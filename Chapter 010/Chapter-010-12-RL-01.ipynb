{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apress - Industrialized Machine Learning Examples\n",
    "\n",
    "Andreas Francois Vermeulen\n",
    "2019\n",
    "\n",
    "### This is an example add-on to a book and needs to be accepted as part of that copyright."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter-010-12-RL-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1968)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_for_gamblers(p_h, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        p_h: Probability of the coin coming up heads\n",
    "    \"\"\"\n",
    "    # The reward is zero on all transitions except those on which the gambler reaches his goal, when it is +1.\n",
    "    rewards = np.zeros(101)\n",
    "    rewards[100] = 1 \n",
    "    \n",
    "    # You should introduce two dummy states corresponding to termination with capital of 0 and 100\n",
    "    V = np.zeros(101)\n",
    "    \n",
    "    def one_step_lookahead(s, V, rewards):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            s: The gamblerâ€™s capital. Integer.\n",
    "            V: The vector that contains values at each state. \n",
    "            rewards: The reward vector.\n",
    "                        \n",
    "        Returns:\n",
    "            A vector containing the expected value of each action. \n",
    "            Its length equals to the number of actions.\n",
    "        \"\"\"\n",
    "        A = np.zeros(101)\n",
    "        stakes = range(1, min(s, 100-s)+1) # Your minimum bet is 1, maximum bet is min(s, 100-s).\n",
    "        for a in stakes:\n",
    "            # rewards[s+a], rewards[s-a] are immediate rewards.\n",
    "            # V[s+a], V[s-a] are values of the next states.\n",
    "            # This is the core of the Bellman equation: The expected value of your action is \n",
    "            # the sum of immediate rewards and the value of the next state.\n",
    "            A[a] = p_h * (rewards[s+a] + V[s+a]*discount_factor) + (1-p_h) * (rewards[s-a] + V[s-a]*discount_factor)\n",
    "        return A\n",
    "    \n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(1, 100):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V, rewards)\n",
    "            # print(s,A,V) # if you want to debug.\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function\n",
    "            V[s] = best_action_value        \n",
    "        # Check if system can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the Optimal Value function\n",
    "    policy = np.zeros(100)\n",
    "    for s in range(1, 100):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V, rewards)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s] = best_action\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 1 - 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, v = value_iteration_for_gamblers(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Policy:\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 12. 11. 15. 16. 17.\n",
      " 18.  6. 20. 21.  3. 23. 24. 25.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.\n",
      " 11. 12. 38. 11. 10.  9. 42.  7. 44.  5. 46. 47. 48. 49. 50.  1.  2.  3.\n",
      "  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 11. 10.  9. 17.  7. 19.  5. 21.\n",
      " 22. 23. 24. 25.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 12. 11.\n",
      " 10.  9.  8.  7.  6.  5.  4.  3.  2.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimized Policy:\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Value Function:\n",
      "[0.00000000e+00 7.24792480e-05 2.89916992e-04 6.95257448e-04\n",
      " 1.16010383e-03 1.76906586e-03 2.78102979e-03 4.03504074e-03\n",
      " 4.66214120e-03 5.59997559e-03 7.08471239e-03 9.03964043e-03\n",
      " 1.11241192e-02 1.56793594e-02 1.61464431e-02 1.69517994e-02\n",
      " 1.86512806e-02 1.98249817e-02 2.24047303e-02 2.73845196e-02\n",
      " 2.83388495e-02 3.04937363e-02 3.61633897e-02 3.84953022e-02\n",
      " 4.44964767e-02 6.25000000e-02 6.27174377e-02 6.33700779e-02\n",
      " 6.45857723e-02 6.59966059e-02 6.78135343e-02 7.08430894e-02\n",
      " 7.46098323e-02 7.64884604e-02 7.93035477e-02 8.37541372e-02\n",
      " 8.96225423e-02 9.58723575e-02 1.09538078e-01 1.10939329e-01\n",
      " 1.13360151e-01 1.18457374e-01 1.21977661e-01 1.29716907e-01\n",
      " 1.44653559e-01 1.47520113e-01 1.53983246e-01 1.70990169e-01\n",
      " 1.77987434e-01 1.95990576e-01 2.50000000e-01 2.50217438e-01\n",
      " 2.50870078e-01 2.52085772e-01 2.53496606e-01 2.55313534e-01\n",
      " 2.58343089e-01 2.62109832e-01 2.63988460e-01 2.66803548e-01\n",
      " 2.71254137e-01 2.77122542e-01 2.83372357e-01 2.97038078e-01\n",
      " 2.98439329e-01 3.00860151e-01 3.05957374e-01 3.09477661e-01\n",
      " 3.17216907e-01 3.32153559e-01 3.35020113e-01 3.41483246e-01\n",
      " 3.58490169e-01 3.65487434e-01 3.83490576e-01 4.37500000e-01\n",
      " 4.38152558e-01 4.40122454e-01 4.43757317e-01 4.47991345e-01\n",
      " 4.53440603e-01 4.62529268e-01 4.73829497e-01 4.79468031e-01\n",
      " 4.87912680e-01 5.01265085e-01 5.18867627e-01 5.37617932e-01\n",
      " 5.78614419e-01 5.82817988e-01 5.90080452e-01 6.05372123e-01\n",
      " 6.15934510e-01 6.39150720e-01 6.83960814e-01 6.92560339e-01\n",
      " 7.11950883e-01 7.62970611e-01 7.83963162e-01 8.37972371e-01\n",
      " 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimized Value Function:\")\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 2 - 55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, v = value_iteration_for_gamblers(0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Policy:\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimized Policy:\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Value Function:\n",
      "[0.         0.17907988 0.3256451  0.44562338 0.54386112 0.62432055\n",
      " 0.69024101 0.74427075 0.78857479 0.82492306 0.8547625  0.87927601\n",
      " 0.89943065 0.916017   0.92968144 0.94095243 0.95026207 0.95796371\n",
      " 0.96434629 0.96964617 0.97405667 0.97773597 0.98081353 0.98339531\n",
      " 0.9855681  0.98740299 0.98895822 0.9902816  0.99141231 0.99238257\n",
      " 0.99321882 0.99394285 0.99457258 0.99512281 0.99560577 0.99603155\n",
      " 0.99640856 0.99674375 0.99704294 0.99731096 0.9975519  0.99776916\n",
      " 0.99796564 0.99814377 0.99830564 0.99845302 0.99858745 0.99871025\n",
      " 0.99882255 0.99892537 0.99901957 0.99910594 0.99918515 0.99925782\n",
      " 0.99932449 0.99938566 0.99944178 0.99949324 0.99954041 0.99958363\n",
      " 0.9996232  0.99965942 0.99969253 0.99972279 0.99975041 0.99977559\n",
      " 0.99979853 0.99981941 0.99983838 0.99985561 0.99987123 0.99988537\n",
      " 0.99989815 0.9999097  0.99992011 0.99992947 0.99993789 0.99994544\n",
      " 0.9999522  0.99995825 0.99996364 0.99996844 0.99997271 0.99997649\n",
      " 0.99997983 0.99998278 0.99998538 0.99998766 0.99998965 0.99999139\n",
      " 0.9999929  0.99999421 0.99999534 0.99999631 0.99999714 0.99999785\n",
      " 0.99999845 0.99999895 0.99999937 0.99999972 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimized Value Function:\")\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! 2019-04-27 14:34:37.614555\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print('Done!',str(now))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
