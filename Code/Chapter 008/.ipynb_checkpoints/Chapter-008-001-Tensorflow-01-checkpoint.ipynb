{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apress - Industrialized Machine Learning Examples\n",
    "\n",
    "Andreas Francois Vermeulen\n",
    "2019\n",
    "\n",
    "### This is an example add-on to a book and needs to be accepted as part of that copyright."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter-008-001-Tensorflow-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=0.196810\n",
    "k=0.201904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1968 dummy x, y data points using formula y = m * x + k\n",
    "x_data = np.random.rand(1968).astype(np.float32)\n",
    "y_data = (m * x_data) + k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seek values for W and b that compute y_data = W * x_data + b\n",
    "W = tf.Variable(tf.random_uniform([1], -5.0, 5.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = W * x_data + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize the mean squared errors.\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5,\n",
    "                                              use_locking=False,\n",
    "                                              name='ZhaanGradientDescent')\n",
    "#train = optimizer.minimize(loss,\n",
    "#                           gate_gradients=optimizer.GATE_GRAPH,\n",
    "#                           name='ZhaanGradientDescentMinimize'\n",
    "#                          )\n",
    "\n",
    "grads_vars=optimizer.compute_gradients(loss,\n",
    "                                       gate_gradients=optimizer.GATE_GRAPH\n",
    "                                      )\n",
    "\n",
    "train=optimizer.apply_gradients(grads_vars,\n",
    "                      name='ZhaanApplyGradients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables.  \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the session graph.\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to learn best fit is W: [0.196810], b: [0.201904]\n",
      "------------------------------------------------------------\n",
      "Step=   0 W=-0.412405 b= 0.743007\n",
      "Step=  20 W= 0.009665 b= 0.302857\n",
      "Step=  40 W= 0.146633 b= 0.228971\n",
      "Step=  60 W= 0.183357 b= 0.209161\n",
      "Step=  80 W= 0.193203 b= 0.203850\n",
      "Step= 100 W= 0.195843 b= 0.202426\n",
      "Step= 120 W= 0.196551 b= 0.202044\n",
      "Step= 140 W= 0.196740 b= 0.201942\n",
      "Step= 160 W= 0.196791 b= 0.201914\n",
      "Step= 180 W= 0.196805 b= 0.201907\n",
      "Step= 200 W= 0.196809 b= 0.201905\n",
      "Step= 220 W= 0.196810 b= 0.201904\n",
      "Step= 240 W= 0.196810 b= 0.201904\n",
      "Step= 260 W= 0.196810 b= 0.201904\n",
      "Step= 280 W= 0.196810 b= 0.201904\n",
      "Step= 300 W= 0.196810 b= 0.201904\n",
      "Step= 320 W= 0.196810 b= 0.201904\n",
      "Step= 340 W= 0.196810 b= 0.201904\n",
      "Step= 360 W= 0.196810 b= 0.201904\n",
      "Step= 380 W= 0.196810 b= 0.201904\n",
      "Step= 400 W= 0.196810 b= 0.201904\n",
      "Step= 420 W= 0.196810 b= 0.201904\n",
      "Step= 440 W= 0.196810 b= 0.201904\n",
      "Step= 460 W= 0.196810 b= 0.201904\n",
      "Step= 480 W= 0.196810 b= 0.201904\n",
      "Step= 500 W= 0.196810 b= 0.201904\n"
     ]
    }
   ],
   "source": [
    "# Fit the solution.\n",
    "print('Need to learn best fit is W: [%0.6f], b: [%0.6f]' % (m, k))\n",
    "print('-'*60)      \n",
    "for step in range(501):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        W_out=sess.run(W)\n",
    "        b_out=sess.run(b)\n",
    "        print('Step=%4d W=%9.6f b=%9.6f' % (step, W_out, b_out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: y = (0.196810 * x) + 0.201904\n"
     ]
    }
   ],
   "source": [
    "print('Solution: y = (%0.6f * x) + %0.6f' % (W_out, b_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! 2019-10-19 21:19:52.683033\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print('Done!',str(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
