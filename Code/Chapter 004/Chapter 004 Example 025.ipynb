{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apress - Industrialized Machine Learning Examples\n",
    "\n",
    "Andreas Francois Vermeulen\n",
    "2019\n",
    "\n",
    "### This is an example add-on to a book and needs to be accepted as part of that copyright."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 004 Example 025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AndreVermeulen\\Documents\\My Book\\apress\\Industrial Machine Learning\\book\\GitHub\\Upload\\industrial-machine-learning\\Data\\Roses01.csv\n"
     ]
    }
   ],
   "source": [
    "fileName = '../../Data/Roses01.csv'\n",
    "fileFullName = os.path.abspath(fileName)\n",
    "print(fileFullName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 6)\n",
      "Index(['F01', 'F02', 'F03', 'F04', 'T', 'T2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "rosedf= pd.read_csv(fileFullName, header=0)\n",
    "print(rosedf.shape)\n",
    "print(rosedf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rose = np.array(rosedf)\n",
    "rose_data = np.array(rosedf[['F01', 'F02', 'F03', 'F04']].copy(deep=True))\n",
    "rose_target = np.array(rosedf['T'].copy(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning on dataset rose\n",
      "training: constant learning-rate\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.03169308\n",
      "Iteration 3, loss = 1.01164057\n",
      "Iteration 4, loss = 0.99231479\n",
      "Iteration 5, loss = 0.97288808\n",
      "Iteration 6, loss = 0.95295324\n",
      "Iteration 7, loss = 0.93316384\n",
      "Iteration 8, loss = 0.91358201\n",
      "Iteration 9, loss = 0.89448215\n",
      "Iteration 10, loss = 0.87559231\n",
      "Iteration 11, loss = 0.85657310\n",
      "Iteration 12, loss = 0.83798337\n",
      "Iteration 13, loss = 0.82008910\n",
      "Iteration 14, loss = 0.80268342\n",
      "Iteration 15, loss = 0.78564117\n",
      "Iteration 16, loss = 0.76902675\n",
      "Iteration 17, loss = 0.75293209\n",
      "Iteration 18, loss = 0.73739946\n",
      "Iteration 19, loss = 0.72239835\n",
      "Iteration 20, loss = 0.70786694\n",
      "Iteration 21, loss = 0.69380993\n",
      "Iteration 22, loss = 0.68022047\n",
      "Iteration 23, loss = 0.66709115\n",
      "Iteration 24, loss = 0.65442376\n",
      "Iteration 25, loss = 0.64220671\n",
      "Iteration 26, loss = 0.63044138\n",
      "Iteration 27, loss = 0.61912946\n",
      "Iteration 28, loss = 0.60825704\n",
      "Iteration 29, loss = 0.59781314\n",
      "Iteration 30, loss = 0.58778543\n",
      "Iteration 31, loss = 0.57816109\n",
      "Iteration 32, loss = 0.56892304\n",
      "Iteration 33, loss = 0.56005060\n",
      "Iteration 34, loss = 0.55153417\n",
      "Iteration 35, loss = 0.54335818\n",
      "Iteration 36, loss = 0.53550030\n",
      "Iteration 37, loss = 0.52794485\n",
      "Iteration 38, loss = 0.52067373\n",
      "Iteration 39, loss = 0.51367420\n",
      "Iteration 40, loss = 0.50693436\n",
      "Iteration 41, loss = 0.50043424\n",
      "Iteration 42, loss = 0.49416394\n",
      "Iteration 43, loss = 0.48811593\n",
      "Iteration 44, loss = 0.48228033\n",
      "Iteration 45, loss = 0.47663935\n",
      "Iteration 46, loss = 0.47118602\n",
      "Iteration 47, loss = 0.46590887\n",
      "Iteration 48, loss = 0.46079178\n",
      "Iteration 49, loss = 0.45582632\n",
      "Iteration 50, loss = 0.45100839\n",
      "Iteration 51, loss = 0.44632533\n",
      "Iteration 52, loss = 0.44176553\n",
      "Iteration 53, loss = 0.43732948\n",
      "Iteration 54, loss = 0.43300869\n",
      "Iteration 55, loss = 0.42879605\n",
      "Iteration 56, loss = 0.42468855\n",
      "Iteration 57, loss = 0.42067872\n",
      "Iteration 58, loss = 0.41676544\n",
      "Iteration 59, loss = 0.41294003\n",
      "Iteration 60, loss = 0.40919735\n",
      "Iteration 61, loss = 0.40553291\n",
      "Iteration 62, loss = 0.40193690\n",
      "Iteration 63, loss = 0.39840948\n",
      "Iteration 64, loss = 0.39494640\n",
      "Iteration 65, loss = 0.39154670\n",
      "Iteration 66, loss = 0.38820422\n",
      "Iteration 67, loss = 0.38492235\n",
      "Iteration 68, loss = 0.38169940\n",
      "Iteration 69, loss = 0.37852644\n",
      "Iteration 70, loss = 0.37541338\n",
      "Iteration 71, loss = 0.37236000\n",
      "Iteration 72, loss = 0.36935330\n",
      "Iteration 73, loss = 0.36639677\n",
      "Iteration 74, loss = 0.36348861\n",
      "Iteration 75, loss = 0.36063099\n",
      "Iteration 76, loss = 0.35782548\n",
      "Iteration 77, loss = 0.35506386\n",
      "Iteration 78, loss = 0.35234599\n",
      "Iteration 79, loss = 0.34966686\n",
      "Iteration 80, loss = 0.34701876\n",
      "Iteration 81, loss = 0.34440217\n",
      "Iteration 82, loss = 0.34182147\n",
      "Iteration 83, loss = 0.33927292\n",
      "Iteration 84, loss = 0.33675985\n",
      "Iteration 85, loss = 0.33427908\n",
      "Iteration 86, loss = 0.33183036\n",
      "Iteration 87, loss = 0.32941355\n",
      "Iteration 88, loss = 0.32702612\n",
      "Iteration 89, loss = 0.32466712\n",
      "Iteration 90, loss = 0.32233727\n",
      "Iteration 91, loss = 0.32004644\n",
      "Iteration 92, loss = 0.31778689\n",
      "Iteration 93, loss = 0.31555430\n",
      "Iteration 94, loss = 0.31334682\n",
      "Iteration 95, loss = 0.31116713\n",
      "Iteration 96, loss = 0.30901466\n",
      "Iteration 97, loss = 0.30688718\n",
      "Iteration 98, loss = 0.30478615\n",
      "Iteration 99, loss = 0.30270876\n",
      "Iteration 100, loss = 0.30065512\n",
      "Iteration 101, loss = 0.29862615\n",
      "Iteration 102, loss = 0.29662085\n",
      "Iteration 103, loss = 0.29463729\n",
      "Iteration 104, loss = 0.29267615\n",
      "Iteration 105, loss = 0.29073681\n",
      "Iteration 106, loss = 0.28881782\n",
      "Iteration 107, loss = 0.28691959\n",
      "Iteration 108, loss = 0.28504171\n",
      "Iteration 109, loss = 0.28318359\n",
      "Iteration 110, loss = 0.28134521\n",
      "Iteration 111, loss = 0.27952565\n",
      "Iteration 112, loss = 0.27772486\n",
      "Iteration 113, loss = 0.27594333\n",
      "Iteration 114, loss = 0.27418044\n",
      "Iteration 115, loss = 0.27243571\n",
      "Iteration 116, loss = 0.27070894\n",
      "Iteration 117, loss = 0.26899963\n",
      "Iteration 118, loss = 0.26730721\n",
      "Iteration 119, loss = 0.26563152\n",
      "Iteration 120, loss = 0.26397287\n",
      "Iteration 121, loss = 0.26233165\n",
      "Iteration 122, loss = 0.26070679\n",
      "Iteration 123, loss = 0.25909844\n",
      "Iteration 124, loss = 0.25750720\n",
      "Iteration 125, loss = 0.25593262\n",
      "Iteration 126, loss = 0.25437376\n",
      "Iteration 127, loss = 0.25283116\n",
      "Iteration 128, loss = 0.25130382\n",
      "Iteration 129, loss = 0.24979189\n",
      "Iteration 130, loss = 0.24829522\n",
      "Iteration 131, loss = 0.24681264\n",
      "Iteration 132, loss = 0.24534501\n",
      "Iteration 133, loss = 0.24389222\n",
      "Iteration 134, loss = 0.24245458\n",
      "Iteration 135, loss = 0.24103085\n",
      "Iteration 136, loss = 0.23962153\n",
      "Iteration 137, loss = 0.23822621\n",
      "Iteration 138, loss = 0.23684468\n",
      "Iteration 139, loss = 0.23547759\n",
      "Iteration 140, loss = 0.23412383\n",
      "Iteration 141, loss = 0.23278479\n",
      "Iteration 142, loss = 0.23145944\n",
      "Iteration 143, loss = 0.23014747\n",
      "Iteration 144, loss = 0.22884837\n",
      "Iteration 145, loss = 0.22756220\n",
      "Iteration 146, loss = 0.22628855\n",
      "Iteration 147, loss = 0.22502770\n",
      "Iteration 148, loss = 0.22377981\n",
      "Iteration 149, loss = 0.22254416\n",
      "Iteration 150, loss = 0.22132004\n",
      "Iteration 151, loss = 0.22010673\n",
      "Iteration 152, loss = 0.21890507\n",
      "Iteration 153, loss = 0.21771519\n",
      "Iteration 154, loss = 0.21653690\n",
      "Iteration 155, loss = 0.21537012\n",
      "Iteration 156, loss = 0.21421476\n",
      "Iteration 157, loss = 0.21307105\n",
      "Iteration 158, loss = 0.21193984\n",
      "Iteration 159, loss = 0.21081915\n",
      "Iteration 160, loss = 0.20970886\n",
      "Iteration 161, loss = 0.20860904\n",
      "Iteration 162, loss = 0.20751961\n",
      "Iteration 163, loss = 0.20644075\n",
      "Iteration 164, loss = 0.20537209\n",
      "Iteration 165, loss = 0.20431408\n",
      "Iteration 166, loss = 0.20326605\n",
      "Iteration 167, loss = 0.20222812\n",
      "Iteration 168, loss = 0.20120030\n",
      "Iteration 169, loss = 0.20018263\n",
      "Iteration 170, loss = 0.19917452\n",
      "Iteration 171, loss = 0.19817623\n",
      "Iteration 172, loss = 0.19718724\n",
      "Iteration 173, loss = 0.19620765\n",
      "Iteration 174, loss = 0.19523738\n",
      "Iteration 175, loss = 0.19427651\n",
      "Iteration 176, loss = 0.19332449\n",
      "Iteration 177, loss = 0.19238192\n",
      "Iteration 178, loss = 0.19144840\n",
      "Iteration 179, loss = 0.19052367\n",
      "Iteration 180, loss = 0.18960755\n",
      "Iteration 181, loss = 0.18870024\n",
      "Iteration 182, loss = 0.18780150\n",
      "Iteration 183, loss = 0.18691114\n",
      "Iteration 184, loss = 0.18602931\n",
      "Iteration 185, loss = 0.18515594\n",
      "Iteration 186, loss = 0.18429078\n",
      "Iteration 187, loss = 0.18343353\n",
      "Iteration 188, loss = 0.18258421\n",
      "Iteration 189, loss = 0.18174264\n",
      "Iteration 190, loss = 0.18090917\n",
      "Iteration 191, loss = 0.18008418\n",
      "Iteration 192, loss = 0.17926661\n",
      "Iteration 193, loss = 0.17845653\n",
      "Iteration 194, loss = 0.17765401\n",
      "Iteration 195, loss = 0.17685863\n",
      "Iteration 196, loss = 0.17607052\n",
      "Iteration 197, loss = 0.17528947\n",
      "Iteration 198, loss = 0.17451541\n",
      "Iteration 199, loss = 0.17374844\n",
      "Iteration 200, loss = 0.17298824\n",
      "Iteration 201, loss = 0.17223491\n",
      "Iteration 202, loss = 0.17148885\n",
      "Iteration 203, loss = 0.17074951\n",
      "Iteration 204, loss = 0.17001679\n",
      "Iteration 205, loss = 0.16929062\n",
      "Iteration 206, loss = 0.16857089\n",
      "Iteration 207, loss = 0.16785770\n",
      "Iteration 208, loss = 0.16715072\n",
      "Iteration 209, loss = 0.16644997\n",
      "Iteration 210, loss = 0.16575550\n",
      "Iteration 211, loss = 0.16506718\n",
      "Iteration 212, loss = 0.16438500\n",
      "Iteration 213, loss = 0.16370887\n",
      "Iteration 214, loss = 0.16303863\n",
      "Iteration 215, loss = 0.16237404\n",
      "Iteration 216, loss = 0.16171541\n",
      "Iteration 217, loss = 0.16106233\n",
      "Iteration 218, loss = 0.16041481\n",
      "Iteration 219, loss = 0.15977297\n",
      "Iteration 220, loss = 0.15913650\n",
      "Iteration 221, loss = 0.15850554\n",
      "Iteration 222, loss = 0.15787988\n",
      "Iteration 223, loss = 0.15725958\n",
      "Iteration 224, loss = 0.15664454\n",
      "Iteration 225, loss = 0.15603467\n",
      "Iteration 226, loss = 0.15542995\n",
      "Iteration 227, loss = 0.15483036\n",
      "Iteration 228, loss = 0.15423584\n",
      "Iteration 229, loss = 0.15364645\n",
      "Iteration 230, loss = 0.15306198\n",
      "Iteration 231, loss = 0.15248234\n",
      "Iteration 232, loss = 0.15190759\n",
      "Iteration 233, loss = 0.15133751\n",
      "Iteration 234, loss = 0.15077220\n",
      "Iteration 235, loss = 0.15021171\n",
      "Iteration 236, loss = 0.14965564\n",
      "Iteration 237, loss = 0.14910417\n",
      "Iteration 238, loss = 0.14855729\n",
      "Iteration 239, loss = 0.14801477\n",
      "Iteration 240, loss = 0.14747683\n",
      "Iteration 241, loss = 0.14694319\n",
      "Iteration 242, loss = 0.14641376\n",
      "Iteration 243, loss = 0.14588883\n",
      "Iteration 244, loss = 0.14536803\n",
      "Iteration 245, loss = 0.14485143\n",
      "Iteration 246, loss = 0.14433898\n",
      "Iteration 247, loss = 0.14383059\n",
      "Iteration 248, loss = 0.14332646\n",
      "Iteration 249, loss = 0.14282623\n",
      "Iteration 250, loss = 0.14233001\n",
      "Iteration 251, loss = 0.14183790\n",
      "Iteration 252, loss = 0.14134951\n",
      "Iteration 253, loss = 0.14086510\n",
      "Iteration 254, loss = 0.14038441\n",
      "Iteration 255, loss = 0.13990752\n",
      "Iteration 256, loss = 0.13943435\n",
      "Iteration 257, loss = 0.13896486\n",
      "Iteration 258, loss = 0.13849916\n",
      "Iteration 259, loss = 0.13803694\n",
      "Iteration 260, loss = 0.13757849\n",
      "Iteration 261, loss = 0.13712343\n",
      "Iteration 262, loss = 0.13667197\n",
      "Iteration 263, loss = 0.13622402\n",
      "Iteration 264, loss = 0.13577954\n",
      "Iteration 265, loss = 0.13533848\n",
      "Iteration 266, loss = 0.13490078\n",
      "Iteration 267, loss = 0.13446642\n",
      "Iteration 268, loss = 0.13403529\n",
      "Iteration 269, loss = 0.13360758\n",
      "Iteration 270, loss = 0.13318293\n",
      "Iteration 271, loss = 0.13276170\n",
      "Iteration 272, loss = 0.13234348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 273, loss = 0.13192843\n",
      "Iteration 274, loss = 0.13151645\n",
      "Iteration 275, loss = 0.13110767\n",
      "Iteration 276, loss = 0.13070188\n",
      "Iteration 277, loss = 0.13029918\n",
      "Iteration 278, loss = 0.12989945\n",
      "Iteration 279, loss = 0.12950270\n",
      "Iteration 280, loss = 0.12910890\n",
      "Iteration 281, loss = 0.12871801\n",
      "Iteration 282, loss = 0.12833001\n",
      "Iteration 283, loss = 0.12794485\n",
      "Iteration 284, loss = 0.12756253\n",
      "Iteration 285, loss = 0.12718295\n",
      "Iteration 286, loss = 0.12680623\n",
      "Iteration 287, loss = 0.12643213\n",
      "Iteration 288, loss = 0.12606086\n",
      "Iteration 289, loss = 0.12569215\n",
      "Iteration 290, loss = 0.12532622\n",
      "Iteration 291, loss = 0.12496282\n",
      "Iteration 292, loss = 0.12460209\n",
      "Iteration 293, loss = 0.12424391\n",
      "Iteration 294, loss = 0.12388833\n",
      "Iteration 295, loss = 0.12353526\n",
      "Iteration 296, loss = 0.12318470\n",
      "Iteration 297, loss = 0.12283664\n",
      "Iteration 298, loss = 0.12249101\n",
      "Iteration 299, loss = 0.12214788\n",
      "Iteration 300, loss = 0.12180710\n",
      "Iteration 301, loss = 0.12146879\n",
      "Iteration 302, loss = 0.12113277\n",
      "Iteration 303, loss = 0.12079919\n",
      "Iteration 304, loss = 0.12046788\n",
      "Iteration 305, loss = 0.12013897\n",
      "Iteration 306, loss = 0.11981224\n",
      "Iteration 307, loss = 0.11948792\n",
      "Iteration 308, loss = 0.11916569\n",
      "Iteration 309, loss = 0.11884586\n",
      "Iteration 310, loss = 0.11852805\n",
      "Iteration 311, loss = 0.11821262\n",
      "Iteration 312, loss = 0.11789918\n",
      "Iteration 313, loss = 0.11758802\n",
      "Iteration 314, loss = 0.11727888\n",
      "Iteration 315, loss = 0.11697192\n",
      "Iteration 316, loss = 0.11666701\n",
      "Iteration 317, loss = 0.11636416\n",
      "Iteration 318, loss = 0.11606343\n",
      "Iteration 319, loss = 0.11576465\n",
      "Iteration 320, loss = 0.11546802\n",
      "Iteration 321, loss = 0.11517323\n",
      "Iteration 322, loss = 0.11488060\n",
      "Iteration 323, loss = 0.11458976\n",
      "Iteration 324, loss = 0.11430099\n",
      "Iteration 325, loss = 0.11401408\n",
      "Iteration 326, loss = 0.11372911\n",
      "Iteration 327, loss = 0.11344604\n",
      "Iteration 328, loss = 0.11316481\n",
      "Iteration 329, loss = 0.11288549\n",
      "Iteration 330, loss = 0.11260781\n",
      "Iteration 331, loss = 0.11233190\n",
      "Iteration 332, loss = 0.11205771\n",
      "Iteration 333, loss = 0.11178540\n",
      "Iteration 334, loss = 0.11151480\n",
      "Iteration 335, loss = 0.11124601\n",
      "Iteration 336, loss = 0.11097896\n",
      "Iteration 337, loss = 0.11071358\n",
      "Iteration 338, loss = 0.11045004\n",
      "Iteration 339, loss = 0.11018811\n",
      "Iteration 340, loss = 0.10992802\n",
      "Iteration 341, loss = 0.10966954\n",
      "Iteration 342, loss = 0.10941275\n",
      "Iteration 343, loss = 0.10915762\n",
      "Iteration 344, loss = 0.10890409\n",
      "Iteration 345, loss = 0.10865224\n",
      "Iteration 346, loss = 0.10840187\n",
      "Iteration 347, loss = 0.10815322\n",
      "Iteration 348, loss = 0.10790607\n",
      "Iteration 349, loss = 0.10766049\n",
      "Iteration 350, loss = 0.10741651\n",
      "Iteration 351, loss = 0.10717397\n",
      "Iteration 352, loss = 0.10693305\n",
      "Iteration 353, loss = 0.10669355\n",
      "Iteration 354, loss = 0.10645561\n",
      "Iteration 355, loss = 0.10621915\n",
      "Iteration 356, loss = 0.10598406\n",
      "Iteration 357, loss = 0.10575012\n",
      "Iteration 358, loss = 0.10551754\n",
      "Iteration 359, loss = 0.10528643\n",
      "Iteration 360, loss = 0.10505676\n",
      "Iteration 361, loss = 0.10482846\n",
      "Iteration 362, loss = 0.10460167\n",
      "Iteration 363, loss = 0.10437615\n",
      "Iteration 364, loss = 0.10415207\n",
      "Iteration 365, loss = 0.10392933\n",
      "Iteration 366, loss = 0.10370788\n",
      "Iteration 367, loss = 0.10348783\n",
      "Iteration 368, loss = 0.10326900\n",
      "Iteration 369, loss = 0.10305155\n",
      "Iteration 370, loss = 0.10283541\n",
      "Iteration 371, loss = 0.10262056\n",
      "Iteration 372, loss = 0.10240716\n",
      "Iteration 373, loss = 0.10219507\n",
      "Iteration 374, loss = 0.10198445\n",
      "Iteration 375, loss = 0.10177520\n",
      "Iteration 376, loss = 0.10156712\n",
      "Iteration 377, loss = 0.10136041\n",
      "Iteration 378, loss = 0.10115484\n",
      "Iteration 379, loss = 0.10095049\n",
      "Iteration 380, loss = 0.10074741\n",
      "Iteration 381, loss = 0.10054545\n",
      "Iteration 382, loss = 0.10034474\n",
      "Iteration 383, loss = 0.10014518\n",
      "Iteration 384, loss = 0.09994681\n",
      "Iteration 385, loss = 0.09974966\n",
      "Iteration 386, loss = 0.09955340\n",
      "Iteration 387, loss = 0.09935831\n",
      "Iteration 388, loss = 0.09916440\n",
      "Iteration 389, loss = 0.09897161\n",
      "Iteration 390, loss = 0.09877997\n",
      "Iteration 391, loss = 0.09858940\n",
      "Iteration 392, loss = 0.09839991\n",
      "Iteration 393, loss = 0.09821157\n",
      "Iteration 394, loss = 0.09802425\n",
      "Iteration 395, loss = 0.09783801\n",
      "Iteration 396, loss = 0.09765286\n",
      "Iteration 397, loss = 0.09746874\n",
      "Iteration 398, loss = 0.09728570\n",
      "Iteration 399, loss = 0.09710365\n",
      "Iteration 400, loss = 0.09692234\n",
      "Iteration 401, loss = 0.09674213\n",
      "Iteration 402, loss = 0.09656286\n",
      "Iteration 403, loss = 0.09638460\n",
      "Iteration 404, loss = 0.09620743\n",
      "Iteration 405, loss = 0.09603117\n",
      "Iteration 406, loss = 0.09585597\n",
      "Iteration 407, loss = 0.09568180\n",
      "Iteration 408, loss = 0.09550859\n",
      "Iteration 409, loss = 0.09533665\n",
      "Iteration 410, loss = 0.09516565\n",
      "Iteration 411, loss = 0.09499554\n",
      "Iteration 412, loss = 0.09482644\n",
      "Iteration 413, loss = 0.09465825\n",
      "Iteration 414, loss = 0.09449095\n",
      "Iteration 415, loss = 0.09432462\n",
      "Iteration 416, loss = 0.09415920\n",
      "Iteration 417, loss = 0.09399463\n",
      "Iteration 418, loss = 0.09383102\n",
      "Iteration 419, loss = 0.09366832\n",
      "Iteration 420, loss = 0.09350659\n",
      "Iteration 421, loss = 0.09334580\n",
      "Iteration 422, loss = 0.09318582\n",
      "Iteration 423, loss = 0.09302673\n",
      "Iteration 424, loss = 0.09286853\n",
      "Iteration 425, loss = 0.09271112\n",
      "Iteration 426, loss = 0.09255460\n",
      "Iteration 427, loss = 0.09239895\n",
      "Iteration 428, loss = 0.09224407\n",
      "Iteration 429, loss = 0.09209008\n",
      "Iteration 430, loss = 0.09193701\n",
      "Iteration 431, loss = 0.09178470\n",
      "Iteration 432, loss = 0.09163325\n",
      "Iteration 433, loss = 0.09148260\n",
      "Iteration 434, loss = 0.09133271\n",
      "Iteration 435, loss = 0.09118364\n",
      "Iteration 436, loss = 0.09103536\n",
      "Iteration 437, loss = 0.09088783\n",
      "Iteration 438, loss = 0.09074109\n",
      "Iteration 439, loss = 0.09059513\n",
      "Iteration 440, loss = 0.09044991\n",
      "Iteration 441, loss = 0.09030545\n",
      "Iteration 442, loss = 0.09016176\n",
      "Iteration 443, loss = 0.09001874\n",
      "Iteration 444, loss = 0.08987592\n",
      "Iteration 445, loss = 0.08973391\n",
      "Iteration 446, loss = 0.08959253\n",
      "Iteration 447, loss = 0.08945188\n",
      "Iteration 448, loss = 0.08931196\n",
      "Iteration 449, loss = 0.08917272\n",
      "Iteration 450, loss = 0.08903442\n",
      "Iteration 451, loss = 0.08889675\n",
      "Iteration 452, loss = 0.08875973\n",
      "Iteration 453, loss = 0.08862341\n",
      "Iteration 454, loss = 0.08848777\n",
      "Iteration 455, loss = 0.08835275\n",
      "Iteration 456, loss = 0.08821831\n",
      "Iteration 457, loss = 0.08808405\n",
      "Iteration 458, loss = 0.08794952\n",
      "Iteration 459, loss = 0.08781667\n",
      "Iteration 460, loss = 0.08768459\n",
      "Iteration 461, loss = 0.08755381\n",
      "Iteration 462, loss = 0.08742369\n",
      "Iteration 463, loss = 0.08729428\n",
      "Iteration 464, loss = 0.08716550\n",
      "Iteration 465, loss = 0.08703735\n",
      "Iteration 466, loss = 0.08690986\n",
      "Iteration 467, loss = 0.08678293\n",
      "Iteration 468, loss = 0.08665645\n",
      "Iteration 469, loss = 0.08653075\n",
      "Iteration 470, loss = 0.08640568\n",
      "Iteration 471, loss = 0.08628131\n",
      "Iteration 472, loss = 0.08615766\n",
      "Iteration 473, loss = 0.08603458\n",
      "Iteration 474, loss = 0.08591213\n",
      "Iteration 475, loss = 0.08579028\n",
      "Iteration 476, loss = 0.08566903\n",
      "Iteration 477, loss = 0.08554834\n",
      "Iteration 478, loss = 0.08542824\n",
      "Iteration 479, loss = 0.08530875\n",
      "Iteration 480, loss = 0.08518975\n",
      "Iteration 481, loss = 0.08507138\n",
      "Iteration 482, loss = 0.08495367\n",
      "Iteration 483, loss = 0.08483651\n",
      "Iteration 484, loss = 0.08471988\n",
      "Iteration 485, loss = 0.08460389\n",
      "Iteration 486, loss = 0.08448838\n",
      "Iteration 487, loss = 0.08437343\n",
      "Iteration 488, loss = 0.08425906\n",
      "Iteration 489, loss = 0.08414521\n",
      "Iteration 490, loss = 0.08403187\n",
      "Iteration 491, loss = 0.08391911\n",
      "Iteration 492, loss = 0.08380690\n",
      "Iteration 493, loss = 0.08369518\n",
      "Iteration 494, loss = 0.08358400\n",
      "Iteration 495, loss = 0.08347337\n",
      "Iteration 496, loss = 0.08336322\n",
      "Iteration 497, loss = 0.08325362\n",
      "Iteration 498, loss = 0.08314461\n",
      "Iteration 499, loss = 0.08303609\n",
      "Iteration 500, loss = 0.08292807\n",
      "Iteration 501, loss = 0.08282055\n",
      "Iteration 502, loss = 0.08271353\n",
      "Iteration 503, loss = 0.08260699\n",
      "Iteration 504, loss = 0.08250093\n",
      "Iteration 505, loss = 0.08239544\n",
      "Iteration 506, loss = 0.08229033\n",
      "Iteration 507, loss = 0.08218575\n",
      "Iteration 508, loss = 0.08208162\n",
      "Iteration 509, loss = 0.08197802\n",
      "Iteration 510, loss = 0.08187483\n",
      "Iteration 511, loss = 0.08177213\n",
      "Iteration 512, loss = 0.08166992\n",
      "Iteration 513, loss = 0.08156813\n",
      "Iteration 514, loss = 0.08146680\n",
      "Iteration 515, loss = 0.08136594\n",
      "Iteration 516, loss = 0.08126555\n",
      "Iteration 517, loss = 0.08116556\n",
      "Iteration 518, loss = 0.08106606\n",
      "Iteration 519, loss = 0.08096701\n",
      "Iteration 520, loss = 0.08086837\n",
      "Iteration 521, loss = 0.08077019\n",
      "Iteration 522, loss = 0.08067244\n",
      "Iteration 523, loss = 0.08057517\n",
      "Iteration 524, loss = 0.08047826\n",
      "Iteration 525, loss = 0.08038182\n",
      "Iteration 526, loss = 0.08028581\n",
      "Iteration 527, loss = 0.08019020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.980000\n",
      "Training set loss: 0.080190\n",
      "training: constant with momentum\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.03169308\n",
      "Iteration 3, loss = 0.99435081\n",
      "Iteration 4, loss = 0.94494013\n",
      "Iteration 5, loss = 0.88462739\n",
      "Iteration 6, loss = 0.81528498\n",
      "Iteration 7, loss = 0.74424065\n",
      "Iteration 8, loss = 0.67212121\n",
      "Iteration 9, loss = 0.60257981\n",
      "Iteration 10, loss = 0.53971027\n",
      "Iteration 11, loss = 0.48582472\n",
      "Iteration 12, loss = 0.44093531\n",
      "Iteration 13, loss = 0.40358107\n",
      "Iteration 14, loss = 0.37206419\n",
      "Iteration 15, loss = 0.34491884\n",
      "Iteration 16, loss = 0.32091627\n",
      "Iteration 17, loss = 0.29924247\n",
      "Iteration 18, loss = 0.27936705\n",
      "Iteration 19, loss = 0.26102326\n",
      "Iteration 20, loss = 0.24403246\n",
      "Iteration 21, loss = 0.22821738\n",
      "Iteration 22, loss = 0.21355478\n",
      "Iteration 23, loss = 0.19999300\n",
      "Iteration 24, loss = 0.18752084\n",
      "Iteration 25, loss = 0.17615042\n",
      "Iteration 26, loss = 0.16583613\n",
      "Iteration 27, loss = 0.15649638\n",
      "Iteration 28, loss = 0.14808470\n",
      "Iteration 29, loss = 0.14050426\n",
      "Iteration 30, loss = 0.13367100\n",
      "Iteration 31, loss = 0.12751686\n",
      "Iteration 32, loss = 0.12195171\n",
      "Iteration 33, loss = 0.11690583\n",
      "Iteration 34, loss = 0.11233870\n",
      "Iteration 35, loss = 0.10820115\n",
      "Iteration 36, loss = 0.10443034\n",
      "Iteration 37, loss = 0.10098472\n",
      "Iteration 38, loss = 0.09783288\n",
      "Iteration 39, loss = 0.09494294\n",
      "Iteration 40, loss = 0.09228787\n",
      "Iteration 41, loss = 0.08984609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 0.08759074\n",
      "Iteration 43, loss = 0.08550706\n",
      "Iteration 44, loss = 0.08358159\n",
      "Iteration 45, loss = 0.08179669\n",
      "Iteration 46, loss = 0.08013567\n",
      "Iteration 47, loss = 0.07859092\n",
      "Iteration 48, loss = 0.07715418\n",
      "Iteration 49, loss = 0.07581545\n",
      "Iteration 50, loss = 0.07456405\n",
      "Iteration 51, loss = 0.07339779\n",
      "Iteration 52, loss = 0.07231014\n",
      "Iteration 53, loss = 0.07130028\n",
      "Iteration 54, loss = 0.07035466\n",
      "Iteration 55, loss = 0.06946935\n",
      "Iteration 56, loss = 0.06863895\n",
      "Iteration 57, loss = 0.06786085\n",
      "Iteration 58, loss = 0.06713102\n",
      "Iteration 59, loss = 0.06644423\n",
      "Iteration 60, loss = 0.06579700\n",
      "Iteration 61, loss = 0.06518699\n",
      "Iteration 62, loss = 0.06461110\n",
      "Iteration 63, loss = 0.06406631\n",
      "Iteration 64, loss = 0.06355071\n",
      "Iteration 65, loss = 0.06306216\n",
      "Iteration 66, loss = 0.06259826\n",
      "Iteration 67, loss = 0.06215701\n",
      "Iteration 68, loss = 0.06173693\n",
      "Iteration 69, loss = 0.06133645\n",
      "Iteration 70, loss = 0.06095394\n",
      "Iteration 71, loss = 0.06058803\n",
      "Iteration 72, loss = 0.06023775\n",
      "Iteration 73, loss = 0.05990224\n",
      "Iteration 74, loss = 0.05958032\n",
      "Iteration 75, loss = 0.05927106\n",
      "Iteration 76, loss = 0.05897355\n",
      "Iteration 77, loss = 0.05868691\n",
      "Iteration 78, loss = 0.05841045\n",
      "Iteration 79, loss = 0.05814396\n",
      "Iteration 80, loss = 0.05788597\n",
      "Iteration 81, loss = 0.05763605\n",
      "Iteration 82, loss = 0.05739448\n",
      "Iteration 83, loss = 0.05716140\n",
      "Iteration 84, loss = 0.05693526\n",
      "Iteration 85, loss = 0.05671564\n",
      "Iteration 86, loss = 0.05650210\n",
      "Iteration 87, loss = 0.05629428\n",
      "Iteration 88, loss = 0.05609186\n",
      "Iteration 89, loss = 0.05589457\n",
      "Iteration 90, loss = 0.05570214\n",
      "Iteration 91, loss = 0.05551433\n",
      "Iteration 92, loss = 0.05533088\n",
      "Iteration 93, loss = 0.05515161\n",
      "Iteration 94, loss = 0.05497630\n",
      "Iteration 95, loss = 0.05480476\n",
      "Iteration 96, loss = 0.05463684\n",
      "Iteration 97, loss = 0.05447237\n",
      "Iteration 98, loss = 0.05431121\n",
      "Iteration 99, loss = 0.05415322\n",
      "Iteration 100, loss = 0.05399826\n",
      "Iteration 101, loss = 0.05384622\n",
      "Iteration 102, loss = 0.05369698\n",
      "Iteration 103, loss = 0.05355045\n",
      "Iteration 104, loss = 0.05340651\n",
      "Iteration 105, loss = 0.05326508\n",
      "Iteration 106, loss = 0.05312607\n",
      "Iteration 107, loss = 0.05298939\n",
      "Iteration 108, loss = 0.05285498\n",
      "Iteration 109, loss = 0.05272275\n",
      "Iteration 110, loss = 0.05259265\n",
      "Iteration 111, loss = 0.05246460\n",
      "Iteration 112, loss = 0.05233854\n",
      "Iteration 113, loss = 0.05221442\n",
      "Iteration 114, loss = 0.05209218\n",
      "Iteration 115, loss = 0.05197177\n",
      "Iteration 116, loss = 0.05185314\n",
      "Iteration 117, loss = 0.05173624\n",
      "Iteration 118, loss = 0.05162103\n",
      "Iteration 119, loss = 0.05150747\n",
      "Iteration 120, loss = 0.05139551\n",
      "Iteration 121, loss = 0.05128514\n",
      "Iteration 122, loss = 0.05117629\n",
      "Iteration 123, loss = 0.05106893\n",
      "Iteration 124, loss = 0.05096303\n",
      "Iteration 125, loss = 0.05085856\n",
      "Iteration 126, loss = 0.05075548\n",
      "Iteration 127, loss = 0.05065377\n",
      "Iteration 128, loss = 0.05055340\n",
      "Iteration 129, loss = 0.05045434\n",
      "Iteration 130, loss = 0.05035656\n",
      "Iteration 131, loss = 0.05026004\n",
      "Iteration 132, loss = 0.05016476\n",
      "Iteration 133, loss = 0.05007069\n",
      "Iteration 134, loss = 0.04997781\n",
      "Iteration 135, loss = 0.04988609\n",
      "Iteration 136, loss = 0.04979551\n",
      "Iteration 137, loss = 0.04970606\n",
      "Iteration 138, loss = 0.04961770\n",
      "Iteration 139, loss = 0.04953043\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.980000\n",
      "Training set loss: 0.049530\n",
      "training: constant with Nesterov's momentum\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.01343901\n",
      "Iteration 3, loss = 0.96292777\n",
      "Iteration 4, loss = 0.90034172\n",
      "Iteration 5, loss = 0.82770885\n",
      "Iteration 6, loss = 0.75319809\n",
      "Iteration 7, loss = 0.67880650\n",
      "Iteration 8, loss = 0.60866879\n",
      "Iteration 9, loss = 0.54558441\n",
      "Iteration 10, loss = 0.49132855\n",
      "Iteration 11, loss = 0.44611795\n",
      "Iteration 12, loss = 0.40852952\n",
      "Iteration 13, loss = 0.37685448\n",
      "Iteration 14, loss = 0.34946349\n",
      "Iteration 15, loss = 0.32520662\n",
      "Iteration 16, loss = 0.30324227\n",
      "Iteration 17, loss = 0.28312156\n",
      "Iteration 18, loss = 0.26458018\n",
      "Iteration 19, loss = 0.24745431\n",
      "Iteration 20, loss = 0.23155474\n",
      "Iteration 21, loss = 0.21681679\n",
      "Iteration 22, loss = 0.20323218\n",
      "Iteration 23, loss = 0.19078329\n",
      "Iteration 24, loss = 0.17944913\n",
      "Iteration 25, loss = 0.16917614\n",
      "Iteration 26, loss = 0.15988408\n",
      "Iteration 27, loss = 0.15149708\n",
      "Iteration 28, loss = 0.14391904\n",
      "Iteration 29, loss = 0.13705833\n",
      "Iteration 30, loss = 0.13085219\n",
      "Iteration 31, loss = 0.12524471\n",
      "Iteration 32, loss = 0.12015843\n",
      "Iteration 33, loss = 0.11553435\n",
      "Iteration 34, loss = 0.11132201\n",
      "Iteration 35, loss = 0.10747696\n",
      "Iteration 36, loss = 0.10396295\n",
      "Iteration 37, loss = 0.10074254\n",
      "Iteration 38, loss = 0.09778705\n",
      "Iteration 39, loss = 0.09506460\n",
      "Iteration 40, loss = 0.09255349\n",
      "Iteration 41, loss = 0.09023828\n",
      "Iteration 42, loss = 0.08809592\n",
      "Iteration 43, loss = 0.08611202\n",
      "Iteration 44, loss = 0.08426877\n",
      "Iteration 45, loss = 0.08255908\n",
      "Iteration 46, loss = 0.08096838\n",
      "Iteration 47, loss = 0.07949226\n",
      "Iteration 48, loss = 0.07811632\n",
      "Iteration 49, loss = 0.07683216\n",
      "Iteration 50, loss = 0.07563191\n",
      "Iteration 51, loss = 0.07450868\n",
      "Iteration 52, loss = 0.07345610\n",
      "Iteration 53, loss = 0.07246868\n",
      "Iteration 54, loss = 0.07154140\n",
      "Iteration 55, loss = 0.07066966\n",
      "Iteration 56, loss = 0.06984983\n",
      "Iteration 57, loss = 0.06907750\n",
      "Iteration 58, loss = 0.06834920\n",
      "Iteration 59, loss = 0.06766142\n",
      "Iteration 60, loss = 0.06701121\n",
      "Iteration 61, loss = 0.06639615\n",
      "Iteration 62, loss = 0.06581333\n",
      "Iteration 63, loss = 0.06526177\n",
      "Iteration 64, loss = 0.06473750\n",
      "Iteration 65, loss = 0.06423812\n",
      "Iteration 66, loss = 0.06376187\n",
      "Iteration 67, loss = 0.06330760\n",
      "Iteration 68, loss = 0.06287350\n",
      "Iteration 69, loss = 0.06245982\n",
      "Iteration 70, loss = 0.06206390\n",
      "Iteration 71, loss = 0.06168426\n",
      "Iteration 72, loss = 0.06131982\n",
      "Iteration 73, loss = 0.06096966\n",
      "Iteration 74, loss = 0.06063303\n",
      "Iteration 75, loss = 0.06030887\n",
      "Iteration 76, loss = 0.05999631\n",
      "Iteration 77, loss = 0.05969435\n",
      "Iteration 78, loss = 0.05940262\n",
      "Iteration 79, loss = 0.05912038\n",
      "Iteration 80, loss = 0.05884711\n",
      "Iteration 81, loss = 0.05858237\n",
      "Iteration 82, loss = 0.05832559\n",
      "Iteration 83, loss = 0.05807635\n",
      "Iteration 84, loss = 0.05783413\n",
      "Iteration 85, loss = 0.05759856\n",
      "Iteration 86, loss = 0.05736946\n",
      "Iteration 87, loss = 0.05714620\n",
      "Iteration 88, loss = 0.05692874\n",
      "Iteration 89, loss = 0.05671668\n",
      "Iteration 90, loss = 0.05650975\n",
      "Iteration 91, loss = 0.05630770\n",
      "Iteration 92, loss = 0.05611037\n",
      "Iteration 93, loss = 0.05591741\n",
      "Iteration 94, loss = 0.05572869\n",
      "Iteration 95, loss = 0.05554404\n",
      "Iteration 96, loss = 0.05536329\n",
      "Iteration 97, loss = 0.05518622\n",
      "Iteration 98, loss = 0.05501271\n",
      "Iteration 99, loss = 0.05484262\n",
      "Iteration 100, loss = 0.05467582\n",
      "Iteration 101, loss = 0.05451225\n",
      "Iteration 102, loss = 0.05435166\n",
      "Iteration 103, loss = 0.05419406\n",
      "Iteration 104, loss = 0.05403929\n",
      "Iteration 105, loss = 0.05388725\n",
      "Iteration 106, loss = 0.05373787\n",
      "Iteration 107, loss = 0.05359105\n",
      "Iteration 108, loss = 0.05344670\n",
      "Iteration 109, loss = 0.05330476\n",
      "Iteration 110, loss = 0.05316515\n",
      "Iteration 111, loss = 0.05302784\n",
      "Iteration 112, loss = 0.05289271\n",
      "Iteration 113, loss = 0.05275972\n",
      "Iteration 114, loss = 0.05262882\n",
      "Iteration 115, loss = 0.05249992\n",
      "Iteration 116, loss = 0.05237299\n",
      "Iteration 117, loss = 0.05224797\n",
      "Iteration 118, loss = 0.05212481\n",
      "Iteration 119, loss = 0.05200346\n",
      "Iteration 120, loss = 0.05188389\n",
      "Iteration 121, loss = 0.05176605\n",
      "Iteration 122, loss = 0.05164989\n",
      "Iteration 123, loss = 0.05153539\n",
      "Iteration 124, loss = 0.05142249\n",
      "Iteration 125, loss = 0.05131117\n",
      "Iteration 126, loss = 0.05120139\n",
      "Iteration 127, loss = 0.05109311\n",
      "Iteration 128, loss = 0.05098630\n",
      "Iteration 129, loss = 0.05088094\n",
      "Iteration 130, loss = 0.05077698\n",
      "Iteration 131, loss = 0.05067442\n",
      "Iteration 132, loss = 0.05057321\n",
      "Iteration 133, loss = 0.05047333\n",
      "Iteration 134, loss = 0.05037475\n",
      "Iteration 135, loss = 0.05027744\n",
      "Iteration 136, loss = 0.05018138\n",
      "Iteration 137, loss = 0.05008654\n",
      "Iteration 138, loss = 0.04999292\n",
      "Iteration 139, loss = 0.04990057\n",
      "Iteration 140, loss = 0.04980922\n",
      "Iteration 141, loss = 0.04971908\n",
      "Iteration 142, loss = 0.04963004\n",
      "Iteration 143, loss = 0.04954217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.980000\n",
      "Training set loss: 0.049542\n",
      "training: inv-scaling learning-rate\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.03169308\n",
      "Iteration 3, loss = 1.03003298\n",
      "Iteration 4, loss = 1.02886267\n",
      "Iteration 5, loss = 1.02790848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 1.02708422\n",
      "Iteration 7, loss = 1.02634996\n",
      "Iteration 8, loss = 1.02568121\n",
      "Iteration 9, loss = 1.02506363\n",
      "Iteration 10, loss = 1.02448595\n",
      "Iteration 11, loss = 1.02394271\n",
      "Iteration 12, loss = 1.02342786\n",
      "Iteration 13, loss = 1.02293781\n",
      "Iteration 14, loss = 1.02246867\n",
      "Iteration 15, loss = 1.02201841\n",
      "Iteration 16, loss = 1.02158494\n",
      "Iteration 17, loss = 1.02116645\n",
      "Iteration 18, loss = 1.02076150\n",
      "Iteration 19, loss = 1.02036955\n",
      "Iteration 20, loss = 1.01998922\n",
      "Iteration 21, loss = 1.01961960\n",
      "Iteration 22, loss = 1.01925962\n",
      "Iteration 23, loss = 1.01890874\n",
      "Iteration 24, loss = 1.01856613\n",
      "Iteration 25, loss = 1.01823081\n",
      "Iteration 26, loss = 1.01790284\n",
      "Iteration 27, loss = 1.01758143\n",
      "Iteration 28, loss = 1.01726596\n",
      "Iteration 29, loss = 1.01695630\n",
      "Iteration 30, loss = 1.01665182\n",
      "Iteration 31, loss = 1.01635298\n",
      "Iteration 32, loss = 1.01605903\n",
      "Iteration 33, loss = 1.01576941\n",
      "Iteration 34, loss = 1.01548430\n",
      "Iteration 35, loss = 1.01520374\n",
      "Iteration 36, loss = 1.01492745\n",
      "Iteration 37, loss = 1.01465519\n",
      "Iteration 38, loss = 1.01438632\n",
      "Iteration 39, loss = 1.01412128\n",
      "Iteration 40, loss = 1.01385985\n",
      "Iteration 41, loss = 1.01360180\n",
      "Iteration 42, loss = 1.01334713\n",
      "Iteration 43, loss = 1.01309567\n",
      "Iteration 44, loss = 1.01284721\n",
      "Iteration 45, loss = 1.01260126\n",
      "Iteration 46, loss = 1.01235834\n",
      "Iteration 47, loss = 1.01211850\n",
      "Iteration 48, loss = 1.01188142\n",
      "Iteration 49, loss = 1.01164705\n",
      "Iteration 50, loss = 1.01141530\n",
      "Iteration 51, loss = 1.01118612\n",
      "Iteration 52, loss = 1.01095932\n",
      "Iteration 53, loss = 1.01073480\n",
      "Iteration 54, loss = 1.01051258\n",
      "Iteration 55, loss = 1.01029259\n",
      "Iteration 56, loss = 1.01007472\n",
      "Iteration 57, loss = 1.00985851\n",
      "Iteration 58, loss = 1.00964454\n",
      "Iteration 59, loss = 1.00943252\n",
      "Iteration 60, loss = 1.00922238\n",
      "Iteration 61, loss = 1.00901423\n",
      "Iteration 62, loss = 1.00880789\n",
      "Iteration 63, loss = 1.00860281\n",
      "Iteration 64, loss = 1.00839938\n",
      "Iteration 65, loss = 1.00819765\n",
      "Iteration 66, loss = 1.00799747\n",
      "Iteration 67, loss = 1.00779887\n",
      "Iteration 68, loss = 1.00760175\n",
      "Iteration 69, loss = 1.00740509\n",
      "Iteration 70, loss = 1.00720901\n",
      "Iteration 71, loss = 1.00701448\n",
      "Iteration 72, loss = 1.00682149\n",
      "Iteration 73, loss = 1.00662972\n",
      "Iteration 74, loss = 1.00643893\n",
      "Iteration 75, loss = 1.00624956\n",
      "Iteration 76, loss = 1.00606154\n",
      "Iteration 77, loss = 1.00587498\n",
      "Iteration 78, loss = 1.00568980\n",
      "Iteration 79, loss = 1.00550588\n",
      "Iteration 80, loss = 1.00532321\n",
      "Iteration 81, loss = 1.00514174\n",
      "Iteration 82, loss = 1.00496141\n",
      "Iteration 83, loss = 1.00478223\n",
      "Iteration 84, loss = 1.00460420\n",
      "Iteration 85, loss = 1.00442728\n",
      "Iteration 86, loss = 1.00425149\n",
      "Iteration 87, loss = 1.00407688\n",
      "Iteration 88, loss = 1.00390337\n",
      "Iteration 89, loss = 1.00373100\n",
      "Iteration 90, loss = 1.00355971\n",
      "Iteration 91, loss = 1.00338954\n",
      "Iteration 92, loss = 1.00322055\n",
      "Iteration 93, loss = 1.00305260\n",
      "Iteration 94, loss = 1.00288560\n",
      "Iteration 95, loss = 1.00271955\n",
      "Iteration 96, loss = 1.00255442\n",
      "Iteration 97, loss = 1.00239021\n",
      "Iteration 98, loss = 1.00222688\n",
      "Iteration 99, loss = 1.00206438\n",
      "Iteration 100, loss = 1.00190268\n",
      "Iteration 101, loss = 1.00174183\n",
      "Iteration 102, loss = 1.00158182\n",
      "Iteration 103, loss = 1.00142270\n",
      "Iteration 104, loss = 1.00126445\n",
      "Iteration 105, loss = 1.00110715\n",
      "Iteration 106, loss = 1.00095064\n",
      "Iteration 107, loss = 1.00079500\n",
      "Iteration 108, loss = 1.00064012\n",
      "Iteration 109, loss = 1.00048599\n",
      "Iteration 110, loss = 1.00033261\n",
      "Iteration 111, loss = 1.00017998\n",
      "Iteration 112, loss = 1.00002808\n",
      "Iteration 113, loss = 0.99987690\n",
      "Iteration 114, loss = 0.99972626\n",
      "Iteration 115, loss = 0.99957604\n",
      "Iteration 116, loss = 0.99942649\n",
      "Iteration 117, loss = 0.99927761\n",
      "Iteration 118, loss = 0.99912934\n",
      "Iteration 119, loss = 0.99898173\n",
      "Iteration 120, loss = 0.99883469\n",
      "Iteration 121, loss = 0.99868801\n",
      "Iteration 122, loss = 0.99854196\n",
      "Iteration 123, loss = 0.99839654\n",
      "Iteration 124, loss = 0.99825174\n",
      "Iteration 125, loss = 0.99810755\n",
      "Iteration 126, loss = 0.99796398\n",
      "Iteration 127, loss = 0.99782103\n",
      "Iteration 128, loss = 0.99767866\n",
      "Iteration 129, loss = 0.99753688\n",
      "Iteration 130, loss = 0.99739568\n",
      "Iteration 131, loss = 0.99725504\n",
      "Iteration 132, loss = 0.99711496\n",
      "Iteration 133, loss = 0.99697537\n",
      "Iteration 134, loss = 0.99683636\n",
      "Iteration 135, loss = 0.99669789\n",
      "Iteration 136, loss = 0.99655996\n",
      "Iteration 137, loss = 0.99642255\n",
      "Iteration 138, loss = 0.99628557\n",
      "Iteration 139, loss = 0.99614880\n",
      "Iteration 140, loss = 0.99601216\n",
      "Iteration 141, loss = 0.99587566\n",
      "Iteration 142, loss = 0.99573949\n",
      "Iteration 143, loss = 0.99560355\n",
      "Iteration 144, loss = 0.99546803\n",
      "Iteration 145, loss = 0.99533299\n",
      "Iteration 146, loss = 0.99519845\n",
      "Iteration 147, loss = 0.99506418\n",
      "Iteration 148, loss = 0.99493015\n",
      "Iteration 149, loss = 0.99479654\n",
      "Iteration 150, loss = 0.99466332\n",
      "Iteration 151, loss = 0.99453048\n",
      "Iteration 152, loss = 0.99439810\n",
      "Iteration 153, loss = 0.99426618\n",
      "Iteration 154, loss = 0.99413470\n",
      "Iteration 155, loss = 0.99400366\n",
      "Iteration 156, loss = 0.99387307\n",
      "Iteration 157, loss = 0.99374286\n",
      "Iteration 158, loss = 0.99361269\n",
      "Iteration 159, loss = 0.99348276\n",
      "Iteration 160, loss = 0.99335303\n",
      "Iteration 161, loss = 0.99322372\n",
      "Iteration 162, loss = 0.99309476\n",
      "Iteration 163, loss = 0.99296626\n",
      "Iteration 164, loss = 0.99283804\n",
      "Iteration 165, loss = 0.99270992\n",
      "Iteration 166, loss = 0.99258221\n",
      "Iteration 167, loss = 0.99245489\n",
      "Iteration 168, loss = 0.99232798\n",
      "Iteration 169, loss = 0.99220147\n",
      "Iteration 170, loss = 0.99207534\n",
      "Iteration 171, loss = 0.99194961\n",
      "Iteration 172, loss = 0.99182428\n",
      "Iteration 173, loss = 0.99169933\n",
      "Iteration 174, loss = 0.99157476\n",
      "Iteration 175, loss = 0.99145059\n",
      "Iteration 176, loss = 0.99132678\n",
      "Iteration 177, loss = 0.99120334\n",
      "Iteration 178, loss = 0.99108027\n",
      "Iteration 179, loss = 0.99095754\n",
      "Iteration 180, loss = 0.99083477\n",
      "Iteration 181, loss = 0.99071236\n",
      "Iteration 182, loss = 0.99059031\n",
      "Iteration 183, loss = 0.99046860\n",
      "Iteration 184, loss = 0.99034727\n",
      "Iteration 185, loss = 0.99022642\n",
      "Iteration 186, loss = 0.99010596\n",
      "Iteration 187, loss = 0.98998581\n",
      "Iteration 188, loss = 0.98986596\n",
      "Iteration 189, loss = 0.98974638\n",
      "Iteration 190, loss = 0.98962714\n",
      "Iteration 191, loss = 0.98950822\n",
      "Iteration 192, loss = 0.98938950\n",
      "Iteration 193, loss = 0.98927081\n",
      "Iteration 194, loss = 0.98915243\n",
      "Iteration 195, loss = 0.98903438\n",
      "Iteration 196, loss = 0.98891663\n",
      "Iteration 197, loss = 0.98879880\n",
      "Iteration 198, loss = 0.98868127\n",
      "Iteration 199, loss = 0.98856406\n",
      "Iteration 200, loss = 0.98844715\n",
      "Iteration 201, loss = 0.98833055\n",
      "Iteration 202, loss = 0.98821426\n",
      "Iteration 203, loss = 0.98809826\n",
      "Iteration 204, loss = 0.98798257\n",
      "Iteration 205, loss = 0.98786717\n",
      "Iteration 206, loss = 0.98775208\n",
      "Iteration 207, loss = 0.98763729\n",
      "Iteration 208, loss = 0.98752280\n",
      "Iteration 209, loss = 0.98740859\n",
      "Iteration 210, loss = 0.98729466\n",
      "Iteration 211, loss = 0.98718101\n",
      "Iteration 212, loss = 0.98706759\n",
      "Iteration 213, loss = 0.98695444\n",
      "Iteration 214, loss = 0.98684158\n",
      "Iteration 215, loss = 0.98672903\n",
      "Iteration 216, loss = 0.98661683\n",
      "Iteration 217, loss = 0.98650497\n",
      "Iteration 218, loss = 0.98639342\n",
      "Iteration 219, loss = 0.98628214\n",
      "Iteration 220, loss = 0.98617113\n",
      "Iteration 221, loss = 0.98606038\n",
      "Iteration 222, loss = 0.98594989\n",
      "Iteration 223, loss = 0.98583967\n",
      "Iteration 224, loss = 0.98572973\n",
      "Iteration 225, loss = 0.98562006\n",
      "Iteration 226, loss = 0.98551065\n",
      "Iteration 227, loss = 0.98540149\n",
      "Iteration 228, loss = 0.98529259\n",
      "Iteration 229, loss = 0.98518393\n",
      "Iteration 230, loss = 0.98507553\n",
      "Iteration 231, loss = 0.98496734\n",
      "Iteration 232, loss = 0.98485898\n",
      "Iteration 233, loss = 0.98475082\n",
      "Iteration 234, loss = 0.98464264\n",
      "Iteration 235, loss = 0.98453447\n",
      "Iteration 236, loss = 0.98442656\n",
      "Iteration 237, loss = 0.98431888\n",
      "Iteration 238, loss = 0.98421144\n",
      "Iteration 239, loss = 0.98410424\n",
      "Iteration 240, loss = 0.98399727\n",
      "Iteration 241, loss = 0.98389057\n",
      "Iteration 242, loss = 0.98378414\n",
      "Iteration 243, loss = 0.98367795\n",
      "Iteration 244, loss = 0.98357200\n",
      "Iteration 245, loss = 0.98346627\n",
      "Iteration 246, loss = 0.98336073\n",
      "Iteration 247, loss = 0.98325537\n",
      "Iteration 248, loss = 0.98315025\n",
      "Iteration 249, loss = 0.98304536\n",
      "Iteration 250, loss = 0.98294070\n",
      "Iteration 251, loss = 0.98283628\n",
      "Iteration 252, loss = 0.98273211\n",
      "Iteration 253, loss = 0.98262816\n",
      "Iteration 254, loss = 0.98252441\n",
      "Iteration 255, loss = 0.98242089\n",
      "Iteration 256, loss = 0.98231758\n",
      "Iteration 257, loss = 0.98221446\n",
      "Iteration 258, loss = 0.98211119\n",
      "Iteration 259, loss = 0.98200812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 260, loss = 0.98190528\n",
      "Iteration 261, loss = 0.98180270\n",
      "Iteration 262, loss = 0.98170032\n",
      "Iteration 263, loss = 0.98159814\n",
      "Iteration 264, loss = 0.98149613\n",
      "Iteration 265, loss = 0.98139388\n",
      "Iteration 266, loss = 0.98129184\n",
      "Iteration 267, loss = 0.98118999\n",
      "Iteration 268, loss = 0.98108834\n",
      "Iteration 269, loss = 0.98098690\n",
      "Iteration 270, loss = 0.98088565\n",
      "Iteration 271, loss = 0.98078462\n",
      "Iteration 272, loss = 0.98068378\n",
      "Iteration 273, loss = 0.98058314\n",
      "Iteration 274, loss = 0.98048269\n",
      "Iteration 275, loss = 0.98038243\n",
      "Iteration 276, loss = 0.98028236\n",
      "Iteration 277, loss = 0.98018246\n",
      "Iteration 278, loss = 0.98008269\n",
      "Iteration 279, loss = 0.97998311\n",
      "Iteration 280, loss = 0.97988372\n",
      "Iteration 281, loss = 0.97978452\n",
      "Iteration 282, loss = 0.97968542\n",
      "Iteration 283, loss = 0.97958651\n",
      "Iteration 284, loss = 0.97948778\n",
      "Iteration 285, loss = 0.97938923\n",
      "Iteration 286, loss = 0.97929086\n",
      "Iteration 287, loss = 0.97919267\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.360000\n",
      "Training set loss: 0.979193\n",
      "training: inv-scaling with momentum\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.01343901\n",
      "Iteration 3, loss = 0.99510827\n",
      "Iteration 4, loss = 0.97865956\n",
      "Iteration 5, loss = 0.96370225\n",
      "Iteration 6, loss = 0.94996885\n",
      "Iteration 7, loss = 0.93732218\n",
      "Iteration 8, loss = 0.92563050\n",
      "Iteration 9, loss = 0.91476074\n",
      "Iteration 10, loss = 0.90459802\n",
      "Iteration 11, loss = 0.89504324\n",
      "Iteration 12, loss = 0.88601052\n",
      "Iteration 13, loss = 0.87744420\n",
      "Iteration 14, loss = 0.86930660\n",
      "Iteration 15, loss = 0.86156642\n",
      "Iteration 16, loss = 0.85421123\n",
      "Iteration 17, loss = 0.84720967\n",
      "Iteration 18, loss = 0.84055041\n",
      "Iteration 19, loss = 0.83422639\n",
      "Iteration 20, loss = 0.82820965\n",
      "Iteration 21, loss = 0.82249651\n",
      "Iteration 22, loss = 0.81706306\n",
      "Iteration 23, loss = 0.81188604\n",
      "Iteration 24, loss = 0.80694723\n",
      "Iteration 25, loss = 0.80223587\n",
      "Iteration 26, loss = 0.79773572\n",
      "Iteration 27, loss = 0.79344051\n",
      "Iteration 28, loss = 0.78933308\n",
      "Iteration 29, loss = 0.78539575\n",
      "Iteration 30, loss = 0.78162415\n",
      "Iteration 31, loss = 0.77799554\n",
      "Iteration 32, loss = 0.77450067\n",
      "Iteration 33, loss = 0.77113012\n",
      "Iteration 34, loss = 0.76787916\n",
      "Iteration 35, loss = 0.76473666\n",
      "Iteration 36, loss = 0.76169678\n",
      "Iteration 37, loss = 0.75875505\n",
      "Iteration 38, loss = 0.75590633\n",
      "Iteration 39, loss = 0.75314718\n",
      "Iteration 40, loss = 0.75046858\n",
      "Iteration 41, loss = 0.74786475\n",
      "Iteration 42, loss = 0.74533237\n",
      "Iteration 43, loss = 0.74286718\n",
      "Iteration 44, loss = 0.74046536\n",
      "Iteration 45, loss = 0.73812320\n",
      "Iteration 46, loss = 0.73583702\n",
      "Iteration 47, loss = 0.73360538\n",
      "Iteration 48, loss = 0.73142640\n",
      "Iteration 49, loss = 0.72929621\n",
      "Iteration 50, loss = 0.72721188\n",
      "Iteration 51, loss = 0.72517144\n",
      "Iteration 52, loss = 0.72317296\n",
      "Iteration 53, loss = 0.72121523\n",
      "Iteration 54, loss = 0.71929656\n",
      "Iteration 55, loss = 0.71741375\n",
      "Iteration 56, loss = 0.71556513\n",
      "Iteration 57, loss = 0.71374903\n",
      "Iteration 58, loss = 0.71196423\n",
      "Iteration 59, loss = 0.71020955\n",
      "Iteration 60, loss = 0.70848361\n",
      "Iteration 61, loss = 0.70678608\n",
      "Iteration 62, loss = 0.70511540\n",
      "Iteration 63, loss = 0.70347048\n",
      "Iteration 64, loss = 0.70185115\n",
      "Iteration 65, loss = 0.70025579\n",
      "Iteration 66, loss = 0.69868361\n",
      "Iteration 67, loss = 0.69713368\n",
      "Iteration 68, loss = 0.69560429\n",
      "Iteration 69, loss = 0.69409606\n",
      "Iteration 70, loss = 0.69260764\n",
      "Iteration 71, loss = 0.69113825\n",
      "Iteration 72, loss = 0.68968763\n",
      "Iteration 73, loss = 0.68825536\n",
      "Iteration 74, loss = 0.68684079\n",
      "Iteration 75, loss = 0.68544370\n",
      "Iteration 76, loss = 0.68406321\n",
      "Iteration 77, loss = 0.68269925\n",
      "Iteration 78, loss = 0.68135137\n",
      "Iteration 79, loss = 0.68001884\n",
      "Iteration 80, loss = 0.67870155\n",
      "Iteration 81, loss = 0.67739912\n",
      "Iteration 82, loss = 0.67611139\n",
      "Iteration 83, loss = 0.67483747\n",
      "Iteration 84, loss = 0.67357750\n",
      "Iteration 85, loss = 0.67233137\n",
      "Iteration 86, loss = 0.67109819\n",
      "Iteration 87, loss = 0.66987861\n",
      "Iteration 88, loss = 0.66867230\n",
      "Iteration 89, loss = 0.66747885\n",
      "Iteration 90, loss = 0.66629782\n",
      "Iteration 91, loss = 0.66512886\n",
      "Iteration 92, loss = 0.66397185\n",
      "Iteration 93, loss = 0.66282660\n",
      "Iteration 94, loss = 0.66169248\n",
      "Iteration 95, loss = 0.66056890\n",
      "Iteration 96, loss = 0.65945640\n",
      "Iteration 97, loss = 0.65835442\n",
      "Iteration 98, loss = 0.65726370\n",
      "Iteration 99, loss = 0.65618325\n",
      "Iteration 100, loss = 0.65511264\n",
      "Iteration 101, loss = 0.65405186\n",
      "Iteration 102, loss = 0.65300058\n",
      "Iteration 103, loss = 0.65195851\n",
      "Iteration 104, loss = 0.65092571\n",
      "Iteration 105, loss = 0.64990186\n",
      "Iteration 106, loss = 0.64888711\n",
      "Iteration 107, loss = 0.64788133\n",
      "Iteration 108, loss = 0.64688390\n",
      "Iteration 109, loss = 0.64589468\n",
      "Iteration 110, loss = 0.64491358\n",
      "Iteration 111, loss = 0.64394064\n",
      "Iteration 112, loss = 0.64297590\n",
      "Iteration 113, loss = 0.64201896\n",
      "Iteration 114, loss = 0.64106998\n",
      "Iteration 115, loss = 0.64012902\n",
      "Iteration 116, loss = 0.63919597\n",
      "Iteration 117, loss = 0.63827066\n",
      "Iteration 118, loss = 0.63735274\n",
      "Iteration 119, loss = 0.63644208\n",
      "Iteration 120, loss = 0.63553853\n",
      "Iteration 121, loss = 0.63464187\n",
      "Iteration 122, loss = 0.63375213\n",
      "Iteration 123, loss = 0.63286909\n",
      "Iteration 124, loss = 0.63199286\n",
      "Iteration 125, loss = 0.63112319\n",
      "Iteration 126, loss = 0.63025992\n",
      "Iteration 127, loss = 0.62940291\n",
      "Iteration 128, loss = 0.62855206\n",
      "Iteration 129, loss = 0.62770727\n",
      "Iteration 130, loss = 0.62686853\n",
      "Iteration 131, loss = 0.62603582\n",
      "Iteration 132, loss = 0.62520897\n",
      "Iteration 133, loss = 0.62438785\n",
      "Iteration 134, loss = 0.62357242\n",
      "Iteration 135, loss = 0.62276264\n",
      "Iteration 136, loss = 0.62195843\n",
      "Iteration 137, loss = 0.62115962\n",
      "Iteration 138, loss = 0.62036620\n",
      "Iteration 139, loss = 0.61957823\n",
      "Iteration 140, loss = 0.61879550\n",
      "Iteration 141, loss = 0.61801806\n",
      "Iteration 142, loss = 0.61724582\n",
      "Iteration 143, loss = 0.61647861\n",
      "Iteration 144, loss = 0.61571644\n",
      "Iteration 145, loss = 0.61495929\n",
      "Iteration 146, loss = 0.61420703\n",
      "Iteration 147, loss = 0.61345960\n",
      "Iteration 148, loss = 0.61271691\n",
      "Iteration 149, loss = 0.61197889\n",
      "Iteration 150, loss = 0.61124553\n",
      "Iteration 151, loss = 0.61051678\n",
      "Iteration 152, loss = 0.60979254\n",
      "Iteration 153, loss = 0.60907284\n",
      "Iteration 154, loss = 0.60835763\n",
      "Iteration 155, loss = 0.60764683\n",
      "Iteration 156, loss = 0.60694039\n",
      "Iteration 157, loss = 0.60623825\n",
      "Iteration 158, loss = 0.60554034\n",
      "Iteration 159, loss = 0.60484664\n",
      "Iteration 160, loss = 0.60415709\n",
      "Iteration 161, loss = 0.60347163\n",
      "Iteration 162, loss = 0.60279024\n",
      "Iteration 163, loss = 0.60211290\n",
      "Iteration 164, loss = 0.60143957\n",
      "Iteration 165, loss = 0.60077018\n",
      "Iteration 166, loss = 0.60010473\n",
      "Iteration 167, loss = 0.59944317\n",
      "Iteration 168, loss = 0.59878540\n",
      "Iteration 169, loss = 0.59813139\n",
      "Iteration 170, loss = 0.59748114\n",
      "Iteration 171, loss = 0.59683459\n",
      "Iteration 172, loss = 0.59619169\n",
      "Iteration 173, loss = 0.59555242\n",
      "Iteration 174, loss = 0.59491672\n",
      "Iteration 175, loss = 0.59428454\n",
      "Iteration 176, loss = 0.59365584\n",
      "Iteration 177, loss = 0.59303060\n",
      "Iteration 178, loss = 0.59240879\n",
      "Iteration 179, loss = 0.59179039\n",
      "Iteration 180, loss = 0.59117541\n",
      "Iteration 181, loss = 0.59056375\n",
      "Iteration 182, loss = 0.58995537\n",
      "Iteration 183, loss = 0.58935024\n",
      "Iteration 184, loss = 0.58874834\n",
      "Iteration 185, loss = 0.58814966\n",
      "Iteration 186, loss = 0.58755416\n",
      "Iteration 187, loss = 0.58696179\n",
      "Iteration 188, loss = 0.58637257\n",
      "Iteration 189, loss = 0.58578647\n",
      "Iteration 190, loss = 0.58520342\n",
      "Iteration 191, loss = 0.58462341\n",
      "Iteration 192, loss = 0.58404642\n",
      "Iteration 193, loss = 0.58347247\n",
      "Iteration 194, loss = 0.58290148\n",
      "Iteration 195, loss = 0.58233349\n",
      "Iteration 196, loss = 0.58176848\n",
      "Iteration 197, loss = 0.58120637\n",
      "Iteration 198, loss = 0.58064713\n",
      "Iteration 199, loss = 0.58009074\n",
      "Iteration 200, loss = 0.57953715\n",
      "Iteration 201, loss = 0.57898632\n",
      "Iteration 202, loss = 0.57843825\n",
      "Iteration 203, loss = 0.57789289\n",
      "Iteration 204, loss = 0.57735022\n",
      "Iteration 205, loss = 0.57681021\n",
      "Iteration 206, loss = 0.57627288\n",
      "Iteration 207, loss = 0.57573818\n",
      "Iteration 208, loss = 0.57520607\n",
      "Iteration 209, loss = 0.57467651\n",
      "Iteration 210, loss = 0.57414950\n",
      "Iteration 211, loss = 0.57362500\n",
      "Iteration 212, loss = 0.57310302\n",
      "Iteration 213, loss = 0.57258353\n",
      "Iteration 214, loss = 0.57206645\n",
      "Iteration 215, loss = 0.57155178\n",
      "Iteration 216, loss = 0.57103955\n",
      "Iteration 217, loss = 0.57052972\n",
      "Iteration 218, loss = 0.57002224\n",
      "Iteration 219, loss = 0.56951709\n",
      "Iteration 220, loss = 0.56901423\n",
      "Iteration 221, loss = 0.56851367\n",
      "Iteration 222, loss = 0.56801541\n",
      "Iteration 223, loss = 0.56751942\n",
      "Iteration 224, loss = 0.56702569\n",
      "Iteration 225, loss = 0.56653422\n",
      "Iteration 226, loss = 0.56604496\n",
      "Iteration 227, loss = 0.56555791\n",
      "Iteration 228, loss = 0.56507305\n",
      "Iteration 229, loss = 0.56459037\n",
      "Iteration 230, loss = 0.56410983\n",
      "Iteration 231, loss = 0.56363143\n",
      "Iteration 232, loss = 0.56315514\n",
      "Iteration 233, loss = 0.56268098\n",
      "Iteration 234, loss = 0.56220890\n",
      "Iteration 235, loss = 0.56173892\n",
      "Iteration 236, loss = 0.56127098\n",
      "Iteration 237, loss = 0.56080507\n",
      "Iteration 238, loss = 0.56034117\n",
      "Iteration 239, loss = 0.55987928\n",
      "Iteration 240, loss = 0.55941942\n",
      "Iteration 241, loss = 0.55896153\n",
      "Iteration 242, loss = 0.55850556\n",
      "Iteration 243, loss = 0.55805152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 244, loss = 0.55759941\n",
      "Iteration 245, loss = 0.55714921\n",
      "Iteration 246, loss = 0.55670092\n",
      "Iteration 247, loss = 0.55625447\n",
      "Iteration 248, loss = 0.55580989\n",
      "Iteration 249, loss = 0.55536715\n",
      "Iteration 250, loss = 0.55492624\n",
      "Iteration 251, loss = 0.55448715\n",
      "Iteration 252, loss = 0.55404984\n",
      "Iteration 253, loss = 0.55361433\n",
      "Iteration 254, loss = 0.55318065\n",
      "Iteration 255, loss = 0.55274872\n",
      "Iteration 256, loss = 0.55231854\n",
      "Iteration 257, loss = 0.55189017\n",
      "Iteration 258, loss = 0.55146354\n",
      "Iteration 259, loss = 0.55103864\n",
      "Iteration 260, loss = 0.55061546\n",
      "Iteration 261, loss = 0.55019398\n",
      "Iteration 262, loss = 0.54977421\n",
      "Iteration 263, loss = 0.54935613\n",
      "Iteration 264, loss = 0.54893970\n",
      "Iteration 265, loss = 0.54852489\n",
      "Iteration 266, loss = 0.54811169\n",
      "Iteration 267, loss = 0.54770011\n",
      "Iteration 268, loss = 0.54729015\n",
      "Iteration 269, loss = 0.54688178\n",
      "Iteration 270, loss = 0.54647499\n",
      "Iteration 271, loss = 0.54606979\n",
      "Iteration 272, loss = 0.54566614\n",
      "Iteration 273, loss = 0.54526407\n",
      "Iteration 274, loss = 0.54486355\n",
      "Iteration 275, loss = 0.54446459\n",
      "Iteration 276, loss = 0.54406716\n",
      "Iteration 277, loss = 0.54367125\n",
      "Iteration 278, loss = 0.54327686\n",
      "Iteration 279, loss = 0.54288397\n",
      "Iteration 280, loss = 0.54249257\n",
      "Iteration 281, loss = 0.54210263\n",
      "Iteration 282, loss = 0.54171414\n",
      "Iteration 283, loss = 0.54132710\n",
      "Iteration 284, loss = 0.54094146\n",
      "Iteration 285, loss = 0.54055726\n",
      "Iteration 286, loss = 0.54017447\n",
      "Iteration 287, loss = 0.53979312\n",
      "Iteration 288, loss = 0.53941323\n",
      "Iteration 289, loss = 0.53903475\n",
      "Iteration 290, loss = 0.53865766\n",
      "Iteration 291, loss = 0.53828197\n",
      "Iteration 292, loss = 0.53790770\n",
      "Iteration 293, loss = 0.53753479\n",
      "Iteration 294, loss = 0.53716326\n",
      "Iteration 295, loss = 0.53679309\n",
      "Iteration 296, loss = 0.53642427\n",
      "Iteration 297, loss = 0.53605678\n",
      "Iteration 298, loss = 0.53569064\n",
      "Iteration 299, loss = 0.53532582\n",
      "Iteration 300, loss = 0.53496233\n",
      "Iteration 301, loss = 0.53460013\n",
      "Iteration 302, loss = 0.53423922\n",
      "Iteration 303, loss = 0.53387957\n",
      "Iteration 304, loss = 0.53352118\n",
      "Iteration 305, loss = 0.53316404\n",
      "Iteration 306, loss = 0.53280814\n",
      "Iteration 307, loss = 0.53245345\n",
      "Iteration 308, loss = 0.53209999\n",
      "Iteration 309, loss = 0.53174775\n",
      "Iteration 310, loss = 0.53139672\n",
      "Iteration 311, loss = 0.53104690\n",
      "Iteration 312, loss = 0.53069833\n",
      "Iteration 313, loss = 0.53035095\n",
      "Iteration 314, loss = 0.53000477\n",
      "Iteration 315, loss = 0.52965978\n",
      "Iteration 316, loss = 0.52931592\n",
      "Iteration 317, loss = 0.52897321\n",
      "Iteration 318, loss = 0.52863166\n",
      "Iteration 319, loss = 0.52829126\n",
      "Iteration 320, loss = 0.52795203\n",
      "Iteration 321, loss = 0.52761397\n",
      "Iteration 322, loss = 0.52727700\n",
      "Iteration 323, loss = 0.52694116\n",
      "Iteration 324, loss = 0.52660644\n",
      "Iteration 325, loss = 0.52627285\n",
      "Iteration 326, loss = 0.52594036\n",
      "Iteration 327, loss = 0.52560897\n",
      "Iteration 328, loss = 0.52527868\n",
      "Iteration 329, loss = 0.52494950\n",
      "Iteration 330, loss = 0.52462140\n",
      "Iteration 331, loss = 0.52429437\n",
      "Iteration 332, loss = 0.52396841\n",
      "Iteration 333, loss = 0.52364351\n",
      "Iteration 334, loss = 0.52331967\n",
      "Iteration 335, loss = 0.52299689\n",
      "Iteration 336, loss = 0.52267516\n",
      "Iteration 337, loss = 0.52235446\n",
      "Iteration 338, loss = 0.52203481\n",
      "Iteration 339, loss = 0.52171617\n",
      "Iteration 340, loss = 0.52139855\n",
      "Iteration 341, loss = 0.52108188\n",
      "Iteration 342, loss = 0.52076621\n",
      "Iteration 343, loss = 0.52045153\n",
      "Iteration 344, loss = 0.52013787\n",
      "Iteration 345, loss = 0.51982520\n",
      "Iteration 346, loss = 0.51951356\n",
      "Iteration 347, loss = 0.51920290\n",
      "Iteration 348, loss = 0.51889321\n",
      "Iteration 349, loss = 0.51858451\n",
      "Iteration 350, loss = 0.51827678\n",
      "Iteration 351, loss = 0.51797001\n",
      "Iteration 352, loss = 0.51766420\n",
      "Iteration 353, loss = 0.51735926\n",
      "Iteration 354, loss = 0.51705526\n",
      "Iteration 355, loss = 0.51675218\n",
      "Iteration 356, loss = 0.51645003\n",
      "Iteration 357, loss = 0.51614880\n",
      "Iteration 358, loss = 0.51584849\n",
      "Iteration 359, loss = 0.51554911\n",
      "Iteration 360, loss = 0.51525075\n",
      "Iteration 361, loss = 0.51495331\n",
      "Iteration 362, loss = 0.51465678\n",
      "Iteration 363, loss = 0.51436116\n",
      "Iteration 364, loss = 0.51406643\n",
      "Iteration 365, loss = 0.51377258\n",
      "Iteration 366, loss = 0.51347962\n",
      "Iteration 367, loss = 0.51318754\n",
      "Iteration 368, loss = 0.51289633\n",
      "Iteration 369, loss = 0.51260603\n",
      "Iteration 370, loss = 0.51231669\n",
      "Iteration 371, loss = 0.51202822\n",
      "Iteration 372, loss = 0.51174058\n",
      "Iteration 373, loss = 0.51145378\n",
      "Iteration 374, loss = 0.51116784\n",
      "Iteration 375, loss = 0.51088272\n",
      "Iteration 376, loss = 0.51059847\n",
      "Iteration 377, loss = 0.51031509\n",
      "Iteration 378, loss = 0.51003252\n",
      "Iteration 379, loss = 0.50975071\n",
      "Iteration 380, loss = 0.50946971\n",
      "Iteration 381, loss = 0.50918953\n",
      "Iteration 382, loss = 0.50891016\n",
      "Iteration 383, loss = 0.50863158\n",
      "Iteration 384, loss = 0.50835380\n",
      "Iteration 385, loss = 0.50807682\n",
      "Iteration 386, loss = 0.50780062\n",
      "Iteration 387, loss = 0.50752522\n",
      "Iteration 388, loss = 0.50725062\n",
      "Iteration 389, loss = 0.50697682\n",
      "Iteration 390, loss = 0.50670388\n",
      "Iteration 391, loss = 0.50643171\n",
      "Iteration 392, loss = 0.50616034\n",
      "Iteration 393, loss = 0.50588976\n",
      "Iteration 394, loss = 0.50561994\n",
      "Iteration 395, loss = 0.50535088\n",
      "Iteration 396, loss = 0.50508257\n",
      "Iteration 397, loss = 0.50481501\n",
      "Iteration 398, loss = 0.50454820\n",
      "Iteration 399, loss = 0.50428212\n",
      "Iteration 400, loss = 0.50401678\n",
      "Iteration 401, loss = 0.50375217\n",
      "Iteration 402, loss = 0.50348832\n",
      "Iteration 403, loss = 0.50322519\n",
      "Iteration 404, loss = 0.50296284\n",
      "Iteration 405, loss = 0.50270129\n",
      "Iteration 406, loss = 0.50244044\n",
      "Iteration 407, loss = 0.50218028\n",
      "Iteration 408, loss = 0.50192083\n",
      "Iteration 409, loss = 0.50166211\n",
      "Iteration 410, loss = 0.50140414\n",
      "Iteration 411, loss = 0.50114686\n",
      "Iteration 412, loss = 0.50089029\n",
      "Iteration 413, loss = 0.50063441\n",
      "Iteration 414, loss = 0.50037920\n",
      "Iteration 415, loss = 0.50012466\n",
      "Iteration 416, loss = 0.49987081\n",
      "Iteration 417, loss = 0.49961764\n",
      "Iteration 418, loss = 0.49936514\n",
      "Iteration 419, loss = 0.49911333\n",
      "Iteration 420, loss = 0.49886218\n",
      "Iteration 421, loss = 0.49861171\n",
      "Iteration 422, loss = 0.49836194\n",
      "Iteration 423, loss = 0.49811283\n",
      "Iteration 424, loss = 0.49786442\n",
      "Iteration 425, loss = 0.49761674\n",
      "Iteration 426, loss = 0.49736973\n",
      "Iteration 427, loss = 0.49712338\n",
      "Iteration 428, loss = 0.49687768\n",
      "Iteration 429, loss = 0.49663263\n",
      "Iteration 430, loss = 0.49638822\n",
      "Iteration 431, loss = 0.49614446\n",
      "Iteration 432, loss = 0.49590142\n",
      "Iteration 433, loss = 0.49565900\n",
      "Iteration 434, loss = 0.49541721\n",
      "Iteration 435, loss = 0.49517605\n",
      "Iteration 436, loss = 0.49493553\n",
      "Iteration 437, loss = 0.49469562\n",
      "Iteration 438, loss = 0.49445633\n",
      "Iteration 439, loss = 0.49421766\n",
      "Iteration 440, loss = 0.49397960\n",
      "Iteration 441, loss = 0.49374215\n",
      "Iteration 442, loss = 0.49350532\n",
      "Iteration 443, loss = 0.49326916\n",
      "Iteration 444, loss = 0.49303361\n",
      "Iteration 445, loss = 0.49279868\n",
      "Iteration 446, loss = 0.49256433\n",
      "Iteration 447, loss = 0.49233058\n",
      "Iteration 448, loss = 0.49209742\n",
      "Iteration 449, loss = 0.49186483\n",
      "Iteration 450, loss = 0.49163282\n",
      "Iteration 451, loss = 0.49140138\n",
      "Iteration 452, loss = 0.49117050\n",
      "Iteration 453, loss = 0.49094020\n",
      "Iteration 454, loss = 0.49071047\n",
      "Iteration 455, loss = 0.49048129\n",
      "Iteration 456, loss = 0.49025268\n",
      "Iteration 457, loss = 0.49002463\n",
      "Iteration 458, loss = 0.48979713\n",
      "Iteration 459, loss = 0.48957016\n",
      "Iteration 460, loss = 0.48934374\n",
      "Iteration 461, loss = 0.48911787\n",
      "Iteration 462, loss = 0.48889255\n",
      "Iteration 463, loss = 0.48866777\n",
      "Iteration 464, loss = 0.48844354\n",
      "Iteration 465, loss = 0.48821984\n",
      "Iteration 466, loss = 0.48799665\n",
      "Iteration 467, loss = 0.48777396\n",
      "Iteration 468, loss = 0.48755179\n",
      "Iteration 469, loss = 0.48733015\n",
      "Iteration 470, loss = 0.48710902\n",
      "Iteration 471, loss = 0.48688841\n",
      "Iteration 472, loss = 0.48666832\n",
      "Iteration 473, loss = 0.48644875\n",
      "Iteration 474, loss = 0.48622969\n",
      "Iteration 475, loss = 0.48601114\n",
      "Iteration 476, loss = 0.48579310\n",
      "Iteration 477, loss = 0.48557557\n",
      "Iteration 478, loss = 0.48535855\n",
      "Iteration 479, loss = 0.48514202\n",
      "Iteration 480, loss = 0.48492601\n",
      "Iteration 481, loss = 0.48471049\n",
      "Iteration 482, loss = 0.48449548\n",
      "Iteration 483, loss = 0.48428096\n",
      "Iteration 484, loss = 0.48406695\n",
      "Iteration 485, loss = 0.48385347\n",
      "Iteration 486, loss = 0.48364052\n",
      "Iteration 487, loss = 0.48342806\n",
      "Iteration 488, loss = 0.48321609\n",
      "Iteration 489, loss = 0.48300463\n",
      "Iteration 490, loss = 0.48279367\n",
      "Iteration 491, loss = 0.48258320\n",
      "Iteration 492, loss = 0.48237320\n",
      "Iteration 493, loss = 0.48216369\n",
      "Iteration 494, loss = 0.48195467\n",
      "Iteration 495, loss = 0.48174613\n",
      "Iteration 496, loss = 0.48153812\n",
      "Iteration 497, loss = 0.48133058\n",
      "Iteration 498, loss = 0.48112352\n",
      "Iteration 499, loss = 0.48091692\n",
      "Iteration 500, loss = 0.48071078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 501, loss = 0.48050510\n",
      "Iteration 502, loss = 0.48029988\n",
      "Iteration 503, loss = 0.48009511\n",
      "Iteration 504, loss = 0.47989079\n",
      "Iteration 505, loss = 0.47968693\n",
      "Iteration 506, loss = 0.47948351\n",
      "Iteration 507, loss = 0.47928051\n",
      "Iteration 508, loss = 0.47907794\n",
      "Iteration 509, loss = 0.47887582\n",
      "Iteration 510, loss = 0.47867414\n",
      "Iteration 511, loss = 0.47847285\n",
      "Iteration 512, loss = 0.47827200\n",
      "Iteration 513, loss = 0.47807157\n",
      "Iteration 514, loss = 0.47787158\n",
      "Iteration 515, loss = 0.47767201\n",
      "Iteration 516, loss = 0.47747287\n",
      "Iteration 517, loss = 0.47727415\n",
      "Iteration 518, loss = 0.47707586\n",
      "Iteration 519, loss = 0.47687798\n",
      "Iteration 520, loss = 0.47668056\n",
      "Iteration 521, loss = 0.47648357\n",
      "Iteration 522, loss = 0.47628699\n",
      "Iteration 523, loss = 0.47609079\n",
      "Iteration 524, loss = 0.47589497\n",
      "Iteration 525, loss = 0.47569957\n",
      "Iteration 526, loss = 0.47550458\n",
      "Iteration 527, loss = 0.47531001\n",
      "Iteration 528, loss = 0.47511584\n",
      "Iteration 529, loss = 0.47492209\n",
      "Iteration 530, loss = 0.47472876\n",
      "Iteration 531, loss = 0.47453584\n",
      "Iteration 532, loss = 0.47434331\n",
      "Iteration 533, loss = 0.47415119\n",
      "Iteration 534, loss = 0.47395948\n",
      "Iteration 535, loss = 0.47376815\n",
      "Iteration 536, loss = 0.47357723\n",
      "Iteration 537, loss = 0.47338670\n",
      "Iteration 538, loss = 0.47319656\n",
      "Iteration 539, loss = 0.47300681\n",
      "Iteration 540, loss = 0.47281744\n",
      "Iteration 541, loss = 0.47262841\n",
      "Iteration 542, loss = 0.47243974\n",
      "Iteration 543, loss = 0.47225140\n",
      "Iteration 544, loss = 0.47206341\n",
      "Iteration 545, loss = 0.47187580\n",
      "Iteration 546, loss = 0.47168856\n",
      "Iteration 547, loss = 0.47150170\n",
      "Iteration 548, loss = 0.47131521\n",
      "Iteration 549, loss = 0.47112910\n",
      "Iteration 550, loss = 0.47094335\n",
      "Iteration 551, loss = 0.47075794\n",
      "Iteration 552, loss = 0.47057290\n",
      "Iteration 553, loss = 0.47038825\n",
      "Iteration 554, loss = 0.47020392\n",
      "Iteration 555, loss = 0.47001995\n",
      "Iteration 556, loss = 0.46983635\n",
      "Iteration 557, loss = 0.46965311\n",
      "Iteration 558, loss = 0.46947023\n",
      "Iteration 559, loss = 0.46928771\n",
      "Iteration 560, loss = 0.46910556\n",
      "Iteration 561, loss = 0.46892376\n",
      "Iteration 562, loss = 0.46874231\n",
      "Iteration 563, loss = 0.46856122\n",
      "Iteration 564, loss = 0.46838049\n",
      "Iteration 565, loss = 0.46820011\n",
      "Iteration 566, loss = 0.46802006\n",
      "Iteration 567, loss = 0.46784035\n",
      "Iteration 568, loss = 0.46766098\n",
      "Iteration 569, loss = 0.46748196\n",
      "Iteration 570, loss = 0.46730325\n",
      "Iteration 571, loss = 0.46712487\n",
      "Iteration 572, loss = 0.46694681\n",
      "Iteration 573, loss = 0.46676910\n",
      "Iteration 574, loss = 0.46659172\n",
      "Iteration 575, loss = 0.46641467\n",
      "Iteration 576, loss = 0.46623797\n",
      "Iteration 577, loss = 0.46606159\n",
      "Iteration 578, loss = 0.46588555\n",
      "Iteration 579, loss = 0.46570985\n",
      "Iteration 580, loss = 0.46553448\n",
      "Iteration 581, loss = 0.46535945\n",
      "Iteration 582, loss = 0.46518474\n",
      "Iteration 583, loss = 0.46501039\n",
      "Iteration 584, loss = 0.46483635\n",
      "Iteration 585, loss = 0.46466262\n",
      "Iteration 586, loss = 0.46448922\n",
      "Iteration 587, loss = 0.46431614\n",
      "Iteration 588, loss = 0.46414338\n",
      "Iteration 589, loss = 0.46397095\n",
      "Iteration 590, loss = 0.46379883\n",
      "Iteration 591, loss = 0.46362706\n",
      "Iteration 592, loss = 0.46345561\n",
      "Iteration 593, loss = 0.46328448\n",
      "Iteration 594, loss = 0.46311366\n",
      "Iteration 595, loss = 0.46294319\n",
      "Iteration 596, loss = 0.46277311\n",
      "Iteration 597, loss = 0.46260336\n",
      "Iteration 598, loss = 0.46243389\n",
      "Iteration 599, loss = 0.46226474\n",
      "Iteration 600, loss = 0.46209590\n",
      "Iteration 601, loss = 0.46192737\n",
      "Iteration 602, loss = 0.46175917\n",
      "Iteration 603, loss = 0.46159127\n",
      "Iteration 604, loss = 0.46142369\n",
      "Iteration 605, loss = 0.46125641\n",
      "Iteration 606, loss = 0.46108944\n",
      "Iteration 607, loss = 0.46092278\n",
      "Iteration 608, loss = 0.46075642\n",
      "Iteration 609, loss = 0.46059036\n",
      "Iteration 610, loss = 0.46042459\n",
      "Iteration 611, loss = 0.46025913\n",
      "Iteration 612, loss = 0.46009396\n",
      "Iteration 613, loss = 0.45992909\n",
      "Iteration 614, loss = 0.45976452\n",
      "Iteration 615, loss = 0.45960026\n",
      "Iteration 616, loss = 0.45943629\n",
      "Iteration 617, loss = 0.45927261\n",
      "Iteration 618, loss = 0.45910923\n",
      "Iteration 619, loss = 0.45894615\n",
      "Iteration 620, loss = 0.45878335\n",
      "Iteration 621, loss = 0.45862084\n",
      "Iteration 622, loss = 0.45845862\n",
      "Iteration 623, loss = 0.45829669\n",
      "Iteration 624, loss = 0.45813504\n",
      "Iteration 625, loss = 0.45797368\n",
      "Iteration 626, loss = 0.45781261\n",
      "Iteration 627, loss = 0.45765183\n",
      "Iteration 628, loss = 0.45749130\n",
      "Iteration 629, loss = 0.45733100\n",
      "Iteration 630, loss = 0.45717098\n",
      "Iteration 631, loss = 0.45701122\n",
      "Iteration 632, loss = 0.45685174\n",
      "Iteration 633, loss = 0.45669254\n",
      "Iteration 634, loss = 0.45653361\n",
      "Iteration 635, loss = 0.45637495\n",
      "Iteration 636, loss = 0.45621656\n",
      "Iteration 637, loss = 0.45605844\n",
      "Iteration 638, loss = 0.45590059\n",
      "Iteration 639, loss = 0.45574302\n",
      "Iteration 640, loss = 0.45558571\n",
      "Iteration 641, loss = 0.45542866\n",
      "Iteration 642, loss = 0.45527187\n",
      "Iteration 643, loss = 0.45511534\n",
      "Iteration 644, loss = 0.45495907\n",
      "Iteration 645, loss = 0.45480307\n",
      "Iteration 646, loss = 0.45464732\n",
      "Iteration 647, loss = 0.45449184\n",
      "Iteration 648, loss = 0.45433662\n",
      "Iteration 649, loss = 0.45418166\n",
      "Iteration 650, loss = 0.45402696\n",
      "Iteration 651, loss = 0.45387251\n",
      "Iteration 652, loss = 0.45371833\n",
      "Iteration 653, loss = 0.45356440\n",
      "Iteration 654, loss = 0.45341073\n",
      "Iteration 655, loss = 0.45325731\n",
      "Iteration 656, loss = 0.45310415\n",
      "Iteration 657, loss = 0.45295125\n",
      "Iteration 658, loss = 0.45279855\n",
      "Iteration 659, loss = 0.45264611\n",
      "Iteration 660, loss = 0.45249393\n",
      "Iteration 661, loss = 0.45234197\n",
      "Iteration 662, loss = 0.45219025\n",
      "Iteration 663, loss = 0.45203878\n",
      "Iteration 664, loss = 0.45188755\n",
      "Iteration 665, loss = 0.45173657\n",
      "Iteration 666, loss = 0.45158584\n",
      "Iteration 667, loss = 0.45143535\n",
      "Iteration 668, loss = 0.45128511\n",
      "Iteration 669, loss = 0.45113508\n",
      "Iteration 670, loss = 0.45098528\n",
      "Iteration 671, loss = 0.45083572\n",
      "Iteration 672, loss = 0.45068640\n",
      "Iteration 673, loss = 0.45053732\n",
      "Iteration 674, loss = 0.45038847\n",
      "Iteration 675, loss = 0.45023986\n",
      "Iteration 676, loss = 0.45009150\n",
      "Iteration 677, loss = 0.44994342\n",
      "Iteration 678, loss = 0.44979564\n",
      "Iteration 679, loss = 0.44964810\n",
      "Iteration 680, loss = 0.44950082\n",
      "Iteration 681, loss = 0.44935377\n",
      "Iteration 682, loss = 0.44920691\n",
      "Iteration 683, loss = 0.44906029\n",
      "Iteration 684, loss = 0.44891390\n",
      "Iteration 685, loss = 0.44876775\n",
      "Iteration 686, loss = 0.44862182\n",
      "Iteration 687, loss = 0.44847613\n",
      "Iteration 688, loss = 0.44833066\n",
      "Iteration 689, loss = 0.44818537\n",
      "Iteration 690, loss = 0.44804031\n",
      "Iteration 691, loss = 0.44789547\n",
      "Iteration 692, loss = 0.44775087\n",
      "Iteration 693, loss = 0.44760648\n",
      "Iteration 694, loss = 0.44746237\n",
      "Iteration 695, loss = 0.44731851\n",
      "Iteration 696, loss = 0.44717488\n",
      "Iteration 697, loss = 0.44703148\n",
      "Iteration 698, loss = 0.44688829\n",
      "Iteration 699, loss = 0.44674533\n",
      "Iteration 700, loss = 0.44660259\n",
      "Iteration 701, loss = 0.44646003\n",
      "Iteration 702, loss = 0.44631770\n",
      "Iteration 703, loss = 0.44617558\n",
      "Iteration 704, loss = 0.44603368\n",
      "Iteration 705, loss = 0.44589200\n",
      "Iteration 706, loss = 0.44575054\n",
      "Iteration 707, loss = 0.44560929\n",
      "Iteration 708, loss = 0.44546826\n",
      "Iteration 709, loss = 0.44532744\n",
      "Iteration 710, loss = 0.44518688\n",
      "Iteration 711, loss = 0.44504659\n",
      "Iteration 712, loss = 0.44490656\n",
      "Iteration 713, loss = 0.44476674\n",
      "Iteration 714, loss = 0.44462713\n",
      "Iteration 715, loss = 0.44448775\n",
      "Iteration 716, loss = 0.44434858\n",
      "Iteration 717, loss = 0.44420962\n",
      "Iteration 718, loss = 0.44407088\n",
      "Iteration 719, loss = 0.44393235\n",
      "Iteration 720, loss = 0.44379404\n",
      "Iteration 721, loss = 0.44365595\n",
      "Iteration 722, loss = 0.44351808\n",
      "Iteration 723, loss = 0.44338041\n",
      "Iteration 724, loss = 0.44324303\n",
      "Iteration 725, loss = 0.44310588\n",
      "Iteration 726, loss = 0.44296900\n",
      "Iteration 727, loss = 0.44283234\n",
      "Iteration 728, loss = 0.44269589\n",
      "Iteration 729, loss = 0.44255965\n",
      "Iteration 730, loss = 0.44242362\n",
      "Iteration 731, loss = 0.44228779\n",
      "Iteration 732, loss = 0.44215218\n",
      "Iteration 733, loss = 0.44201678\n",
      "Iteration 734, loss = 0.44188158\n",
      "Iteration 735, loss = 0.44174658\n",
      "Iteration 736, loss = 0.44161179\n",
      "Iteration 737, loss = 0.44147719\n",
      "Iteration 738, loss = 0.44134281\n",
      "Iteration 739, loss = 0.44120862\n",
      "Iteration 740, loss = 0.44107463\n",
      "Iteration 741, loss = 0.44094084\n",
      "Iteration 742, loss = 0.44080724\n",
      "Iteration 743, loss = 0.44067383\n",
      "Iteration 744, loss = 0.44054063\n",
      "Iteration 745, loss = 0.44040769\n",
      "Iteration 746, loss = 0.44027496\n",
      "Iteration 747, loss = 0.44014242\n",
      "Iteration 748, loss = 0.44001008\n",
      "Iteration 749, loss = 0.43987794\n",
      "Iteration 750, loss = 0.43974599\n",
      "Iteration 751, loss = 0.43961424\n",
      "Iteration 752, loss = 0.43948268\n",
      "Iteration 753, loss = 0.43935128\n",
      "Iteration 754, loss = 0.43922006\n",
      "Iteration 755, loss = 0.43908902\n",
      "Iteration 756, loss = 0.43895810\n",
      "Iteration 757, loss = 0.43882741\n",
      "Iteration 758, loss = 0.43869691\n",
      "Iteration 759, loss = 0.43856661\n",
      "Iteration 760, loss = 0.43843650\n",
      "Iteration 761, loss = 0.43830658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 762, loss = 0.43817686\n",
      "Iteration 763, loss = 0.43804732\n",
      "Iteration 764, loss = 0.43791798\n",
      "Iteration 765, loss = 0.43778881\n",
      "Iteration 766, loss = 0.43765980\n",
      "Iteration 767, loss = 0.43753096\n",
      "Iteration 768, loss = 0.43740232\n",
      "Iteration 769, loss = 0.43727388\n",
      "Iteration 770, loss = 0.43714563\n",
      "Iteration 771, loss = 0.43701756\n",
      "Iteration 772, loss = 0.43688967\n",
      "Iteration 773, loss = 0.43676196\n",
      "Iteration 774, loss = 0.43663443\n",
      "Iteration 775, loss = 0.43650706\n",
      "Iteration 776, loss = 0.43637985\n",
      "Iteration 777, loss = 0.43625282\n",
      "Iteration 778, loss = 0.43612597\n",
      "Iteration 779, loss = 0.43599929\n",
      "Iteration 780, loss = 0.43587279\n",
      "Iteration 781, loss = 0.43574646\n",
      "Iteration 782, loss = 0.43562030\n",
      "Iteration 783, loss = 0.43549432\n",
      "Iteration 784, loss = 0.43536851\n",
      "Iteration 785, loss = 0.43524287\n",
      "Iteration 786, loss = 0.43511740\n",
      "Iteration 787, loss = 0.43499212\n",
      "Iteration 788, loss = 0.43486701\n",
      "Iteration 789, loss = 0.43474207\n",
      "Iteration 790, loss = 0.43461730\n",
      "Iteration 791, loss = 0.43449270\n",
      "Iteration 792, loss = 0.43436828\n",
      "Iteration 793, loss = 0.43424401\n",
      "Iteration 794, loss = 0.43411992\n",
      "Iteration 795, loss = 0.43399599\n",
      "Iteration 796, loss = 0.43387223\n",
      "Iteration 797, loss = 0.43374865\n",
      "Iteration 798, loss = 0.43362526\n",
      "Iteration 799, loss = 0.43350212\n",
      "Iteration 800, loss = 0.43337920\n",
      "Iteration 801, loss = 0.43325645\n",
      "Iteration 802, loss = 0.43313388\n",
      "Iteration 803, loss = 0.43301148\n",
      "Iteration 804, loss = 0.43288926\n",
      "Iteration 805, loss = 0.43276720\n",
      "Iteration 806, loss = 0.43264532\n",
      "Iteration 807, loss = 0.43252361\n",
      "Iteration 808, loss = 0.43240207\n",
      "Iteration 809, loss = 0.43228069\n",
      "Iteration 810, loss = 0.43215949\n",
      "Iteration 811, loss = 0.43203846\n",
      "Iteration 812, loss = 0.43191759\n",
      "Iteration 813, loss = 0.43179688\n",
      "Iteration 814, loss = 0.43167634\n",
      "Iteration 815, loss = 0.43155596\n",
      "Iteration 816, loss = 0.43143574\n",
      "Iteration 817, loss = 0.43131568\n",
      "Iteration 818, loss = 0.43119578\n",
      "Iteration 819, loss = 0.43107604\n",
      "Iteration 820, loss = 0.43095646\n",
      "Iteration 821, loss = 0.43083704\n",
      "Iteration 822, loss = 0.43071778\n",
      "Iteration 823, loss = 0.43059867\n",
      "Iteration 824, loss = 0.43047973\n",
      "Iteration 825, loss = 0.43036093\n",
      "Iteration 826, loss = 0.43024230\n",
      "Iteration 827, loss = 0.43012383\n",
      "Iteration 828, loss = 0.43000552\n",
      "Iteration 829, loss = 0.42988743\n",
      "Iteration 830, loss = 0.42976952\n",
      "Iteration 831, loss = 0.42965176\n",
      "Iteration 832, loss = 0.42953415\n",
      "Iteration 833, loss = 0.42941672\n",
      "Iteration 834, loss = 0.42929944\n",
      "Iteration 835, loss = 0.42918232\n",
      "Iteration 836, loss = 0.42906535\n",
      "Iteration 837, loss = 0.42894854\n",
      "Iteration 838, loss = 0.42883188\n",
      "Iteration 839, loss = 0.42871538\n",
      "Iteration 840, loss = 0.42859903\n",
      "Iteration 841, loss = 0.42848283\n",
      "Iteration 842, loss = 0.42836679\n",
      "Iteration 843, loss = 0.42825090\n",
      "Iteration 844, loss = 0.42813515\n",
      "Iteration 845, loss = 0.42801955\n",
      "Iteration 846, loss = 0.42790407\n",
      "Iteration 847, loss = 0.42778875\n",
      "Iteration 848, loss = 0.42767356\n",
      "Iteration 849, loss = 0.42755852\n",
      "Iteration 850, loss = 0.42744363\n",
      "Iteration 851, loss = 0.42732888\n",
      "Iteration 852, loss = 0.42721426\n",
      "Iteration 853, loss = 0.42709977\n",
      "Iteration 854, loss = 0.42698541\n",
      "Iteration 855, loss = 0.42687120\n",
      "Iteration 856, loss = 0.42675711\n",
      "Iteration 857, loss = 0.42664314\n",
      "Iteration 858, loss = 0.42652930\n",
      "Iteration 859, loss = 0.42641561\n",
      "Iteration 860, loss = 0.42630208\n",
      "Iteration 861, loss = 0.42618874\n",
      "Iteration 862, loss = 0.42607557\n",
      "Iteration 863, loss = 0.42596255\n",
      "Iteration 864, loss = 0.42584971\n",
      "Iteration 865, loss = 0.42573702\n",
      "Iteration 866, loss = 0.42562448\n",
      "Iteration 867, loss = 0.42551207\n",
      "Iteration 868, loss = 0.42539981\n",
      "Iteration 869, loss = 0.42528769\n",
      "Iteration 870, loss = 0.42517571\n",
      "Iteration 871, loss = 0.42506388\n",
      "Iteration 872, loss = 0.42495218\n",
      "Iteration 873, loss = 0.42484063\n",
      "Iteration 874, loss = 0.42472920\n",
      "Iteration 875, loss = 0.42461789\n",
      "Iteration 876, loss = 0.42450672\n",
      "Iteration 877, loss = 0.42439568\n",
      "Iteration 878, loss = 0.42428479\n",
      "Iteration 879, loss = 0.42417403\n",
      "Iteration 880, loss = 0.42406340\n",
      "Iteration 881, loss = 0.42395291\n",
      "Iteration 882, loss = 0.42384255\n",
      "Iteration 883, loss = 0.42373233\n",
      "Iteration 884, loss = 0.42362225\n",
      "Iteration 885, loss = 0.42351235\n",
      "Iteration 886, loss = 0.42340263\n",
      "Iteration 887, loss = 0.42329306\n",
      "Iteration 888, loss = 0.42318362\n",
      "Iteration 889, loss = 0.42307430\n",
      "Iteration 890, loss = 0.42296511\n",
      "Iteration 891, loss = 0.42285605\n",
      "Iteration 892, loss = 0.42274712\n",
      "Iteration 893, loss = 0.42263833\n",
      "Iteration 894, loss = 0.42252966\n",
      "Iteration 895, loss = 0.42242110\n",
      "Iteration 896, loss = 0.42231267\n",
      "Iteration 897, loss = 0.42220437\n",
      "Iteration 898, loss = 0.42209622\n",
      "Iteration 899, loss = 0.42198823\n",
      "Iteration 900, loss = 0.42188037\n",
      "Iteration 901, loss = 0.42177261\n",
      "Iteration 902, loss = 0.42166497\n",
      "Iteration 903, loss = 0.42155746\n",
      "Iteration 904, loss = 0.42145006\n",
      "Iteration 905, loss = 0.42134277\n",
      "Iteration 906, loss = 0.42123561\n",
      "Iteration 907, loss = 0.42112857\n",
      "Iteration 908, loss = 0.42102167\n",
      "Iteration 909, loss = 0.42091489\n",
      "Iteration 910, loss = 0.42080824\n",
      "Iteration 911, loss = 0.42070172\n",
      "Iteration 912, loss = 0.42059532\n",
      "Iteration 913, loss = 0.42048904\n",
      "Iteration 914, loss = 0.42038289\n",
      "Iteration 915, loss = 0.42027685\n",
      "Iteration 916, loss = 0.42017095\n",
      "Iteration 917, loss = 0.42006516\n",
      "Iteration 918, loss = 0.41995950\n",
      "Iteration 919, loss = 0.41985396\n",
      "Iteration 920, loss = 0.41974854\n",
      "Iteration 921, loss = 0.41964325\n",
      "Iteration 922, loss = 0.41953807\n",
      "Iteration 923, loss = 0.41943302\n",
      "Iteration 924, loss = 0.41932808\n",
      "Iteration 925, loss = 0.41922326\n",
      "Iteration 926, loss = 0.41911856\n",
      "Iteration 927, loss = 0.41901399\n",
      "Iteration 928, loss = 0.41890953\n",
      "Iteration 929, loss = 0.41880520\n",
      "Iteration 930, loss = 0.41870098\n",
      "Iteration 931, loss = 0.41859688\n",
      "Iteration 932, loss = 0.41849290\n",
      "Iteration 933, loss = 0.41838904\n",
      "Iteration 934, loss = 0.41828527\n",
      "Iteration 935, loss = 0.41818161\n",
      "Iteration 936, loss = 0.41807806\n",
      "Iteration 937, loss = 0.41797463\n",
      "Iteration 938, loss = 0.41787132\n",
      "Iteration 939, loss = 0.41776812\n",
      "Iteration 940, loss = 0.41766505\n",
      "Iteration 941, loss = 0.41756210\n",
      "Iteration 942, loss = 0.41745926\n",
      "Iteration 943, loss = 0.41735654\n",
      "Iteration 944, loss = 0.41725394\n",
      "Iteration 945, loss = 0.41715146\n",
      "Iteration 946, loss = 0.41704906\n",
      "Iteration 947, loss = 0.41694680\n",
      "Iteration 948, loss = 0.41684465\n",
      "Iteration 949, loss = 0.41674261\n",
      "Iteration 950, loss = 0.41664069\n",
      "Iteration 951, loss = 0.41653889\n",
      "Iteration 952, loss = 0.41643720\n",
      "Iteration 953, loss = 0.41633562\n",
      "Iteration 954, loss = 0.41623416\n",
      "Iteration 955, loss = 0.41613281\n",
      "Iteration 956, loss = 0.41603159\n",
      "Iteration 957, loss = 0.41593047\n",
      "Iteration 958, loss = 0.41582947\n",
      "Iteration 959, loss = 0.41572857\n",
      "Iteration 960, loss = 0.41562779\n",
      "Iteration 961, loss = 0.41552712\n",
      "Iteration 962, loss = 0.41542656\n",
      "Iteration 963, loss = 0.41532608\n",
      "Iteration 964, loss = 0.41522571\n",
      "Iteration 965, loss = 0.41512545\n",
      "Iteration 966, loss = 0.41502530\n",
      "Iteration 967, loss = 0.41492525\n",
      "Iteration 968, loss = 0.41482532\n",
      "Iteration 969, loss = 0.41472549\n",
      "Iteration 970, loss = 0.41462576\n",
      "Iteration 971, loss = 0.41452613\n",
      "Iteration 972, loss = 0.41442661\n",
      "Iteration 973, loss = 0.41432723\n",
      "Iteration 974, loss = 0.41422797\n",
      "Iteration 975, loss = 0.41412881\n",
      "Iteration 976, loss = 0.41402976\n",
      "Iteration 977, loss = 0.41393083\n",
      "Iteration 978, loss = 0.41383200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.913333\n",
      "Training set loss: 0.413832\n",
      "training: inv-scaling with Nesterov's momentum\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.03169308\n",
      "Iteration 3, loss = 1.01185616\n",
      "Iteration 4, loss = 0.99403369\n",
      "Iteration 5, loss = 0.97785415\n",
      "Iteration 6, loss = 0.96313564\n",
      "Iteration 7, loss = 0.94963823\n",
      "Iteration 8, loss = 0.93724336\n",
      "Iteration 9, loss = 0.92579975\n",
      "Iteration 10, loss = 0.91517366\n",
      "Iteration 11, loss = 0.90523907\n",
      "Iteration 12, loss = 0.89589542\n",
      "Iteration 13, loss = 0.88706629\n",
      "Iteration 14, loss = 0.87868364\n",
      "Iteration 15, loss = 0.87071417\n",
      "Iteration 16, loss = 0.86311459\n",
      "Iteration 17, loss = 0.85587378\n",
      "Iteration 18, loss = 0.84896385\n",
      "Iteration 19, loss = 0.84237125\n",
      "Iteration 20, loss = 0.83607521\n",
      "Iteration 21, loss = 0.83007069\n",
      "Iteration 22, loss = 0.82434776\n",
      "Iteration 23, loss = 0.81889504\n",
      "Iteration 24, loss = 0.81369494\n",
      "Iteration 25, loss = 0.80871692\n",
      "Iteration 26, loss = 0.80396038\n",
      "Iteration 27, loss = 0.79941647\n",
      "Iteration 28, loss = 0.79506989\n",
      "Iteration 29, loss = 0.79090793\n",
      "Iteration 30, loss = 0.78692033\n",
      "Iteration 31, loss = 0.78309772\n",
      "Iteration 32, loss = 0.77942120\n",
      "Iteration 33, loss = 0.77588193\n",
      "Iteration 34, loss = 0.77247059\n",
      "Iteration 35, loss = 0.76917763\n",
      "Iteration 36, loss = 0.76599683\n",
      "Iteration 37, loss = 0.76292171\n",
      "Iteration 38, loss = 0.75994365\n",
      "Iteration 39, loss = 0.75706093\n",
      "Iteration 40, loss = 0.75427218\n",
      "Iteration 41, loss = 0.75156770\n",
      "Iteration 42, loss = 0.74894044\n",
      "Iteration 43, loss = 0.74638687\n",
      "Iteration 44, loss = 0.74390210\n",
      "Iteration 45, loss = 0.74148247\n",
      "Iteration 46, loss = 0.73912451\n",
      "Iteration 47, loss = 0.73682519\n",
      "Iteration 48, loss = 0.73458182\n",
      "Iteration 49, loss = 0.73239096\n",
      "Iteration 50, loss = 0.73024851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51, loss = 0.72815348\n",
      "Iteration 52, loss = 0.72610339\n",
      "Iteration 53, loss = 0.72409629\n",
      "Iteration 54, loss = 0.72212934\n",
      "Iteration 55, loss = 0.72020103\n",
      "Iteration 56, loss = 0.71830917\n",
      "Iteration 57, loss = 0.71645224\n",
      "Iteration 58, loss = 0.71462908\n",
      "Iteration 59, loss = 0.71283814\n",
      "Iteration 60, loss = 0.71107809\n",
      "Iteration 61, loss = 0.70934781\n",
      "Iteration 62, loss = 0.70764593\n",
      "Iteration 63, loss = 0.70597108\n",
      "Iteration 64, loss = 0.70432199\n",
      "Iteration 65, loss = 0.70269803\n",
      "Iteration 66, loss = 0.70109814\n",
      "Iteration 67, loss = 0.69952135\n",
      "Iteration 68, loss = 0.69796735\n",
      "Iteration 69, loss = 0.69643491\n",
      "Iteration 70, loss = 0.69492301\n",
      "Iteration 71, loss = 0.69343169\n",
      "Iteration 72, loss = 0.69195924\n",
      "Iteration 73, loss = 0.69050561\n",
      "Iteration 74, loss = 0.68907045\n",
      "Iteration 75, loss = 0.68765326\n",
      "Iteration 76, loss = 0.68625312\n",
      "Iteration 77, loss = 0.68486954\n",
      "Iteration 78, loss = 0.68350195\n",
      "Iteration 79, loss = 0.68215041\n",
      "Iteration 80, loss = 0.68081420\n",
      "Iteration 81, loss = 0.67949281\n",
      "Iteration 82, loss = 0.67818623\n",
      "Iteration 83, loss = 0.67689393\n",
      "Iteration 84, loss = 0.67561557\n",
      "Iteration 85, loss = 0.67435112\n",
      "Iteration 86, loss = 0.67309993\n",
      "Iteration 87, loss = 0.67186229\n",
      "Iteration 88, loss = 0.67063751\n",
      "Iteration 89, loss = 0.66942599\n",
      "Iteration 90, loss = 0.66822689\n",
      "Iteration 91, loss = 0.66703989\n",
      "Iteration 92, loss = 0.66586541\n",
      "Iteration 93, loss = 0.66470313\n",
      "Iteration 94, loss = 0.66355234\n",
      "Iteration 95, loss = 0.66241314\n",
      "Iteration 96, loss = 0.66128503\n",
      "Iteration 97, loss = 0.66016750\n",
      "Iteration 98, loss = 0.65906015\n",
      "Iteration 99, loss = 0.65796370\n",
      "Iteration 100, loss = 0.65687732\n",
      "Iteration 101, loss = 0.65580099\n",
      "Iteration 102, loss = 0.65473452\n",
      "Iteration 103, loss = 0.65367757\n",
      "Iteration 104, loss = 0.65263080\n",
      "Iteration 105, loss = 0.65159386\n",
      "Iteration 106, loss = 0.65056615\n",
      "Iteration 107, loss = 0.64954740\n",
      "Iteration 108, loss = 0.64853737\n",
      "Iteration 109, loss = 0.64753581\n",
      "Iteration 110, loss = 0.64654280\n",
      "Iteration 111, loss = 0.64555807\n",
      "Iteration 112, loss = 0.64458175\n",
      "Iteration 113, loss = 0.64361379\n",
      "Iteration 114, loss = 0.64265373\n",
      "Iteration 115, loss = 0.64170161\n",
      "Iteration 116, loss = 0.64075715\n",
      "Iteration 117, loss = 0.63982022\n",
      "Iteration 118, loss = 0.63889097\n",
      "Iteration 119, loss = 0.63796897\n",
      "Iteration 120, loss = 0.63705423\n",
      "Iteration 121, loss = 0.63614685\n",
      "Iteration 122, loss = 0.63524663\n",
      "Iteration 123, loss = 0.63435336\n",
      "Iteration 124, loss = 0.63346709\n",
      "Iteration 125, loss = 0.63258759\n",
      "Iteration 126, loss = 0.63171494\n",
      "Iteration 127, loss = 0.63084876\n",
      "Iteration 128, loss = 0.62998899\n",
      "Iteration 129, loss = 0.62913566\n",
      "Iteration 130, loss = 0.62828858\n",
      "Iteration 131, loss = 0.62744752\n",
      "Iteration 132, loss = 0.62661256\n",
      "Iteration 133, loss = 0.62578354\n",
      "Iteration 134, loss = 0.62496041\n",
      "Iteration 135, loss = 0.62414296\n",
      "Iteration 136, loss = 0.62333126\n",
      "Iteration 137, loss = 0.62252527\n",
      "Iteration 138, loss = 0.62172470\n",
      "Iteration 139, loss = 0.62092958\n",
      "Iteration 140, loss = 0.62013987\n",
      "Iteration 141, loss = 0.61935546\n",
      "Iteration 142, loss = 0.61857636\n",
      "Iteration 143, loss = 0.61780249\n",
      "Iteration 144, loss = 0.61703370\n",
      "Iteration 145, loss = 0.61627001\n",
      "Iteration 146, loss = 0.61551134\n",
      "Iteration 147, loss = 0.61475762\n",
      "Iteration 148, loss = 0.61400874\n",
      "Iteration 149, loss = 0.61326460\n",
      "Iteration 150, loss = 0.61252516\n",
      "Iteration 151, loss = 0.61179035\n",
      "Iteration 152, loss = 0.61106014\n",
      "Iteration 153, loss = 0.61033450\n",
      "Iteration 154, loss = 0.60961337\n",
      "Iteration 155, loss = 0.60889669\n",
      "Iteration 156, loss = 0.60818442\n",
      "Iteration 157, loss = 0.60747655\n",
      "Iteration 158, loss = 0.60677299\n",
      "Iteration 159, loss = 0.60607373\n",
      "Iteration 160, loss = 0.60537873\n",
      "Iteration 161, loss = 0.60468793\n",
      "Iteration 162, loss = 0.60400131\n",
      "Iteration 163, loss = 0.60331880\n",
      "Iteration 164, loss = 0.60264040\n",
      "Iteration 165, loss = 0.60196601\n",
      "Iteration 166, loss = 0.60129554\n",
      "Iteration 167, loss = 0.60062894\n",
      "Iteration 168, loss = 0.59996619\n",
      "Iteration 169, loss = 0.59930724\n",
      "Iteration 170, loss = 0.59865210\n",
      "Iteration 171, loss = 0.59800071\n",
      "Iteration 172, loss = 0.59735303\n",
      "Iteration 173, loss = 0.59670902\n",
      "Iteration 174, loss = 0.59606864\n",
      "Iteration 175, loss = 0.59543183\n",
      "Iteration 176, loss = 0.59479858\n",
      "Iteration 177, loss = 0.59416883\n",
      "Iteration 178, loss = 0.59354259\n",
      "Iteration 179, loss = 0.59291982\n",
      "Iteration 180, loss = 0.59230050\n",
      "Iteration 181, loss = 0.59168455\n",
      "Iteration 182, loss = 0.59107196\n",
      "Iteration 183, loss = 0.59046268\n",
      "Iteration 184, loss = 0.58985666\n",
      "Iteration 185, loss = 0.58925387\n",
      "Iteration 186, loss = 0.58865428\n",
      "Iteration 187, loss = 0.58805786\n",
      "Iteration 188, loss = 0.58746458\n",
      "Iteration 189, loss = 0.58687440\n",
      "Iteration 190, loss = 0.58628730\n",
      "Iteration 191, loss = 0.58570331\n",
      "Iteration 192, loss = 0.58512236\n",
      "Iteration 193, loss = 0.58454443\n",
      "Iteration 194, loss = 0.58396953\n",
      "Iteration 195, loss = 0.58339758\n",
      "Iteration 196, loss = 0.58282858\n",
      "Iteration 197, loss = 0.58226260\n",
      "Iteration 198, loss = 0.58169951\n",
      "Iteration 199, loss = 0.58113930\n",
      "Iteration 200, loss = 0.58058194\n",
      "Iteration 201, loss = 0.58002742\n",
      "Iteration 202, loss = 0.57947569\n",
      "Iteration 203, loss = 0.57892679\n",
      "Iteration 204, loss = 0.57838063\n",
      "Iteration 205, loss = 0.57783722\n",
      "Iteration 206, loss = 0.57729648\n",
      "Iteration 207, loss = 0.57675837\n",
      "Iteration 208, loss = 0.57622287\n",
      "Iteration 209, loss = 0.57569001\n",
      "Iteration 210, loss = 0.57515974\n",
      "Iteration 211, loss = 0.57463203\n",
      "Iteration 212, loss = 0.57410684\n",
      "Iteration 213, loss = 0.57358416\n",
      "Iteration 214, loss = 0.57306394\n",
      "Iteration 215, loss = 0.57254618\n",
      "Iteration 216, loss = 0.57203088\n",
      "Iteration 217, loss = 0.57151800\n",
      "Iteration 218, loss = 0.57100753\n",
      "Iteration 219, loss = 0.57049944\n",
      "Iteration 220, loss = 0.56999370\n",
      "Iteration 221, loss = 0.56949030\n",
      "Iteration 222, loss = 0.56898923\n",
      "Iteration 223, loss = 0.56849048\n",
      "Iteration 224, loss = 0.56799399\n",
      "Iteration 225, loss = 0.56749975\n",
      "Iteration 226, loss = 0.56700775\n",
      "Iteration 227, loss = 0.56651798\n",
      "Iteration 228, loss = 0.56603042\n",
      "Iteration 229, loss = 0.56554503\n",
      "Iteration 230, loss = 0.56506181\n",
      "Iteration 231, loss = 0.56458079\n",
      "Iteration 232, loss = 0.56410192\n",
      "Iteration 233, loss = 0.56362519\n",
      "Iteration 234, loss = 0.56315056\n",
      "Iteration 235, loss = 0.56267801\n",
      "Iteration 236, loss = 0.56220753\n",
      "Iteration 237, loss = 0.56173911\n",
      "Iteration 238, loss = 0.56127274\n",
      "Iteration 239, loss = 0.56080841\n",
      "Iteration 240, loss = 0.56034608\n",
      "Iteration 241, loss = 0.55988573\n",
      "Iteration 242, loss = 0.55942735\n",
      "Iteration 243, loss = 0.55897093\n",
      "Iteration 244, loss = 0.55851645\n",
      "Iteration 245, loss = 0.55806391\n",
      "Iteration 246, loss = 0.55761327\n",
      "Iteration 247, loss = 0.55716455\n",
      "Iteration 248, loss = 0.55671773\n",
      "Iteration 249, loss = 0.55627282\n",
      "Iteration 250, loss = 0.55582981\n",
      "Iteration 251, loss = 0.55538867\n",
      "Iteration 252, loss = 0.55494941\n",
      "Iteration 253, loss = 0.55451198\n",
      "Iteration 254, loss = 0.55407635\n",
      "Iteration 255, loss = 0.55364252\n",
      "Iteration 256, loss = 0.55321048\n",
      "Iteration 257, loss = 0.55278022\n",
      "Iteration 258, loss = 0.55235173\n",
      "Iteration 259, loss = 0.55192498\n",
      "Iteration 260, loss = 0.55149998\n",
      "Iteration 261, loss = 0.55107670\n",
      "Iteration 262, loss = 0.55065511\n",
      "Iteration 263, loss = 0.55023520\n",
      "Iteration 264, loss = 0.54981691\n",
      "Iteration 265, loss = 0.54940029\n",
      "Iteration 266, loss = 0.54898535\n",
      "Iteration 267, loss = 0.54857207\n",
      "Iteration 268, loss = 0.54816042\n",
      "Iteration 269, loss = 0.54775038\n",
      "Iteration 270, loss = 0.54734195\n",
      "Iteration 271, loss = 0.54693513\n",
      "Iteration 272, loss = 0.54652990\n",
      "Iteration 273, loss = 0.54612624\n",
      "Iteration 274, loss = 0.54572414\n",
      "Iteration 275, loss = 0.54532359\n",
      "Iteration 276, loss = 0.54492459\n",
      "Iteration 277, loss = 0.54452714\n",
      "Iteration 278, loss = 0.54413122\n",
      "Iteration 279, loss = 0.54373682\n",
      "Iteration 280, loss = 0.54334392\n",
      "Iteration 281, loss = 0.54295250\n",
      "Iteration 282, loss = 0.54256256\n",
      "Iteration 283, loss = 0.54217409\n",
      "Iteration 284, loss = 0.54178708\n",
      "Iteration 285, loss = 0.54140153\n",
      "Iteration 286, loss = 0.54101741\n",
      "Iteration 287, loss = 0.54063469\n",
      "Iteration 288, loss = 0.54025340\n",
      "Iteration 289, loss = 0.53987351\n",
      "Iteration 290, loss = 0.53949502\n",
      "Iteration 291, loss = 0.53911794\n",
      "Iteration 292, loss = 0.53874227\n",
      "Iteration 293, loss = 0.53836801\n",
      "Iteration 294, loss = 0.53799511\n",
      "Iteration 295, loss = 0.53762357\n",
      "Iteration 296, loss = 0.53725340\n",
      "Iteration 297, loss = 0.53688456\n",
      "Iteration 298, loss = 0.53651707\n",
      "Iteration 299, loss = 0.53615090\n",
      "Iteration 300, loss = 0.53578604\n",
      "Iteration 301, loss = 0.53542250\n",
      "Iteration 302, loss = 0.53506027\n",
      "Iteration 303, loss = 0.53469932\n",
      "Iteration 304, loss = 0.53433964\n",
      "Iteration 305, loss = 0.53398125\n",
      "Iteration 306, loss = 0.53362411\n",
      "Iteration 307, loss = 0.53326825\n",
      "Iteration 308, loss = 0.53291364\n",
      "Iteration 309, loss = 0.53256028\n",
      "Iteration 310, loss = 0.53220816\n",
      "Iteration 311, loss = 0.53185723\n",
      "Iteration 312, loss = 0.53150749\n",
      "Iteration 313, loss = 0.53115896\n",
      "Iteration 314, loss = 0.53081164\n",
      "Iteration 315, loss = 0.53046553\n",
      "Iteration 316, loss = 0.53012067\n",
      "Iteration 317, loss = 0.52977700\n",
      "Iteration 318, loss = 0.52943451\n",
      "Iteration 319, loss = 0.52909319\n",
      "Iteration 320, loss = 0.52875303\n",
      "Iteration 321, loss = 0.52841402\n",
      "Iteration 322, loss = 0.52807615\n",
      "Iteration 323, loss = 0.52773941\n",
      "Iteration 324, loss = 0.52740381\n",
      "Iteration 325, loss = 0.52706932\n",
      "Iteration 326, loss = 0.52673595\n",
      "Iteration 327, loss = 0.52640369\n",
      "Iteration 328, loss = 0.52607253\n",
      "Iteration 329, loss = 0.52574247\n",
      "Iteration 330, loss = 0.52541350\n",
      "Iteration 331, loss = 0.52508562\n",
      "Iteration 332, loss = 0.52475881\n",
      "Iteration 333, loss = 0.52443306\n",
      "Iteration 334, loss = 0.52410839\n",
      "Iteration 335, loss = 0.52378479\n",
      "Iteration 336, loss = 0.52346224\n",
      "Iteration 337, loss = 0.52314075\n",
      "Iteration 338, loss = 0.52282031\n",
      "Iteration 339, loss = 0.52250089\n",
      "Iteration 340, loss = 0.52218253\n",
      "Iteration 341, loss = 0.52186521\n",
      "Iteration 342, loss = 0.52154890\n",
      "Iteration 343, loss = 0.52123360\n",
      "Iteration 344, loss = 0.52091930\n",
      "Iteration 345, loss = 0.52060600\n",
      "Iteration 346, loss = 0.52029370\n",
      "Iteration 347, loss = 0.51998237\n",
      "Iteration 348, loss = 0.51967204\n",
      "Iteration 349, loss = 0.51936270\n",
      "Iteration 350, loss = 0.51905434\n",
      "Iteration 351, loss = 0.51874696\n",
      "Iteration 352, loss = 0.51844053\n",
      "Iteration 353, loss = 0.51813504\n",
      "Iteration 354, loss = 0.51783049\n",
      "Iteration 355, loss = 0.51752691\n",
      "Iteration 356, loss = 0.51722427\n",
      "Iteration 357, loss = 0.51692256\n",
      "Iteration 358, loss = 0.51662177\n",
      "Iteration 359, loss = 0.51632191\n",
      "Iteration 360, loss = 0.51602294\n",
      "Iteration 361, loss = 0.51572487\n",
      "Iteration 362, loss = 0.51542769\n",
      "Iteration 363, loss = 0.51513141\n",
      "Iteration 364, loss = 0.51483601\n",
      "Iteration 365, loss = 0.51454151\n",
      "Iteration 366, loss = 0.51424785\n",
      "Iteration 367, loss = 0.51395507\n",
      "Iteration 368, loss = 0.51366317\n",
      "Iteration 369, loss = 0.51337213\n",
      "Iteration 370, loss = 0.51308197\n",
      "Iteration 371, loss = 0.51279267\n",
      "Iteration 372, loss = 0.51250423\n",
      "Iteration 373, loss = 0.51221664\n",
      "Iteration 374, loss = 0.51192990\n",
      "Iteration 375, loss = 0.51164401\n",
      "Iteration 376, loss = 0.51135896\n",
      "Iteration 377, loss = 0.51107473\n",
      "Iteration 378, loss = 0.51079134\n",
      "Iteration 379, loss = 0.51050877\n",
      "Iteration 380, loss = 0.51022702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 381, loss = 0.50994611\n",
      "Iteration 382, loss = 0.50966604\n",
      "Iteration 383, loss = 0.50938677\n",
      "Iteration 384, loss = 0.50910831\n",
      "Iteration 385, loss = 0.50883064\n",
      "Iteration 386, loss = 0.50855377\n",
      "Iteration 387, loss = 0.50827769\n",
      "Iteration 388, loss = 0.50800240\n",
      "Iteration 389, loss = 0.50772789\n",
      "Iteration 390, loss = 0.50745416\n",
      "Iteration 391, loss = 0.50718125\n",
      "Iteration 392, loss = 0.50690913\n",
      "Iteration 393, loss = 0.50663778\n",
      "Iteration 394, loss = 0.50636719\n",
      "Iteration 395, loss = 0.50609738\n",
      "Iteration 396, loss = 0.50582831\n",
      "Iteration 397, loss = 0.50556000\n",
      "Iteration 398, loss = 0.50529244\n",
      "Iteration 399, loss = 0.50502562\n",
      "Iteration 400, loss = 0.50475956\n",
      "Iteration 401, loss = 0.50449420\n",
      "Iteration 402, loss = 0.50422957\n",
      "Iteration 403, loss = 0.50396567\n",
      "Iteration 404, loss = 0.50370249\n",
      "Iteration 405, loss = 0.50344003\n",
      "Iteration 406, loss = 0.50317829\n",
      "Iteration 407, loss = 0.50291727\n",
      "Iteration 408, loss = 0.50265696\n",
      "Iteration 409, loss = 0.50239736\n",
      "Iteration 410, loss = 0.50213844\n",
      "Iteration 411, loss = 0.50188026\n",
      "Iteration 412, loss = 0.50162279\n",
      "Iteration 413, loss = 0.50136603\n",
      "Iteration 414, loss = 0.50110996\n",
      "Iteration 415, loss = 0.50085459\n",
      "Iteration 416, loss = 0.50059990\n",
      "Iteration 417, loss = 0.50034590\n",
      "Iteration 418, loss = 0.50009258\n",
      "Iteration 419, loss = 0.49983995\n",
      "Iteration 420, loss = 0.49958799\n",
      "Iteration 421, loss = 0.49933674\n",
      "Iteration 422, loss = 0.49908616\n",
      "Iteration 423, loss = 0.49883628\n",
      "Iteration 424, loss = 0.49858706\n",
      "Iteration 425, loss = 0.49833851\n",
      "Iteration 426, loss = 0.49809061\n",
      "Iteration 427, loss = 0.49784337\n",
      "Iteration 428, loss = 0.49759677\n",
      "Iteration 429, loss = 0.49735079\n",
      "Iteration 430, loss = 0.49710545\n",
      "Iteration 431, loss = 0.49686074\n",
      "Iteration 432, loss = 0.49661666\n",
      "Iteration 433, loss = 0.49637321\n",
      "Iteration 434, loss = 0.49613038\n",
      "Iteration 435, loss = 0.49588815\n",
      "Iteration 436, loss = 0.49564653\n",
      "Iteration 437, loss = 0.49540552\n",
      "Iteration 438, loss = 0.49516512\n",
      "Iteration 439, loss = 0.49492533\n",
      "Iteration 440, loss = 0.49468615\n",
      "Iteration 441, loss = 0.49444756\n",
      "Iteration 442, loss = 0.49420957\n",
      "Iteration 443, loss = 0.49397219\n",
      "Iteration 444, loss = 0.49373542\n",
      "Iteration 445, loss = 0.49349923\n",
      "Iteration 446, loss = 0.49326360\n",
      "Iteration 447, loss = 0.49302856\n",
      "Iteration 448, loss = 0.49279410\n",
      "Iteration 449, loss = 0.49256023\n",
      "Iteration 450, loss = 0.49232695\n",
      "Iteration 451, loss = 0.49209424\n",
      "Iteration 452, loss = 0.49186210\n",
      "Iteration 453, loss = 0.49163054\n",
      "Iteration 454, loss = 0.49139954\n",
      "Iteration 455, loss = 0.49116912\n",
      "Iteration 456, loss = 0.49093926\n",
      "Iteration 457, loss = 0.49070996\n",
      "Iteration 458, loss = 0.49048123\n",
      "Iteration 459, loss = 0.49025306\n",
      "Iteration 460, loss = 0.49002544\n",
      "Iteration 461, loss = 0.48979837\n",
      "Iteration 462, loss = 0.48957185\n",
      "Iteration 463, loss = 0.48934587\n",
      "Iteration 464, loss = 0.48912043\n",
      "Iteration 465, loss = 0.48889554\n",
      "Iteration 466, loss = 0.48867119\n",
      "Iteration 467, loss = 0.48844737\n",
      "Iteration 468, loss = 0.48822409\n",
      "Iteration 469, loss = 0.48800134\n",
      "Iteration 470, loss = 0.48777912\n",
      "Iteration 471, loss = 0.48755742\n",
      "Iteration 472, loss = 0.48733623\n",
      "Iteration 473, loss = 0.48711556\n",
      "Iteration 474, loss = 0.48689541\n",
      "Iteration 475, loss = 0.48667577\n",
      "Iteration 476, loss = 0.48645664\n",
      "Iteration 477, loss = 0.48623803\n",
      "Iteration 478, loss = 0.48601992\n",
      "Iteration 479, loss = 0.48580232\n",
      "Iteration 480, loss = 0.48558523\n",
      "Iteration 481, loss = 0.48536867\n",
      "Iteration 482, loss = 0.48515263\n",
      "Iteration 483, loss = 0.48493708\n",
      "Iteration 484, loss = 0.48472203\n",
      "Iteration 485, loss = 0.48450744\n",
      "Iteration 486, loss = 0.48429332\n",
      "Iteration 487, loss = 0.48407969\n",
      "Iteration 488, loss = 0.48386658\n",
      "Iteration 489, loss = 0.48365396\n",
      "Iteration 490, loss = 0.48344182\n",
      "Iteration 491, loss = 0.48323016\n",
      "Iteration 492, loss = 0.48301894\n",
      "Iteration 493, loss = 0.48280820\n",
      "Iteration 494, loss = 0.48259794\n",
      "Iteration 495, loss = 0.48238814\n",
      "Iteration 496, loss = 0.48217881\n",
      "Iteration 497, loss = 0.48196999\n",
      "Iteration 498, loss = 0.48176164\n",
      "Iteration 499, loss = 0.48155376\n",
      "Iteration 500, loss = 0.48134634\n",
      "Iteration 501, loss = 0.48113939\n",
      "Iteration 502, loss = 0.48093289\n",
      "Iteration 503, loss = 0.48072686\n",
      "Iteration 504, loss = 0.48052128\n",
      "Iteration 505, loss = 0.48031615\n",
      "Iteration 506, loss = 0.48011144\n",
      "Iteration 507, loss = 0.47990719\n",
      "Iteration 508, loss = 0.47970338\n",
      "Iteration 509, loss = 0.47950002\n",
      "Iteration 510, loss = 0.47929711\n",
      "Iteration 511, loss = 0.47909465\n",
      "Iteration 512, loss = 0.47889264\n",
      "Iteration 513, loss = 0.47869107\n",
      "Iteration 514, loss = 0.47848994\n",
      "Iteration 515, loss = 0.47828919\n",
      "Iteration 516, loss = 0.47808887\n",
      "Iteration 517, loss = 0.47788898\n",
      "Iteration 518, loss = 0.47768951\n",
      "Iteration 519, loss = 0.47749047\n",
      "Iteration 520, loss = 0.47729185\n",
      "Iteration 521, loss = 0.47709361\n",
      "Iteration 522, loss = 0.47689577\n",
      "Iteration 523, loss = 0.47669832\n",
      "Iteration 524, loss = 0.47650127\n",
      "Iteration 525, loss = 0.47630459\n",
      "Iteration 526, loss = 0.47610830\n",
      "Iteration 527, loss = 0.47591240\n",
      "Iteration 528, loss = 0.47571691\n",
      "Iteration 529, loss = 0.47552179\n",
      "Iteration 530, loss = 0.47532704\n",
      "Iteration 531, loss = 0.47513269\n",
      "Iteration 532, loss = 0.47493873\n",
      "Iteration 533, loss = 0.47474516\n",
      "Iteration 534, loss = 0.47455199\n",
      "Iteration 535, loss = 0.47435918\n",
      "Iteration 536, loss = 0.47416676\n",
      "Iteration 537, loss = 0.47397483\n",
      "Iteration 538, loss = 0.47378329\n",
      "Iteration 539, loss = 0.47359216\n",
      "Iteration 540, loss = 0.47340143\n",
      "Iteration 541, loss = 0.47321110\n",
      "Iteration 542, loss = 0.47302115\n",
      "Iteration 543, loss = 0.47283160\n",
      "Iteration 544, loss = 0.47264242\n",
      "Iteration 545, loss = 0.47245361\n",
      "Iteration 546, loss = 0.47226519\n",
      "Iteration 547, loss = 0.47207716\n",
      "Iteration 548, loss = 0.47188957\n",
      "Iteration 549, loss = 0.47170241\n",
      "Iteration 550, loss = 0.47151564\n",
      "Iteration 551, loss = 0.47132925\n",
      "Iteration 552, loss = 0.47114325\n",
      "Iteration 553, loss = 0.47095764\n",
      "Iteration 554, loss = 0.47077241\n",
      "Iteration 555, loss = 0.47058756\n",
      "Iteration 556, loss = 0.47040309\n",
      "Iteration 557, loss = 0.47021904\n",
      "Iteration 558, loss = 0.47003535\n",
      "Iteration 559, loss = 0.46985200\n",
      "Iteration 560, loss = 0.46966901\n",
      "Iteration 561, loss = 0.46948639\n",
      "Iteration 562, loss = 0.46930409\n",
      "Iteration 563, loss = 0.46912215\n",
      "Iteration 564, loss = 0.46894056\n",
      "Iteration 565, loss = 0.46875934\n",
      "Iteration 566, loss = 0.46857846\n",
      "Iteration 567, loss = 0.46839794\n",
      "Iteration 568, loss = 0.46821778\n",
      "Iteration 569, loss = 0.46803797\n",
      "Iteration 570, loss = 0.46785851\n",
      "Iteration 571, loss = 0.46767938\n",
      "Iteration 572, loss = 0.46750057\n",
      "Iteration 573, loss = 0.46732218\n",
      "Iteration 574, loss = 0.46714415\n",
      "Iteration 575, loss = 0.46696647\n",
      "Iteration 576, loss = 0.46678915\n",
      "Iteration 577, loss = 0.46661214\n",
      "Iteration 578, loss = 0.46643546\n",
      "Iteration 579, loss = 0.46625912\n",
      "Iteration 580, loss = 0.46608312\n",
      "Iteration 581, loss = 0.46590748\n",
      "Iteration 582, loss = 0.46573219\n",
      "Iteration 583, loss = 0.46555724\n",
      "Iteration 584, loss = 0.46538265\n",
      "Iteration 585, loss = 0.46520841\n",
      "Iteration 586, loss = 0.46503450\n",
      "Iteration 587, loss = 0.46486092\n",
      "Iteration 588, loss = 0.46468769\n",
      "Iteration 589, loss = 0.46451478\n",
      "Iteration 590, loss = 0.46434217\n",
      "Iteration 591, loss = 0.46416996\n",
      "Iteration 592, loss = 0.46399808\n",
      "Iteration 593, loss = 0.46382653\n",
      "Iteration 594, loss = 0.46365527\n",
      "Iteration 595, loss = 0.46348432\n",
      "Iteration 596, loss = 0.46331370\n",
      "Iteration 597, loss = 0.46314340\n",
      "Iteration 598, loss = 0.46297342\n",
      "Iteration 599, loss = 0.46280376\n",
      "Iteration 600, loss = 0.46263442\n",
      "Iteration 601, loss = 0.46246540\n",
      "Iteration 602, loss = 0.46229669\n",
      "Iteration 603, loss = 0.46212829\n",
      "Iteration 604, loss = 0.46196017\n",
      "Iteration 605, loss = 0.46179235\n",
      "Iteration 606, loss = 0.46162484\n",
      "Iteration 607, loss = 0.46145765\n",
      "Iteration 608, loss = 0.46129077\n",
      "Iteration 609, loss = 0.46112419\n",
      "Iteration 610, loss = 0.46095790\n",
      "Iteration 611, loss = 0.46079187\n",
      "Iteration 612, loss = 0.46062614\n",
      "Iteration 613, loss = 0.46046070\n",
      "Iteration 614, loss = 0.46029555\n",
      "Iteration 615, loss = 0.46013075\n",
      "Iteration 616, loss = 0.45996627\n",
      "Iteration 617, loss = 0.45980208\n",
      "Iteration 618, loss = 0.45963820\n",
      "Iteration 619, loss = 0.45947462\n",
      "Iteration 620, loss = 0.45931133\n",
      "Iteration 621, loss = 0.45914833\n",
      "Iteration 622, loss = 0.45898563\n",
      "Iteration 623, loss = 0.45882329\n",
      "Iteration 624, loss = 0.45866125\n",
      "Iteration 625, loss = 0.45849950\n",
      "Iteration 626, loss = 0.45833804\n",
      "Iteration 627, loss = 0.45817687\n",
      "Iteration 628, loss = 0.45801599\n",
      "Iteration 629, loss = 0.45785541\n",
      "Iteration 630, loss = 0.45769512\n",
      "Iteration 631, loss = 0.45753511\n",
      "Iteration 632, loss = 0.45737538\n",
      "Iteration 633, loss = 0.45721592\n",
      "Iteration 634, loss = 0.45705672\n",
      "Iteration 635, loss = 0.45689783\n",
      "Iteration 636, loss = 0.45673926\n",
      "Iteration 637, loss = 0.45658096\n",
      "Iteration 638, loss = 0.45642295\n",
      "Iteration 639, loss = 0.45626519\n",
      "Iteration 640, loss = 0.45610768\n",
      "Iteration 641, loss = 0.45595044\n",
      "Iteration 642, loss = 0.45579348\n",
      "Iteration 643, loss = 0.45563678\n",
      "Iteration 644, loss = 0.45548037\n",
      "Iteration 645, loss = 0.45532423\n",
      "Iteration 646, loss = 0.45516836\n",
      "Iteration 647, loss = 0.45501276\n",
      "Iteration 648, loss = 0.45485741\n",
      "Iteration 649, loss = 0.45470232\n",
      "Iteration 650, loss = 0.45454748\n",
      "Iteration 651, loss = 0.45439291\n",
      "Iteration 652, loss = 0.45423859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 653, loss = 0.45408454\n",
      "Iteration 654, loss = 0.45393074\n",
      "Iteration 655, loss = 0.45377720\n",
      "Iteration 656, loss = 0.45362392\n",
      "Iteration 657, loss = 0.45347090\n",
      "Iteration 658, loss = 0.45331814\n",
      "Iteration 659, loss = 0.45316564\n",
      "Iteration 660, loss = 0.45301338\n",
      "Iteration 661, loss = 0.45286137\n",
      "Iteration 662, loss = 0.45270961\n",
      "Iteration 663, loss = 0.45255810\n",
      "Iteration 664, loss = 0.45240681\n",
      "Iteration 665, loss = 0.45225576\n",
      "Iteration 666, loss = 0.45210496\n",
      "Iteration 667, loss = 0.45195440\n",
      "Iteration 668, loss = 0.45180409\n",
      "Iteration 669, loss = 0.45165404\n",
      "Iteration 670, loss = 0.45150425\n",
      "Iteration 671, loss = 0.45135475\n",
      "Iteration 672, loss = 0.45120550\n",
      "Iteration 673, loss = 0.45105649\n",
      "Iteration 674, loss = 0.45090773\n",
      "Iteration 675, loss = 0.45075921\n",
      "Iteration 676, loss = 0.45061094\n",
      "Iteration 677, loss = 0.45046291\n",
      "Iteration 678, loss = 0.45031511\n",
      "Iteration 679, loss = 0.45016752\n",
      "Iteration 680, loss = 0.45002016\n",
      "Iteration 681, loss = 0.44987304\n",
      "Iteration 682, loss = 0.44972617\n",
      "Iteration 683, loss = 0.44957958\n",
      "Iteration 684, loss = 0.44943323\n",
      "Iteration 685, loss = 0.44928715\n",
      "Iteration 686, loss = 0.44914133\n",
      "Iteration 687, loss = 0.44899575\n",
      "Iteration 688, loss = 0.44885041\n",
      "Iteration 689, loss = 0.44870530\n",
      "Iteration 690, loss = 0.44856044\n",
      "Iteration 691, loss = 0.44841580\n",
      "Iteration 692, loss = 0.44827139\n",
      "Iteration 693, loss = 0.44812721\n",
      "Iteration 694, loss = 0.44798327\n",
      "Iteration 695, loss = 0.44783955\n",
      "Iteration 696, loss = 0.44769606\n",
      "Iteration 697, loss = 0.44755280\n",
      "Iteration 698, loss = 0.44740978\n",
      "Iteration 699, loss = 0.44726699\n",
      "Iteration 700, loss = 0.44712443\n",
      "Iteration 701, loss = 0.44698210\n",
      "Iteration 702, loss = 0.44683999\n",
      "Iteration 703, loss = 0.44669811\n",
      "Iteration 704, loss = 0.44655650\n",
      "Iteration 705, loss = 0.44641513\n",
      "Iteration 706, loss = 0.44627399\n",
      "Iteration 707, loss = 0.44613308\n",
      "Iteration 708, loss = 0.44599239\n",
      "Iteration 709, loss = 0.44585193\n",
      "Iteration 710, loss = 0.44571168\n",
      "Iteration 711, loss = 0.44557166\n",
      "Iteration 712, loss = 0.44543185\n",
      "Iteration 713, loss = 0.44529226\n",
      "Iteration 714, loss = 0.44515289\n",
      "Iteration 715, loss = 0.44501374\n",
      "Iteration 716, loss = 0.44487478\n",
      "Iteration 717, loss = 0.44473603\n",
      "Iteration 718, loss = 0.44459748\n",
      "Iteration 719, loss = 0.44445915\n",
      "Iteration 720, loss = 0.44432102\n",
      "Iteration 721, loss = 0.44418311\n",
      "Iteration 722, loss = 0.44404540\n",
      "Iteration 723, loss = 0.44390790\n",
      "Iteration 724, loss = 0.44377061\n",
      "Iteration 725, loss = 0.44363353\n",
      "Iteration 726, loss = 0.44349666\n",
      "Iteration 727, loss = 0.44335999\n",
      "Iteration 728, loss = 0.44322353\n",
      "Iteration 729, loss = 0.44308727\n",
      "Iteration 730, loss = 0.44295121\n",
      "Iteration 731, loss = 0.44281533\n",
      "Iteration 732, loss = 0.44267966\n",
      "Iteration 733, loss = 0.44254419\n",
      "Iteration 734, loss = 0.44240893\n",
      "Iteration 735, loss = 0.44227386\n",
      "Iteration 736, loss = 0.44213898\n",
      "Iteration 737, loss = 0.44200431\n",
      "Iteration 738, loss = 0.44186984\n",
      "Iteration 739, loss = 0.44173557\n",
      "Iteration 740, loss = 0.44160149\n",
      "Iteration 741, loss = 0.44146763\n",
      "Iteration 742, loss = 0.44133396\n",
      "Iteration 743, loss = 0.44120050\n",
      "Iteration 744, loss = 0.44106723\n",
      "Iteration 745, loss = 0.44093420\n",
      "Iteration 746, loss = 0.44080137\n",
      "Iteration 747, loss = 0.44066874\n",
      "Iteration 748, loss = 0.44053630\n",
      "Iteration 749, loss = 0.44040406\n",
      "Iteration 750, loss = 0.44027201\n",
      "Iteration 751, loss = 0.44014016\n",
      "Iteration 752, loss = 0.44000850\n",
      "Iteration 753, loss = 0.43987701\n",
      "Iteration 754, loss = 0.43974569\n",
      "Iteration 755, loss = 0.43961454\n",
      "Iteration 756, loss = 0.43948357\n",
      "Iteration 757, loss = 0.43935279\n",
      "Iteration 758, loss = 0.43922220\n",
      "Iteration 759, loss = 0.43909180\n",
      "Iteration 760, loss = 0.43896161\n",
      "Iteration 761, loss = 0.43883159\n",
      "Iteration 762, loss = 0.43870177\n",
      "Iteration 763, loss = 0.43857214\n",
      "Iteration 764, loss = 0.43844271\n",
      "Iteration 765, loss = 0.43831344\n",
      "Iteration 766, loss = 0.43818436\n",
      "Iteration 767, loss = 0.43805545\n",
      "Iteration 768, loss = 0.43792673\n",
      "Iteration 769, loss = 0.43779820\n",
      "Iteration 770, loss = 0.43766984\n",
      "Iteration 771, loss = 0.43754166\n",
      "Iteration 772, loss = 0.43741368\n",
      "Iteration 773, loss = 0.43728587\n",
      "Iteration 774, loss = 0.43715824\n",
      "Iteration 775, loss = 0.43703079\n",
      "Iteration 776, loss = 0.43690352\n",
      "Iteration 777, loss = 0.43677642\n",
      "Iteration 778, loss = 0.43664951\n",
      "Iteration 779, loss = 0.43652277\n",
      "Iteration 780, loss = 0.43639621\n",
      "Iteration 781, loss = 0.43626983\n",
      "Iteration 782, loss = 0.43614363\n",
      "Iteration 783, loss = 0.43601760\n",
      "Iteration 784, loss = 0.43589173\n",
      "Iteration 785, loss = 0.43576601\n",
      "Iteration 786, loss = 0.43564046\n",
      "Iteration 787, loss = 0.43551507\n",
      "Iteration 788, loss = 0.43538986\n",
      "Iteration 789, loss = 0.43526482\n",
      "Iteration 790, loss = 0.43513995\n",
      "Iteration 791, loss = 0.43501525\n",
      "Iteration 792, loss = 0.43489073\n",
      "Iteration 793, loss = 0.43476638\n",
      "Iteration 794, loss = 0.43464220\n",
      "Iteration 795, loss = 0.43451819\n",
      "Iteration 796, loss = 0.43439435\n",
      "Iteration 797, loss = 0.43427068\n",
      "Iteration 798, loss = 0.43414718\n",
      "Iteration 799, loss = 0.43402387\n",
      "Iteration 800, loss = 0.43390072\n",
      "Iteration 801, loss = 0.43377773\n",
      "Iteration 802, loss = 0.43365488\n",
      "Iteration 803, loss = 0.43353221\n",
      "Iteration 804, loss = 0.43340969\n",
      "Iteration 805, loss = 0.43328734\n",
      "Iteration 806, loss = 0.43316516\n",
      "Iteration 807, loss = 0.43304314\n",
      "Iteration 808, loss = 0.43292128\n",
      "Iteration 809, loss = 0.43279958\n",
      "Iteration 810, loss = 0.43267805\n",
      "Iteration 811, loss = 0.43255669\n",
      "Iteration 812, loss = 0.43243550\n",
      "Iteration 813, loss = 0.43231450\n",
      "Iteration 814, loss = 0.43219370\n",
      "Iteration 815, loss = 0.43207306\n",
      "Iteration 816, loss = 0.43195258\n",
      "Iteration 817, loss = 0.43183227\n",
      "Iteration 818, loss = 0.43171212\n",
      "Iteration 819, loss = 0.43159214\n",
      "Iteration 820, loss = 0.43147232\n",
      "Iteration 821, loss = 0.43135267\n",
      "Iteration 822, loss = 0.43123317\n",
      "Iteration 823, loss = 0.43111384\n",
      "Iteration 824, loss = 0.43099467\n",
      "Iteration 825, loss = 0.43087564\n",
      "Iteration 826, loss = 0.43075675\n",
      "Iteration 827, loss = 0.43063802\n",
      "Iteration 828, loss = 0.43051945\n",
      "Iteration 829, loss = 0.43040103\n",
      "Iteration 830, loss = 0.43028279\n",
      "Iteration 831, loss = 0.43016470\n",
      "Iteration 832, loss = 0.43004674\n",
      "Iteration 833, loss = 0.42992893\n",
      "Iteration 834, loss = 0.42981128\n",
      "Iteration 835, loss = 0.42969376\n",
      "Iteration 836, loss = 0.42957637\n",
      "Iteration 837, loss = 0.42945914\n",
      "Iteration 838, loss = 0.42934211\n",
      "Iteration 839, loss = 0.42922526\n",
      "Iteration 840, loss = 0.42910857\n",
      "Iteration 841, loss = 0.42899203\n",
      "Iteration 842, loss = 0.42887564\n",
      "Iteration 843, loss = 0.42875941\n",
      "Iteration 844, loss = 0.42864333\n",
      "Iteration 845, loss = 0.42852740\n",
      "Iteration 846, loss = 0.42841162\n",
      "Iteration 847, loss = 0.42829599\n",
      "Iteration 848, loss = 0.42818050\n",
      "Iteration 849, loss = 0.42806517\n",
      "Iteration 850, loss = 0.42794998\n",
      "Iteration 851, loss = 0.42783494\n",
      "Iteration 852, loss = 0.42772004\n",
      "Iteration 853, loss = 0.42760525\n",
      "Iteration 854, loss = 0.42749060\n",
      "Iteration 855, loss = 0.42737610\n",
      "Iteration 856, loss = 0.42726173\n",
      "Iteration 857, loss = 0.42714751\n",
      "Iteration 858, loss = 0.42703343\n",
      "Iteration 859, loss = 0.42691949\n",
      "Iteration 860, loss = 0.42680569\n",
      "Iteration 861, loss = 0.42669204\n",
      "Iteration 862, loss = 0.42657852\n",
      "Iteration 863, loss = 0.42646514\n",
      "Iteration 864, loss = 0.42635195\n",
      "Iteration 865, loss = 0.42623891\n",
      "Iteration 866, loss = 0.42612600\n",
      "Iteration 867, loss = 0.42601324\n",
      "Iteration 868, loss = 0.42590062\n",
      "Iteration 869, loss = 0.42578814\n",
      "Iteration 870, loss = 0.42567580\n",
      "Iteration 871, loss = 0.42556360\n",
      "Iteration 872, loss = 0.42545154\n",
      "Iteration 873, loss = 0.42533962\n",
      "Iteration 874, loss = 0.42522782\n",
      "Iteration 875, loss = 0.42511615\n",
      "Iteration 876, loss = 0.42500462\n",
      "Iteration 877, loss = 0.42489322\n",
      "Iteration 878, loss = 0.42478196\n",
      "Iteration 879, loss = 0.42467083\n",
      "Iteration 880, loss = 0.42455985\n",
      "Iteration 881, loss = 0.42444898\n",
      "Iteration 882, loss = 0.42433823\n",
      "Iteration 883, loss = 0.42422760\n",
      "Iteration 884, loss = 0.42411710\n",
      "Iteration 885, loss = 0.42400672\n",
      "Iteration 886, loss = 0.42389647\n",
      "Iteration 887, loss = 0.42378636\n",
      "Iteration 888, loss = 0.42367637\n",
      "Iteration 889, loss = 0.42356651\n",
      "Iteration 890, loss = 0.42345679\n",
      "Iteration 891, loss = 0.42334720\n",
      "Iteration 892, loss = 0.42323774\n",
      "Iteration 893, loss = 0.42312841\n",
      "Iteration 894, loss = 0.42301921\n",
      "Iteration 895, loss = 0.42291014\n",
      "Iteration 896, loss = 0.42280120\n",
      "Iteration 897, loss = 0.42269237\n",
      "Iteration 898, loss = 0.42258366\n",
      "Iteration 899, loss = 0.42247506\n",
      "Iteration 900, loss = 0.42236657\n",
      "Iteration 901, loss = 0.42225819\n",
      "Iteration 902, loss = 0.42214994\n",
      "Iteration 903, loss = 0.42204181\n",
      "Iteration 904, loss = 0.42193381\n",
      "Iteration 905, loss = 0.42182595\n",
      "Iteration 906, loss = 0.42171821\n",
      "Iteration 907, loss = 0.42161060\n",
      "Iteration 908, loss = 0.42150315\n",
      "Iteration 909, loss = 0.42139585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 910, loss = 0.42128867\n",
      "Iteration 911, loss = 0.42118163\n",
      "Iteration 912, loss = 0.42107471\n",
      "Iteration 913, loss = 0.42096792\n",
      "Iteration 914, loss = 0.42086124\n",
      "Iteration 915, loss = 0.42075465\n",
      "Iteration 916, loss = 0.42064818\n",
      "Iteration 917, loss = 0.42054182\n",
      "Iteration 918, loss = 0.42043559\n",
      "Iteration 919, loss = 0.42032947\n",
      "Iteration 920, loss = 0.42022348\n",
      "Iteration 921, loss = 0.42011760\n",
      "Iteration 922, loss = 0.42001182\n",
      "Iteration 923, loss = 0.41990614\n",
      "Iteration 924, loss = 0.41980057\n",
      "Iteration 925, loss = 0.41969512\n",
      "Iteration 926, loss = 0.41958976\n",
      "Iteration 927, loss = 0.41948450\n",
      "Iteration 928, loss = 0.41937936\n",
      "Iteration 929, loss = 0.41927433\n",
      "Iteration 930, loss = 0.41916942\n",
      "Iteration 931, loss = 0.41906462\n",
      "Iteration 932, loss = 0.41895994\n",
      "Iteration 933, loss = 0.41885537\n",
      "Iteration 934, loss = 0.41875092\n",
      "Iteration 935, loss = 0.41864658\n",
      "Iteration 936, loss = 0.41854236\n",
      "Iteration 937, loss = 0.41843825\n",
      "Iteration 938, loss = 0.41833426\n",
      "Iteration 939, loss = 0.41823039\n",
      "Iteration 940, loss = 0.41812664\n",
      "Iteration 941, loss = 0.41802301\n",
      "Iteration 942, loss = 0.41791950\n",
      "Iteration 943, loss = 0.41781610\n",
      "Iteration 944, loss = 0.41771281\n",
      "Iteration 945, loss = 0.41760965\n",
      "Iteration 946, loss = 0.41750659\n",
      "Iteration 947, loss = 0.41740370\n",
      "Iteration 948, loss = 0.41730094\n",
      "Iteration 949, loss = 0.41719831\n",
      "Iteration 950, loss = 0.41709579\n",
      "Iteration 951, loss = 0.41699340\n",
      "Iteration 952, loss = 0.41689112\n",
      "Iteration 953, loss = 0.41678896\n",
      "Iteration 954, loss = 0.41668692\n",
      "Iteration 955, loss = 0.41658502\n",
      "Iteration 956, loss = 0.41648323\n",
      "Iteration 957, loss = 0.41638156\n",
      "Iteration 958, loss = 0.41628000\n",
      "Iteration 959, loss = 0.41617855\n",
      "Iteration 960, loss = 0.41607722\n",
      "Iteration 961, loss = 0.41597606\n",
      "Iteration 962, loss = 0.41587502\n",
      "Iteration 963, loss = 0.41577409\n",
      "Iteration 964, loss = 0.41567330\n",
      "Iteration 965, loss = 0.41557262\n",
      "Iteration 966, loss = 0.41547206\n",
      "Iteration 967, loss = 0.41537161\n",
      "Iteration 968, loss = 0.41527128\n",
      "Iteration 969, loss = 0.41517106\n",
      "Iteration 970, loss = 0.41507095\n",
      "Iteration 971, loss = 0.41497093\n",
      "Iteration 972, loss = 0.41487103\n",
      "Iteration 973, loss = 0.41477123\n",
      "Iteration 974, loss = 0.41467154\n",
      "Iteration 975, loss = 0.41457196\n",
      "Iteration 976, loss = 0.41447246\n",
      "Iteration 977, loss = 0.41437306\n",
      "Iteration 978, loss = 0.41427377\n",
      "Iteration 979, loss = 0.41417458\n",
      "Iteration 980, loss = 0.41407548\n",
      "Iteration 981, loss = 0.41397649\n",
      "Iteration 982, loss = 0.41387757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.913333\n",
      "Training set loss: 0.413878\n",
      "training: adam\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.00270013\n",
      "Iteration 3, loss = 0.95682085\n",
      "Iteration 4, loss = 0.91151167\n",
      "Iteration 5, loss = 0.86576511\n",
      "Iteration 6, loss = 0.82112995\n",
      "Iteration 7, loss = 0.77847520\n",
      "Iteration 8, loss = 0.73645678\n",
      "Iteration 9, loss = 0.69496229\n",
      "Iteration 10, loss = 0.65467840\n",
      "Iteration 11, loss = 0.61636567\n",
      "Iteration 12, loss = 0.58054656\n",
      "Iteration 13, loss = 0.54745038\n",
      "Iteration 14, loss = 0.51718493\n",
      "Iteration 15, loss = 0.48968444\n",
      "Iteration 16, loss = 0.46476917\n",
      "Iteration 17, loss = 0.44221236\n",
      "Iteration 18, loss = 0.42179290\n",
      "Iteration 19, loss = 0.40314052\n",
      "Iteration 20, loss = 0.38593677\n",
      "Iteration 21, loss = 0.36999165\n",
      "Iteration 22, loss = 0.35521206\n",
      "Iteration 23, loss = 0.34140816\n",
      "Iteration 24, loss = 0.32837464\n",
      "Iteration 25, loss = 0.31593766\n",
      "Iteration 26, loss = 0.30402141\n",
      "Iteration 27, loss = 0.29258774\n",
      "Iteration 28, loss = 0.28159377\n",
      "Iteration 29, loss = 0.27101125\n",
      "Iteration 30, loss = 0.26082378\n",
      "Iteration 31, loss = 0.25101834\n",
      "Iteration 32, loss = 0.24159608\n",
      "Iteration 33, loss = 0.23254591\n",
      "Iteration 34, loss = 0.22386227\n",
      "Iteration 35, loss = 0.21554094\n",
      "Iteration 36, loss = 0.20757572\n",
      "Iteration 37, loss = 0.19994423\n",
      "Iteration 38, loss = 0.19264524\n",
      "Iteration 39, loss = 0.18567642\n",
      "Iteration 40, loss = 0.17902749\n",
      "Iteration 41, loss = 0.17268590\n",
      "Iteration 42, loss = 0.16663760\n",
      "Iteration 43, loss = 0.16087787\n",
      "Iteration 44, loss = 0.15539803\n",
      "Iteration 45, loss = 0.15018384\n",
      "Iteration 46, loss = 0.14522187\n",
      "Iteration 47, loss = 0.14050016\n",
      "Iteration 48, loss = 0.13600805\n",
      "Iteration 49, loss = 0.13174114\n",
      "Iteration 50, loss = 0.12768924\n",
      "Iteration 51, loss = 0.12384273\n",
      "Iteration 52, loss = 0.12019833\n",
      "Iteration 53, loss = 0.11674967\n",
      "Iteration 54, loss = 0.11348743\n",
      "Iteration 55, loss = 0.11040013\n",
      "Iteration 56, loss = 0.10748306\n",
      "Iteration 57, loss = 0.10472537\n",
      "Iteration 58, loss = 0.10211987\n",
      "Iteration 59, loss = 0.09965920\n",
      "Iteration 60, loss = 0.09733799\n",
      "Iteration 61, loss = 0.09514610\n",
      "Iteration 62, loss = 0.09307671\n",
      "Iteration 63, loss = 0.09112328\n",
      "Iteration 64, loss = 0.08927881\n",
      "Iteration 65, loss = 0.08753591\n",
      "Iteration 66, loss = 0.08588897\n",
      "Iteration 67, loss = 0.08433234\n",
      "Iteration 68, loss = 0.08285993\n",
      "Iteration 69, loss = 0.08146676\n",
      "Iteration 70, loss = 0.08014810\n",
      "Iteration 71, loss = 0.07889799\n",
      "Iteration 72, loss = 0.07771260\n",
      "Iteration 73, loss = 0.07658724\n",
      "Iteration 74, loss = 0.07551787\n",
      "Iteration 75, loss = 0.07450468\n",
      "Iteration 76, loss = 0.07353900\n",
      "Iteration 77, loss = 0.07261923\n",
      "Iteration 78, loss = 0.07174308\n",
      "Iteration 79, loss = 0.07090642\n",
      "Iteration 80, loss = 0.07010690\n",
      "Iteration 81, loss = 0.06934225\n",
      "Iteration 82, loss = 0.06861006\n",
      "Iteration 83, loss = 0.06790735\n",
      "Iteration 84, loss = 0.06723319\n",
      "Iteration 85, loss = 0.06658483\n",
      "Iteration 86, loss = 0.06596145\n",
      "Iteration 87, loss = 0.06536122\n",
      "Iteration 88, loss = 0.06478243\n",
      "Iteration 89, loss = 0.06422377\n",
      "Iteration 90, loss = 0.06368453\n",
      "Iteration 91, loss = 0.06316397\n",
      "Iteration 92, loss = 0.06266180\n",
      "Iteration 93, loss = 0.06217640\n",
      "Iteration 94, loss = 0.06170580\n",
      "Iteration 95, loss = 0.06125061\n",
      "Iteration 96, loss = 0.06080839\n",
      "Iteration 97, loss = 0.06037899\n",
      "Iteration 98, loss = 0.05996241\n",
      "Iteration 99, loss = 0.05955715\n",
      "Iteration 100, loss = 0.05916341\n",
      "Iteration 101, loss = 0.05878127\n",
      "Iteration 102, loss = 0.05841105\n",
      "Iteration 103, loss = 0.05805106\n",
      "Iteration 104, loss = 0.05770012\n",
      "Iteration 105, loss = 0.05736001\n",
      "Iteration 106, loss = 0.05702833\n",
      "Iteration 107, loss = 0.05670329\n",
      "Iteration 108, loss = 0.05638863\n",
      "Iteration 109, loss = 0.05608246\n",
      "Iteration 110, loss = 0.05578276\n",
      "Iteration 111, loss = 0.05549293\n",
      "Iteration 112, loss = 0.05520856\n",
      "Iteration 113, loss = 0.05493104\n",
      "Iteration 114, loss = 0.05466012\n",
      "Iteration 115, loss = 0.05439574\n",
      "Iteration 116, loss = 0.05413915\n",
      "Iteration 117, loss = 0.05388929\n",
      "Iteration 118, loss = 0.05364456\n",
      "Iteration 119, loss = 0.05340641\n",
      "Iteration 120, loss = 0.05317236\n",
      "Iteration 121, loss = 0.05294306\n",
      "Iteration 122, loss = 0.05271940\n",
      "Iteration 123, loss = 0.05250179\n",
      "Iteration 124, loss = 0.05228851\n",
      "Iteration 125, loss = 0.05207827\n",
      "Iteration 126, loss = 0.05187400\n",
      "Iteration 127, loss = 0.05167415\n",
      "Iteration 128, loss = 0.05147776\n",
      "Iteration 129, loss = 0.05128567\n",
      "Iteration 130, loss = 0.05109655\n",
      "Iteration 131, loss = 0.05091129\n",
      "Iteration 132, loss = 0.05073016\n",
      "Iteration 133, loss = 0.05055451\n",
      "Iteration 134, loss = 0.05038130\n",
      "Iteration 135, loss = 0.05020957\n",
      "Iteration 136, loss = 0.05004171\n",
      "Iteration 137, loss = 0.04987855\n",
      "Iteration 138, loss = 0.04971873\n",
      "Iteration 139, loss = 0.04956154\n",
      "Iteration 140, loss = 0.04940630\n",
      "Iteration 141, loss = 0.04925439\n",
      "Iteration 142, loss = 0.04910593\n",
      "Iteration 143, loss = 0.04896006\n",
      "Iteration 144, loss = 0.04881680\n",
      "Iteration 145, loss = 0.04867647\n",
      "Iteration 146, loss = 0.04853885\n",
      "Iteration 147, loss = 0.04840322\n",
      "Iteration 148, loss = 0.04827077\n",
      "Iteration 149, loss = 0.04814034\n",
      "Iteration 150, loss = 0.04801149\n",
      "Iteration 151, loss = 0.04788647\n",
      "Iteration 152, loss = 0.04776367\n",
      "Iteration 153, loss = 0.04764251\n",
      "Iteration 154, loss = 0.04752286\n",
      "Iteration 155, loss = 0.04740562\n",
      "Iteration 156, loss = 0.04729114\n",
      "Iteration 157, loss = 0.04717767\n",
      "Iteration 158, loss = 0.04706629\n",
      "Iteration 159, loss = 0.04695781\n",
      "Iteration 160, loss = 0.04685136\n",
      "Iteration 161, loss = 0.04674644\n",
      "Iteration 162, loss = 0.04664283\n",
      "Iteration 163, loss = 0.04654072\n",
      "Iteration 164, loss = 0.04644181\n",
      "Iteration 165, loss = 0.04634300\n",
      "Iteration 166, loss = 0.04624763\n",
      "Iteration 167, loss = 0.04615298\n",
      "Iteration 168, loss = 0.04605910\n",
      "Iteration 169, loss = 0.04596757\n",
      "Iteration 170, loss = 0.04587744\n",
      "Iteration 171, loss = 0.04578831\n",
      "Iteration 172, loss = 0.04570108\n",
      "Iteration 173, loss = 0.04561463\n",
      "Iteration 174, loss = 0.04553083\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.980000\n",
      "Training set loss: 0.045531\n",
      "\n",
      "learning on dataset digits\n",
      "training: constant learning-rate\n",
      "Iteration 1, loss = 2.16498699\n",
      "Iteration 2, loss = 1.74530399\n",
      "Iteration 3, loss = 1.35033016\n",
      "Iteration 4, loss = 1.02209366\n",
      "Iteration 5, loss = 0.78633710\n",
      "Iteration 6, loss = 0.63162398\n",
      "Iteration 7, loss = 0.52768449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.44966734\n",
      "Iteration 9, loss = 0.39599675\n",
      "Iteration 10, loss = 0.35910073\n",
      "Iteration 11, loss = 0.32342909\n",
      "Iteration 12, loss = 0.29851906\n",
      "Iteration 13, loss = 0.27719303\n",
      "Iteration 14, loss = 0.25945514\n",
      "Iteration 15, loss = 0.24380177\n",
      "Iteration 16, loss = 0.22995671\n",
      "Iteration 17, loss = 0.21926040\n",
      "Iteration 18, loss = 0.21013782\n",
      "Iteration 19, loss = 0.19933330\n",
      "Iteration 20, loss = 0.19267995\n",
      "Iteration 21, loss = 0.18506944\n",
      "Iteration 22, loss = 0.17803897\n",
      "Iteration 23, loss = 0.17285625\n",
      "Iteration 24, loss = 0.16716209\n",
      "Iteration 25, loss = 0.16166121\n",
      "Iteration 26, loss = 0.15874452\n",
      "Iteration 27, loss = 0.15420410\n",
      "Iteration 28, loss = 0.14980178\n",
      "Iteration 29, loss = 0.14511583\n",
      "Iteration 30, loss = 0.14259421\n",
      "Iteration 31, loss = 0.13847250\n",
      "Iteration 32, loss = 0.13540302\n",
      "Iteration 33, loss = 0.13327335\n",
      "Iteration 34, loss = 0.13097354\n",
      "Iteration 35, loss = 0.12866558\n",
      "Iteration 36, loss = 0.12590667\n",
      "Iteration 37, loss = 0.12239277\n",
      "Iteration 38, loss = 0.12076932\n",
      "Iteration 39, loss = 0.11907401\n",
      "Iteration 40, loss = 0.11764229\n",
      "Iteration 41, loss = 0.11553560\n",
      "Iteration 42, loss = 0.11344602\n",
      "Iteration 43, loss = 0.11131037\n",
      "Iteration 44, loss = 0.10937329\n",
      "Iteration 45, loss = 0.10708640\n",
      "Iteration 46, loss = 0.10708888\n",
      "Iteration 47, loss = 0.10529455\n",
      "Iteration 48, loss = 0.10238278\n",
      "Iteration 49, loss = 0.10102945\n",
      "Iteration 50, loss = 0.10057132\n",
      "Iteration 51, loss = 0.09831381\n",
      "Iteration 52, loss = 0.09658239\n",
      "Iteration 53, loss = 0.09621629\n",
      "Iteration 54, loss = 0.09465787\n",
      "Iteration 55, loss = 0.09387531\n",
      "Iteration 56, loss = 0.09239810\n",
      "Iteration 57, loss = 0.09092286\n",
      "Iteration 58, loss = 0.08979073\n",
      "Iteration 59, loss = 0.08858941\n",
      "Iteration 60, loss = 0.08907982\n",
      "Iteration 61, loss = 0.08747867\n",
      "Iteration 62, loss = 0.08557326\n",
      "Iteration 63, loss = 0.08425338\n",
      "Iteration 64, loss = 0.08401039\n",
      "Iteration 65, loss = 0.08229614\n",
      "Iteration 66, loss = 0.08179469\n",
      "Iteration 67, loss = 0.08023440\n",
      "Iteration 68, loss = 0.07988230\n",
      "Iteration 69, loss = 0.07892006\n",
      "Iteration 70, loss = 0.07746839\n",
      "Iteration 71, loss = 0.07739962\n",
      "Iteration 72, loss = 0.07650759\n",
      "Iteration 73, loss = 0.07571359\n",
      "Iteration 74, loss = 0.07461541\n",
      "Iteration 75, loss = 0.07336294\n",
      "Iteration 76, loss = 0.07333056\n",
      "Iteration 77, loss = 0.07197396\n",
      "Iteration 78, loss = 0.07235172\n",
      "Iteration 79, loss = 0.07056655\n",
      "Iteration 80, loss = 0.07095431\n",
      "Iteration 81, loss = 0.06951204\n",
      "Iteration 82, loss = 0.06862762\n",
      "Iteration 83, loss = 0.06839593\n",
      "Iteration 84, loss = 0.06729402\n",
      "Iteration 85, loss = 0.06603662\n",
      "Iteration 86, loss = 0.06568560\n",
      "Iteration 87, loss = 0.06509332\n",
      "Iteration 88, loss = 0.06502096\n",
      "Iteration 89, loss = 0.06429180\n",
      "Iteration 90, loss = 0.06419809\n",
      "Iteration 91, loss = 0.06327113\n",
      "Iteration 92, loss = 0.06236513\n",
      "Iteration 93, loss = 0.06180175\n",
      "Iteration 94, loss = 0.06132965\n",
      "Iteration 95, loss = 0.06107310\n",
      "Iteration 96, loss = 0.06025700\n",
      "Iteration 97, loss = 0.05961185\n",
      "Iteration 98, loss = 0.05944942\n",
      "Iteration 99, loss = 0.05848761\n",
      "Iteration 100, loss = 0.05829811\n",
      "Iteration 101, loss = 0.05754788\n",
      "Iteration 102, loss = 0.05754779\n",
      "Iteration 103, loss = 0.05608570\n",
      "Iteration 104, loss = 0.05725867\n",
      "Iteration 105, loss = 0.05527524\n",
      "Iteration 106, loss = 0.05532556\n",
      "Iteration 107, loss = 0.05491274\n",
      "Iteration 108, loss = 0.05439134\n",
      "Iteration 109, loss = 0.05381726\n",
      "Iteration 110, loss = 0.05333293\n",
      "Iteration 111, loss = 0.05273940\n",
      "Iteration 112, loss = 0.05301174\n",
      "Iteration 113, loss = 0.05200888\n",
      "Iteration 114, loss = 0.05194732\n",
      "Iteration 115, loss = 0.05090401\n",
      "Iteration 116, loss = 0.05104885\n",
      "Iteration 117, loss = 0.05016337\n",
      "Iteration 118, loss = 0.04981476\n",
      "Iteration 119, loss = 0.04963180\n",
      "Iteration 120, loss = 0.04998483\n",
      "Iteration 121, loss = 0.04857417\n",
      "Iteration 122, loss = 0.04882359\n",
      "Iteration 123, loss = 0.04759870\n",
      "Iteration 124, loss = 0.04742696\n",
      "Iteration 125, loss = 0.04807861\n",
      "Iteration 126, loss = 0.04707977\n",
      "Iteration 127, loss = 0.04679688\n",
      "Iteration 128, loss = 0.04655974\n",
      "Iteration 129, loss = 0.04535563\n",
      "Iteration 130, loss = 0.04563025\n",
      "Iteration 131, loss = 0.04594915\n",
      "Iteration 132, loss = 0.04553307\n",
      "Iteration 133, loss = 0.04483690\n",
      "Iteration 134, loss = 0.04475029\n",
      "Iteration 135, loss = 0.04439603\n",
      "Iteration 136, loss = 0.04330217\n",
      "Iteration 137, loss = 0.04334214\n",
      "Iteration 138, loss = 0.04306304\n",
      "Iteration 139, loss = 0.04310811\n",
      "Iteration 140, loss = 0.04224584\n",
      "Iteration 141, loss = 0.04226891\n",
      "Iteration 142, loss = 0.04191759\n",
      "Iteration 143, loss = 0.04163256\n",
      "Iteration 144, loss = 0.04120347\n",
      "Iteration 145, loss = 0.04158778\n",
      "Iteration 146, loss = 0.04048230\n",
      "Iteration 147, loss = 0.04095239\n",
      "Iteration 148, loss = 0.04033507\n",
      "Iteration 149, loss = 0.03967420\n",
      "Iteration 150, loss = 0.03944944\n",
      "Iteration 151, loss = 0.03945216\n",
      "Iteration 152, loss = 0.03947306\n",
      "Iteration 153, loss = 0.03969199\n",
      "Iteration 154, loss = 0.03908281\n",
      "Iteration 155, loss = 0.03842460\n",
      "Iteration 156, loss = 0.03796942\n",
      "Iteration 157, loss = 0.03757053\n",
      "Iteration 158, loss = 0.03784392\n",
      "Iteration 159, loss = 0.03762265\n",
      "Iteration 160, loss = 0.03724375\n",
      "Iteration 161, loss = 0.03726417\n",
      "Iteration 162, loss = 0.03680841\n",
      "Iteration 163, loss = 0.03640311\n",
      "Iteration 164, loss = 0.03648826\n",
      "Iteration 165, loss = 0.03583253\n",
      "Iteration 166, loss = 0.03573202\n",
      "Iteration 167, loss = 0.03512175\n",
      "Iteration 168, loss = 0.03532971\n",
      "Iteration 169, loss = 0.03487716\n",
      "Iteration 170, loss = 0.03499794\n",
      "Iteration 171, loss = 0.03468134\n",
      "Iteration 172, loss = 0.03458787\n",
      "Iteration 173, loss = 0.03418060\n",
      "Iteration 174, loss = 0.03435586\n",
      "Iteration 175, loss = 0.03401404\n",
      "Iteration 176, loss = 0.03339063\n",
      "Iteration 177, loss = 0.03363726\n",
      "Iteration 178, loss = 0.03314638\n",
      "Iteration 179, loss = 0.03290184\n",
      "Iteration 180, loss = 0.03275944\n",
      "Iteration 181, loss = 0.03271316\n",
      "Iteration 182, loss = 0.03223870\n",
      "Iteration 183, loss = 0.03200183\n",
      "Iteration 184, loss = 0.03222657\n",
      "Iteration 185, loss = 0.03197111\n",
      "Iteration 186, loss = 0.03149742\n",
      "Iteration 187, loss = 0.03184922\n",
      "Iteration 188, loss = 0.03144920\n",
      "Iteration 189, loss = 0.03118975\n",
      "Iteration 190, loss = 0.03113611\n",
      "Iteration 191, loss = 0.03092330\n",
      "Iteration 192, loss = 0.03053712\n",
      "Iteration 193, loss = 0.03028015\n",
      "Iteration 194, loss = 0.03027096\n",
      "Iteration 195, loss = 0.03007165\n",
      "Iteration 196, loss = 0.02978221\n",
      "Iteration 197, loss = 0.02982907\n",
      "Iteration 198, loss = 0.02955864\n",
      "Iteration 199, loss = 0.02911117\n",
      "Iteration 200, loss = 0.02960542\n",
      "Iteration 201, loss = 0.02900652\n",
      "Iteration 202, loss = 0.02882300\n",
      "Iteration 203, loss = 0.02861926\n",
      "Iteration 204, loss = 0.02870810\n",
      "Iteration 205, loss = 0.02864185\n",
      "Iteration 206, loss = 0.02851413\n",
      "Iteration 207, loss = 0.02797513\n",
      "Iteration 208, loss = 0.02804296\n",
      "Iteration 209, loss = 0.02778233\n",
      "Iteration 210, loss = 0.02736573\n",
      "Iteration 211, loss = 0.02768752\n",
      "Iteration 212, loss = 0.02742222\n",
      "Iteration 213, loss = 0.02720636\n",
      "Iteration 214, loss = 0.02720448\n",
      "Iteration 215, loss = 0.02727963\n",
      "Iteration 216, loss = 0.02683233\n",
      "Iteration 217, loss = 0.02662574\n",
      "Iteration 218, loss = 0.02656884\n",
      "Iteration 219, loss = 0.02639615\n",
      "Iteration 220, loss = 0.02642345\n",
      "Iteration 221, loss = 0.02619636\n",
      "Iteration 222, loss = 0.02643170\n",
      "Iteration 223, loss = 0.02571742\n",
      "Iteration 224, loss = 0.02582322\n",
      "Iteration 225, loss = 0.02552162\n",
      "Iteration 226, loss = 0.02531354\n",
      "Iteration 227, loss = 0.02521255\n",
      "Iteration 228, loss = 0.02509752\n",
      "Iteration 229, loss = 0.02499280\n",
      "Iteration 230, loss = 0.02489727\n",
      "Iteration 231, loss = 0.02476278\n",
      "Iteration 232, loss = 0.02475142\n",
      "Iteration 233, loss = 0.02446200\n",
      "Iteration 234, loss = 0.02453657\n",
      "Iteration 235, loss = 0.02448618\n",
      "Iteration 236, loss = 0.02408102\n",
      "Iteration 237, loss = 0.02400327\n",
      "Iteration 238, loss = 0.02383260\n",
      "Iteration 239, loss = 0.02387853\n",
      "Iteration 240, loss = 0.02381829\n",
      "Iteration 241, loss = 0.02385824\n",
      "Iteration 242, loss = 0.02350918\n",
      "Iteration 243, loss = 0.02383921\n",
      "Iteration 244, loss = 0.02316803\n",
      "Iteration 245, loss = 0.02322943\n",
      "Iteration 246, loss = 0.02309459\n",
      "Iteration 247, loss = 0.02303311\n",
      "Iteration 248, loss = 0.02278186\n",
      "Iteration 249, loss = 0.02279671\n",
      "Iteration 250, loss = 0.02280479\n",
      "Iteration 251, loss = 0.02275956\n",
      "Iteration 252, loss = 0.02247327\n",
      "Iteration 253, loss = 0.02240053\n",
      "Iteration 254, loss = 0.02204907\n",
      "Iteration 255, loss = 0.02215677\n",
      "Iteration 256, loss = 0.02243945\n",
      "Iteration 257, loss = 0.02211744\n",
      "Iteration 258, loss = 0.02178905\n",
      "Iteration 259, loss = 0.02168546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 260, loss = 0.02152832\n",
      "Iteration 261, loss = 0.02130605\n",
      "Iteration 262, loss = 0.02122868\n",
      "Iteration 263, loss = 0.02129158\n",
      "Iteration 264, loss = 0.02109658\n",
      "Iteration 265, loss = 0.02096417\n",
      "Iteration 266, loss = 0.02082436\n",
      "Iteration 267, loss = 0.02112102\n",
      "Iteration 268, loss = 0.02075650\n",
      "Iteration 269, loss = 0.02074914\n",
      "Iteration 270, loss = 0.02048330\n",
      "Iteration 271, loss = 0.02075509\n",
      "Iteration 272, loss = 0.02031076\n",
      "Iteration 273, loss = 0.02044365\n",
      "Iteration 274, loss = 0.02017100\n",
      "Iteration 275, loss = 0.02019884\n",
      "Iteration 276, loss = 0.02026111\n",
      "Iteration 277, loss = 0.02013594\n",
      "Iteration 278, loss = 0.02026186\n",
      "Iteration 279, loss = 0.02003610\n",
      "Iteration 280, loss = 0.01956865\n",
      "Iteration 281, loss = 0.01971913\n",
      "Iteration 282, loss = 0.01958778\n",
      "Iteration 283, loss = 0.01911345\n",
      "Iteration 284, loss = 0.01945197\n",
      "Iteration 285, loss = 0.01937069\n",
      "Iteration 286, loss = 0.01915067\n",
      "Iteration 287, loss = 0.01929086\n",
      "Iteration 288, loss = 0.01920142\n",
      "Iteration 289, loss = 0.01901817\n",
      "Iteration 290, loss = 0.01890359\n",
      "Iteration 291, loss = 0.01873483\n",
      "Iteration 292, loss = 0.01860464\n",
      "Iteration 293, loss = 0.01851124\n",
      "Iteration 294, loss = 0.01849434\n",
      "Iteration 295, loss = 0.01861113\n",
      "Iteration 296, loss = 0.01824453\n",
      "Iteration 297, loss = 0.01825420\n",
      "Iteration 298, loss = 0.01836762\n",
      "Iteration 299, loss = 0.01818377\n",
      "Iteration 300, loss = 0.01801968\n",
      "Iteration 301, loss = 0.01804129\n",
      "Iteration 302, loss = 0.01791751\n",
      "Iteration 303, loss = 0.01804687\n",
      "Iteration 304, loss = 0.01781527\n",
      "Iteration 305, loss = 0.01773428\n",
      "Iteration 306, loss = 0.01760659\n",
      "Iteration 307, loss = 0.01754036\n",
      "Iteration 308, loss = 0.01735476\n",
      "Iteration 309, loss = 0.01752930\n",
      "Iteration 310, loss = 0.01723961\n",
      "Iteration 311, loss = 0.01731658\n",
      "Iteration 312, loss = 0.01706340\n",
      "Iteration 313, loss = 0.01713817\n",
      "Iteration 314, loss = 0.01686948\n",
      "Iteration 315, loss = 0.01709471\n",
      "Iteration 316, loss = 0.01690659\n",
      "Iteration 317, loss = 0.01700995\n",
      "Iteration 318, loss = 0.01675899\n",
      "Iteration 319, loss = 0.01691208\n",
      "Iteration 320, loss = 0.01668516\n",
      "Iteration 321, loss = 0.01669953\n",
      "Iteration 322, loss = 0.01652758\n",
      "Iteration 323, loss = 0.01626349\n",
      "Iteration 324, loss = 0.01645705\n",
      "Iteration 325, loss = 0.01646350\n",
      "Iteration 326, loss = 0.01623070\n",
      "Iteration 327, loss = 0.01620146\n",
      "Iteration 328, loss = 0.01624381\n",
      "Iteration 329, loss = 0.01617312\n",
      "Iteration 330, loss = 0.01603730\n",
      "Iteration 331, loss = 0.01594505\n",
      "Iteration 332, loss = 0.01586500\n",
      "Iteration 333, loss = 0.01575196\n",
      "Iteration 334, loss = 0.01574995\n",
      "Iteration 335, loss = 0.01562385\n",
      "Iteration 336, loss = 0.01574151\n",
      "Iteration 337, loss = 0.01587791\n",
      "Iteration 338, loss = 0.01541523\n",
      "Iteration 339, loss = 0.01550392\n",
      "Iteration 340, loss = 0.01534337\n",
      "Iteration 341, loss = 0.01532537\n",
      "Iteration 342, loss = 0.01520869\n",
      "Iteration 343, loss = 0.01518758\n",
      "Iteration 344, loss = 0.01512355\n",
      "Iteration 345, loss = 0.01502611\n",
      "Iteration 346, loss = 0.01489961\n",
      "Iteration 347, loss = 0.01496480\n",
      "Iteration 348, loss = 0.01490130\n",
      "Iteration 349, loss = 0.01501452\n",
      "Iteration 350, loss = 0.01489118\n",
      "Iteration 351, loss = 0.01478542\n",
      "Iteration 352, loss = 0.01482021\n",
      "Iteration 353, loss = 0.01467023\n",
      "Iteration 354, loss = 0.01461795\n",
      "Iteration 355, loss = 0.01457543\n",
      "Iteration 356, loss = 0.01450448\n",
      "Iteration 357, loss = 0.01428273\n",
      "Iteration 358, loss = 0.01412620\n",
      "Iteration 359, loss = 0.01441596\n",
      "Iteration 360, loss = 0.01414592\n",
      "Iteration 361, loss = 0.01433804\n",
      "Iteration 362, loss = 0.01411213\n",
      "Iteration 363, loss = 0.01415772\n",
      "Iteration 364, loss = 0.01410194\n",
      "Iteration 365, loss = 0.01419327\n",
      "Iteration 366, loss = 0.01401318\n",
      "Iteration 367, loss = 0.01385976\n",
      "Iteration 368, loss = 0.01391991\n",
      "Iteration 369, loss = 0.01382997\n",
      "Iteration 370, loss = 0.01362431\n",
      "Iteration 371, loss = 0.01371622\n",
      "Iteration 372, loss = 0.01367463\n",
      "Iteration 373, loss = 0.01359612\n",
      "Iteration 374, loss = 0.01349286\n",
      "Iteration 375, loss = 0.01350906\n",
      "Iteration 376, loss = 0.01352188\n",
      "Iteration 377, loss = 0.01340880\n",
      "Iteration 378, loss = 0.01348195\n",
      "Iteration 379, loss = 0.01323400\n",
      "Iteration 380, loss = 0.01318361\n",
      "Iteration 381, loss = 0.01336841\n",
      "Iteration 382, loss = 0.01328835\n",
      "Iteration 383, loss = 0.01305042\n",
      "Iteration 384, loss = 0.01316021\n",
      "Iteration 385, loss = 0.01321811\n",
      "Iteration 386, loss = 0.01294491\n",
      "Iteration 387, loss = 0.01303133\n",
      "Iteration 388, loss = 0.01293885\n",
      "Iteration 389, loss = 0.01292839\n",
      "Iteration 390, loss = 0.01286880\n",
      "Iteration 391, loss = 0.01272003\n",
      "Iteration 392, loss = 0.01285663\n",
      "Iteration 393, loss = 0.01273363\n",
      "Iteration 394, loss = 0.01267148\n",
      "Iteration 395, loss = 0.01265703\n",
      "Iteration 396, loss = 0.01256597\n",
      "Iteration 397, loss = 0.01252497\n",
      "Iteration 398, loss = 0.01251868\n",
      "Iteration 399, loss = 0.01247307\n",
      "Iteration 400, loss = 0.01250816\n",
      "Iteration 401, loss = 0.01237097\n",
      "Iteration 402, loss = 0.01237136\n",
      "Iteration 403, loss = 0.01232586\n",
      "Iteration 404, loss = 0.01237544\n",
      "Iteration 405, loss = 0.01216009\n",
      "Iteration 406, loss = 0.01217362\n",
      "Iteration 407, loss = 0.01220245\n",
      "Iteration 408, loss = 0.01218060\n",
      "Iteration 409, loss = 0.01199371\n",
      "Iteration 410, loss = 0.01206826\n",
      "Iteration 411, loss = 0.01195327\n",
      "Iteration 412, loss = 0.01186773\n",
      "Iteration 413, loss = 0.01190948\n",
      "Iteration 414, loss = 0.01184905\n",
      "Iteration 415, loss = 0.01190274\n",
      "Iteration 416, loss = 0.01173942\n",
      "Iteration 417, loss = 0.01164648\n",
      "Iteration 418, loss = 0.01163008\n",
      "Iteration 419, loss = 0.01172901\n",
      "Iteration 420, loss = 0.01157425\n",
      "Iteration 421, loss = 0.01156236\n",
      "Iteration 422, loss = 0.01155746\n",
      "Iteration 423, loss = 0.01161785\n",
      "Iteration 424, loss = 0.01154908\n",
      "Iteration 425, loss = 0.01142369\n",
      "Iteration 426, loss = 0.01143741\n",
      "Iteration 427, loss = 0.01146162\n",
      "Iteration 428, loss = 0.01132494\n",
      "Iteration 429, loss = 0.01131299\n",
      "Iteration 430, loss = 0.01129559\n",
      "Iteration 431, loss = 0.01131205\n",
      "Iteration 432, loss = 0.01125442\n",
      "Iteration 433, loss = 0.01114795\n",
      "Iteration 434, loss = 0.01109577\n",
      "Iteration 435, loss = 0.01101839\n",
      "Iteration 436, loss = 0.01103933\n",
      "Iteration 437, loss = 0.01106968\n",
      "Iteration 438, loss = 0.01097765\n",
      "Iteration 439, loss = 0.01093152\n",
      "Iteration 440, loss = 0.01092551\n",
      "Iteration 441, loss = 0.01078054\n",
      "Iteration 442, loss = 0.01081067\n",
      "Iteration 443, loss = 0.01092431\n",
      "Iteration 444, loss = 0.01065785\n",
      "Iteration 445, loss = 0.01077053\n",
      "Iteration 446, loss = 0.01081972\n",
      "Iteration 447, loss = 0.01063999\n",
      "Iteration 448, loss = 0.01069416\n",
      "Iteration 449, loss = 0.01064727\n",
      "Iteration 450, loss = 0.01058582\n",
      "Iteration 451, loss = 0.01050140\n",
      "Iteration 452, loss = 0.01049049\n",
      "Iteration 453, loss = 0.01049436\n",
      "Iteration 454, loss = 0.01040999\n",
      "Iteration 455, loss = 0.01047323\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 1.000000\n",
      "Training set loss: 0.010473\n",
      "training: constant with momentum\n",
      "Iteration 1, loss = 1.94238221\n",
      "Iteration 2, loss = 0.67913298\n",
      "Iteration 3, loss = 0.28130619\n",
      "Iteration 4, loss = 0.19674528\n",
      "Iteration 5, loss = 0.15501248\n",
      "Iteration 6, loss = 0.12093421\n",
      "Iteration 7, loss = 0.09747044\n",
      "Iteration 8, loss = 0.08059499\n",
      "Iteration 9, loss = 0.07003580\n",
      "Iteration 10, loss = 0.06036287\n",
      "Iteration 11, loss = 0.05587655\n",
      "Iteration 12, loss = 0.05304725\n",
      "Iteration 13, loss = 0.05098537\n",
      "Iteration 14, loss = 0.04729062\n",
      "Iteration 15, loss = 0.04129728\n",
      "Iteration 16, loss = 0.03560138\n",
      "Iteration 17, loss = 0.03526845\n",
      "Iteration 18, loss = 0.03438420\n",
      "Iteration 19, loss = 0.03767033\n",
      "Iteration 20, loss = 0.03117248\n",
      "Iteration 21, loss = 0.03026350\n",
      "Iteration 22, loss = 0.02764813\n",
      "Iteration 23, loss = 0.02546351\n",
      "Iteration 24, loss = 0.02132695\n",
      "Iteration 25, loss = 0.02062711\n",
      "Iteration 26, loss = 0.01932537\n",
      "Iteration 27, loss = 0.01873234\n",
      "Iteration 28, loss = 0.01820819\n",
      "Iteration 29, loss = 0.01819218\n",
      "Iteration 30, loss = 0.01707523\n",
      "Iteration 31, loss = 0.01614730\n",
      "Iteration 32, loss = 0.01488048\n",
      "Iteration 33, loss = 0.01442725\n",
      "Iteration 34, loss = 0.01469079\n",
      "Iteration 35, loss = 0.01316747\n",
      "Iteration 36, loss = 0.01324977\n",
      "Iteration 37, loss = 0.01235918\n",
      "Iteration 38, loss = 0.01226651\n",
      "Iteration 39, loss = 0.01220291\n",
      "Iteration 40, loss = 0.01159487\n",
      "Iteration 41, loss = 0.01179890\n",
      "Iteration 42, loss = 0.01171029\n",
      "Iteration 43, loss = 0.01172573\n",
      "Iteration 44, loss = 0.01045805\n",
      "Iteration 45, loss = 0.01016403\n",
      "Iteration 46, loss = 0.01020108\n",
      "Iteration 47, loss = 0.00941769\n",
      "Iteration 48, loss = 0.00838996\n",
      "Iteration 49, loss = 0.00839287\n",
      "Iteration 50, loss = 0.00819317\n",
      "Iteration 51, loss = 0.00774008\n",
      "Iteration 52, loss = 0.00755298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53, loss = 0.00725135\n",
      "Iteration 54, loss = 0.00776415\n",
      "Iteration 55, loss = 0.00691098\n",
      "Iteration 56, loss = 0.00694474\n",
      "Iteration 57, loss = 0.00678754\n",
      "Iteration 58, loss = 0.00692740\n",
      "Iteration 59, loss = 0.00647471\n",
      "Iteration 60, loss = 0.00611788\n",
      "Iteration 61, loss = 0.00596831\n",
      "Iteration 62, loss = 0.00572886\n",
      "Iteration 63, loss = 0.00578571\n",
      "Iteration 64, loss = 0.00567181\n",
      "Iteration 65, loss = 0.00537117\n",
      "Iteration 66, loss = 0.00522246\n",
      "Iteration 67, loss = 0.00517043\n",
      "Iteration 68, loss = 0.00504730\n",
      "Iteration 69, loss = 0.00485173\n",
      "Iteration 70, loss = 0.00513171\n",
      "Iteration 71, loss = 0.00491846\n",
      "Iteration 72, loss = 0.00467479\n",
      "Iteration 73, loss = 0.00452780\n",
      "Iteration 74, loss = 0.00439970\n",
      "Iteration 75, loss = 0.00447187\n",
      "Iteration 76, loss = 0.00431771\n",
      "Iteration 77, loss = 0.00439819\n",
      "Iteration 78, loss = 0.00430839\n",
      "Iteration 79, loss = 0.00410993\n",
      "Iteration 80, loss = 0.00403998\n",
      "Iteration 81, loss = 0.00404303\n",
      "Iteration 82, loss = 0.00387875\n",
      "Iteration 83, loss = 0.00385048\n",
      "Iteration 84, loss = 0.00388751\n",
      "Iteration 85, loss = 0.00373039\n",
      "Iteration 86, loss = 0.00366336\n",
      "Iteration 87, loss = 0.00361291\n",
      "Iteration 88, loss = 0.00361577\n",
      "Iteration 89, loss = 0.00371165\n",
      "Iteration 90, loss = 0.00352901\n",
      "Iteration 91, loss = 0.00350651\n",
      "Iteration 92, loss = 0.00338928\n",
      "Iteration 93, loss = 0.00341287\n",
      "Iteration 94, loss = 0.00329155\n",
      "Iteration 95, loss = 0.00326604\n",
      "Iteration 96, loss = 0.00321307\n",
      "Iteration 97, loss = 0.00313163\n",
      "Iteration 98, loss = 0.00317417\n",
      "Iteration 99, loss = 0.00305119\n",
      "Iteration 100, loss = 0.00302550\n",
      "Iteration 101, loss = 0.00302139\n",
      "Iteration 102, loss = 0.00297283\n",
      "Iteration 103, loss = 0.00291672\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 1.000000\n",
      "Training set loss: 0.002917\n",
      "training: constant with Nesterov's momentum\n",
      "Iteration 1, loss = 1.80716619\n",
      "Iteration 2, loss = 0.52351487\n",
      "Iteration 3, loss = 0.22289129\n",
      "Iteration 4, loss = 0.15015899\n",
      "Iteration 5, loss = 0.11903042\n",
      "Iteration 6, loss = 0.10084898\n",
      "Iteration 7, loss = 0.09181560\n",
      "Iteration 8, loss = 0.07722248\n",
      "Iteration 9, loss = 0.06639360\n",
      "Iteration 10, loss = 0.06426221\n",
      "Iteration 11, loss = 0.05592063\n",
      "Iteration 12, loss = 0.04981325\n",
      "Iteration 13, loss = 0.04807141\n",
      "Iteration 14, loss = 0.04414123\n",
      "Iteration 15, loss = 0.04289821\n",
      "Iteration 16, loss = 0.03870224\n",
      "Iteration 17, loss = 0.03594533\n",
      "Iteration 18, loss = 0.03386465\n",
      "Iteration 19, loss = 0.03322058\n",
      "Iteration 20, loss = 0.03222873\n",
      "Iteration 21, loss = 0.02859296\n",
      "Iteration 22, loss = 0.02693005\n",
      "Iteration 23, loss = 0.02654318\n",
      "Iteration 24, loss = 0.02449836\n",
      "Iteration 25, loss = 0.02379817\n",
      "Iteration 26, loss = 0.02197868\n",
      "Iteration 27, loss = 0.02127737\n",
      "Iteration 28, loss = 0.02106824\n",
      "Iteration 29, loss = 0.01987292\n",
      "Iteration 30, loss = 0.01848127\n",
      "Iteration 31, loss = 0.01780117\n",
      "Iteration 32, loss = 0.01674050\n",
      "Iteration 33, loss = 0.01599648\n",
      "Iteration 34, loss = 0.01570591\n",
      "Iteration 35, loss = 0.01489586\n",
      "Iteration 36, loss = 0.01451466\n",
      "Iteration 37, loss = 0.01351594\n",
      "Iteration 38, loss = 0.01355057\n",
      "Iteration 39, loss = 0.01300040\n",
      "Iteration 40, loss = 0.01293277\n",
      "Iteration 41, loss = 0.01272983\n",
      "Iteration 42, loss = 0.01138628\n",
      "Iteration 43, loss = 0.01113758\n",
      "Iteration 44, loss = 0.01083067\n",
      "Iteration 45, loss = 0.01012067\n",
      "Iteration 46, loss = 0.01041462\n",
      "Iteration 47, loss = 0.00983660\n",
      "Iteration 48, loss = 0.00921491\n",
      "Iteration 49, loss = 0.00894779\n",
      "Iteration 50, loss = 0.00897813\n",
      "Iteration 51, loss = 0.00859326\n",
      "Iteration 52, loss = 0.00847525\n",
      "Iteration 53, loss = 0.00814966\n",
      "Iteration 54, loss = 0.00832979\n",
      "Iteration 55, loss = 0.00790984\n",
      "Iteration 56, loss = 0.00762892\n",
      "Iteration 57, loss = 0.00729059\n",
      "Iteration 58, loss = 0.00727643\n",
      "Iteration 59, loss = 0.00695939\n",
      "Iteration 60, loss = 0.00683185\n",
      "Iteration 61, loss = 0.00671484\n",
      "Iteration 62, loss = 0.00637854\n",
      "Iteration 63, loss = 0.00627511\n",
      "Iteration 64, loss = 0.00622654\n",
      "Iteration 65, loss = 0.00619005\n",
      "Iteration 66, loss = 0.00582342\n",
      "Iteration 67, loss = 0.00575442\n",
      "Iteration 68, loss = 0.00567624\n",
      "Iteration 69, loss = 0.00550452\n",
      "Iteration 70, loss = 0.00562141\n",
      "Iteration 71, loss = 0.00553506\n",
      "Iteration 72, loss = 0.00522682\n",
      "Iteration 73, loss = 0.00509408\n",
      "Iteration 74, loss = 0.00495889\n",
      "Iteration 75, loss = 0.00495071\n",
      "Iteration 76, loss = 0.00485350\n",
      "Iteration 77, loss = 0.00479913\n",
      "Iteration 78, loss = 0.00472752\n",
      "Iteration 79, loss = 0.00461603\n",
      "Iteration 80, loss = 0.00447737\n",
      "Iteration 81, loss = 0.00443442\n",
      "Iteration 82, loss = 0.00438026\n",
      "Iteration 83, loss = 0.00431030\n",
      "Iteration 84, loss = 0.00430504\n",
      "Iteration 85, loss = 0.00419877\n",
      "Iteration 86, loss = 0.00407762\n",
      "Iteration 87, loss = 0.00399076\n",
      "Iteration 88, loss = 0.00405316\n",
      "Iteration 89, loss = 0.00406563\n",
      "Iteration 90, loss = 0.00402517\n",
      "Iteration 91, loss = 0.00386382\n",
      "Iteration 92, loss = 0.00373269\n",
      "Iteration 93, loss = 0.00380193\n",
      "Iteration 94, loss = 0.00369387\n",
      "Iteration 95, loss = 0.00362287\n",
      "Iteration 96, loss = 0.00352923\n",
      "Iteration 97, loss = 0.00345706\n",
      "Iteration 98, loss = 0.00347761\n",
      "Iteration 99, loss = 0.00337685\n",
      "Iteration 100, loss = 0.00332075\n",
      "Iteration 101, loss = 0.00334605\n",
      "Iteration 102, loss = 0.00325211\n",
      "Iteration 103, loss = 0.00321132\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 1.000000\n",
      "Training set loss: 0.003211\n",
      "training: inv-scaling learning-rate\n",
      "Iteration 1, loss = 2.16498699\n",
      "Iteration 2, loss = 1.90870076\n",
      "Iteration 3, loss = 1.90020042\n",
      "Iteration 4, loss = 1.89396007\n",
      "Iteration 5, loss = 1.88874063\n",
      "Iteration 6, loss = 1.88420884\n",
      "Iteration 7, loss = 1.88008198\n",
      "Iteration 8, loss = 1.87632970\n",
      "Iteration 9, loss = 1.87283244\n",
      "Iteration 10, loss = 1.86956168\n",
      "Iteration 11, loss = 1.86646120\n",
      "Iteration 12, loss = 1.86352830\n",
      "Iteration 13, loss = 1.86073380\n",
      "Iteration 14, loss = 1.85802879\n",
      "Iteration 15, loss = 1.85546541\n",
      "Iteration 16, loss = 1.85296455\n",
      "Iteration 17, loss = 1.85055307\n",
      "Iteration 18, loss = 1.84823971\n",
      "Iteration 19, loss = 1.84597284\n",
      "Iteration 20, loss = 1.84375621\n",
      "Iteration 21, loss = 1.84162808\n",
      "Iteration 22, loss = 1.83952686\n",
      "Iteration 23, loss = 1.83749175\n",
      "Iteration 24, loss = 1.83548885\n",
      "Iteration 25, loss = 1.83354117\n",
      "Iteration 26, loss = 1.83163157\n",
      "Iteration 27, loss = 1.82975799\n",
      "Iteration 28, loss = 1.82792425\n",
      "Iteration 29, loss = 1.82612052\n",
      "Iteration 30, loss = 1.82434522\n",
      "Iteration 31, loss = 1.82260947\n",
      "Iteration 32, loss = 1.82089044\n",
      "Iteration 33, loss = 1.81921385\n",
      "Iteration 34, loss = 1.81755508\n",
      "Iteration 35, loss = 1.81591149\n",
      "Iteration 36, loss = 1.81430143\n",
      "Iteration 37, loss = 1.81271937\n",
      "Iteration 38, loss = 1.81115748\n",
      "Iteration 39, loss = 1.80961505\n",
      "Iteration 40, loss = 1.80807800\n",
      "Iteration 41, loss = 1.80657755\n",
      "Iteration 42, loss = 1.80509236\n",
      "Iteration 43, loss = 1.80362457\n",
      "Iteration 44, loss = 1.80217673\n",
      "Iteration 45, loss = 1.80073744\n",
      "Iteration 46, loss = 1.79932099\n",
      "Iteration 47, loss = 1.79791841\n",
      "Iteration 48, loss = 1.79653333\n",
      "Iteration 49, loss = 1.79515679\n",
      "Iteration 50, loss = 1.79379562\n",
      "Iteration 51, loss = 1.79244900\n",
      "Iteration 52, loss = 1.79111982\n",
      "Iteration 53, loss = 1.78980221\n",
      "Iteration 54, loss = 1.78849656\n",
      "Iteration 55, loss = 1.78720038\n",
      "Iteration 56, loss = 1.78591643\n",
      "Iteration 57, loss = 1.78465102\n",
      "Iteration 58, loss = 1.78338176\n",
      "Iteration 59, loss = 1.78213472\n",
      "Iteration 60, loss = 1.78090498\n",
      "Iteration 61, loss = 1.77967285\n",
      "Iteration 62, loss = 1.77844750\n",
      "Iteration 63, loss = 1.77723861\n",
      "Iteration 64, loss = 1.77604526\n",
      "Iteration 65, loss = 1.77485570\n",
      "Iteration 66, loss = 1.77366849\n",
      "Iteration 67, loss = 1.77249900\n",
      "Iteration 68, loss = 1.77133538\n",
      "Iteration 69, loss = 1.77017246\n",
      "Iteration 70, loss = 1.76902925\n",
      "Iteration 71, loss = 1.76788966\n",
      "Iteration 72, loss = 1.76675757\n",
      "Iteration 73, loss = 1.76563542\n",
      "Iteration 74, loss = 1.76451627\n",
      "Iteration 75, loss = 1.76340284\n",
      "Iteration 76, loss = 1.76230331\n",
      "Iteration 77, loss = 1.76120621\n",
      "Iteration 78, loss = 1.76011935\n",
      "Iteration 79, loss = 1.75903650\n",
      "Iteration 80, loss = 1.75796089\n",
      "Iteration 81, loss = 1.75689492\n",
      "Iteration 82, loss = 1.75583046\n",
      "Iteration 83, loss = 1.75477146\n",
      "Iteration 84, loss = 1.75372171\n",
      "Iteration 85, loss = 1.75267681\n",
      "Iteration 86, loss = 1.75164496\n",
      "Iteration 87, loss = 1.75061311\n",
      "Iteration 88, loss = 1.74958655\n",
      "Iteration 89, loss = 1.74856215\n",
      "Iteration 90, loss = 1.74755564\n",
      "Iteration 91, loss = 1.74654579\n",
      "Iteration 92, loss = 1.74553883\n",
      "Iteration 93, loss = 1.74454888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 94, loss = 1.74355099\n",
      "Iteration 95, loss = 1.74256480\n",
      "Iteration 96, loss = 1.74158415\n",
      "Iteration 97, loss = 1.74060671\n",
      "Iteration 98, loss = 1.73963496\n",
      "Iteration 99, loss = 1.73867134\n",
      "Iteration 100, loss = 1.73770373\n",
      "Iteration 101, loss = 1.73675242\n",
      "Iteration 102, loss = 1.73579705\n",
      "Iteration 103, loss = 1.73484656\n",
      "Iteration 104, loss = 1.73390272\n",
      "Iteration 105, loss = 1.73296570\n",
      "Iteration 106, loss = 1.73202927\n",
      "Iteration 107, loss = 1.73110500\n",
      "Iteration 108, loss = 1.73017292\n",
      "Iteration 109, loss = 1.72924995\n",
      "Iteration 110, loss = 1.72833766\n",
      "Iteration 111, loss = 1.72742023\n",
      "Iteration 112, loss = 1.72651042\n",
      "Iteration 113, loss = 1.72560933\n",
      "Iteration 114, loss = 1.72471141\n",
      "Iteration 115, loss = 1.72381654\n",
      "Iteration 116, loss = 1.72292001\n",
      "Iteration 117, loss = 1.72202817\n",
      "Iteration 118, loss = 1.72114131\n",
      "Iteration 119, loss = 1.72025877\n",
      "Iteration 120, loss = 1.71938683\n",
      "Iteration 121, loss = 1.71851043\n",
      "Iteration 122, loss = 1.71764045\n",
      "Iteration 123, loss = 1.71677214\n",
      "Iteration 124, loss = 1.71591252\n",
      "Iteration 125, loss = 1.71505229\n",
      "Iteration 126, loss = 1.71419832\n",
      "Iteration 127, loss = 1.71334119\n",
      "Iteration 128, loss = 1.71249343\n",
      "Iteration 129, loss = 1.71164699\n",
      "Iteration 130, loss = 1.71080489\n",
      "Iteration 131, loss = 1.70996200\n",
      "Iteration 132, loss = 1.70912387\n",
      "Iteration 133, loss = 1.70829309\n",
      "Iteration 134, loss = 1.70746397\n",
      "Iteration 135, loss = 1.70663507\n",
      "Iteration 136, loss = 1.70580856\n",
      "Iteration 137, loss = 1.70498833\n",
      "Iteration 138, loss = 1.70417188\n",
      "Iteration 139, loss = 1.70335643\n",
      "Iteration 140, loss = 1.70254260\n",
      "Iteration 141, loss = 1.70173589\n",
      "Iteration 142, loss = 1.70093217\n",
      "Iteration 143, loss = 1.70012325\n",
      "Iteration 144, loss = 1.69932313\n",
      "Iteration 145, loss = 1.69852049\n",
      "Iteration 146, loss = 1.69772446\n",
      "Iteration 147, loss = 1.69693438\n",
      "Iteration 148, loss = 1.69614193\n",
      "Iteration 149, loss = 1.69535399\n",
      "Iteration 150, loss = 1.69456560\n",
      "Iteration 151, loss = 1.69378370\n",
      "Iteration 152, loss = 1.69300156\n",
      "Iteration 153, loss = 1.69222078\n",
      "Iteration 154, loss = 1.69144454\n",
      "Iteration 155, loss = 1.69067404\n",
      "Iteration 156, loss = 1.68989985\n",
      "Iteration 157, loss = 1.68913366\n",
      "Iteration 158, loss = 1.68836444\n",
      "Iteration 159, loss = 1.68760238\n",
      "Iteration 160, loss = 1.68683804\n",
      "Iteration 161, loss = 1.68608277\n",
      "Iteration 162, loss = 1.68532307\n",
      "Iteration 163, loss = 1.68456905\n",
      "Iteration 164, loss = 1.68381655\n",
      "Iteration 165, loss = 1.68306602\n",
      "Iteration 166, loss = 1.68232662\n",
      "Iteration 167, loss = 1.68157301\n",
      "Iteration 168, loss = 1.68083213\n",
      "Iteration 169, loss = 1.68009001\n",
      "Iteration 170, loss = 1.67934837\n",
      "Iteration 171, loss = 1.67861070\n",
      "Iteration 172, loss = 1.67787531\n",
      "Iteration 173, loss = 1.67714090\n",
      "Iteration 174, loss = 1.67641429\n",
      "Iteration 175, loss = 1.67568295\n",
      "Iteration 176, loss = 1.67495646\n",
      "Iteration 177, loss = 1.67423232\n",
      "Iteration 178, loss = 1.67350737\n",
      "Iteration 179, loss = 1.67278890\n",
      "Iteration 180, loss = 1.67206728\n",
      "Iteration 181, loss = 1.67135148\n",
      "Iteration 182, loss = 1.67063675\n",
      "Iteration 183, loss = 1.66992838\n",
      "Iteration 184, loss = 1.66921162\n",
      "Iteration 185, loss = 1.66850679\n",
      "Iteration 186, loss = 1.66779601\n",
      "Iteration 187, loss = 1.66708924\n",
      "Iteration 188, loss = 1.66638609\n",
      "Iteration 189, loss = 1.66568903\n",
      "Iteration 190, loss = 1.66498554\n",
      "Iteration 191, loss = 1.66429147\n",
      "Iteration 192, loss = 1.66359448\n",
      "Iteration 193, loss = 1.66289981\n",
      "Iteration 194, loss = 1.66220525\n",
      "Iteration 195, loss = 1.66151817\n",
      "Iteration 196, loss = 1.66082877\n",
      "Iteration 197, loss = 1.66013905\n",
      "Iteration 198, loss = 1.65945611\n",
      "Iteration 199, loss = 1.65877108\n",
      "Iteration 200, loss = 1.65809053\n",
      "Iteration 201, loss = 1.65740969\n",
      "Iteration 202, loss = 1.65673013\n",
      "Iteration 203, loss = 1.65605459\n",
      "Iteration 204, loss = 1.65537974\n",
      "Iteration 205, loss = 1.65470937\n",
      "Iteration 206, loss = 1.65403330\n",
      "Iteration 207, loss = 1.65336652\n",
      "Iteration 208, loss = 1.65269652\n",
      "Iteration 209, loss = 1.65203078\n",
      "Iteration 210, loss = 1.65136618\n",
      "Iteration 211, loss = 1.65070211\n",
      "Iteration 212, loss = 1.65003678\n",
      "Iteration 213, loss = 1.64937675\n",
      "Iteration 214, loss = 1.64871815\n",
      "Iteration 215, loss = 1.64806042\n",
      "Iteration 216, loss = 1.64740142\n",
      "Iteration 217, loss = 1.64674590\n",
      "Iteration 218, loss = 1.64609587\n",
      "Iteration 219, loss = 1.64544380\n",
      "Iteration 220, loss = 1.64479009\n",
      "Iteration 221, loss = 1.64414481\n",
      "Iteration 222, loss = 1.64349564\n",
      "Iteration 223, loss = 1.64285101\n",
      "Iteration 224, loss = 1.64220816\n",
      "Iteration 225, loss = 1.64156313\n",
      "Iteration 226, loss = 1.64091714\n",
      "Iteration 227, loss = 1.64027747\n",
      "Iteration 228, loss = 1.63964121\n",
      "Iteration 229, loss = 1.63900109\n",
      "Iteration 230, loss = 1.63836524\n",
      "Iteration 231, loss = 1.63772965\n",
      "Iteration 232, loss = 1.63709675\n",
      "Iteration 233, loss = 1.63646380\n",
      "Iteration 234, loss = 1.63583157\n",
      "Iteration 235, loss = 1.63520508\n",
      "Iteration 236, loss = 1.63457548\n",
      "Iteration 237, loss = 1.63394598\n",
      "Iteration 238, loss = 1.63332093\n",
      "Iteration 239, loss = 1.63269652\n",
      "Iteration 240, loss = 1.63207284\n",
      "Iteration 241, loss = 1.63145158\n",
      "Iteration 242, loss = 1.63083518\n",
      "Iteration 243, loss = 1.63021448\n",
      "Iteration 244, loss = 1.62959370\n",
      "Iteration 245, loss = 1.62897848\n",
      "Iteration 246, loss = 1.62836435\n",
      "Iteration 247, loss = 1.62774870\n",
      "Iteration 248, loss = 1.62713445\n",
      "Iteration 249, loss = 1.62652621\n",
      "Iteration 250, loss = 1.62591875\n",
      "Iteration 251, loss = 1.62530934\n",
      "Iteration 252, loss = 1.62470310\n",
      "Iteration 253, loss = 1.62409489\n",
      "Iteration 254, loss = 1.62349000\n",
      "Iteration 255, loss = 1.62288661\n",
      "Iteration 256, loss = 1.62228519\n",
      "Iteration 257, loss = 1.62168578\n",
      "Iteration 258, loss = 1.62108098\n",
      "Iteration 259, loss = 1.62048127\n",
      "Iteration 260, loss = 1.61988297\n",
      "Iteration 261, loss = 1.61928805\n",
      "Iteration 262, loss = 1.61869245\n",
      "Iteration 263, loss = 1.61809786\n",
      "Iteration 264, loss = 1.61750768\n",
      "Iteration 265, loss = 1.61691374\n",
      "Iteration 266, loss = 1.61632250\n",
      "Iteration 267, loss = 1.61573079\n",
      "Iteration 268, loss = 1.61513976\n",
      "Iteration 269, loss = 1.61455130\n",
      "Iteration 270, loss = 1.61396546\n",
      "Iteration 271, loss = 1.61337962\n",
      "Iteration 272, loss = 1.61279544\n",
      "Iteration 273, loss = 1.61221208\n",
      "Iteration 274, loss = 1.61162685\n",
      "Iteration 275, loss = 1.61104978\n",
      "Iteration 276, loss = 1.61046678\n",
      "Iteration 277, loss = 1.60988712\n",
      "Iteration 278, loss = 1.60930944\n",
      "Iteration 279, loss = 1.60873074\n",
      "Iteration 280, loss = 1.60815322\n",
      "Iteration 281, loss = 1.60757857\n",
      "Iteration 282, loss = 1.60700453\n",
      "Iteration 283, loss = 1.60642973\n",
      "Iteration 284, loss = 1.60585850\n",
      "Iteration 285, loss = 1.60528700\n",
      "Iteration 286, loss = 1.60471622\n",
      "Iteration 287, loss = 1.60414728\n",
      "Iteration 288, loss = 1.60358122\n",
      "Iteration 289, loss = 1.60300967\n",
      "Iteration 290, loss = 1.60244502\n",
      "Iteration 291, loss = 1.60188200\n",
      "Iteration 292, loss = 1.60131889\n",
      "Iteration 293, loss = 1.60075550\n",
      "Iteration 294, loss = 1.60019306\n",
      "Iteration 295, loss = 1.59963158\n",
      "Iteration 296, loss = 1.59906884\n",
      "Iteration 297, loss = 1.59851026\n",
      "Iteration 298, loss = 1.59795318\n",
      "Iteration 299, loss = 1.59739319\n",
      "Iteration 300, loss = 1.59683715\n",
      "Iteration 301, loss = 1.59628108\n",
      "Iteration 302, loss = 1.59572367\n",
      "Iteration 303, loss = 1.59517265\n",
      "Iteration 304, loss = 1.59461872\n",
      "Iteration 305, loss = 1.59406876\n",
      "Iteration 306, loss = 1.59351576\n",
      "Iteration 307, loss = 1.59296245\n",
      "Iteration 308, loss = 1.59241619\n",
      "Iteration 309, loss = 1.59186548\n",
      "Iteration 310, loss = 1.59131958\n",
      "Iteration 311, loss = 1.59077439\n",
      "Iteration 312, loss = 1.59022581\n",
      "Iteration 313, loss = 1.58968201\n",
      "Iteration 314, loss = 1.58913713\n",
      "Iteration 315, loss = 1.58859455\n",
      "Iteration 316, loss = 1.58805289\n",
      "Iteration 317, loss = 1.58751400\n",
      "Iteration 318, loss = 1.58697179\n",
      "Iteration 319, loss = 1.58643253\n",
      "Iteration 320, loss = 1.58589117\n",
      "Iteration 321, loss = 1.58535331\n",
      "Iteration 322, loss = 1.58481866\n",
      "Iteration 323, loss = 1.58428031\n",
      "Iteration 324, loss = 1.58374783\n",
      "Iteration 325, loss = 1.58321133\n",
      "Iteration 326, loss = 1.58267574\n",
      "Iteration 327, loss = 1.58214550\n",
      "Iteration 328, loss = 1.58161218\n",
      "Iteration 329, loss = 1.58108169\n",
      "Iteration 330, loss = 1.58055215\n",
      "Iteration 331, loss = 1.58001974\n",
      "Iteration 332, loss = 1.57949037\n",
      "Iteration 333, loss = 1.57896173\n",
      "Iteration 334, loss = 1.57843595\n",
      "Iteration 335, loss = 1.57790944\n",
      "Iteration 336, loss = 1.57738367\n",
      "Iteration 337, loss = 1.57686354\n",
      "Iteration 338, loss = 1.57633740\n",
      "Iteration 339, loss = 1.57581348\n",
      "Iteration 340, loss = 1.57529094\n",
      "Iteration 341, loss = 1.57477224\n",
      "Iteration 342, loss = 1.57425097\n",
      "Iteration 343, loss = 1.57373230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 344, loss = 1.57321294\n",
      "Iteration 345, loss = 1.57269424\n",
      "Iteration 346, loss = 1.57217594\n",
      "Iteration 347, loss = 1.57166154\n",
      "Iteration 348, loss = 1.57114842\n",
      "Iteration 349, loss = 1.57063308\n",
      "Iteration 350, loss = 1.57011989\n",
      "Iteration 351, loss = 1.56960637\n",
      "Iteration 352, loss = 1.56909525\n",
      "Iteration 353, loss = 1.56858229\n",
      "Iteration 354, loss = 1.56807312\n",
      "Iteration 355, loss = 1.56756350\n",
      "Iteration 356, loss = 1.56705407\n",
      "Iteration 357, loss = 1.56654768\n",
      "Iteration 358, loss = 1.56603912\n",
      "Iteration 359, loss = 1.56553577\n",
      "Iteration 360, loss = 1.56502946\n",
      "Iteration 361, loss = 1.56452293\n",
      "Iteration 362, loss = 1.56401946\n",
      "Iteration 363, loss = 1.56351503\n",
      "Iteration 364, loss = 1.56300976\n",
      "Iteration 365, loss = 1.56251181\n",
      "Iteration 366, loss = 1.56200867\n",
      "Iteration 367, loss = 1.56150813\n",
      "Iteration 368, loss = 1.56100839\n",
      "Iteration 369, loss = 1.56050804\n",
      "Iteration 370, loss = 1.56000862\n",
      "Iteration 371, loss = 1.55951196\n",
      "Iteration 372, loss = 1.55901283\n",
      "Iteration 373, loss = 1.55852012\n",
      "Iteration 374, loss = 1.55802139\n",
      "Iteration 375, loss = 1.55752660\n",
      "Iteration 376, loss = 1.55703589\n",
      "Iteration 377, loss = 1.55653873\n",
      "Iteration 378, loss = 1.55605078\n",
      "Iteration 379, loss = 1.55555477\n",
      "Iteration 380, loss = 1.55506403\n",
      "Iteration 381, loss = 1.55457309\n",
      "Iteration 382, loss = 1.55408523\n",
      "Iteration 383, loss = 1.55359525\n",
      "Iteration 384, loss = 1.55310523\n",
      "Iteration 385, loss = 1.55261811\n",
      "Iteration 386, loss = 1.55213270\n",
      "Iteration 387, loss = 1.55164445\n",
      "Iteration 388, loss = 1.55116268\n",
      "Iteration 389, loss = 1.55067303\n",
      "Iteration 390, loss = 1.55018813\n",
      "Iteration 391, loss = 1.54970557\n",
      "Iteration 392, loss = 1.54922236\n",
      "Iteration 393, loss = 1.54874075\n",
      "Iteration 394, loss = 1.54825718\n",
      "Iteration 395, loss = 1.54777598\n",
      "Iteration 396, loss = 1.54729669\n",
      "Iteration 397, loss = 1.54681537\n",
      "Iteration 398, loss = 1.54633583\n",
      "Iteration 399, loss = 1.54585785\n",
      "Iteration 400, loss = 1.54538045\n",
      "Iteration 401, loss = 1.54490169\n",
      "Iteration 402, loss = 1.54442487\n",
      "Iteration 403, loss = 1.54394949\n",
      "Iteration 404, loss = 1.54347404\n",
      "Iteration 405, loss = 1.54299923\n",
      "Iteration 406, loss = 1.54252551\n",
      "Iteration 407, loss = 1.54205185\n",
      "Iteration 408, loss = 1.54158165\n",
      "Iteration 409, loss = 1.54110854\n",
      "Iteration 410, loss = 1.54063772\n",
      "Iteration 411, loss = 1.54016520\n",
      "Iteration 412, loss = 1.53969432\n",
      "Iteration 413, loss = 1.53922390\n",
      "Iteration 414, loss = 1.53875586\n",
      "Iteration 415, loss = 1.53828710\n",
      "Iteration 416, loss = 1.53781954\n",
      "Iteration 417, loss = 1.53735205\n",
      "Iteration 418, loss = 1.53688441\n",
      "Iteration 419, loss = 1.53641876\n",
      "Iteration 420, loss = 1.53595378\n",
      "Iteration 421, loss = 1.53549058\n",
      "Iteration 422, loss = 1.53502233\n",
      "Iteration 423, loss = 1.53456083\n",
      "Iteration 424, loss = 1.53409714\n",
      "Iteration 425, loss = 1.53363398\n",
      "Iteration 426, loss = 1.53317458\n",
      "Iteration 427, loss = 1.53271286\n",
      "Iteration 428, loss = 1.53224902\n",
      "Iteration 429, loss = 1.53179250\n",
      "Iteration 430, loss = 1.53132876\n",
      "Iteration 431, loss = 1.53087348\n",
      "Iteration 432, loss = 1.53041398\n",
      "Iteration 433, loss = 1.52995503\n",
      "Iteration 434, loss = 1.52950017\n",
      "Iteration 435, loss = 1.52904186\n",
      "Iteration 436, loss = 1.52858521\n",
      "Iteration 437, loss = 1.52813439\n",
      "Iteration 438, loss = 1.52767478\n",
      "Iteration 439, loss = 1.52722129\n",
      "Iteration 440, loss = 1.52676771\n",
      "Iteration 441, loss = 1.52631470\n",
      "Iteration 442, loss = 1.52586274\n",
      "Iteration 443, loss = 1.52541269\n",
      "Iteration 444, loss = 1.52495758\n",
      "Iteration 445, loss = 1.52450465\n",
      "Iteration 446, loss = 1.52405708\n",
      "Iteration 447, loss = 1.52360589\n",
      "Iteration 448, loss = 1.52315826\n",
      "Iteration 449, loss = 1.52270790\n",
      "Iteration 450, loss = 1.52226042\n",
      "Iteration 451, loss = 1.52181221\n",
      "Iteration 452, loss = 1.52136741\n",
      "Iteration 453, loss = 1.52092278\n",
      "Iteration 454, loss = 1.52047481\n",
      "Iteration 455, loss = 1.52003004\n",
      "Iteration 456, loss = 1.51958440\n",
      "Iteration 457, loss = 1.51913901\n",
      "Iteration 458, loss = 1.51869703\n",
      "Iteration 459, loss = 1.51825207\n",
      "Iteration 460, loss = 1.51780730\n",
      "Iteration 461, loss = 1.51736959\n",
      "Iteration 462, loss = 1.51692644\n",
      "Iteration 463, loss = 1.51648737\n",
      "Iteration 464, loss = 1.51604737\n",
      "Iteration 465, loss = 1.51560420\n",
      "Iteration 466, loss = 1.51516336\n",
      "Iteration 467, loss = 1.51472663\n",
      "Iteration 468, loss = 1.51428815\n",
      "Iteration 469, loss = 1.51385024\n",
      "Iteration 470, loss = 1.51341222\n",
      "Iteration 471, loss = 1.51297778\n",
      "Iteration 472, loss = 1.51254048\n",
      "Iteration 473, loss = 1.51210600\n",
      "Iteration 474, loss = 1.51166900\n",
      "Iteration 475, loss = 1.51123390\n",
      "Iteration 476, loss = 1.51079948\n",
      "Iteration 477, loss = 1.51036410\n",
      "Iteration 478, loss = 1.50993126\n",
      "Iteration 479, loss = 1.50949904\n",
      "Iteration 480, loss = 1.50906474\n",
      "Iteration 481, loss = 1.50863502\n",
      "Iteration 482, loss = 1.50820118\n",
      "Iteration 483, loss = 1.50776929\n",
      "Iteration 484, loss = 1.50734039\n",
      "Iteration 485, loss = 1.50691215\n",
      "Iteration 486, loss = 1.50647875\n",
      "Iteration 487, loss = 1.50605287\n",
      "Iteration 488, loss = 1.50562321\n",
      "Iteration 489, loss = 1.50519651\n",
      "Iteration 490, loss = 1.50476657\n",
      "Iteration 491, loss = 1.50434013\n",
      "Iteration 492, loss = 1.50391469\n",
      "Iteration 493, loss = 1.50348892\n",
      "Iteration 494, loss = 1.50306235\n",
      "Iteration 495, loss = 1.50263644\n",
      "Iteration 496, loss = 1.50221360\n",
      "Iteration 497, loss = 1.50179016\n",
      "Iteration 498, loss = 1.50136558\n",
      "Iteration 499, loss = 1.50094210\n",
      "Iteration 500, loss = 1.50052246\n",
      "Iteration 501, loss = 1.50009617\n",
      "Iteration 502, loss = 1.49967672\n",
      "Iteration 503, loss = 1.49925555\n",
      "Iteration 504, loss = 1.49883166\n",
      "Iteration 505, loss = 1.49840900\n",
      "Iteration 506, loss = 1.49799049\n",
      "Iteration 507, loss = 1.49756983\n",
      "Iteration 508, loss = 1.49715289\n",
      "Iteration 509, loss = 1.49673026\n",
      "Iteration 510, loss = 1.49631270\n",
      "Iteration 511, loss = 1.49589410\n",
      "Iteration 512, loss = 1.49547815\n",
      "Iteration 513, loss = 1.49506232\n",
      "Iteration 514, loss = 1.49464514\n",
      "Iteration 515, loss = 1.49422710\n",
      "Iteration 516, loss = 1.49381040\n",
      "Iteration 517, loss = 1.49339476\n",
      "Iteration 518, loss = 1.49298012\n",
      "Iteration 519, loss = 1.49256463\n",
      "Iteration 520, loss = 1.49215036\n",
      "Iteration 521, loss = 1.49173946\n",
      "Iteration 522, loss = 1.49132472\n",
      "Iteration 523, loss = 1.49090864\n",
      "Iteration 524, loss = 1.49049674\n",
      "Iteration 525, loss = 1.49008463\n",
      "Iteration 526, loss = 1.48967370\n",
      "Iteration 527, loss = 1.48926206\n",
      "Iteration 528, loss = 1.48885121\n",
      "Iteration 529, loss = 1.48844032\n",
      "Iteration 530, loss = 1.48803042\n",
      "Iteration 531, loss = 1.48762098\n",
      "Iteration 532, loss = 1.48721243\n",
      "Iteration 533, loss = 1.48680427\n",
      "Iteration 534, loss = 1.48639544\n",
      "Iteration 535, loss = 1.48598533\n",
      "Iteration 536, loss = 1.48558065\n",
      "Iteration 537, loss = 1.48517278\n",
      "Iteration 538, loss = 1.48476718\n",
      "Iteration 539, loss = 1.48436090\n",
      "Iteration 540, loss = 1.48395405\n",
      "Iteration 541, loss = 1.48354878\n",
      "Iteration 542, loss = 1.48314296\n",
      "Iteration 543, loss = 1.48274140\n",
      "Iteration 544, loss = 1.48233665\n",
      "Iteration 545, loss = 1.48193526\n",
      "Iteration 546, loss = 1.48153172\n",
      "Iteration 547, loss = 1.48112775\n",
      "Iteration 548, loss = 1.48072511\n",
      "Iteration 549, loss = 1.48032477\n",
      "Iteration 550, loss = 1.47992090\n",
      "Iteration 551, loss = 1.47952332\n",
      "Iteration 552, loss = 1.47912029\n",
      "Iteration 553, loss = 1.47871908\n",
      "Iteration 554, loss = 1.47832210\n",
      "Iteration 555, loss = 1.47792024\n",
      "Iteration 556, loss = 1.47752296\n",
      "Iteration 557, loss = 1.47712574\n",
      "Iteration 558, loss = 1.47672693\n",
      "Iteration 559, loss = 1.47632935\n",
      "Iteration 560, loss = 1.47593337\n",
      "Iteration 561, loss = 1.47553650\n",
      "Iteration 562, loss = 1.47514216\n",
      "Iteration 563, loss = 1.47474638\n",
      "Iteration 564, loss = 1.47435124\n",
      "Iteration 565, loss = 1.47395328\n",
      "Iteration 566, loss = 1.47355930\n",
      "Iteration 567, loss = 1.47316782\n",
      "Iteration 568, loss = 1.47277305\n",
      "Iteration 569, loss = 1.47237866\n",
      "Iteration 570, loss = 1.47198753\n",
      "Iteration 571, loss = 1.47159527\n",
      "Iteration 572, loss = 1.47120248\n",
      "Iteration 573, loss = 1.47080908\n",
      "Iteration 574, loss = 1.47041818\n",
      "Iteration 575, loss = 1.47002957\n",
      "Iteration 576, loss = 1.46963708\n",
      "Iteration 577, loss = 1.46924678\n",
      "Iteration 578, loss = 1.46885758\n",
      "Iteration 579, loss = 1.46846718\n",
      "Iteration 580, loss = 1.46807788\n",
      "Iteration 581, loss = 1.46769218\n",
      "Iteration 582, loss = 1.46730150\n",
      "Iteration 583, loss = 1.46691436\n",
      "Iteration 584, loss = 1.46652854\n",
      "Iteration 585, loss = 1.46613811\n",
      "Iteration 586, loss = 1.46575245\n",
      "Iteration 587, loss = 1.46536592\n",
      "Iteration 588, loss = 1.46498110\n",
      "Iteration 589, loss = 1.46459519\n",
      "Iteration 590, loss = 1.46420782\n",
      "Iteration 591, loss = 1.46382348\n",
      "Iteration 592, loss = 1.46343804\n",
      "Iteration 593, loss = 1.46305581\n",
      "Iteration 594, loss = 1.46267233\n",
      "Iteration 595, loss = 1.46228787\n",
      "Iteration 596, loss = 1.46190542\n",
      "Iteration 597, loss = 1.46152353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 598, loss = 1.46113919\n",
      "Iteration 599, loss = 1.46075670\n",
      "Iteration 600, loss = 1.46037540\n",
      "Iteration 601, loss = 1.45999654\n",
      "Iteration 602, loss = 1.45961329\n",
      "Iteration 603, loss = 1.45923150\n",
      "Iteration 604, loss = 1.45885048\n",
      "Iteration 605, loss = 1.45847475\n",
      "Iteration 606, loss = 1.45809255\n",
      "Iteration 607, loss = 1.45771411\n",
      "Iteration 608, loss = 1.45733398\n",
      "Iteration 609, loss = 1.45695718\n",
      "Iteration 610, loss = 1.45657922\n",
      "Iteration 611, loss = 1.45620101\n",
      "Iteration 612, loss = 1.45582277\n",
      "Iteration 613, loss = 1.45544454\n",
      "Iteration 614, loss = 1.45506859\n",
      "Iteration 615, loss = 1.45469319\n",
      "Iteration 616, loss = 1.45431698\n",
      "Iteration 617, loss = 1.45394014\n",
      "Iteration 618, loss = 1.45356549\n",
      "Iteration 619, loss = 1.45318955\n",
      "Iteration 620, loss = 1.45281646\n",
      "Iteration 621, loss = 1.45244384\n",
      "Iteration 622, loss = 1.45206940\n",
      "Iteration 623, loss = 1.45169536\n",
      "Iteration 624, loss = 1.45132248\n",
      "Iteration 625, loss = 1.45094816\n",
      "Iteration 626, loss = 1.45057803\n",
      "Iteration 627, loss = 1.45020701\n",
      "Iteration 628, loss = 1.44983367\n",
      "Iteration 629, loss = 1.44946226\n",
      "Iteration 630, loss = 1.44909054\n",
      "Iteration 631, loss = 1.44872048\n",
      "Iteration 632, loss = 1.44835188\n",
      "Iteration 633, loss = 1.44798037\n",
      "Iteration 634, loss = 1.44761119\n",
      "Iteration 635, loss = 1.44724017\n",
      "Iteration 636, loss = 1.44687282\n",
      "Iteration 637, loss = 1.44650257\n",
      "Iteration 638, loss = 1.44613534\n",
      "Iteration 639, loss = 1.44576760\n",
      "Iteration 640, loss = 1.44540005\n",
      "Iteration 641, loss = 1.44503020\n",
      "Iteration 642, loss = 1.44466698\n",
      "Iteration 643, loss = 1.44429887\n",
      "Iteration 644, loss = 1.44393121\n",
      "Iteration 645, loss = 1.44356630\n",
      "Iteration 646, loss = 1.44320136\n",
      "Iteration 647, loss = 1.44283504\n",
      "Iteration 648, loss = 1.44246907\n",
      "Iteration 649, loss = 1.44210331\n",
      "Iteration 650, loss = 1.44174016\n",
      "Iteration 651, loss = 1.44137507\n",
      "Iteration 652, loss = 1.44101280\n",
      "Iteration 653, loss = 1.44064913\n",
      "Iteration 654, loss = 1.44028391\n",
      "Iteration 655, loss = 1.43992181\n",
      "Iteration 656, loss = 1.43955949\n",
      "Iteration 657, loss = 1.43919526\n",
      "Iteration 658, loss = 1.43883284\n",
      "Iteration 659, loss = 1.43847285\n",
      "Iteration 660, loss = 1.43810992\n",
      "Iteration 661, loss = 1.43775102\n",
      "Iteration 662, loss = 1.43739041\n",
      "Iteration 663, loss = 1.43702853\n",
      "Iteration 664, loss = 1.43666921\n",
      "Iteration 665, loss = 1.43630785\n",
      "Iteration 666, loss = 1.43595181\n",
      "Iteration 667, loss = 1.43558896\n",
      "Iteration 668, loss = 1.43523070\n",
      "Iteration 669, loss = 1.43487348\n",
      "Iteration 670, loss = 1.43451420\n",
      "Iteration 671, loss = 1.43415661\n",
      "Iteration 672, loss = 1.43380044\n",
      "Iteration 673, loss = 1.43344152\n",
      "Iteration 674, loss = 1.43308454\n",
      "Iteration 675, loss = 1.43272959\n",
      "Iteration 676, loss = 1.43237551\n",
      "Iteration 677, loss = 1.43201704\n",
      "Iteration 678, loss = 1.43166270\n",
      "Iteration 679, loss = 1.43130868\n",
      "Iteration 680, loss = 1.43095196\n",
      "Iteration 681, loss = 1.43059848\n",
      "Iteration 682, loss = 1.43024473\n",
      "Iteration 683, loss = 1.42988915\n",
      "Iteration 684, loss = 1.42953616\n",
      "Iteration 685, loss = 1.42918225\n",
      "Iteration 686, loss = 1.42883195\n",
      "Iteration 687, loss = 1.42847720\n",
      "Iteration 688, loss = 1.42812797\n",
      "Iteration 689, loss = 1.42777068\n",
      "Iteration 690, loss = 1.42742182\n",
      "Iteration 691, loss = 1.42707063\n",
      "Iteration 692, loss = 1.42671895\n",
      "Iteration 693, loss = 1.42636832\n",
      "Iteration 694, loss = 1.42601746\n",
      "Iteration 695, loss = 1.42566849\n",
      "Iteration 696, loss = 1.42531745\n",
      "Iteration 697, loss = 1.42496826\n",
      "Iteration 698, loss = 1.42461781\n",
      "Iteration 699, loss = 1.42427127\n",
      "Iteration 700, loss = 1.42392075\n",
      "Iteration 701, loss = 1.42357320\n",
      "Iteration 702, loss = 1.42322574\n",
      "Iteration 703, loss = 1.42287685\n",
      "Iteration 704, loss = 1.42252941\n",
      "Iteration 705, loss = 1.42218271\n",
      "Iteration 706, loss = 1.42183451\n",
      "Iteration 707, loss = 1.42148775\n",
      "Iteration 708, loss = 1.42114371\n",
      "Iteration 709, loss = 1.42079599\n",
      "Iteration 710, loss = 1.42044979\n",
      "Iteration 711, loss = 1.42010534\n",
      "Iteration 712, loss = 1.41975810\n",
      "Iteration 713, loss = 1.41941431\n",
      "Iteration 714, loss = 1.41906932\n",
      "Iteration 715, loss = 1.41872434\n",
      "Iteration 716, loss = 1.41838155\n",
      "Iteration 717, loss = 1.41803583\n",
      "Iteration 718, loss = 1.41769291\n",
      "Iteration 719, loss = 1.41735152\n",
      "Iteration 720, loss = 1.41700661\n",
      "Iteration 721, loss = 1.41666372\n",
      "Iteration 722, loss = 1.41631992\n",
      "Iteration 723, loss = 1.41597681\n",
      "Iteration 724, loss = 1.41563357\n",
      "Iteration 725, loss = 1.41529310\n",
      "Iteration 726, loss = 1.41495372\n",
      "Iteration 727, loss = 1.41460838\n",
      "Iteration 728, loss = 1.41426835\n",
      "Iteration 729, loss = 1.41392556\n",
      "Iteration 730, loss = 1.41358718\n",
      "Iteration 731, loss = 1.41324712\n",
      "Iteration 732, loss = 1.41290644\n",
      "Iteration 733, loss = 1.41256771\n",
      "Iteration 734, loss = 1.41222734\n",
      "Iteration 735, loss = 1.41188770\n",
      "Iteration 736, loss = 1.41154709\n",
      "Iteration 737, loss = 1.41121132\n",
      "Iteration 738, loss = 1.41087375\n",
      "Iteration 739, loss = 1.41053349\n",
      "Iteration 740, loss = 1.41019459\n",
      "Iteration 741, loss = 1.40985668\n",
      "Iteration 742, loss = 1.40952302\n",
      "Iteration 743, loss = 1.40918417\n",
      "Iteration 744, loss = 1.40884699\n",
      "Iteration 745, loss = 1.40850964\n",
      "Iteration 746, loss = 1.40817229\n",
      "Iteration 747, loss = 1.40783742\n",
      "Iteration 748, loss = 1.40750129\n",
      "Iteration 749, loss = 1.40716506\n",
      "Iteration 750, loss = 1.40683336\n",
      "Iteration 751, loss = 1.40649663\n",
      "Iteration 752, loss = 1.40616278\n",
      "Iteration 753, loss = 1.40582870\n",
      "Iteration 754, loss = 1.40549459\n",
      "Iteration 755, loss = 1.40515883\n",
      "Iteration 756, loss = 1.40482775\n",
      "Iteration 757, loss = 1.40449120\n",
      "Iteration 758, loss = 1.40415860\n",
      "Iteration 759, loss = 1.40382559\n",
      "Iteration 760, loss = 1.40349510\n",
      "Iteration 761, loss = 1.40316191\n",
      "Iteration 762, loss = 1.40283022\n",
      "Iteration 763, loss = 1.40249708\n",
      "Iteration 764, loss = 1.40216859\n",
      "Iteration 765, loss = 1.40183492\n",
      "Iteration 766, loss = 1.40150720\n",
      "Iteration 767, loss = 1.40117496\n",
      "Iteration 768, loss = 1.40084172\n",
      "Iteration 769, loss = 1.40051488\n",
      "Iteration 770, loss = 1.40018374\n",
      "Iteration 771, loss = 1.39985449\n",
      "Iteration 772, loss = 1.39952466\n",
      "Iteration 773, loss = 1.39919802\n",
      "Iteration 774, loss = 1.39886818\n",
      "Iteration 775, loss = 1.39853845\n",
      "Iteration 776, loss = 1.39821193\n",
      "Iteration 777, loss = 1.39788294\n",
      "Iteration 778, loss = 1.39755468\n",
      "Iteration 779, loss = 1.39722834\n",
      "Iteration 780, loss = 1.39690084\n",
      "Iteration 781, loss = 1.39657279\n",
      "Iteration 782, loss = 1.39624619\n",
      "Iteration 783, loss = 1.39592036\n",
      "Iteration 784, loss = 1.39559263\n",
      "Iteration 785, loss = 1.39526844\n",
      "Iteration 786, loss = 1.39493996\n",
      "Iteration 787, loss = 1.39461542\n",
      "Iteration 788, loss = 1.39429148\n",
      "Iteration 789, loss = 1.39396466\n",
      "Iteration 790, loss = 1.39364176\n",
      "Iteration 791, loss = 1.39331560\n",
      "Iteration 792, loss = 1.39299180\n",
      "Iteration 793, loss = 1.39266838\n",
      "Iteration 794, loss = 1.39234536\n",
      "Iteration 795, loss = 1.39201885\n",
      "Iteration 796, loss = 1.39169778\n",
      "Iteration 797, loss = 1.39137404\n",
      "Iteration 798, loss = 1.39105096\n",
      "Iteration 799, loss = 1.39072863\n",
      "Iteration 800, loss = 1.39040653\n",
      "Iteration 801, loss = 1.39008346\n",
      "Iteration 802, loss = 1.38976282\n",
      "Iteration 803, loss = 1.38944218\n",
      "Iteration 804, loss = 1.38911876\n",
      "Iteration 805, loss = 1.38880008\n",
      "Iteration 806, loss = 1.38847961\n",
      "Iteration 807, loss = 1.38815782\n",
      "Iteration 808, loss = 1.38783850\n",
      "Iteration 809, loss = 1.38751680\n",
      "Iteration 810, loss = 1.38719903\n",
      "Iteration 811, loss = 1.38687863\n",
      "Iteration 812, loss = 1.38655866\n",
      "Iteration 813, loss = 1.38624073\n",
      "Iteration 814, loss = 1.38592104\n",
      "Iteration 815, loss = 1.38560318\n",
      "Iteration 816, loss = 1.38528356\n",
      "Iteration 817, loss = 1.38496596\n",
      "Iteration 818, loss = 1.38464892\n",
      "Iteration 819, loss = 1.38432887\n",
      "Iteration 820, loss = 1.38401193\n",
      "Iteration 821, loss = 1.38369637\n",
      "Iteration 822, loss = 1.38337801\n",
      "Iteration 823, loss = 1.38306107\n",
      "Iteration 824, loss = 1.38274477\n",
      "Iteration 825, loss = 1.38242811\n",
      "Iteration 826, loss = 1.38211303\n",
      "Iteration 827, loss = 1.38179585\n",
      "Iteration 828, loss = 1.38147960\n",
      "Iteration 829, loss = 1.38116519\n",
      "Iteration 830, loss = 1.38085114\n",
      "Iteration 831, loss = 1.38053628\n",
      "Iteration 832, loss = 1.38022172\n",
      "Iteration 833, loss = 1.37990735\n",
      "Iteration 834, loss = 1.37959203\n",
      "Iteration 835, loss = 1.37928142\n",
      "Iteration 836, loss = 1.37896662\n",
      "Iteration 837, loss = 1.37865106\n",
      "Iteration 838, loss = 1.37833920\n",
      "Iteration 839, loss = 1.37802528\n",
      "Iteration 840, loss = 1.37771533\n",
      "Iteration 841, loss = 1.37740036\n",
      "Iteration 842, loss = 1.37708972\n",
      "Iteration 843, loss = 1.37677796\n",
      "Iteration 844, loss = 1.37646573\n",
      "Iteration 845, loss = 1.37615511\n",
      "Iteration 846, loss = 1.37584149\n",
      "Iteration 847, loss = 1.37552882\n",
      "Iteration 848, loss = 1.37522149\n",
      "Iteration 849, loss = 1.37490985\n",
      "Iteration 850, loss = 1.37459900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 851, loss = 1.37428869\n",
      "Iteration 852, loss = 1.37397943\n",
      "Iteration 853, loss = 1.37366978\n",
      "Iteration 854, loss = 1.37335837\n",
      "Iteration 855, loss = 1.37304907\n",
      "Iteration 856, loss = 1.37274007\n",
      "Iteration 857, loss = 1.37243201\n",
      "Iteration 858, loss = 1.37212275\n",
      "Iteration 859, loss = 1.37181390\n",
      "Iteration 860, loss = 1.37150494\n",
      "Iteration 861, loss = 1.37119767\n",
      "Iteration 862, loss = 1.37089027\n",
      "Iteration 863, loss = 1.37058185\n",
      "Iteration 864, loss = 1.37027604\n",
      "Iteration 865, loss = 1.36996939\n",
      "Iteration 866, loss = 1.36966262\n",
      "Iteration 867, loss = 1.36935225\n",
      "Iteration 868, loss = 1.36904829\n",
      "Iteration 869, loss = 1.36874154\n",
      "Iteration 870, loss = 1.36843487\n",
      "Iteration 871, loss = 1.36812959\n",
      "Iteration 872, loss = 1.36782352\n",
      "Iteration 873, loss = 1.36751809\n",
      "Iteration 874, loss = 1.36721190\n",
      "Iteration 875, loss = 1.36690637\n",
      "Iteration 876, loss = 1.36660324\n",
      "Iteration 877, loss = 1.36629851\n",
      "Iteration 878, loss = 1.36599260\n",
      "Iteration 879, loss = 1.36568816\n",
      "Iteration 880, loss = 1.36538569\n",
      "Iteration 881, loss = 1.36508097\n",
      "Iteration 882, loss = 1.36477704\n",
      "Iteration 883, loss = 1.36447277\n",
      "Iteration 884, loss = 1.36417075\n",
      "Iteration 885, loss = 1.36387004\n",
      "Iteration 886, loss = 1.36356505\n",
      "Iteration 887, loss = 1.36326451\n",
      "Iteration 888, loss = 1.36296232\n",
      "Iteration 889, loss = 1.36265886\n",
      "Iteration 890, loss = 1.36235947\n",
      "Iteration 891, loss = 1.36205958\n",
      "Iteration 892, loss = 1.36175713\n",
      "Iteration 893, loss = 1.36145483\n",
      "Iteration 894, loss = 1.36115430\n",
      "Iteration 895, loss = 1.36085302\n",
      "Iteration 896, loss = 1.36055215\n",
      "Iteration 897, loss = 1.36025248\n",
      "Iteration 898, loss = 1.35995450\n",
      "Iteration 899, loss = 1.35965298\n",
      "Iteration 900, loss = 1.35935492\n",
      "Iteration 901, loss = 1.35905652\n",
      "Iteration 902, loss = 1.35875902\n",
      "Iteration 903, loss = 1.35845818\n",
      "Iteration 904, loss = 1.35816092\n",
      "Iteration 905, loss = 1.35786209\n",
      "Iteration 906, loss = 1.35756171\n",
      "Iteration 907, loss = 1.35726584\n",
      "Iteration 908, loss = 1.35696757\n",
      "Iteration 909, loss = 1.35667085\n",
      "Iteration 910, loss = 1.35637239\n",
      "Iteration 911, loss = 1.35607611\n",
      "Iteration 912, loss = 1.35578008\n",
      "Iteration 913, loss = 1.35548505\n",
      "Iteration 914, loss = 1.35518863\n",
      "Iteration 915, loss = 1.35489319\n",
      "Iteration 916, loss = 1.35459545\n",
      "Iteration 917, loss = 1.35429984\n",
      "Iteration 918, loss = 1.35400403\n",
      "Iteration 919, loss = 1.35370811\n",
      "Iteration 920, loss = 1.35341243\n",
      "Iteration 921, loss = 1.35311970\n",
      "Iteration 922, loss = 1.35282332\n",
      "Iteration 923, loss = 1.35253112\n",
      "Iteration 924, loss = 1.35223427\n",
      "Iteration 925, loss = 1.35194088\n",
      "Iteration 926, loss = 1.35164766\n",
      "Iteration 927, loss = 1.35135360\n",
      "Iteration 928, loss = 1.35105850\n",
      "Iteration 929, loss = 1.35076780\n",
      "Iteration 930, loss = 1.35047326\n",
      "Iteration 931, loss = 1.35017930\n",
      "Iteration 932, loss = 1.34988800\n",
      "Iteration 933, loss = 1.34959553\n",
      "Iteration 934, loss = 1.34930336\n",
      "Iteration 935, loss = 1.34901144\n",
      "Iteration 936, loss = 1.34872002\n",
      "Iteration 937, loss = 1.34842774\n",
      "Iteration 938, loss = 1.34813415\n",
      "Iteration 939, loss = 1.34784644\n",
      "Iteration 940, loss = 1.34755382\n",
      "Iteration 941, loss = 1.34726352\n",
      "Iteration 942, loss = 1.34697314\n",
      "Iteration 943, loss = 1.34667925\n",
      "Iteration 944, loss = 1.34639125\n",
      "Iteration 945, loss = 1.34609975\n",
      "Iteration 946, loss = 1.34581017\n",
      "Iteration 947, loss = 1.34551950\n",
      "Iteration 948, loss = 1.34522898\n",
      "Iteration 949, loss = 1.34494084\n",
      "Iteration 950, loss = 1.34465090\n",
      "Iteration 951, loss = 1.34436207\n",
      "Iteration 952, loss = 1.34407363\n",
      "Iteration 953, loss = 1.34378616\n",
      "Iteration 954, loss = 1.34349514\n",
      "Iteration 955, loss = 1.34320799\n",
      "Iteration 956, loss = 1.34291928\n",
      "Iteration 957, loss = 1.34263145\n",
      "Iteration 958, loss = 1.34234309\n",
      "Iteration 959, loss = 1.34205693\n",
      "Iteration 960, loss = 1.34176863\n",
      "Iteration 961, loss = 1.34148272\n",
      "Iteration 962, loss = 1.34119538\n",
      "Iteration 963, loss = 1.34090934\n",
      "Iteration 964, loss = 1.34062113\n",
      "Iteration 965, loss = 1.34033550\n",
      "Iteration 966, loss = 1.34004950\n",
      "Iteration 967, loss = 1.33976293\n",
      "Iteration 968, loss = 1.33947882\n",
      "Iteration 969, loss = 1.33919035\n",
      "Iteration 970, loss = 1.33890675\n",
      "Iteration 971, loss = 1.33862193\n",
      "Iteration 972, loss = 1.33833573\n",
      "Iteration 973, loss = 1.33805235\n",
      "Iteration 974, loss = 1.33776603\n",
      "Iteration 975, loss = 1.33748216\n",
      "Iteration 976, loss = 1.33719687\n",
      "Iteration 977, loss = 1.33691374\n",
      "Iteration 978, loss = 1.33662896\n",
      "Iteration 979, loss = 1.33634701\n",
      "Iteration 980, loss = 1.33606118\n",
      "Iteration 981, loss = 1.33577833\n",
      "Iteration 982, loss = 1.33549553\n",
      "Iteration 983, loss = 1.33521290\n",
      "Iteration 984, loss = 1.33493044\n",
      "Iteration 985, loss = 1.33464699\n",
      "Iteration 986, loss = 1.33436516\n",
      "Iteration 987, loss = 1.33408312\n",
      "Iteration 988, loss = 1.33380005\n",
      "Iteration 989, loss = 1.33351921\n",
      "Iteration 990, loss = 1.33323606\n",
      "Iteration 991, loss = 1.33295508\n",
      "Iteration 992, loss = 1.33267373\n",
      "Iteration 993, loss = 1.33239433\n",
      "Iteration 994, loss = 1.33211296\n",
      "Iteration 995, loss = 1.33183128\n",
      "Iteration 996, loss = 1.33155090\n",
      "Iteration 997, loss = 1.33127241\n",
      "Iteration 998, loss = 1.33099106\n",
      "Iteration 999, loss = 1.33071271\n",
      "Iteration 1000, loss = 1.33043041\n",
      "Iteration 1001, loss = 1.33015164\n",
      "Iteration 1002, loss = 1.32987215\n",
      "Iteration 1003, loss = 1.32959473\n",
      "Iteration 1004, loss = 1.32931512\n",
      "Iteration 1005, loss = 1.32903573\n",
      "Iteration 1006, loss = 1.32875604\n",
      "Iteration 1007, loss = 1.32848073\n",
      "Iteration 1008, loss = 1.32820148\n",
      "Iteration 1009, loss = 1.32792041\n",
      "Iteration 1010, loss = 1.32764295\n",
      "Iteration 1011, loss = 1.32736605\n",
      "Iteration 1012, loss = 1.32708818\n",
      "Iteration 1013, loss = 1.32681378\n",
      "Iteration 1014, loss = 1.32653166\n",
      "Iteration 1015, loss = 1.32625536\n",
      "Iteration 1016, loss = 1.32597914\n",
      "Iteration 1017, loss = 1.32570271\n",
      "Iteration 1018, loss = 1.32542609\n",
      "Iteration 1019, loss = 1.32514935\n",
      "Iteration 1020, loss = 1.32487386\n",
      "Iteration 1021, loss = 1.32459784\n",
      "Iteration 1022, loss = 1.32432192\n",
      "Iteration 1023, loss = 1.32404679\n",
      "Iteration 1024, loss = 1.32377148\n",
      "Iteration 1025, loss = 1.32349750\n",
      "Iteration 1026, loss = 1.32322142\n",
      "Iteration 1027, loss = 1.32294686\n",
      "Iteration 1028, loss = 1.32267054\n",
      "Iteration 1029, loss = 1.32239574\n",
      "Iteration 1030, loss = 1.32212152\n",
      "Iteration 1031, loss = 1.32184831\n",
      "Iteration 1032, loss = 1.32157421\n",
      "Iteration 1033, loss = 1.32129750\n",
      "Iteration 1034, loss = 1.32102461\n",
      "Iteration 1035, loss = 1.32075165\n",
      "Iteration 1036, loss = 1.32047757\n",
      "Iteration 1037, loss = 1.32020351\n",
      "Iteration 1038, loss = 1.31993013\n",
      "Iteration 1039, loss = 1.31965810\n",
      "Iteration 1040, loss = 1.31938507\n",
      "Iteration 1041, loss = 1.31911462\n",
      "Iteration 1042, loss = 1.31883841\n",
      "Iteration 1043, loss = 1.31856772\n",
      "Iteration 1044, loss = 1.31829621\n",
      "Iteration 1045, loss = 1.31802409\n",
      "Iteration 1046, loss = 1.31774933\n",
      "Iteration 1047, loss = 1.31747906\n",
      "Iteration 1048, loss = 1.31720761\n",
      "Iteration 1049, loss = 1.31693803\n",
      "Iteration 1050, loss = 1.31666518\n",
      "Iteration 1051, loss = 1.31639615\n",
      "Iteration 1052, loss = 1.31612479\n",
      "Iteration 1053, loss = 1.31585221\n",
      "Iteration 1054, loss = 1.31558321\n",
      "Iteration 1055, loss = 1.31531151\n",
      "Iteration 1056, loss = 1.31504095\n",
      "Iteration 1057, loss = 1.31477119\n",
      "Iteration 1058, loss = 1.31450172\n",
      "Iteration 1059, loss = 1.31423253\n",
      "Iteration 1060, loss = 1.31396124\n",
      "Iteration 1061, loss = 1.31369306\n",
      "Iteration 1062, loss = 1.31342224\n",
      "Iteration 1063, loss = 1.31315461\n",
      "Iteration 1064, loss = 1.31288441\n",
      "Iteration 1065, loss = 1.31261742\n",
      "Iteration 1066, loss = 1.31234877\n",
      "Iteration 1067, loss = 1.31207964\n",
      "Iteration 1068, loss = 1.31181219\n",
      "Iteration 1069, loss = 1.31154495\n",
      "Iteration 1070, loss = 1.31127665\n",
      "Iteration 1071, loss = 1.31100921\n",
      "Iteration 1072, loss = 1.31074131\n",
      "Iteration 1073, loss = 1.31047263\n",
      "Iteration 1074, loss = 1.31020767\n",
      "Iteration 1075, loss = 1.30993904\n",
      "Iteration 1076, loss = 1.30967230\n",
      "Iteration 1077, loss = 1.30940736\n",
      "Iteration 1078, loss = 1.30914052\n",
      "Iteration 1079, loss = 1.30887255\n",
      "Iteration 1080, loss = 1.30860812\n",
      "Iteration 1081, loss = 1.30834292\n",
      "Iteration 1082, loss = 1.30807584\n",
      "Iteration 1083, loss = 1.30780924\n",
      "Iteration 1084, loss = 1.30754485\n",
      "Iteration 1085, loss = 1.30727984\n",
      "Iteration 1086, loss = 1.30701244\n",
      "Iteration 1087, loss = 1.30674804\n",
      "Iteration 1088, loss = 1.30648317\n",
      "Iteration 1089, loss = 1.30622004\n",
      "Iteration 1090, loss = 1.30595650\n",
      "Iteration 1091, loss = 1.30569032\n",
      "Iteration 1092, loss = 1.30542709\n",
      "Iteration 1093, loss = 1.30516358\n",
      "Iteration 1094, loss = 1.30490056\n",
      "Iteration 1095, loss = 1.30463633\n",
      "Iteration 1096, loss = 1.30437344\n",
      "Iteration 1097, loss = 1.30411241\n",
      "Iteration 1098, loss = 1.30384631\n",
      "Iteration 1099, loss = 1.30358313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1100, loss = 1.30331989\n",
      "Iteration 1101, loss = 1.30305746\n",
      "Iteration 1102, loss = 1.30279555\n",
      "Iteration 1103, loss = 1.30253308\n",
      "Iteration 1104, loss = 1.30227268\n",
      "Iteration 1105, loss = 1.30200782\n",
      "Iteration 1106, loss = 1.30174754\n",
      "Iteration 1107, loss = 1.30148433\n",
      "Iteration 1108, loss = 1.30122242\n",
      "Iteration 1109, loss = 1.30096195\n",
      "Iteration 1110, loss = 1.30070127\n",
      "Iteration 1111, loss = 1.30044010\n",
      "Iteration 1112, loss = 1.30017841\n",
      "Iteration 1113, loss = 1.29991792\n",
      "Iteration 1114, loss = 1.29965781\n",
      "Iteration 1115, loss = 1.29939724\n",
      "Iteration 1116, loss = 1.29913784\n",
      "Iteration 1117, loss = 1.29887574\n",
      "Iteration 1118, loss = 1.29861624\n",
      "Iteration 1119, loss = 1.29835444\n",
      "Iteration 1120, loss = 1.29809666\n",
      "Iteration 1121, loss = 1.29783602\n",
      "Iteration 1122, loss = 1.29757452\n",
      "Iteration 1123, loss = 1.29731610\n",
      "Iteration 1124, loss = 1.29705594\n",
      "Iteration 1125, loss = 1.29679896\n",
      "Iteration 1126, loss = 1.29653791\n",
      "Iteration 1127, loss = 1.29627949\n",
      "Iteration 1128, loss = 1.29602019\n",
      "Iteration 1129, loss = 1.29576234\n",
      "Iteration 1130, loss = 1.29550469\n",
      "Iteration 1131, loss = 1.29524587\n",
      "Iteration 1132, loss = 1.29498874\n",
      "Iteration 1133, loss = 1.29472975\n",
      "Iteration 1134, loss = 1.29447061\n",
      "Iteration 1135, loss = 1.29421464\n",
      "Iteration 1136, loss = 1.29395712\n",
      "Iteration 1137, loss = 1.29369895\n",
      "Iteration 1138, loss = 1.29344273\n",
      "Iteration 1139, loss = 1.29318494\n",
      "Iteration 1140, loss = 1.29292828\n",
      "Iteration 1141, loss = 1.29267245\n",
      "Iteration 1142, loss = 1.29241573\n",
      "Iteration 1143, loss = 1.29215887\n",
      "Iteration 1144, loss = 1.29190326\n",
      "Iteration 1145, loss = 1.29164811\n",
      "Iteration 1146, loss = 1.29139183\n",
      "Iteration 1147, loss = 1.29113568\n",
      "Iteration 1148, loss = 1.29088012\n",
      "Iteration 1149, loss = 1.29062572\n",
      "Iteration 1150, loss = 1.29037149\n",
      "Iteration 1151, loss = 1.29011613\n",
      "Iteration 1152, loss = 1.28986023\n",
      "Iteration 1153, loss = 1.28960620\n",
      "Iteration 1154, loss = 1.28935106\n",
      "Iteration 1155, loss = 1.28909556\n",
      "Iteration 1156, loss = 1.28884244\n",
      "Iteration 1157, loss = 1.28858984\n",
      "Iteration 1158, loss = 1.28833657\n",
      "Iteration 1159, loss = 1.28808037\n",
      "Iteration 1160, loss = 1.28782717\n",
      "Iteration 1161, loss = 1.28757470\n",
      "Iteration 1162, loss = 1.28732054\n",
      "Iteration 1163, loss = 1.28706722\n",
      "Iteration 1164, loss = 1.28681381\n",
      "Iteration 1165, loss = 1.28656249\n",
      "Iteration 1166, loss = 1.28630985\n",
      "Iteration 1167, loss = 1.28605565\n",
      "Iteration 1168, loss = 1.28580474\n",
      "Iteration 1169, loss = 1.28555083\n",
      "Iteration 1170, loss = 1.28529903\n",
      "Iteration 1171, loss = 1.28504846\n",
      "Iteration 1172, loss = 1.28479381\n",
      "Iteration 1173, loss = 1.28454289\n",
      "Iteration 1174, loss = 1.28429201\n",
      "Iteration 1175, loss = 1.28404047\n",
      "Iteration 1176, loss = 1.28378958\n",
      "Iteration 1177, loss = 1.28353826\n",
      "Iteration 1178, loss = 1.28328758\n",
      "Iteration 1179, loss = 1.28303549\n",
      "Iteration 1180, loss = 1.28278604\n",
      "Iteration 1181, loss = 1.28253568\n",
      "Iteration 1182, loss = 1.28228495\n",
      "Iteration 1183, loss = 1.28203327\n",
      "Iteration 1184, loss = 1.28178613\n",
      "Iteration 1185, loss = 1.28153374\n",
      "Iteration 1186, loss = 1.28128426\n",
      "Iteration 1187, loss = 1.28103277\n",
      "Iteration 1188, loss = 1.28078553\n",
      "Iteration 1189, loss = 1.28053519\n",
      "Iteration 1190, loss = 1.28028827\n",
      "Iteration 1191, loss = 1.28003605\n",
      "Iteration 1192, loss = 1.27978669\n",
      "Iteration 1193, loss = 1.27953755\n",
      "Iteration 1194, loss = 1.27929055\n",
      "Iteration 1195, loss = 1.27904040\n",
      "Iteration 1196, loss = 1.27879150\n",
      "Iteration 1197, loss = 1.27854392\n",
      "Iteration 1198, loss = 1.27829589\n",
      "Iteration 1199, loss = 1.27804795\n",
      "Iteration 1200, loss = 1.27779942\n",
      "Iteration 1201, loss = 1.27755147\n",
      "Iteration 1202, loss = 1.27730321\n",
      "Iteration 1203, loss = 1.27705487\n",
      "Iteration 1204, loss = 1.27680803\n",
      "Iteration 1205, loss = 1.27656099\n",
      "Iteration 1206, loss = 1.27631499\n",
      "Iteration 1207, loss = 1.27606875\n",
      "Iteration 1208, loss = 1.27581934\n",
      "Iteration 1209, loss = 1.27557250\n",
      "Iteration 1210, loss = 1.27532568\n",
      "Iteration 1211, loss = 1.27508014\n",
      "Iteration 1212, loss = 1.27483201\n",
      "Iteration 1213, loss = 1.27458608\n",
      "Iteration 1214, loss = 1.27434039\n",
      "Iteration 1215, loss = 1.27409470\n",
      "Iteration 1216, loss = 1.27384928\n",
      "Iteration 1217, loss = 1.27360075\n",
      "Iteration 1218, loss = 1.27335680\n",
      "Iteration 1219, loss = 1.27311141\n",
      "Iteration 1220, loss = 1.27286631\n",
      "Iteration 1221, loss = 1.27262034\n",
      "Iteration 1222, loss = 1.27237706\n",
      "Iteration 1223, loss = 1.27213228\n",
      "Iteration 1224, loss = 1.27188497\n",
      "Iteration 1225, loss = 1.27164173\n",
      "Iteration 1226, loss = 1.27139729\n",
      "Iteration 1227, loss = 1.27115469\n",
      "Iteration 1228, loss = 1.27090823\n",
      "Iteration 1229, loss = 1.27066351\n",
      "Iteration 1230, loss = 1.27042159\n",
      "Iteration 1231, loss = 1.27017897\n",
      "Iteration 1232, loss = 1.26993483\n",
      "Iteration 1233, loss = 1.26969128\n",
      "Iteration 1234, loss = 1.26944746\n",
      "Iteration 1235, loss = 1.26920496\n",
      "Iteration 1236, loss = 1.26896154\n",
      "Iteration 1237, loss = 1.26871853\n",
      "Iteration 1238, loss = 1.26847710\n",
      "Iteration 1239, loss = 1.26823537\n",
      "Iteration 1240, loss = 1.26799000\n",
      "Iteration 1241, loss = 1.26774807\n",
      "Iteration 1242, loss = 1.26750613\n",
      "Iteration 1243, loss = 1.26726359\n",
      "Iteration 1244, loss = 1.26702276\n",
      "Iteration 1245, loss = 1.26678191\n",
      "Iteration 1246, loss = 1.26653830\n",
      "Iteration 1247, loss = 1.26629771\n",
      "Iteration 1248, loss = 1.26605618\n",
      "Iteration 1249, loss = 1.26581399\n",
      "Iteration 1250, loss = 1.26557223\n",
      "Iteration 1251, loss = 1.26533177\n",
      "Iteration 1252, loss = 1.26509080\n",
      "Iteration 1253, loss = 1.26484987\n",
      "Iteration 1254, loss = 1.26460813\n",
      "Iteration 1255, loss = 1.26436896\n",
      "Iteration 1256, loss = 1.26412817\n",
      "Iteration 1257, loss = 1.26388641\n",
      "Iteration 1258, loss = 1.26364719\n",
      "Iteration 1259, loss = 1.26340649\n",
      "Iteration 1260, loss = 1.26316974\n",
      "Iteration 1261, loss = 1.26292929\n",
      "Iteration 1262, loss = 1.26268747\n",
      "Iteration 1263, loss = 1.26244812\n",
      "Iteration 1264, loss = 1.26220726\n",
      "Iteration 1265, loss = 1.26197139\n",
      "Iteration 1266, loss = 1.26172924\n",
      "Iteration 1267, loss = 1.26148925\n",
      "Iteration 1268, loss = 1.26125301\n",
      "Iteration 1269, loss = 1.26101286\n",
      "Iteration 1270, loss = 1.26077483\n",
      "Iteration 1271, loss = 1.26053570\n",
      "Iteration 1272, loss = 1.26029778\n",
      "Iteration 1273, loss = 1.26006000\n",
      "Iteration 1274, loss = 1.25982083\n",
      "Iteration 1275, loss = 1.25958270\n",
      "Iteration 1276, loss = 1.25934416\n",
      "Iteration 1277, loss = 1.25910706\n",
      "Iteration 1278, loss = 1.25886974\n",
      "Iteration 1279, loss = 1.25863308\n",
      "Iteration 1280, loss = 1.25839498\n",
      "Iteration 1281, loss = 1.25815659\n",
      "Iteration 1282, loss = 1.25792071\n",
      "Iteration 1283, loss = 1.25768290\n",
      "Iteration 1284, loss = 1.25744666\n",
      "Iteration 1285, loss = 1.25721067\n",
      "Iteration 1286, loss = 1.25697346\n",
      "Iteration 1287, loss = 1.25673754\n",
      "Iteration 1288, loss = 1.25649933\n",
      "Iteration 1289, loss = 1.25626520\n",
      "Iteration 1290, loss = 1.25602858\n",
      "Iteration 1291, loss = 1.25579369\n",
      "Iteration 1292, loss = 1.25555654\n",
      "Iteration 1293, loss = 1.25532306\n",
      "Iteration 1294, loss = 1.25508599\n",
      "Iteration 1295, loss = 1.25485023\n",
      "Iteration 1296, loss = 1.25461407\n",
      "Iteration 1297, loss = 1.25438126\n",
      "Iteration 1298, loss = 1.25414405\n",
      "Iteration 1299, loss = 1.25391081\n",
      "Iteration 1300, loss = 1.25367529\n",
      "Iteration 1301, loss = 1.25343996\n",
      "Iteration 1302, loss = 1.25320505\n",
      "Iteration 1303, loss = 1.25297310\n",
      "Iteration 1304, loss = 1.25273718\n",
      "Iteration 1305, loss = 1.25250383\n",
      "Iteration 1306, loss = 1.25226992\n",
      "Iteration 1307, loss = 1.25203542\n",
      "Iteration 1308, loss = 1.25180163\n",
      "Iteration 1309, loss = 1.25156923\n",
      "Iteration 1310, loss = 1.25133409\n",
      "Iteration 1311, loss = 1.25110156\n",
      "Iteration 1312, loss = 1.25086696\n",
      "Iteration 1313, loss = 1.25063551\n",
      "Iteration 1314, loss = 1.25040325\n",
      "Iteration 1315, loss = 1.25016883\n",
      "Iteration 1316, loss = 1.24993631\n",
      "Iteration 1317, loss = 1.24970480\n",
      "Iteration 1318, loss = 1.24947073\n",
      "Iteration 1319, loss = 1.24923831\n",
      "Iteration 1320, loss = 1.24900624\n",
      "Iteration 1321, loss = 1.24877544\n",
      "Iteration 1322, loss = 1.24854139\n",
      "Iteration 1323, loss = 1.24830967\n",
      "Iteration 1324, loss = 1.24807798\n",
      "Iteration 1325, loss = 1.24784618\n",
      "Iteration 1326, loss = 1.24761381\n",
      "Iteration 1327, loss = 1.24738324\n",
      "Iteration 1328, loss = 1.24715203\n",
      "Iteration 1329, loss = 1.24692052\n",
      "Iteration 1330, loss = 1.24668844\n",
      "Iteration 1331, loss = 1.24645739\n",
      "Iteration 1332, loss = 1.24622940\n",
      "Iteration 1333, loss = 1.24599835\n",
      "Iteration 1334, loss = 1.24576724\n",
      "Iteration 1335, loss = 1.24553567\n",
      "Iteration 1336, loss = 1.24530563\n",
      "Iteration 1337, loss = 1.24507561\n",
      "Iteration 1338, loss = 1.24484558\n",
      "Iteration 1339, loss = 1.24461650\n",
      "Iteration 1340, loss = 1.24438608\n",
      "Iteration 1341, loss = 1.24415702\n",
      "Iteration 1342, loss = 1.24392726\n",
      "Iteration 1343, loss = 1.24369671\n",
      "Iteration 1344, loss = 1.24346794\n",
      "Iteration 1345, loss = 1.24323928\n",
      "Iteration 1346, loss = 1.24301079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1347, loss = 1.24278145\n",
      "Iteration 1348, loss = 1.24255280\n",
      "Iteration 1349, loss = 1.24232465\n",
      "Iteration 1350, loss = 1.24209712\n",
      "Iteration 1351, loss = 1.24186731\n",
      "Iteration 1352, loss = 1.24163815\n",
      "Iteration 1353, loss = 1.24141130\n",
      "Iteration 1354, loss = 1.24118297\n",
      "Iteration 1355, loss = 1.24095393\n",
      "Iteration 1356, loss = 1.24072857\n",
      "Iteration 1357, loss = 1.24050034\n",
      "Iteration 1358, loss = 1.24027232\n",
      "Iteration 1359, loss = 1.24004494\n",
      "Iteration 1360, loss = 1.23981689\n",
      "Iteration 1361, loss = 1.23958925\n",
      "Iteration 1362, loss = 1.23936410\n",
      "Iteration 1363, loss = 1.23913698\n",
      "Iteration 1364, loss = 1.23890995\n",
      "Iteration 1365, loss = 1.23868376\n",
      "Iteration 1366, loss = 1.23845680\n",
      "Iteration 1367, loss = 1.23823352\n",
      "Iteration 1368, loss = 1.23800535\n",
      "Iteration 1369, loss = 1.23777769\n",
      "Iteration 1370, loss = 1.23755178\n",
      "Iteration 1371, loss = 1.23732576\n",
      "Iteration 1372, loss = 1.23709889\n",
      "Iteration 1373, loss = 1.23687390\n",
      "Iteration 1374, loss = 1.23664884\n",
      "Iteration 1375, loss = 1.23642354\n",
      "Iteration 1376, loss = 1.23619909\n",
      "Iteration 1377, loss = 1.23597393\n",
      "Iteration 1378, loss = 1.23574756\n",
      "Iteration 1379, loss = 1.23552420\n",
      "Iteration 1380, loss = 1.23529821\n",
      "Iteration 1381, loss = 1.23507385\n",
      "Iteration 1382, loss = 1.23485036\n",
      "Iteration 1383, loss = 1.23462499\n",
      "Iteration 1384, loss = 1.23440102\n",
      "Iteration 1385, loss = 1.23417714\n",
      "Iteration 1386, loss = 1.23395230\n",
      "Iteration 1387, loss = 1.23372894\n",
      "Iteration 1388, loss = 1.23350527\n",
      "Iteration 1389, loss = 1.23328204\n",
      "Iteration 1390, loss = 1.23305764\n",
      "Iteration 1391, loss = 1.23283471\n",
      "Iteration 1392, loss = 1.23260916\n",
      "Iteration 1393, loss = 1.23238704\n",
      "Iteration 1394, loss = 1.23216350\n",
      "Iteration 1395, loss = 1.23194049\n",
      "Iteration 1396, loss = 1.23171909\n",
      "Iteration 1397, loss = 1.23149515\n",
      "Iteration 1398, loss = 1.23127157\n",
      "Iteration 1399, loss = 1.23104976\n",
      "Iteration 1400, loss = 1.23082720\n",
      "Iteration 1401, loss = 1.23060599\n",
      "Iteration 1402, loss = 1.23038162\n",
      "Iteration 1403, loss = 1.23015876\n",
      "Iteration 1404, loss = 1.22993815\n",
      "Iteration 1405, loss = 1.22971603\n",
      "Iteration 1406, loss = 1.22949305\n",
      "Iteration 1407, loss = 1.22927214\n",
      "Iteration 1408, loss = 1.22905008\n",
      "Iteration 1409, loss = 1.22882954\n",
      "Iteration 1410, loss = 1.22860714\n",
      "Iteration 1411, loss = 1.22838683\n",
      "Iteration 1412, loss = 1.22816442\n",
      "Iteration 1413, loss = 1.22794437\n",
      "Iteration 1414, loss = 1.22772501\n",
      "Iteration 1415, loss = 1.22750222\n",
      "Iteration 1416, loss = 1.22728487\n",
      "Iteration 1417, loss = 1.22706183\n",
      "Iteration 1418, loss = 1.22684289\n",
      "Iteration 1419, loss = 1.22662152\n",
      "Iteration 1420, loss = 1.22639920\n",
      "Iteration 1421, loss = 1.22618103\n",
      "Iteration 1422, loss = 1.22596125\n",
      "Iteration 1423, loss = 1.22574136\n",
      "Iteration 1424, loss = 1.22552019\n",
      "Iteration 1425, loss = 1.22530132\n",
      "Iteration 1426, loss = 1.22508109\n",
      "Iteration 1427, loss = 1.22486277\n",
      "Iteration 1428, loss = 1.22464424\n",
      "Iteration 1429, loss = 1.22442353\n",
      "Iteration 1430, loss = 1.22420506\n",
      "Iteration 1431, loss = 1.22398539\n",
      "Iteration 1432, loss = 1.22376678\n",
      "Iteration 1433, loss = 1.22354895\n",
      "Iteration 1434, loss = 1.22333058\n",
      "Iteration 1435, loss = 1.22311153\n",
      "Iteration 1436, loss = 1.22289317\n",
      "Iteration 1437, loss = 1.22267483\n",
      "Iteration 1438, loss = 1.22245656\n",
      "Iteration 1439, loss = 1.22224077\n",
      "Iteration 1440, loss = 1.22202231\n",
      "Iteration 1441, loss = 1.22180308\n",
      "Iteration 1442, loss = 1.22158593\n",
      "Iteration 1443, loss = 1.22137074\n",
      "Iteration 1444, loss = 1.22115052\n",
      "Iteration 1445, loss = 1.22093394\n",
      "Iteration 1446, loss = 1.22071536\n",
      "Iteration 1447, loss = 1.22049987\n",
      "Iteration 1448, loss = 1.22028259\n",
      "Iteration 1449, loss = 1.22006549\n",
      "Iteration 1450, loss = 1.21984775\n",
      "Iteration 1451, loss = 1.21963182\n",
      "Iteration 1452, loss = 1.21941562\n",
      "Iteration 1453, loss = 1.21919917\n",
      "Iteration 1454, loss = 1.21898368\n",
      "Iteration 1455, loss = 1.21876753\n",
      "Iteration 1456, loss = 1.21855167\n",
      "Iteration 1457, loss = 1.21833498\n",
      "Iteration 1458, loss = 1.21812036\n",
      "Iteration 1459, loss = 1.21790426\n",
      "Iteration 1460, loss = 1.21768806\n",
      "Iteration 1461, loss = 1.21747248\n",
      "Iteration 1462, loss = 1.21725899\n",
      "Iteration 1463, loss = 1.21704102\n",
      "Iteration 1464, loss = 1.21682749\n",
      "Iteration 1465, loss = 1.21661289\n",
      "Iteration 1466, loss = 1.21639729\n",
      "Iteration 1467, loss = 1.21618301\n",
      "Iteration 1468, loss = 1.21596909\n",
      "Iteration 1469, loss = 1.21575419\n",
      "Iteration 1470, loss = 1.21553862\n",
      "Iteration 1471, loss = 1.21532506\n",
      "Iteration 1472, loss = 1.21511120\n",
      "Iteration 1473, loss = 1.21489842\n",
      "Iteration 1474, loss = 1.21468323\n",
      "Iteration 1475, loss = 1.21446892\n",
      "Iteration 1476, loss = 1.21425643\n",
      "Iteration 1477, loss = 1.21404031\n",
      "Iteration 1478, loss = 1.21382823\n",
      "Iteration 1479, loss = 1.21361459\n",
      "Iteration 1480, loss = 1.21340073\n",
      "Iteration 1481, loss = 1.21318746\n",
      "Iteration 1482, loss = 1.21297469\n",
      "Iteration 1483, loss = 1.21276195\n",
      "Iteration 1484, loss = 1.21254867\n",
      "Iteration 1485, loss = 1.21233472\n",
      "Iteration 1486, loss = 1.21212277\n",
      "Iteration 1487, loss = 1.21191024\n",
      "Iteration 1488, loss = 1.21169775\n",
      "Iteration 1489, loss = 1.21148545\n",
      "Iteration 1490, loss = 1.21127337\n",
      "Iteration 1491, loss = 1.21106208\n",
      "Iteration 1492, loss = 1.21084875\n",
      "Iteration 1493, loss = 1.21063618\n",
      "Iteration 1494, loss = 1.21042349\n",
      "Iteration 1495, loss = 1.21021266\n",
      "Iteration 1496, loss = 1.21000202\n",
      "Iteration 1497, loss = 1.20978987\n",
      "Iteration 1498, loss = 1.20957785\n",
      "Iteration 1499, loss = 1.20936690\n",
      "Iteration 1500, loss = 1.20915477\n",
      "Iteration 1501, loss = 1.20894413\n",
      "Iteration 1502, loss = 1.20873350\n",
      "Iteration 1503, loss = 1.20852217\n",
      "Iteration 1504, loss = 1.20831106\n",
      "Iteration 1505, loss = 1.20810151\n",
      "Iteration 1506, loss = 1.20789052\n",
      "Iteration 1507, loss = 1.20767934\n",
      "Iteration 1508, loss = 1.20746810\n",
      "Iteration 1509, loss = 1.20725842\n",
      "Iteration 1510, loss = 1.20704983\n",
      "Iteration 1511, loss = 1.20683762\n",
      "Iteration 1512, loss = 1.20662809\n",
      "Iteration 1513, loss = 1.20641946\n",
      "Iteration 1514, loss = 1.20620797\n",
      "Iteration 1515, loss = 1.20600020\n",
      "Iteration 1516, loss = 1.20578879\n",
      "Iteration 1517, loss = 1.20558042\n",
      "Iteration 1518, loss = 1.20537087\n",
      "Iteration 1519, loss = 1.20516086\n",
      "Iteration 1520, loss = 1.20495269\n",
      "Iteration 1521, loss = 1.20474189\n",
      "Iteration 1522, loss = 1.20453443\n",
      "Iteration 1523, loss = 1.20432389\n",
      "Iteration 1524, loss = 1.20411574\n",
      "Iteration 1525, loss = 1.20390745\n",
      "Iteration 1526, loss = 1.20369856\n",
      "Iteration 1527, loss = 1.20349175\n",
      "Iteration 1528, loss = 1.20328104\n",
      "Iteration 1529, loss = 1.20307489\n",
      "Iteration 1530, loss = 1.20286522\n",
      "Iteration 1531, loss = 1.20265828\n",
      "Iteration 1532, loss = 1.20244916\n",
      "Iteration 1533, loss = 1.20224196\n",
      "Iteration 1534, loss = 1.20203446\n",
      "Iteration 1535, loss = 1.20182514\n",
      "Iteration 1536, loss = 1.20161882\n",
      "Iteration 1537, loss = 1.20141089\n",
      "Iteration 1538, loss = 1.20120419\n",
      "Iteration 1539, loss = 1.20099691\n",
      "Iteration 1540, loss = 1.20079146\n",
      "Iteration 1541, loss = 1.20058378\n",
      "Iteration 1542, loss = 1.20037515\n",
      "Iteration 1543, loss = 1.20016901\n",
      "Iteration 1544, loss = 1.19996154\n",
      "Iteration 1545, loss = 1.19975541\n",
      "Iteration 1546, loss = 1.19954768\n",
      "Iteration 1547, loss = 1.19934182\n",
      "Iteration 1548, loss = 1.19913535\n",
      "Iteration 1549, loss = 1.19893020\n",
      "Iteration 1550, loss = 1.19872213\n",
      "Iteration 1551, loss = 1.19851751\n",
      "Iteration 1552, loss = 1.19831175\n",
      "Iteration 1553, loss = 1.19810688\n",
      "Iteration 1554, loss = 1.19789909\n",
      "Iteration 1555, loss = 1.19769342\n",
      "Iteration 1556, loss = 1.19748889\n",
      "Iteration 1557, loss = 1.19728419\n",
      "Iteration 1558, loss = 1.19707835\n",
      "Iteration 1559, loss = 1.19687252\n",
      "Iteration 1560, loss = 1.19666704\n",
      "Iteration 1561, loss = 1.19646264\n",
      "Iteration 1562, loss = 1.19625705\n",
      "Iteration 1563, loss = 1.19605444\n",
      "Iteration 1564, loss = 1.19584948\n",
      "Iteration 1565, loss = 1.19564426\n",
      "Iteration 1566, loss = 1.19544011\n",
      "Iteration 1567, loss = 1.19523613\n",
      "Iteration 1568, loss = 1.19503136\n",
      "Iteration 1569, loss = 1.19482728\n",
      "Iteration 1570, loss = 1.19462241\n",
      "Iteration 1571, loss = 1.19441951\n",
      "Iteration 1572, loss = 1.19421646\n",
      "Iteration 1573, loss = 1.19401008\n",
      "Iteration 1574, loss = 1.19380816\n",
      "Iteration 1575, loss = 1.19360454\n",
      "Iteration 1576, loss = 1.19340071\n",
      "Iteration 1577, loss = 1.19319757\n",
      "Iteration 1578, loss = 1.19299310\n",
      "Iteration 1579, loss = 1.19279097\n",
      "Iteration 1580, loss = 1.19258724\n",
      "Iteration 1581, loss = 1.19238511\n",
      "Iteration 1582, loss = 1.19218161\n",
      "Iteration 1583, loss = 1.19197944\n",
      "Iteration 1584, loss = 1.19177643\n",
      "Iteration 1585, loss = 1.19157355\n",
      "Iteration 1586, loss = 1.19137079\n",
      "Iteration 1587, loss = 1.19116861\n",
      "Iteration 1588, loss = 1.19096731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1589, loss = 1.19076554\n",
      "Iteration 1590, loss = 1.19056349\n",
      "Iteration 1591, loss = 1.19036137\n",
      "Iteration 1592, loss = 1.19016018\n",
      "Iteration 1593, loss = 1.18995799\n",
      "Iteration 1594, loss = 1.18975626\n",
      "Iteration 1595, loss = 1.18955475\n",
      "Iteration 1596, loss = 1.18935382\n",
      "Iteration 1597, loss = 1.18915226\n",
      "Iteration 1598, loss = 1.18895072\n",
      "Iteration 1599, loss = 1.18874948\n",
      "Iteration 1600, loss = 1.18854922\n",
      "Iteration 1601, loss = 1.18834938\n",
      "Iteration 1602, loss = 1.18814653\n",
      "Iteration 1603, loss = 1.18794535\n",
      "Iteration 1604, loss = 1.18774458\n",
      "Iteration 1605, loss = 1.18754381\n",
      "Iteration 1606, loss = 1.18734563\n",
      "Iteration 1607, loss = 1.18714467\n",
      "Iteration 1608, loss = 1.18694269\n",
      "Iteration 1609, loss = 1.18674244\n",
      "Iteration 1610, loss = 1.18654302\n",
      "Iteration 1611, loss = 1.18634292\n",
      "Iteration 1612, loss = 1.18614305\n",
      "Iteration 1613, loss = 1.18594279\n",
      "Iteration 1614, loss = 1.18574418\n",
      "Iteration 1615, loss = 1.18554327\n",
      "Iteration 1616, loss = 1.18534468\n",
      "Iteration 1617, loss = 1.18514509\n",
      "Iteration 1618, loss = 1.18494680\n",
      "Iteration 1619, loss = 1.18474600\n",
      "Iteration 1620, loss = 1.18454689\n",
      "Iteration 1621, loss = 1.18434800\n",
      "Iteration 1622, loss = 1.18414994\n",
      "Iteration 1623, loss = 1.18395096\n",
      "Iteration 1624, loss = 1.18375315\n",
      "Iteration 1625, loss = 1.18355281\n",
      "Iteration 1626, loss = 1.18335485\n",
      "Iteration 1627, loss = 1.18315655\n",
      "Iteration 1628, loss = 1.18295861\n",
      "Iteration 1629, loss = 1.18276049\n",
      "Iteration 1630, loss = 1.18256198\n",
      "Iteration 1631, loss = 1.18236522\n",
      "Iteration 1632, loss = 1.18216586\n",
      "Iteration 1633, loss = 1.18196736\n",
      "Iteration 1634, loss = 1.18176987\n",
      "Iteration 1635, loss = 1.18157146\n",
      "Iteration 1636, loss = 1.18137463\n",
      "Iteration 1637, loss = 1.18117741\n",
      "Iteration 1638, loss = 1.18098021\n",
      "Iteration 1639, loss = 1.18078259\n",
      "Iteration 1640, loss = 1.18058597\n",
      "Iteration 1641, loss = 1.18038801\n",
      "Iteration 1642, loss = 1.18019065\n",
      "Iteration 1643, loss = 1.17999449\n",
      "Iteration 1644, loss = 1.17979824\n",
      "Iteration 1645, loss = 1.17960104\n",
      "Iteration 1646, loss = 1.17940304\n",
      "Iteration 1647, loss = 1.17920749\n",
      "Iteration 1648, loss = 1.17901149\n",
      "Iteration 1649, loss = 1.17881509\n",
      "Iteration 1650, loss = 1.17861906\n",
      "Iteration 1651, loss = 1.17842279\n",
      "Iteration 1652, loss = 1.17822724\n",
      "Iteration 1653, loss = 1.17803016\n",
      "Iteration 1654, loss = 1.17783382\n",
      "Iteration 1655, loss = 1.17763793\n",
      "Iteration 1656, loss = 1.17744243\n",
      "Iteration 1657, loss = 1.17724812\n",
      "Iteration 1658, loss = 1.17705179\n",
      "Iteration 1659, loss = 1.17685649\n",
      "Iteration 1660, loss = 1.17666043\n",
      "Iteration 1661, loss = 1.17646666\n",
      "Iteration 1662, loss = 1.17626966\n",
      "Iteration 1663, loss = 1.17607508\n",
      "Iteration 1664, loss = 1.17588030\n",
      "Iteration 1665, loss = 1.17568450\n",
      "Iteration 1666, loss = 1.17549059\n",
      "Iteration 1667, loss = 1.17529548\n",
      "Iteration 1668, loss = 1.17510041\n",
      "Iteration 1669, loss = 1.17490654\n",
      "Iteration 1670, loss = 1.17471171\n",
      "Iteration 1671, loss = 1.17451861\n",
      "Iteration 1672, loss = 1.17432301\n",
      "Iteration 1673, loss = 1.17413024\n",
      "Iteration 1674, loss = 1.17393455\n",
      "Iteration 1675, loss = 1.17374198\n",
      "Iteration 1676, loss = 1.17354706\n",
      "Iteration 1677, loss = 1.17335518\n",
      "Iteration 1678, loss = 1.17316078\n",
      "Iteration 1679, loss = 1.17296565\n",
      "Iteration 1680, loss = 1.17277278\n",
      "Iteration 1681, loss = 1.17257892\n",
      "Iteration 1682, loss = 1.17238611\n",
      "Iteration 1683, loss = 1.17219287\n",
      "Iteration 1684, loss = 1.17199963\n",
      "Iteration 1685, loss = 1.17180559\n",
      "Iteration 1686, loss = 1.17161337\n",
      "Iteration 1687, loss = 1.17141952\n",
      "Iteration 1688, loss = 1.17122767\n",
      "Iteration 1689, loss = 1.17103459\n",
      "Iteration 1690, loss = 1.17084382\n",
      "Iteration 1691, loss = 1.17064893\n",
      "Iteration 1692, loss = 1.17045733\n",
      "Iteration 1693, loss = 1.17026460\n",
      "Iteration 1694, loss = 1.17007264\n",
      "Iteration 1695, loss = 1.16988150\n",
      "Iteration 1696, loss = 1.16968946\n",
      "Iteration 1697, loss = 1.16949786\n",
      "Iteration 1698, loss = 1.16930587\n",
      "Iteration 1699, loss = 1.16911495\n",
      "Iteration 1700, loss = 1.16892165\n",
      "Iteration 1701, loss = 1.16872985\n",
      "Iteration 1702, loss = 1.16853841\n",
      "Iteration 1703, loss = 1.16834697\n",
      "Iteration 1704, loss = 1.16815765\n",
      "Iteration 1705, loss = 1.16796389\n",
      "Iteration 1706, loss = 1.16777369\n",
      "Iteration 1707, loss = 1.16758330\n",
      "Iteration 1708, loss = 1.16739202\n",
      "Iteration 1709, loss = 1.16720134\n",
      "Iteration 1710, loss = 1.16700993\n",
      "Iteration 1711, loss = 1.16681965\n",
      "Iteration 1712, loss = 1.16662980\n",
      "Iteration 1713, loss = 1.16643699\n",
      "Iteration 1714, loss = 1.16624947\n",
      "Iteration 1715, loss = 1.16605813\n",
      "Iteration 1716, loss = 1.16586974\n",
      "Iteration 1717, loss = 1.16567921\n",
      "Iteration 1718, loss = 1.16548836\n",
      "Iteration 1719, loss = 1.16529848\n",
      "Iteration 1720, loss = 1.16510684\n",
      "Iteration 1721, loss = 1.16491753\n",
      "Iteration 1722, loss = 1.16472861\n",
      "Iteration 1723, loss = 1.16453873\n",
      "Iteration 1724, loss = 1.16435001\n",
      "Iteration 1725, loss = 1.16416186\n",
      "Iteration 1726, loss = 1.16397026\n",
      "Iteration 1727, loss = 1.16378069\n",
      "Iteration 1728, loss = 1.16359163\n",
      "Iteration 1729, loss = 1.16340266\n",
      "Iteration 1730, loss = 1.16321438\n",
      "Iteration 1731, loss = 1.16302403\n",
      "Iteration 1732, loss = 1.16283522\n",
      "Iteration 1733, loss = 1.16264773\n",
      "Iteration 1734, loss = 1.16246050\n",
      "Iteration 1735, loss = 1.16227025\n",
      "Iteration 1736, loss = 1.16208188\n",
      "Iteration 1737, loss = 1.16189258\n",
      "Iteration 1738, loss = 1.16170490\n",
      "Iteration 1739, loss = 1.16151689\n",
      "Iteration 1740, loss = 1.16132844\n",
      "Iteration 1741, loss = 1.16114141\n",
      "Iteration 1742, loss = 1.16095345\n",
      "Iteration 1743, loss = 1.16076634\n",
      "Iteration 1744, loss = 1.16057818\n",
      "Iteration 1745, loss = 1.16039009\n",
      "Iteration 1746, loss = 1.16020269\n",
      "Iteration 1747, loss = 1.16001544\n",
      "Iteration 1748, loss = 1.15982719\n",
      "Iteration 1749, loss = 1.15964089\n",
      "Iteration 1750, loss = 1.15945280\n",
      "Iteration 1751, loss = 1.15926723\n",
      "Iteration 1752, loss = 1.15907967\n",
      "Iteration 1753, loss = 1.15889310\n",
      "Iteration 1754, loss = 1.15870597\n",
      "Iteration 1755, loss = 1.15851707\n",
      "Iteration 1756, loss = 1.15833155\n",
      "Iteration 1757, loss = 1.15814592\n",
      "Iteration 1758, loss = 1.15795930\n",
      "Iteration 1759, loss = 1.15777295\n",
      "Iteration 1760, loss = 1.15758654\n",
      "Iteration 1761, loss = 1.15740086\n",
      "Iteration 1762, loss = 1.15721317\n",
      "Iteration 1763, loss = 1.15702820\n",
      "Iteration 1764, loss = 1.15684117\n",
      "Iteration 1765, loss = 1.15665527\n",
      "Iteration 1766, loss = 1.15647011\n",
      "Iteration 1767, loss = 1.15628334\n",
      "Iteration 1768, loss = 1.15609843\n",
      "Iteration 1769, loss = 1.15591313\n",
      "Iteration 1770, loss = 1.15572744\n",
      "Iteration 1771, loss = 1.15554208\n",
      "Iteration 1772, loss = 1.15535663\n",
      "Iteration 1773, loss = 1.15517315\n",
      "Iteration 1774, loss = 1.15498746\n",
      "Iteration 1775, loss = 1.15480221\n",
      "Iteration 1776, loss = 1.15461663\n",
      "Iteration 1777, loss = 1.15443108\n",
      "Iteration 1778, loss = 1.15424559\n",
      "Iteration 1779, loss = 1.15406367\n",
      "Iteration 1780, loss = 1.15387782\n",
      "Iteration 1781, loss = 1.15369306\n",
      "Iteration 1782, loss = 1.15350860\n",
      "Iteration 1783, loss = 1.15332273\n",
      "Iteration 1784, loss = 1.15313927\n",
      "Iteration 1785, loss = 1.15295428\n",
      "Iteration 1786, loss = 1.15276952\n",
      "Iteration 1787, loss = 1.15258696\n",
      "Iteration 1788, loss = 1.15240158\n",
      "Iteration 1789, loss = 1.15222089\n",
      "Iteration 1790, loss = 1.15203635\n",
      "Iteration 1791, loss = 1.15185132\n",
      "Iteration 1792, loss = 1.15166789\n",
      "Iteration 1793, loss = 1.15148607\n",
      "Iteration 1794, loss = 1.15130102\n",
      "Iteration 1795, loss = 1.15111811\n",
      "Iteration 1796, loss = 1.15093310\n",
      "Iteration 1797, loss = 1.15075085\n",
      "Iteration 1798, loss = 1.15056781\n",
      "Iteration 1799, loss = 1.15038392\n",
      "Iteration 1800, loss = 1.15020144\n",
      "Iteration 1801, loss = 1.15001852\n",
      "Iteration 1802, loss = 1.14983573\n",
      "Iteration 1803, loss = 1.14965120\n",
      "Iteration 1804, loss = 1.14946854\n",
      "Iteration 1805, loss = 1.14928643\n",
      "Iteration 1806, loss = 1.14910329\n",
      "Iteration 1807, loss = 1.14892016\n",
      "Iteration 1808, loss = 1.14873682\n",
      "Iteration 1809, loss = 1.14855623\n",
      "Iteration 1810, loss = 1.14837293\n",
      "Iteration 1811, loss = 1.14819065\n",
      "Iteration 1812, loss = 1.14800980\n",
      "Iteration 1813, loss = 1.14782688\n",
      "Iteration 1814, loss = 1.14764514\n",
      "Iteration 1815, loss = 1.14746385\n",
      "Iteration 1816, loss = 1.14728215\n",
      "Iteration 1817, loss = 1.14709923\n",
      "Iteration 1818, loss = 1.14691776\n",
      "Iteration 1819, loss = 1.14673659\n",
      "Iteration 1820, loss = 1.14655557\n",
      "Iteration 1821, loss = 1.14637268\n",
      "Iteration 1822, loss = 1.14619217\n",
      "Iteration 1823, loss = 1.14601054\n",
      "Iteration 1824, loss = 1.14583072\n",
      "Iteration 1825, loss = 1.14564827\n",
      "Iteration 1826, loss = 1.14546849\n",
      "Iteration 1827, loss = 1.14528562\n",
      "Iteration 1828, loss = 1.14510477\n",
      "Iteration 1829, loss = 1.14492327\n",
      "Iteration 1830, loss = 1.14474322\n",
      "Iteration 1831, loss = 1.14456324\n",
      "Iteration 1832, loss = 1.14438222\n",
      "Iteration 1833, loss = 1.14420301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1834, loss = 1.14402154\n",
      "Iteration 1835, loss = 1.14384074\n",
      "Iteration 1836, loss = 1.14366240\n",
      "Iteration 1837, loss = 1.14347981\n",
      "Iteration 1838, loss = 1.14330136\n",
      "Iteration 1839, loss = 1.14312110\n",
      "Iteration 1840, loss = 1.14293957\n",
      "Iteration 1841, loss = 1.14276071\n",
      "Iteration 1842, loss = 1.14257990\n",
      "Iteration 1843, loss = 1.14240018\n",
      "Iteration 1844, loss = 1.14222098\n",
      "Iteration 1845, loss = 1.14204147\n",
      "Iteration 1846, loss = 1.14186301\n",
      "Iteration 1847, loss = 1.14168353\n",
      "Iteration 1848, loss = 1.14150271\n",
      "Iteration 1849, loss = 1.14132536\n",
      "Iteration 1850, loss = 1.14114392\n",
      "Iteration 1851, loss = 1.14096521\n",
      "Iteration 1852, loss = 1.14078511\n",
      "Iteration 1853, loss = 1.14060761\n",
      "Iteration 1854, loss = 1.14042816\n",
      "Iteration 1855, loss = 1.14025133\n",
      "Iteration 1856, loss = 1.14007078\n",
      "Iteration 1857, loss = 1.13989145\n",
      "Iteration 1858, loss = 1.13971501\n",
      "Iteration 1859, loss = 1.13953613\n",
      "Iteration 1860, loss = 1.13935636\n",
      "Iteration 1861, loss = 1.13917804\n",
      "Iteration 1862, loss = 1.13900063\n",
      "Iteration 1863, loss = 1.13882126\n",
      "Iteration 1864, loss = 1.13864346\n",
      "Iteration 1865, loss = 1.13846631\n",
      "Iteration 1866, loss = 1.13828804\n",
      "Iteration 1867, loss = 1.13811021\n",
      "Iteration 1868, loss = 1.13793108\n",
      "Iteration 1869, loss = 1.13775427\n",
      "Iteration 1870, loss = 1.13757699\n",
      "Iteration 1871, loss = 1.13739729\n",
      "Iteration 1872, loss = 1.13722028\n",
      "Iteration 1873, loss = 1.13704406\n",
      "Iteration 1874, loss = 1.13686664\n",
      "Iteration 1875, loss = 1.13668932\n",
      "Iteration 1876, loss = 1.13651183\n",
      "Iteration 1877, loss = 1.13633533\n",
      "Iteration 1878, loss = 1.13615699\n",
      "Iteration 1879, loss = 1.13598015\n",
      "Iteration 1880, loss = 1.13580276\n",
      "Iteration 1881, loss = 1.13562665\n",
      "Iteration 1882, loss = 1.13545032\n",
      "Iteration 1883, loss = 1.13527265\n",
      "Iteration 1884, loss = 1.13509656\n",
      "Iteration 1885, loss = 1.13491982\n",
      "Iteration 1886, loss = 1.13474266\n",
      "Iteration 1887, loss = 1.13456783\n",
      "Iteration 1888, loss = 1.13439068\n",
      "Iteration 1889, loss = 1.13421426\n",
      "Iteration 1890, loss = 1.13403926\n",
      "Iteration 1891, loss = 1.13386145\n",
      "Iteration 1892, loss = 1.13368682\n",
      "Iteration 1893, loss = 1.13351016\n",
      "Iteration 1894, loss = 1.13333469\n",
      "Iteration 1895, loss = 1.13315983\n",
      "Iteration 1896, loss = 1.13298480\n",
      "Iteration 1897, loss = 1.13280683\n",
      "Iteration 1898, loss = 1.13263272\n",
      "Iteration 1899, loss = 1.13245698\n",
      "Iteration 1900, loss = 1.13228151\n",
      "Iteration 1901, loss = 1.13210637\n",
      "Iteration 1902, loss = 1.13193118\n",
      "Iteration 1903, loss = 1.13175624\n",
      "Iteration 1904, loss = 1.13158069\n",
      "Iteration 1905, loss = 1.13140497\n",
      "Iteration 1906, loss = 1.13123073\n",
      "Iteration 1907, loss = 1.13105658\n",
      "Iteration 1908, loss = 1.13088167\n",
      "Iteration 1909, loss = 1.13070665\n",
      "Iteration 1910, loss = 1.13053177\n",
      "Iteration 1911, loss = 1.13035791\n",
      "Iteration 1912, loss = 1.13018205\n",
      "Iteration 1913, loss = 1.13000806\n",
      "Iteration 1914, loss = 1.12983442\n",
      "Iteration 1915, loss = 1.12965985\n",
      "Iteration 1916, loss = 1.12948690\n",
      "Iteration 1917, loss = 1.12931233\n",
      "Iteration 1918, loss = 1.12913870\n",
      "Iteration 1919, loss = 1.12896445\n",
      "Iteration 1920, loss = 1.12879079\n",
      "Iteration 1921, loss = 1.12861742\n",
      "Iteration 1922, loss = 1.12844336\n",
      "Iteration 1923, loss = 1.12827033\n",
      "Iteration 1924, loss = 1.12809688\n",
      "Iteration 1925, loss = 1.12792290\n",
      "Iteration 1926, loss = 1.12775022\n",
      "Iteration 1927, loss = 1.12757622\n",
      "Iteration 1928, loss = 1.12740255\n",
      "Iteration 1929, loss = 1.12722913\n",
      "Iteration 1930, loss = 1.12706014\n",
      "Iteration 1931, loss = 1.12688470\n",
      "Iteration 1932, loss = 1.12671083\n",
      "Iteration 1933, loss = 1.12653950\n",
      "Iteration 1934, loss = 1.12636572\n",
      "Iteration 1935, loss = 1.12619220\n",
      "Iteration 1936, loss = 1.12602079\n",
      "Iteration 1937, loss = 1.12584866\n",
      "Iteration 1938, loss = 1.12567577\n",
      "Iteration 1939, loss = 1.12550182\n",
      "Iteration 1940, loss = 1.12533005\n",
      "Iteration 1941, loss = 1.12515892\n",
      "Iteration 1942, loss = 1.12498774\n",
      "Iteration 1943, loss = 1.12481387\n",
      "Iteration 1944, loss = 1.12464238\n",
      "Iteration 1945, loss = 1.12447001\n",
      "Iteration 1946, loss = 1.12430025\n",
      "Iteration 1947, loss = 1.12412704\n",
      "Iteration 1948, loss = 1.12395495\n",
      "Iteration 1949, loss = 1.12378329\n",
      "Iteration 1950, loss = 1.12361249\n",
      "Iteration 1951, loss = 1.12344022\n",
      "Iteration 1952, loss = 1.12326859\n",
      "Iteration 1953, loss = 1.12309791\n",
      "Iteration 1954, loss = 1.12292684\n",
      "Iteration 1955, loss = 1.12275502\n",
      "Iteration 1956, loss = 1.12258339\n",
      "Iteration 1957, loss = 1.12241214\n",
      "Iteration 1958, loss = 1.12224061\n",
      "Iteration 1959, loss = 1.12207105\n",
      "Iteration 1960, loss = 1.12189987\n",
      "Iteration 1961, loss = 1.12172987\n",
      "Iteration 1962, loss = 1.12155802\n",
      "Iteration 1963, loss = 1.12138803\n",
      "Iteration 1964, loss = 1.12121592\n",
      "Iteration 1965, loss = 1.12104653\n",
      "Iteration 1966, loss = 1.12087558\n",
      "Iteration 1967, loss = 1.12070505\n",
      "Iteration 1968, loss = 1.12053479\n",
      "Iteration 1969, loss = 1.12036597\n",
      "Iteration 1970, loss = 1.12019679\n",
      "Iteration 1971, loss = 1.12002522\n",
      "Iteration 1972, loss = 1.11985497\n",
      "Iteration 1973, loss = 1.11968473\n",
      "Iteration 1974, loss = 1.11951526\n",
      "Iteration 1975, loss = 1.11934571\n",
      "Iteration 1976, loss = 1.11917641\n",
      "Iteration 1977, loss = 1.11900622\n",
      "Iteration 1978, loss = 1.11883582\n",
      "Iteration 1979, loss = 1.11866725\n",
      "Iteration 1980, loss = 1.11849766\n",
      "Iteration 1981, loss = 1.11832905\n",
      "Iteration 1982, loss = 1.11815853\n",
      "Iteration 1983, loss = 1.11799017\n",
      "Iteration 1984, loss = 1.11782067\n",
      "Iteration 1985, loss = 1.11765181\n",
      "Iteration 1986, loss = 1.11748292\n",
      "Iteration 1987, loss = 1.11731488\n",
      "Iteration 1988, loss = 1.11714539\n",
      "Iteration 1989, loss = 1.11697608\n",
      "Iteration 1990, loss = 1.11680869\n",
      "Iteration 1991, loss = 1.11663990\n",
      "Iteration 1992, loss = 1.11647134\n",
      "Iteration 1993, loss = 1.11630213\n",
      "Iteration 1994, loss = 1.11613453\n",
      "Iteration 1995, loss = 1.11596725\n",
      "Iteration 1996, loss = 1.11579710\n",
      "Iteration 1997, loss = 1.11562961\n",
      "Iteration 1998, loss = 1.11546106\n",
      "Iteration 1999, loss = 1.11529247\n",
      "Iteration 2000, loss = 1.11512658\n",
      "Iteration 2001, loss = 1.11495824\n",
      "Iteration 2002, loss = 1.11479084\n",
      "Iteration 2003, loss = 1.11462115\n",
      "Iteration 2004, loss = 1.11445407\n",
      "Iteration 2005, loss = 1.11428665\n",
      "Iteration 2006, loss = 1.11412053\n",
      "Iteration 2007, loss = 1.11395250\n",
      "Iteration 2008, loss = 1.11378436\n",
      "Iteration 2009, loss = 1.11361617\n",
      "Iteration 2010, loss = 1.11345053\n",
      "Iteration 2011, loss = 1.11328289\n",
      "Iteration 2012, loss = 1.11311612\n",
      "Iteration 2013, loss = 1.11294828\n",
      "Iteration 2014, loss = 1.11278175\n",
      "Iteration 2015, loss = 1.11261389\n",
      "Iteration 2016, loss = 1.11244725\n",
      "Iteration 2017, loss = 1.11228133\n",
      "Iteration 2018, loss = 1.11211481\n",
      "Iteration 2019, loss = 1.11194726\n",
      "Iteration 2020, loss = 1.11178166\n",
      "Iteration 2021, loss = 1.11161479\n",
      "Iteration 2022, loss = 1.11144760\n",
      "Iteration 2023, loss = 1.11128117\n",
      "Iteration 2024, loss = 1.11111435\n",
      "Iteration 2025, loss = 1.11094828\n",
      "Iteration 2026, loss = 1.11078240\n",
      "Iteration 2027, loss = 1.11061669\n",
      "Iteration 2028, loss = 1.11045242\n",
      "Iteration 2029, loss = 1.11028514\n",
      "Iteration 2030, loss = 1.11011871\n",
      "Iteration 2031, loss = 1.10995465\n",
      "Iteration 2032, loss = 1.10978803\n",
      "Iteration 2033, loss = 1.10962281\n",
      "Iteration 2034, loss = 1.10945838\n",
      "Iteration 2035, loss = 1.10928997\n",
      "Iteration 2036, loss = 1.10912561\n",
      "Iteration 2037, loss = 1.10895941\n",
      "Iteration 2038, loss = 1.10879600\n",
      "Iteration 2039, loss = 1.10863041\n",
      "Iteration 2040, loss = 1.10846510\n",
      "Iteration 2041, loss = 1.10830084\n",
      "Iteration 2042, loss = 1.10813466\n",
      "Iteration 2043, loss = 1.10796999\n",
      "Iteration 2044, loss = 1.10780604\n",
      "Iteration 2045, loss = 1.10764078\n",
      "Iteration 2046, loss = 1.10747721\n",
      "Iteration 2047, loss = 1.10731287\n",
      "Iteration 2048, loss = 1.10714692\n",
      "Iteration 2049, loss = 1.10698297\n",
      "Iteration 2050, loss = 1.10681939\n",
      "Iteration 2051, loss = 1.10665393\n",
      "Iteration 2052, loss = 1.10648874\n",
      "Iteration 2053, loss = 1.10632545\n",
      "Iteration 2054, loss = 1.10616104\n",
      "Iteration 2055, loss = 1.10599749\n",
      "Iteration 2056, loss = 1.10583358\n",
      "Iteration 2057, loss = 1.10566959\n",
      "Iteration 2058, loss = 1.10550421\n",
      "Iteration 2059, loss = 1.10534087\n",
      "Iteration 2060, loss = 1.10517758\n",
      "Iteration 2061, loss = 1.10501562\n",
      "Iteration 2062, loss = 1.10484965\n",
      "Iteration 2063, loss = 1.10468661\n",
      "Iteration 2064, loss = 1.10452296\n",
      "Iteration 2065, loss = 1.10436006\n",
      "Iteration 2066, loss = 1.10419738\n",
      "Iteration 2067, loss = 1.10403328\n",
      "Iteration 2068, loss = 1.10387047\n",
      "Iteration 2069, loss = 1.10370639\n",
      "Iteration 2070, loss = 1.10354352\n",
      "Iteration 2071, loss = 1.10338023\n",
      "Iteration 2072, loss = 1.10321697\n",
      "Iteration 2073, loss = 1.10305441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2074, loss = 1.10289080\n",
      "Iteration 2075, loss = 1.10272875\n",
      "Iteration 2076, loss = 1.10256572\n",
      "Iteration 2077, loss = 1.10240427\n",
      "Iteration 2078, loss = 1.10223964\n",
      "Iteration 2079, loss = 1.10207912\n",
      "Iteration 2080, loss = 1.10191611\n",
      "Iteration 2081, loss = 1.10175321\n",
      "Iteration 2082, loss = 1.10159090\n",
      "Iteration 2083, loss = 1.10142929\n",
      "Iteration 2084, loss = 1.10126739\n",
      "Iteration 2085, loss = 1.10110437\n",
      "Iteration 2086, loss = 1.10094258\n",
      "Iteration 2087, loss = 1.10078053\n",
      "Iteration 2088, loss = 1.10061917\n",
      "Iteration 2089, loss = 1.10045673\n",
      "Iteration 2090, loss = 1.10029520\n",
      "Iteration 2091, loss = 1.10013435\n",
      "Iteration 2092, loss = 1.09997293\n",
      "Iteration 2093, loss = 1.09981068\n",
      "Iteration 2094, loss = 1.09964847\n",
      "Iteration 2095, loss = 1.09948834\n",
      "Iteration 2096, loss = 1.09932690\n",
      "Iteration 2097, loss = 1.09916579\n",
      "Iteration 2098, loss = 1.09900375\n",
      "Iteration 2099, loss = 1.09884155\n",
      "Iteration 2100, loss = 1.09868256\n",
      "Iteration 2101, loss = 1.09852040\n",
      "Iteration 2102, loss = 1.09835951\n",
      "Iteration 2103, loss = 1.09819894\n",
      "Iteration 2104, loss = 1.09803898\n",
      "Iteration 2105, loss = 1.09787860\n",
      "Iteration 2106, loss = 1.09771649\n",
      "Iteration 2107, loss = 1.09755643\n",
      "Iteration 2108, loss = 1.09739608\n",
      "Iteration 2109, loss = 1.09723543\n",
      "Iteration 2110, loss = 1.09707420\n",
      "Iteration 2111, loss = 1.09691547\n",
      "Iteration 2112, loss = 1.09675482\n",
      "Iteration 2113, loss = 1.09659472\n",
      "Iteration 2114, loss = 1.09643478\n",
      "Iteration 2115, loss = 1.09627585\n",
      "Iteration 2116, loss = 1.09611503\n",
      "Iteration 2117, loss = 1.09595470\n",
      "Iteration 2118, loss = 1.09579526\n",
      "Iteration 2119, loss = 1.09563614\n",
      "Iteration 2120, loss = 1.09547520\n",
      "Iteration 2121, loss = 1.09531495\n",
      "Iteration 2122, loss = 1.09515543\n",
      "Iteration 2123, loss = 1.09499697\n",
      "Iteration 2124, loss = 1.09483737\n",
      "Iteration 2125, loss = 1.09467780\n",
      "Iteration 2126, loss = 1.09451763\n",
      "Iteration 2127, loss = 1.09435885\n",
      "Iteration 2128, loss = 1.09420053\n",
      "Iteration 2129, loss = 1.09404090\n",
      "Iteration 2130, loss = 1.09388183\n",
      "Iteration 2131, loss = 1.09372208\n",
      "Iteration 2132, loss = 1.09356329\n",
      "Iteration 2133, loss = 1.09340478\n",
      "Iteration 2134, loss = 1.09324659\n",
      "Iteration 2135, loss = 1.09308761\n",
      "Iteration 2136, loss = 1.09292869\n",
      "Iteration 2137, loss = 1.09277002\n",
      "Iteration 2138, loss = 1.09261138\n",
      "Iteration 2139, loss = 1.09245283\n",
      "Iteration 2140, loss = 1.09229597\n",
      "Iteration 2141, loss = 1.09213544\n",
      "Iteration 2142, loss = 1.09197834\n",
      "Iteration 2143, loss = 1.09181976\n",
      "Iteration 2144, loss = 1.09166133\n",
      "Iteration 2145, loss = 1.09150365\n",
      "Iteration 2146, loss = 1.09134542\n",
      "Iteration 2147, loss = 1.09118772\n",
      "Iteration 2148, loss = 1.09102969\n",
      "Iteration 2149, loss = 1.09087179\n",
      "Iteration 2150, loss = 1.09071355\n",
      "Iteration 2151, loss = 1.09055598\n",
      "Iteration 2152, loss = 1.09039743\n",
      "Iteration 2153, loss = 1.09024073\n",
      "Iteration 2154, loss = 1.09008384\n",
      "Iteration 2155, loss = 1.08992555\n",
      "Iteration 2156, loss = 1.08976808\n",
      "Iteration 2157, loss = 1.08961126\n",
      "Iteration 2158, loss = 1.08945304\n",
      "Iteration 2159, loss = 1.08929664\n",
      "Iteration 2160, loss = 1.08913973\n",
      "Iteration 2161, loss = 1.08898185\n",
      "Iteration 2162, loss = 1.08882537\n",
      "Iteration 2163, loss = 1.08866915\n",
      "Iteration 2164, loss = 1.08851128\n",
      "Iteration 2165, loss = 1.08835360\n",
      "Iteration 2166, loss = 1.08819711\n",
      "Iteration 2167, loss = 1.08804140\n",
      "Iteration 2168, loss = 1.08788287\n",
      "Iteration 2169, loss = 1.08772654\n",
      "Iteration 2170, loss = 1.08757189\n",
      "Iteration 2171, loss = 1.08741355\n",
      "Iteration 2172, loss = 1.08725789\n",
      "Iteration 2173, loss = 1.08710162\n",
      "Iteration 2174, loss = 1.08694412\n",
      "Iteration 2175, loss = 1.08678884\n",
      "Iteration 2176, loss = 1.08663277\n",
      "Iteration 2177, loss = 1.08647658\n",
      "Iteration 2178, loss = 1.08632205\n",
      "Iteration 2179, loss = 1.08616410\n",
      "Iteration 2180, loss = 1.08600837\n",
      "Iteration 2181, loss = 1.08585243\n",
      "Iteration 2182, loss = 1.08569567\n",
      "Iteration 2183, loss = 1.08554169\n",
      "Iteration 2184, loss = 1.08538514\n",
      "Iteration 2185, loss = 1.08523101\n",
      "Iteration 2186, loss = 1.08507322\n",
      "Iteration 2187, loss = 1.08491869\n",
      "Iteration 2188, loss = 1.08476286\n",
      "Iteration 2189, loss = 1.08460780\n",
      "Iteration 2190, loss = 1.08445212\n",
      "Iteration 2191, loss = 1.08429780\n",
      "Iteration 2192, loss = 1.08414107\n",
      "Iteration 2193, loss = 1.08398681\n",
      "Iteration 2194, loss = 1.08383218\n",
      "Iteration 2195, loss = 1.08367653\n",
      "Iteration 2196, loss = 1.08352280\n",
      "Iteration 2197, loss = 1.08336651\n",
      "Iteration 2198, loss = 1.08321394\n",
      "Iteration 2199, loss = 1.08305730\n",
      "Iteration 2200, loss = 1.08290386\n",
      "Iteration 2201, loss = 1.08274771\n",
      "Iteration 2202, loss = 1.08259415\n",
      "Iteration 2203, loss = 1.08243865\n",
      "Iteration 2204, loss = 1.08228439\n",
      "Iteration 2205, loss = 1.08213020\n",
      "Iteration 2206, loss = 1.08197641\n",
      "Iteration 2207, loss = 1.08182319\n",
      "Iteration 2208, loss = 1.08166756\n",
      "Iteration 2209, loss = 1.08151285\n",
      "Iteration 2210, loss = 1.08135936\n",
      "Iteration 2211, loss = 1.08120631\n",
      "Iteration 2212, loss = 1.08105060\n",
      "Iteration 2213, loss = 1.08089846\n",
      "Iteration 2214, loss = 1.08074355\n",
      "Iteration 2215, loss = 1.08059011\n",
      "Iteration 2216, loss = 1.08043660\n",
      "Iteration 2217, loss = 1.08028220\n",
      "Iteration 2218, loss = 1.08012828\n",
      "Iteration 2219, loss = 1.07997488\n",
      "Iteration 2220, loss = 1.07982214\n",
      "Iteration 2221, loss = 1.07966885\n",
      "Iteration 2222, loss = 1.07951486\n",
      "Iteration 2223, loss = 1.07936204\n",
      "Iteration 2224, loss = 1.07920904\n",
      "Iteration 2225, loss = 1.07905455\n",
      "Iteration 2226, loss = 1.07890153\n",
      "Iteration 2227, loss = 1.07874887\n",
      "Iteration 2228, loss = 1.07859516\n",
      "Iteration 2229, loss = 1.07844123\n",
      "Iteration 2230, loss = 1.07829070\n",
      "Iteration 2231, loss = 1.07813712\n",
      "Iteration 2232, loss = 1.07798412\n",
      "Iteration 2233, loss = 1.07783181\n",
      "Iteration 2234, loss = 1.07767860\n",
      "Iteration 2235, loss = 1.07752602\n",
      "Iteration 2236, loss = 1.07737439\n",
      "Iteration 2237, loss = 1.07722203\n",
      "Iteration 2238, loss = 1.07706742\n",
      "Iteration 2239, loss = 1.07691586\n",
      "Iteration 2240, loss = 1.07676364\n",
      "Iteration 2241, loss = 1.07661140\n",
      "Iteration 2242, loss = 1.07645891\n",
      "Iteration 2243, loss = 1.07630743\n",
      "Iteration 2244, loss = 1.07615534\n",
      "Iteration 2245, loss = 1.07600323\n",
      "Iteration 2246, loss = 1.07585166\n",
      "Iteration 2247, loss = 1.07570092\n",
      "Iteration 2248, loss = 1.07554756\n",
      "Iteration 2249, loss = 1.07539531\n",
      "Iteration 2250, loss = 1.07524413\n",
      "Iteration 2251, loss = 1.07509175\n",
      "Iteration 2252, loss = 1.07494098\n",
      "Iteration 2253, loss = 1.07478819\n",
      "Iteration 2254, loss = 1.07463746\n",
      "Iteration 2255, loss = 1.07448736\n",
      "Iteration 2256, loss = 1.07433436\n",
      "Iteration 2257, loss = 1.07418279\n",
      "Iteration 2258, loss = 1.07403229\n",
      "Iteration 2259, loss = 1.07388001\n",
      "Iteration 2260, loss = 1.07373000\n",
      "Iteration 2261, loss = 1.07357729\n",
      "Iteration 2262, loss = 1.07342859\n",
      "Iteration 2263, loss = 1.07327607\n",
      "Iteration 2264, loss = 1.07312451\n",
      "Iteration 2265, loss = 1.07297404\n",
      "Iteration 2266, loss = 1.07282261\n",
      "Iteration 2267, loss = 1.07267351\n",
      "Iteration 2268, loss = 1.07252248\n",
      "Iteration 2269, loss = 1.07237287\n",
      "Iteration 2270, loss = 1.07222087\n",
      "Iteration 2271, loss = 1.07207088\n",
      "Iteration 2272, loss = 1.07192005\n",
      "Iteration 2273, loss = 1.07176968\n",
      "Iteration 2274, loss = 1.07161930\n",
      "Iteration 2275, loss = 1.07146988\n",
      "Iteration 2276, loss = 1.07131975\n",
      "Iteration 2277, loss = 1.07116865\n",
      "Iteration 2278, loss = 1.07101856\n",
      "Iteration 2279, loss = 1.07086925\n",
      "Iteration 2280, loss = 1.07071842\n",
      "Iteration 2281, loss = 1.07057058\n",
      "Iteration 2282, loss = 1.07041959\n",
      "Iteration 2283, loss = 1.07026928\n",
      "Iteration 2284, loss = 1.07011938\n",
      "Iteration 2285, loss = 1.06996990\n",
      "Iteration 2286, loss = 1.06982297\n",
      "Iteration 2287, loss = 1.06967144\n",
      "Iteration 2288, loss = 1.06952137\n",
      "Iteration 2289, loss = 1.06937260\n",
      "Iteration 2290, loss = 1.06922407\n",
      "Iteration 2291, loss = 1.06907456\n",
      "Iteration 2292, loss = 1.06892536\n",
      "Iteration 2293, loss = 1.06877608\n",
      "Iteration 2294, loss = 1.06862645\n",
      "Iteration 2295, loss = 1.06847750\n",
      "Iteration 2296, loss = 1.06832794\n",
      "Iteration 2297, loss = 1.06818052\n",
      "Iteration 2298, loss = 1.06803143\n",
      "Iteration 2299, loss = 1.06788208\n",
      "Iteration 2300, loss = 1.06773332\n",
      "Iteration 2301, loss = 1.06758527\n",
      "Iteration 2302, loss = 1.06743694\n",
      "Iteration 2303, loss = 1.06728715\n",
      "Iteration 2304, loss = 1.06713909\n",
      "Iteration 2305, loss = 1.06699146\n",
      "Iteration 2306, loss = 1.06684155\n",
      "Iteration 2307, loss = 1.06669470\n",
      "Iteration 2308, loss = 1.06654518\n",
      "Iteration 2309, loss = 1.06639771\n",
      "Iteration 2310, loss = 1.06624910\n",
      "Iteration 2311, loss = 1.06610119\n",
      "Iteration 2312, loss = 1.06595345\n",
      "Iteration 2313, loss = 1.06580616\n",
      "Iteration 2314, loss = 1.06565682\n",
      "Iteration 2315, loss = 1.06550942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2316, loss = 1.06536218\n",
      "Iteration 2317, loss = 1.06521389\n",
      "Iteration 2318, loss = 1.06506715\n",
      "Iteration 2319, loss = 1.06491913\n",
      "Iteration 2320, loss = 1.06477121\n",
      "Iteration 2321, loss = 1.06462619\n",
      "Iteration 2322, loss = 1.06447714\n",
      "Iteration 2323, loss = 1.06432847\n",
      "Iteration 2324, loss = 1.06418275\n",
      "Iteration 2325, loss = 1.06403568\n",
      "Iteration 2326, loss = 1.06388737\n",
      "Iteration 2327, loss = 1.06374039\n",
      "Iteration 2328, loss = 1.06359414\n",
      "Iteration 2329, loss = 1.06344737\n",
      "Iteration 2330, loss = 1.06329999\n",
      "Iteration 2331, loss = 1.06315212\n",
      "Iteration 2332, loss = 1.06300551\n",
      "Iteration 2333, loss = 1.06286112\n",
      "Iteration 2334, loss = 1.06271198\n",
      "Iteration 2335, loss = 1.06256716\n",
      "Iteration 2336, loss = 1.06241903\n",
      "Iteration 2337, loss = 1.06227222\n",
      "Iteration 2338, loss = 1.06212588\n",
      "Iteration 2339, loss = 1.06197953\n",
      "Iteration 2340, loss = 1.06183323\n",
      "Iteration 2341, loss = 1.06168736\n",
      "Iteration 2342, loss = 1.06154271\n",
      "Iteration 2343, loss = 1.06139527\n",
      "Iteration 2344, loss = 1.06124736\n",
      "Iteration 2345, loss = 1.06110236\n",
      "Iteration 2346, loss = 1.06095761\n",
      "Iteration 2347, loss = 1.06080947\n",
      "Iteration 2348, loss = 1.06066422\n",
      "Iteration 2349, loss = 1.06051841\n",
      "Iteration 2350, loss = 1.06037281\n",
      "Iteration 2351, loss = 1.06022751\n",
      "Iteration 2352, loss = 1.06008164\n",
      "Iteration 2353, loss = 1.05993522\n",
      "Iteration 2354, loss = 1.05978976\n",
      "Iteration 2355, loss = 1.05964502\n",
      "Iteration 2356, loss = 1.05949868\n",
      "Iteration 2357, loss = 1.05935319\n",
      "Iteration 2358, loss = 1.05920773\n",
      "Iteration 2359, loss = 1.05906313\n",
      "Iteration 2360, loss = 1.05891683\n",
      "Iteration 2361, loss = 1.05877268\n",
      "Iteration 2362, loss = 1.05862607\n",
      "Iteration 2363, loss = 1.05848204\n",
      "Iteration 2364, loss = 1.05833831\n",
      "Iteration 2365, loss = 1.05819200\n",
      "Iteration 2366, loss = 1.05804624\n",
      "Iteration 2367, loss = 1.05790151\n",
      "Iteration 2368, loss = 1.05775742\n",
      "Iteration 2369, loss = 1.05761158\n",
      "Iteration 2370, loss = 1.05746682\n",
      "Iteration 2371, loss = 1.05732220\n",
      "Iteration 2372, loss = 1.05717796\n",
      "Iteration 2373, loss = 1.05703376\n",
      "Iteration 2374, loss = 1.05688862\n",
      "Iteration 2375, loss = 1.05674348\n",
      "Iteration 2376, loss = 1.05660074\n",
      "Iteration 2377, loss = 1.05645489\n",
      "Iteration 2378, loss = 1.05631238\n",
      "Iteration 2379, loss = 1.05616645\n",
      "Iteration 2380, loss = 1.05602272\n",
      "Iteration 2381, loss = 1.05587909\n",
      "Iteration 2382, loss = 1.05573397\n",
      "Iteration 2383, loss = 1.05559113\n",
      "Iteration 2384, loss = 1.05544638\n",
      "Iteration 2385, loss = 1.05530484\n",
      "Iteration 2386, loss = 1.05515826\n",
      "Iteration 2387, loss = 1.05501609\n",
      "Iteration 2388, loss = 1.05487271\n",
      "Iteration 2389, loss = 1.05472759\n",
      "Iteration 2390, loss = 1.05458417\n",
      "Iteration 2391, loss = 1.05444014\n",
      "Iteration 2392, loss = 1.05429700\n",
      "Iteration 2393, loss = 1.05415325\n",
      "Iteration 2394, loss = 1.05401037\n",
      "Iteration 2395, loss = 1.05386779\n",
      "Iteration 2396, loss = 1.05372355\n",
      "Iteration 2397, loss = 1.05358104\n",
      "Iteration 2398, loss = 1.05343709\n",
      "Iteration 2399, loss = 1.05329469\n",
      "Iteration 2400, loss = 1.05315113\n",
      "Iteration 2401, loss = 1.05300725\n",
      "Iteration 2402, loss = 1.05286603\n",
      "Iteration 2403, loss = 1.05272270\n",
      "Iteration 2404, loss = 1.05258022\n",
      "Iteration 2405, loss = 1.05243652\n",
      "Iteration 2406, loss = 1.05229417\n",
      "Iteration 2407, loss = 1.05215248\n",
      "Iteration 2408, loss = 1.05200851\n",
      "Iteration 2409, loss = 1.05186611\n",
      "Iteration 2410, loss = 1.05172367\n",
      "Iteration 2411, loss = 1.05158184\n",
      "Iteration 2412, loss = 1.05143876\n",
      "Iteration 2413, loss = 1.05129643\n",
      "Iteration 2414, loss = 1.05115531\n",
      "Iteration 2415, loss = 1.05101213\n",
      "Iteration 2416, loss = 1.05086955\n",
      "Iteration 2417, loss = 1.05072810\n",
      "Iteration 2418, loss = 1.05058497\n",
      "Iteration 2419, loss = 1.05044422\n",
      "Iteration 2420, loss = 1.05030144\n",
      "Iteration 2421, loss = 1.05015917\n",
      "Iteration 2422, loss = 1.05001812\n",
      "Iteration 2423, loss = 1.04987603\n",
      "Iteration 2424, loss = 1.04973475\n",
      "Iteration 2425, loss = 1.04959350\n",
      "Iteration 2426, loss = 1.04945216\n",
      "Iteration 2427, loss = 1.04930966\n",
      "Iteration 2428, loss = 1.04916806\n",
      "Iteration 2429, loss = 1.04902694\n",
      "Iteration 2430, loss = 1.04888607\n",
      "Iteration 2431, loss = 1.04874376\n",
      "Iteration 2432, loss = 1.04860176\n",
      "Iteration 2433, loss = 1.04846204\n",
      "Iteration 2434, loss = 1.04831998\n",
      "Iteration 2435, loss = 1.04817878\n",
      "Iteration 2436, loss = 1.04803692\n",
      "Iteration 2437, loss = 1.04789727\n",
      "Iteration 2438, loss = 1.04775533\n",
      "Iteration 2439, loss = 1.04761440\n",
      "Iteration 2440, loss = 1.04747382\n",
      "Iteration 2441, loss = 1.04733287\n",
      "Iteration 2442, loss = 1.04719210\n",
      "Iteration 2443, loss = 1.04705090\n",
      "Iteration 2444, loss = 1.04691163\n",
      "Iteration 2445, loss = 1.04677003\n",
      "Iteration 2446, loss = 1.04662930\n",
      "Iteration 2447, loss = 1.04648893\n",
      "Iteration 2448, loss = 1.04634849\n",
      "Iteration 2449, loss = 1.04620741\n",
      "Iteration 2450, loss = 1.04606688\n",
      "Iteration 2451, loss = 1.04592754\n",
      "Iteration 2452, loss = 1.04578626\n",
      "Iteration 2453, loss = 1.04564667\n",
      "Iteration 2454, loss = 1.04550596\n",
      "Iteration 2455, loss = 1.04536599\n",
      "Iteration 2456, loss = 1.04522672\n",
      "Iteration 2457, loss = 1.04508607\n",
      "Iteration 2458, loss = 1.04494626\n",
      "Iteration 2459, loss = 1.04480644\n",
      "Iteration 2460, loss = 1.04466671\n",
      "Iteration 2461, loss = 1.04452664\n",
      "Iteration 2462, loss = 1.04438699\n",
      "Iteration 2463, loss = 1.04424839\n",
      "Iteration 2464, loss = 1.04410919\n",
      "Iteration 2465, loss = 1.04396886\n",
      "Iteration 2466, loss = 1.04382810\n",
      "Iteration 2467, loss = 1.04369010\n",
      "Iteration 2468, loss = 1.04354982\n",
      "Iteration 2469, loss = 1.04341090\n",
      "Iteration 2470, loss = 1.04327211\n",
      "Iteration 2471, loss = 1.04313175\n",
      "Iteration 2472, loss = 1.04299185\n",
      "Iteration 2473, loss = 1.04285400\n",
      "Iteration 2474, loss = 1.04271355\n",
      "Iteration 2475, loss = 1.04257438\n",
      "Iteration 2476, loss = 1.04243679\n",
      "Iteration 2477, loss = 1.04229638\n",
      "Iteration 2478, loss = 1.04215886\n",
      "Iteration 2479, loss = 1.04201757\n",
      "Iteration 2480, loss = 1.04188083\n",
      "Iteration 2481, loss = 1.04174185\n",
      "Iteration 2482, loss = 1.04160231\n",
      "Iteration 2483, loss = 1.04146429\n",
      "Iteration 2484, loss = 1.04132508\n",
      "Iteration 2485, loss = 1.04118680\n",
      "Iteration 2486, loss = 1.04104756\n",
      "Iteration 2487, loss = 1.04090820\n",
      "Iteration 2488, loss = 1.04077041\n",
      "Iteration 2489, loss = 1.04063164\n",
      "Iteration 2490, loss = 1.04049543\n",
      "Iteration 2491, loss = 1.04035768\n",
      "Iteration 2492, loss = 1.04021782\n",
      "Iteration 2493, loss = 1.04007996\n",
      "Iteration 2494, loss = 1.03994193\n",
      "Iteration 2495, loss = 1.03980288\n",
      "Iteration 2496, loss = 1.03966496\n",
      "Iteration 2497, loss = 1.03952682\n",
      "Iteration 2498, loss = 1.03939004\n",
      "Iteration 2499, loss = 1.03925102\n",
      "Iteration 2500, loss = 1.03911405\n",
      "Iteration 2501, loss = 1.03897631\n",
      "Iteration 2502, loss = 1.03883641\n",
      "Iteration 2503, loss = 1.03870135\n",
      "Iteration 2504, loss = 1.03856268\n",
      "Iteration 2505, loss = 1.03842429\n",
      "Iteration 2506, loss = 1.03828693\n",
      "Iteration 2507, loss = 1.03814921\n",
      "Iteration 2508, loss = 1.03801176\n",
      "Iteration 2509, loss = 1.03787477\n",
      "Iteration 2510, loss = 1.03773878\n",
      "Iteration 2511, loss = 1.03760031\n",
      "Iteration 2512, loss = 1.03746301\n",
      "Iteration 2513, loss = 1.03732533\n",
      "Iteration 2514, loss = 1.03718817\n",
      "Iteration 2515, loss = 1.03705313\n",
      "Iteration 2516, loss = 1.03691388\n",
      "Iteration 2517, loss = 1.03677789\n",
      "Iteration 2518, loss = 1.03663974\n",
      "Iteration 2519, loss = 1.03650362\n",
      "Iteration 2520, loss = 1.03636618\n",
      "Iteration 2521, loss = 1.03623034\n",
      "Iteration 2522, loss = 1.03609260\n",
      "Iteration 2523, loss = 1.03595666\n",
      "Iteration 2524, loss = 1.03581985\n",
      "Iteration 2525, loss = 1.03568278\n",
      "Iteration 2526, loss = 1.03554734\n",
      "Iteration 2527, loss = 1.03541049\n",
      "Iteration 2528, loss = 1.03527390\n",
      "Iteration 2529, loss = 1.03513805\n",
      "Iteration 2530, loss = 1.03500209\n",
      "Iteration 2531, loss = 1.03486579\n",
      "Iteration 2532, loss = 1.03472876\n",
      "Iteration 2533, loss = 1.03459228\n",
      "Iteration 2534, loss = 1.03445535\n",
      "Iteration 2535, loss = 1.03431996\n",
      "Iteration 2536, loss = 1.03418389\n",
      "Iteration 2537, loss = 1.03404836\n",
      "Iteration 2538, loss = 1.03391165\n",
      "Iteration 2539, loss = 1.03377785\n",
      "Iteration 2540, loss = 1.03364048\n",
      "Iteration 2541, loss = 1.03350407\n",
      "Iteration 2542, loss = 1.03336919\n",
      "Iteration 2543, loss = 1.03323210\n",
      "Iteration 2544, loss = 1.03309660\n",
      "Iteration 2545, loss = 1.03296114\n",
      "Iteration 2546, loss = 1.03282644\n",
      "Iteration 2547, loss = 1.03269162\n",
      "Iteration 2548, loss = 1.03255606\n",
      "Iteration 2549, loss = 1.03242057\n",
      "Iteration 2550, loss = 1.03228506\n",
      "Iteration 2551, loss = 1.03214969\n",
      "Iteration 2552, loss = 1.03201439\n",
      "Iteration 2553, loss = 1.03187928\n",
      "Iteration 2554, loss = 1.03174506\n",
      "Iteration 2555, loss = 1.03160903\n",
      "Iteration 2556, loss = 1.03147363\n",
      "Iteration 2557, loss = 1.03133846\n",
      "Iteration 2558, loss = 1.03120267\n",
      "Iteration 2559, loss = 1.03106907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2560, loss = 1.03093406\n",
      "Iteration 2561, loss = 1.03079958\n",
      "Iteration 2562, loss = 1.03066482\n",
      "Iteration 2563, loss = 1.03052965\n",
      "Iteration 2564, loss = 1.03039616\n",
      "Iteration 2565, loss = 1.03026157\n",
      "Iteration 2566, loss = 1.03012701\n",
      "Iteration 2567, loss = 1.02999276\n",
      "Iteration 2568, loss = 1.02985827\n",
      "Iteration 2569, loss = 1.02972199\n",
      "Iteration 2570, loss = 1.02958804\n",
      "Iteration 2571, loss = 1.02945424\n",
      "Iteration 2572, loss = 1.02931964\n",
      "Iteration 2573, loss = 1.02918573\n",
      "Iteration 2574, loss = 1.02905078\n",
      "Iteration 2575, loss = 1.02891761\n",
      "Iteration 2576, loss = 1.02878356\n",
      "Iteration 2577, loss = 1.02864925\n",
      "Iteration 2578, loss = 1.02851470\n",
      "Iteration 2579, loss = 1.02838239\n",
      "Iteration 2580, loss = 1.02824707\n",
      "Iteration 2581, loss = 1.02811409\n",
      "Iteration 2582, loss = 1.02798103\n",
      "Iteration 2583, loss = 1.02784594\n",
      "Iteration 2584, loss = 1.02771250\n",
      "Iteration 2585, loss = 1.02757942\n",
      "Iteration 2586, loss = 1.02744540\n",
      "Iteration 2587, loss = 1.02731178\n",
      "Iteration 2588, loss = 1.02717809\n",
      "Iteration 2589, loss = 1.02704588\n",
      "Iteration 2590, loss = 1.02691188\n",
      "Iteration 2591, loss = 1.02677786\n",
      "Iteration 2592, loss = 1.02664584\n",
      "Iteration 2593, loss = 1.02651265\n",
      "Iteration 2594, loss = 1.02637804\n",
      "Iteration 2595, loss = 1.02624592\n",
      "Iteration 2596, loss = 1.02611252\n",
      "Iteration 2597, loss = 1.02597910\n",
      "Iteration 2598, loss = 1.02584675\n",
      "Iteration 2599, loss = 1.02571364\n",
      "Iteration 2600, loss = 1.02558061\n",
      "Iteration 2601, loss = 1.02544723\n",
      "Iteration 2602, loss = 1.02531536\n",
      "Iteration 2603, loss = 1.02518302\n",
      "Iteration 2604, loss = 1.02504940\n",
      "Iteration 2605, loss = 1.02491733\n",
      "Iteration 2606, loss = 1.02478431\n",
      "Iteration 2607, loss = 1.02465196\n",
      "Iteration 2608, loss = 1.02451908\n",
      "Iteration 2609, loss = 1.02438811\n",
      "Iteration 2610, loss = 1.02425444\n",
      "Iteration 2611, loss = 1.02412337\n",
      "Iteration 2612, loss = 1.02399070\n",
      "Iteration 2613, loss = 1.02385651\n",
      "Iteration 2614, loss = 1.02372468\n",
      "Iteration 2615, loss = 1.02359404\n",
      "Iteration 2616, loss = 1.02346166\n",
      "Iteration 2617, loss = 1.02332977\n",
      "Iteration 2618, loss = 1.02319712\n",
      "Iteration 2619, loss = 1.02306492\n",
      "Iteration 2620, loss = 1.02293449\n",
      "Iteration 2621, loss = 1.02280190\n",
      "Iteration 2622, loss = 1.02266988\n",
      "Iteration 2623, loss = 1.02253933\n",
      "Iteration 2624, loss = 1.02240755\n",
      "Iteration 2625, loss = 1.02227488\n",
      "Iteration 2626, loss = 1.02214335\n",
      "Iteration 2627, loss = 1.02201130\n",
      "Iteration 2628, loss = 1.02188052\n",
      "Iteration 2629, loss = 1.02174952\n",
      "Iteration 2630, loss = 1.02161674\n",
      "Iteration 2631, loss = 1.02148595\n",
      "Iteration 2632, loss = 1.02135533\n",
      "Iteration 2633, loss = 1.02122333\n",
      "Iteration 2634, loss = 1.02109208\n",
      "Iteration 2635, loss = 1.02095999\n",
      "Iteration 2636, loss = 1.02082973\n",
      "Iteration 2637, loss = 1.02069917\n",
      "Iteration 2638, loss = 1.02056845\n",
      "Iteration 2639, loss = 1.02043660\n",
      "Iteration 2640, loss = 1.02030592\n",
      "Iteration 2641, loss = 1.02017494\n",
      "Iteration 2642, loss = 1.02004408\n",
      "Iteration 2643, loss = 1.01991363\n",
      "Iteration 2644, loss = 1.01978311\n",
      "Iteration 2645, loss = 1.01965208\n",
      "Iteration 2646, loss = 1.01952266\n",
      "Iteration 2647, loss = 1.01939096\n",
      "Iteration 2648, loss = 1.01925943\n",
      "Iteration 2649, loss = 1.01912907\n",
      "Iteration 2650, loss = 1.01899960\n",
      "Iteration 2651, loss = 1.01886856\n",
      "Iteration 2652, loss = 1.01873800\n",
      "Iteration 2653, loss = 1.01860917\n",
      "Iteration 2654, loss = 1.01847869\n",
      "Iteration 2655, loss = 1.01834682\n",
      "Iteration 2656, loss = 1.01821787\n",
      "Iteration 2657, loss = 1.01808783\n",
      "Iteration 2658, loss = 1.01795718\n",
      "Iteration 2659, loss = 1.01782662\n",
      "Iteration 2660, loss = 1.01769778\n",
      "Iteration 2661, loss = 1.01756671\n",
      "Iteration 2662, loss = 1.01743736\n",
      "Iteration 2663, loss = 1.01730676\n",
      "Iteration 2664, loss = 1.01717782\n",
      "Iteration 2665, loss = 1.01704860\n",
      "Iteration 2666, loss = 1.01691847\n",
      "Iteration 2667, loss = 1.01678669\n",
      "Iteration 2668, loss = 1.01665787\n",
      "Iteration 2669, loss = 1.01652992\n",
      "Iteration 2670, loss = 1.01640050\n",
      "Iteration 2671, loss = 1.01627026\n",
      "Iteration 2672, loss = 1.01613937\n",
      "Iteration 2673, loss = 1.01601114\n",
      "Iteration 2674, loss = 1.01588038\n",
      "Iteration 2675, loss = 1.01575222\n",
      "Iteration 2676, loss = 1.01562234\n",
      "Iteration 2677, loss = 1.01549464\n",
      "Iteration 2678, loss = 1.01536406\n",
      "Iteration 2679, loss = 1.01523426\n",
      "Iteration 2680, loss = 1.01510585\n",
      "Iteration 2681, loss = 1.01497741\n",
      "Iteration 2682, loss = 1.01484844\n",
      "Iteration 2683, loss = 1.01471906\n",
      "Iteration 2684, loss = 1.01458996\n",
      "Iteration 2685, loss = 1.01446119\n",
      "Iteration 2686, loss = 1.01433289\n",
      "Iteration 2687, loss = 1.01420389\n",
      "Iteration 2688, loss = 1.01407518\n",
      "Iteration 2689, loss = 1.01394608\n",
      "Iteration 2690, loss = 1.01381799\n",
      "Iteration 2691, loss = 1.01368996\n",
      "Iteration 2692, loss = 1.01355985\n",
      "Iteration 2693, loss = 1.01343278\n",
      "Iteration 2694, loss = 1.01330290\n",
      "Iteration 2695, loss = 1.01317485\n",
      "Iteration 2696, loss = 1.01304734\n",
      "Iteration 2697, loss = 1.01291934\n",
      "Iteration 2698, loss = 1.01279066\n",
      "Iteration 2699, loss = 1.01266291\n",
      "Iteration 2700, loss = 1.01253492\n",
      "Iteration 2701, loss = 1.01240543\n",
      "Iteration 2702, loss = 1.01227831\n",
      "Iteration 2703, loss = 1.01214897\n",
      "Iteration 2704, loss = 1.01202318\n",
      "Iteration 2705, loss = 1.01189456\n",
      "Iteration 2706, loss = 1.01176612\n",
      "Iteration 2707, loss = 1.01163823\n",
      "Iteration 2708, loss = 1.01151027\n",
      "Iteration 2709, loss = 1.01138278\n",
      "Iteration 2710, loss = 1.01125413\n",
      "Iteration 2711, loss = 1.01112658\n",
      "Iteration 2712, loss = 1.01099949\n",
      "Iteration 2713, loss = 1.01087178\n",
      "Iteration 2714, loss = 1.01074375\n",
      "Iteration 2715, loss = 1.01061668\n",
      "Iteration 2716, loss = 1.01049047\n",
      "Iteration 2717, loss = 1.01036158\n",
      "Iteration 2718, loss = 1.01023390\n",
      "Iteration 2719, loss = 1.01010730\n",
      "Iteration 2720, loss = 1.00998059\n",
      "Iteration 2721, loss = 1.00985230\n",
      "Iteration 2722, loss = 1.00972666\n",
      "Iteration 2723, loss = 1.00959803\n",
      "Iteration 2724, loss = 1.00947091\n",
      "Iteration 2725, loss = 1.00934461\n",
      "Iteration 2726, loss = 1.00921742\n",
      "Iteration 2727, loss = 1.00908984\n",
      "Iteration 2728, loss = 1.00896263\n",
      "Iteration 2729, loss = 1.00883581\n",
      "Iteration 2730, loss = 1.00870941\n",
      "Iteration 2731, loss = 1.00858274\n",
      "Iteration 2732, loss = 1.00845652\n",
      "Iteration 2733, loss = 1.00833007\n",
      "Iteration 2734, loss = 1.00820177\n",
      "Iteration 2735, loss = 1.00807593\n",
      "Iteration 2736, loss = 1.00794994\n",
      "Iteration 2737, loss = 1.00782343\n",
      "Iteration 2738, loss = 1.00769654\n",
      "Iteration 2739, loss = 1.00756940\n",
      "Iteration 2740, loss = 1.00744365\n",
      "Iteration 2741, loss = 1.00731701\n",
      "Iteration 2742, loss = 1.00719054\n",
      "Iteration 2743, loss = 1.00706444\n",
      "Iteration 2744, loss = 1.00693812\n",
      "Iteration 2745, loss = 1.00681327\n",
      "Iteration 2746, loss = 1.00668575\n",
      "Iteration 2747, loss = 1.00656056\n",
      "Iteration 2748, loss = 1.00643321\n",
      "Iteration 2749, loss = 1.00630809\n",
      "Iteration 2750, loss = 1.00618164\n",
      "Iteration 2751, loss = 1.00605695\n",
      "Iteration 2752, loss = 1.00593008\n",
      "Iteration 2753, loss = 1.00580423\n",
      "Iteration 2754, loss = 1.00567936\n",
      "Iteration 2755, loss = 1.00555436\n",
      "Iteration 2756, loss = 1.00542725\n",
      "Iteration 2757, loss = 1.00530182\n",
      "Iteration 2758, loss = 1.00517768\n",
      "Iteration 2759, loss = 1.00504948\n",
      "Iteration 2760, loss = 1.00492590\n",
      "Iteration 2761, loss = 1.00480071\n",
      "Iteration 2762, loss = 1.00467548\n",
      "Iteration 2763, loss = 1.00454985\n",
      "Iteration 2764, loss = 1.00442312\n",
      "Iteration 2765, loss = 1.00429924\n",
      "Iteration 2766, loss = 1.00417215\n",
      "Iteration 2767, loss = 1.00404772\n",
      "Iteration 2768, loss = 1.00392329\n",
      "Iteration 2769, loss = 1.00379798\n",
      "Iteration 2770, loss = 1.00367338\n",
      "Iteration 2771, loss = 1.00354788\n",
      "Iteration 2772, loss = 1.00342170\n",
      "Iteration 2773, loss = 1.00329754\n",
      "Iteration 2774, loss = 1.00317328\n",
      "Iteration 2775, loss = 1.00304767\n",
      "Iteration 2776, loss = 1.00292355\n",
      "Iteration 2777, loss = 1.00279920\n",
      "Iteration 2778, loss = 1.00267447\n",
      "Iteration 2779, loss = 1.00254909\n",
      "Iteration 2780, loss = 1.00242406\n",
      "Iteration 2781, loss = 1.00229924\n",
      "Iteration 2782, loss = 1.00217502\n",
      "Iteration 2783, loss = 1.00205105\n",
      "Iteration 2784, loss = 1.00192737\n",
      "Iteration 2785, loss = 1.00180196\n",
      "Iteration 2786, loss = 1.00167850\n",
      "Iteration 2787, loss = 1.00155384\n",
      "Iteration 2788, loss = 1.00143005\n",
      "Iteration 2789, loss = 1.00130474\n",
      "Iteration 2790, loss = 1.00118094\n",
      "Iteration 2791, loss = 1.00105664\n",
      "Iteration 2792, loss = 1.00093318\n",
      "Iteration 2793, loss = 1.00080863\n",
      "Iteration 2794, loss = 1.00068435\n",
      "Iteration 2795, loss = 1.00056157\n",
      "Iteration 2796, loss = 1.00043654\n",
      "Iteration 2797, loss = 1.00031389\n",
      "Iteration 2798, loss = 1.00018991\n",
      "Iteration 2799, loss = 1.00006485\n",
      "Iteration 2800, loss = 0.99994210\n",
      "Iteration 2801, loss = 0.99981897\n",
      "Iteration 2802, loss = 0.99969629\n",
      "Iteration 2803, loss = 0.99957154\n",
      "Iteration 2804, loss = 0.99944812\n",
      "Iteration 2805, loss = 0.99932467\n",
      "Iteration 2806, loss = 0.99920069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2807, loss = 0.99907804\n",
      "Iteration 2808, loss = 0.99895488\n",
      "Iteration 2809, loss = 0.99883104\n",
      "Iteration 2810, loss = 0.99870708\n",
      "Iteration 2811, loss = 0.99858470\n",
      "Iteration 2812, loss = 0.99846156\n",
      "Iteration 2813, loss = 0.99833739\n",
      "Iteration 2814, loss = 0.99821511\n",
      "Iteration 2815, loss = 0.99809213\n",
      "Iteration 2816, loss = 0.99796835\n",
      "Iteration 2817, loss = 0.99784518\n",
      "Iteration 2818, loss = 0.99772299\n",
      "Iteration 2819, loss = 0.99759988\n",
      "Iteration 2820, loss = 0.99747735\n",
      "Iteration 2821, loss = 0.99735390\n",
      "Iteration 2822, loss = 0.99723097\n",
      "Iteration 2823, loss = 0.99710850\n",
      "Iteration 2824, loss = 0.99698598\n",
      "Iteration 2825, loss = 0.99686538\n",
      "Iteration 2826, loss = 0.99674138\n",
      "Iteration 2827, loss = 0.99661851\n",
      "Iteration 2828, loss = 0.99649626\n",
      "Iteration 2829, loss = 0.99637316\n",
      "Iteration 2830, loss = 0.99625018\n",
      "Iteration 2831, loss = 0.99612873\n",
      "Iteration 2832, loss = 0.99600600\n",
      "Iteration 2833, loss = 0.99588390\n",
      "Iteration 2834, loss = 0.99576060\n",
      "Iteration 2835, loss = 0.99563811\n",
      "Iteration 2836, loss = 0.99551591\n",
      "Iteration 2837, loss = 0.99539450\n",
      "Iteration 2838, loss = 0.99527237\n",
      "Iteration 2839, loss = 0.99515063\n",
      "Iteration 2840, loss = 0.99502823\n",
      "Iteration 2841, loss = 0.99490594\n",
      "Iteration 2842, loss = 0.99478405\n",
      "Iteration 2843, loss = 0.99466181\n",
      "Iteration 2844, loss = 0.99454051\n",
      "Iteration 2845, loss = 0.99441855\n",
      "Iteration 2846, loss = 0.99429693\n",
      "Iteration 2847, loss = 0.99417459\n",
      "Iteration 2848, loss = 0.99405289\n",
      "Iteration 2849, loss = 0.99393243\n",
      "Iteration 2850, loss = 0.99381011\n",
      "Iteration 2851, loss = 0.99368850\n",
      "Iteration 2852, loss = 0.99356683\n",
      "Iteration 2853, loss = 0.99344575\n",
      "Iteration 2854, loss = 0.99332478\n",
      "Iteration 2855, loss = 0.99320274\n",
      "Iteration 2856, loss = 0.99308112\n",
      "Iteration 2857, loss = 0.99295949\n",
      "Iteration 2858, loss = 0.99283778\n",
      "Iteration 2859, loss = 0.99271689\n",
      "Iteration 2860, loss = 0.99259594\n",
      "Iteration 2861, loss = 0.99247584\n",
      "Iteration 2862, loss = 0.99235340\n",
      "Iteration 2863, loss = 0.99223274\n",
      "Iteration 2864, loss = 0.99211103\n",
      "Iteration 2865, loss = 0.99199018\n",
      "Iteration 2866, loss = 0.99186853\n",
      "Iteration 2867, loss = 0.99174901\n",
      "Iteration 2868, loss = 0.99162707\n",
      "Iteration 2869, loss = 0.99150817\n",
      "Iteration 2870, loss = 0.99138602\n",
      "Iteration 2871, loss = 0.99126381\n",
      "Iteration 2872, loss = 0.99114418\n",
      "Iteration 2873, loss = 0.99102337\n",
      "Iteration 2874, loss = 0.99090201\n",
      "Iteration 2875, loss = 0.99078202\n",
      "Iteration 2876, loss = 0.99066255\n",
      "Iteration 2877, loss = 0.99054126\n",
      "Iteration 2878, loss = 0.99042141\n",
      "Iteration 2879, loss = 0.99030111\n",
      "Iteration 2880, loss = 0.99017975\n",
      "Iteration 2881, loss = 0.99006045\n",
      "Iteration 2882, loss = 0.98993917\n",
      "Iteration 2883, loss = 0.98981966\n",
      "Iteration 2884, loss = 0.98969855\n",
      "Iteration 2885, loss = 0.98957784\n",
      "Iteration 2886, loss = 0.98945794\n",
      "Iteration 2887, loss = 0.98933898\n",
      "Iteration 2888, loss = 0.98921765\n",
      "Iteration 2889, loss = 0.98909843\n",
      "Iteration 2890, loss = 0.98897780\n",
      "Iteration 2891, loss = 0.98885811\n",
      "Iteration 2892, loss = 0.98873874\n",
      "Iteration 2893, loss = 0.98861801\n",
      "Iteration 2894, loss = 0.98849871\n",
      "Iteration 2895, loss = 0.98837969\n",
      "Iteration 2896, loss = 0.98825919\n",
      "Iteration 2897, loss = 0.98813924\n",
      "Iteration 2898, loss = 0.98802058\n",
      "Iteration 2899, loss = 0.98789933\n",
      "Iteration 2900, loss = 0.98778029\n",
      "Iteration 2901, loss = 0.98766109\n",
      "Iteration 2902, loss = 0.98754103\n",
      "Iteration 2903, loss = 0.98742128\n",
      "Iteration 2904, loss = 0.98730197\n",
      "Iteration 2905, loss = 0.98718434\n",
      "Iteration 2906, loss = 0.98706378\n",
      "Iteration 2907, loss = 0.98694422\n",
      "Iteration 2908, loss = 0.98682601\n",
      "Iteration 2909, loss = 0.98670537\n",
      "Iteration 2910, loss = 0.98658716\n",
      "Iteration 2911, loss = 0.98646683\n",
      "Iteration 2912, loss = 0.98634836\n",
      "Iteration 2913, loss = 0.98622974\n",
      "Iteration 2914, loss = 0.98611040\n",
      "Iteration 2915, loss = 0.98599172\n",
      "Iteration 2916, loss = 0.98587278\n",
      "Iteration 2917, loss = 0.98575430\n",
      "Iteration 2918, loss = 0.98563457\n",
      "Iteration 2919, loss = 0.98551673\n",
      "Iteration 2920, loss = 0.98539661\n",
      "Iteration 2921, loss = 0.98527857\n",
      "Iteration 2922, loss = 0.98516038\n",
      "Iteration 2923, loss = 0.98504149\n",
      "Iteration 2924, loss = 0.98492250\n",
      "Iteration 2925, loss = 0.98480427\n",
      "Iteration 2926, loss = 0.98468514\n",
      "Iteration 2927, loss = 0.98456659\n",
      "Iteration 2928, loss = 0.98444863\n",
      "Iteration 2929, loss = 0.98432947\n",
      "Iteration 2930, loss = 0.98421152\n",
      "Iteration 2931, loss = 0.98409256\n",
      "Iteration 2932, loss = 0.98397569\n",
      "Iteration 2933, loss = 0.98385653\n",
      "Iteration 2934, loss = 0.98373869\n",
      "Iteration 2935, loss = 0.98362066\n",
      "Iteration 2936, loss = 0.98350215\n",
      "Iteration 2937, loss = 0.98338400\n",
      "Iteration 2938, loss = 0.98326534\n",
      "Iteration 2939, loss = 0.98314868\n",
      "Iteration 2940, loss = 0.98303082\n",
      "Iteration 2941, loss = 0.98291263\n",
      "Iteration 2942, loss = 0.98279548\n",
      "Iteration 2943, loss = 0.98267715\n",
      "Iteration 2944, loss = 0.98255814\n",
      "Iteration 2945, loss = 0.98244038\n",
      "Iteration 2946, loss = 0.98232372\n",
      "Iteration 2947, loss = 0.98220577\n",
      "Iteration 2948, loss = 0.98208786\n",
      "Iteration 2949, loss = 0.98197110\n",
      "Iteration 2950, loss = 0.98185355\n",
      "Iteration 2951, loss = 0.98173585\n",
      "Iteration 2952, loss = 0.98161749\n",
      "Iteration 2953, loss = 0.98150070\n",
      "Iteration 2954, loss = 0.98138293\n",
      "Iteration 2955, loss = 0.98126596\n",
      "Iteration 2956, loss = 0.98114790\n",
      "Iteration 2957, loss = 0.98103158\n",
      "Iteration 2958, loss = 0.98091436\n",
      "Iteration 2959, loss = 0.98079734\n",
      "Iteration 2960, loss = 0.98067899\n",
      "Iteration 2961, loss = 0.98056225\n",
      "Iteration 2962, loss = 0.98044546\n",
      "Iteration 2963, loss = 0.98032930\n",
      "Iteration 2964, loss = 0.98021171\n",
      "Iteration 2965, loss = 0.98009379\n",
      "Iteration 2966, loss = 0.97997662\n",
      "Iteration 2967, loss = 0.97986133\n",
      "Iteration 2968, loss = 0.97974452\n",
      "Iteration 2969, loss = 0.97962701\n",
      "Iteration 2970, loss = 0.97951008\n",
      "Iteration 2971, loss = 0.97939340\n",
      "Iteration 2972, loss = 0.97927740\n",
      "Iteration 2973, loss = 0.97916042\n",
      "Iteration 2974, loss = 0.97904372\n",
      "Iteration 2975, loss = 0.97892712\n",
      "Iteration 2976, loss = 0.97880939\n",
      "Iteration 2977, loss = 0.97869322\n",
      "Iteration 2978, loss = 0.97857648\n",
      "Iteration 2979, loss = 0.97846095\n",
      "Iteration 2980, loss = 0.97834377\n",
      "Iteration 2981, loss = 0.97822723\n",
      "Iteration 2982, loss = 0.97811164\n",
      "Iteration 2983, loss = 0.97799470\n",
      "Iteration 2984, loss = 0.97787930\n",
      "Iteration 2985, loss = 0.97776280\n",
      "Iteration 2986, loss = 0.97764682\n",
      "Iteration 2987, loss = 0.97753093\n",
      "Iteration 2988, loss = 0.97741419\n",
      "Iteration 2989, loss = 0.97729824\n",
      "Iteration 2990, loss = 0.97718329\n",
      "Iteration 2991, loss = 0.97706689\n",
      "Iteration 2992, loss = 0.97695105\n",
      "Iteration 2993, loss = 0.97683349\n",
      "Iteration 2994, loss = 0.97671842\n",
      "Iteration 2995, loss = 0.97660218\n",
      "Iteration 2996, loss = 0.97648670\n",
      "Iteration 2997, loss = 0.97637039\n",
      "Iteration 2998, loss = 0.97625532\n",
      "Iteration 2999, loss = 0.97613952\n",
      "Iteration 3000, loss = 0.97602420\n",
      "Iteration 3001, loss = 0.97590820\n",
      "Iteration 3002, loss = 0.97579174\n",
      "Iteration 3003, loss = 0.97567706\n",
      "Iteration 3004, loss = 0.97556100\n",
      "Iteration 3005, loss = 0.97544651\n",
      "Iteration 3006, loss = 0.97533124\n",
      "Iteration 3007, loss = 0.97521537\n",
      "Iteration 3008, loss = 0.97509991\n",
      "Iteration 3009, loss = 0.97498491\n",
      "Iteration 3010, loss = 0.97486874\n",
      "Iteration 3011, loss = 0.97475523\n",
      "Iteration 3012, loss = 0.97463875\n",
      "Iteration 3013, loss = 0.97452460\n",
      "Iteration 3014, loss = 0.97440781\n",
      "Iteration 3015, loss = 0.97429295\n",
      "Iteration 3016, loss = 0.97417796\n",
      "Iteration 3017, loss = 0.97406334\n",
      "Iteration 3018, loss = 0.97394841\n",
      "Iteration 3019, loss = 0.97383407\n",
      "Iteration 3020, loss = 0.97371855\n",
      "Iteration 3021, loss = 0.97360391\n",
      "Iteration 3022, loss = 0.97348890\n",
      "Iteration 3023, loss = 0.97337339\n",
      "Iteration 3024, loss = 0.97325950\n",
      "Iteration 3025, loss = 0.97314399\n",
      "Iteration 3026, loss = 0.97302964\n",
      "Iteration 3027, loss = 0.97291588\n",
      "Iteration 3028, loss = 0.97280170\n",
      "Iteration 3029, loss = 0.97268529\n",
      "Iteration 3030, loss = 0.97257100\n",
      "Iteration 3031, loss = 0.97245611\n",
      "Iteration 3032, loss = 0.97234164\n",
      "Iteration 3033, loss = 0.97222715\n",
      "Iteration 3034, loss = 0.97211321\n",
      "Iteration 3035, loss = 0.97199838\n",
      "Iteration 3036, loss = 0.97188432\n",
      "Iteration 3037, loss = 0.97177010\n",
      "Iteration 3038, loss = 0.97165506\n",
      "Iteration 3039, loss = 0.97154222\n",
      "Iteration 3040, loss = 0.97142718\n",
      "Iteration 3041, loss = 0.97131241\n",
      "Iteration 3042, loss = 0.97120076\n",
      "Iteration 3043, loss = 0.97108487\n",
      "Iteration 3044, loss = 0.97097117\n",
      "Iteration 3045, loss = 0.97085746\n",
      "Iteration 3046, loss = 0.97074290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3047, loss = 0.97063041\n",
      "Iteration 3048, loss = 0.97051523\n",
      "Iteration 3049, loss = 0.97040122\n",
      "Iteration 3050, loss = 0.97028751\n",
      "Iteration 3051, loss = 0.97017403\n",
      "Iteration 3052, loss = 0.97006049\n",
      "Iteration 3053, loss = 0.96994703\n",
      "Iteration 3054, loss = 0.96983312\n",
      "Iteration 3055, loss = 0.96971939\n",
      "Iteration 3056, loss = 0.96960620\n",
      "Iteration 3057, loss = 0.96949252\n",
      "Iteration 3058, loss = 0.96937903\n",
      "Iteration 3059, loss = 0.96926450\n",
      "Iteration 3060, loss = 0.96915090\n",
      "Iteration 3061, loss = 0.96903863\n",
      "Iteration 3062, loss = 0.96892486\n",
      "Iteration 3063, loss = 0.96881194\n",
      "Iteration 3064, loss = 0.96869855\n",
      "Iteration 3065, loss = 0.96858603\n",
      "Iteration 3066, loss = 0.96847317\n",
      "Iteration 3067, loss = 0.96835883\n",
      "Iteration 3068, loss = 0.96824512\n",
      "Iteration 3069, loss = 0.96813270\n",
      "Iteration 3070, loss = 0.96802048\n",
      "Iteration 3071, loss = 0.96790734\n",
      "Iteration 3072, loss = 0.96779460\n",
      "Iteration 3073, loss = 0.96768120\n",
      "Iteration 3074, loss = 0.96756759\n",
      "Iteration 3075, loss = 0.96745582\n",
      "Iteration 3076, loss = 0.96734245\n",
      "Iteration 3077, loss = 0.96722935\n",
      "Iteration 3078, loss = 0.96711757\n",
      "Iteration 3079, loss = 0.96700423\n",
      "Iteration 3080, loss = 0.96689090\n",
      "Iteration 3081, loss = 0.96677842\n",
      "Iteration 3082, loss = 0.96666535\n",
      "Iteration 3083, loss = 0.96655280\n",
      "Iteration 3084, loss = 0.96644031\n",
      "Iteration 3085, loss = 0.96632750\n",
      "Iteration 3086, loss = 0.96621711\n",
      "Iteration 3087, loss = 0.96610357\n",
      "Iteration 3088, loss = 0.96599094\n",
      "Iteration 3089, loss = 0.96587906\n",
      "Iteration 3090, loss = 0.96576648\n",
      "Iteration 3091, loss = 0.96565355\n",
      "Iteration 3092, loss = 0.96554141\n",
      "Iteration 3093, loss = 0.96542918\n",
      "Iteration 3094, loss = 0.96531672\n",
      "Iteration 3095, loss = 0.96520463\n",
      "Iteration 3096, loss = 0.96509240\n",
      "Iteration 3097, loss = 0.96498041\n",
      "Iteration 3098, loss = 0.96486852\n",
      "Iteration 3099, loss = 0.96475610\n",
      "Iteration 3100, loss = 0.96464445\n",
      "Iteration 3101, loss = 0.96453273\n",
      "Iteration 3102, loss = 0.96441980\n",
      "Iteration 3103, loss = 0.96430729\n",
      "Iteration 3104, loss = 0.96419709\n",
      "Iteration 3105, loss = 0.96408390\n",
      "Iteration 3106, loss = 0.96397293\n",
      "Iteration 3107, loss = 0.96386050\n",
      "Iteration 3108, loss = 0.96375010\n",
      "Iteration 3109, loss = 0.96363743\n",
      "Iteration 3110, loss = 0.96352578\n",
      "Iteration 3111, loss = 0.96341406\n",
      "Iteration 3112, loss = 0.96330202\n",
      "Iteration 3113, loss = 0.96319000\n",
      "Iteration 3114, loss = 0.96307901\n",
      "Iteration 3115, loss = 0.96296830\n",
      "Iteration 3116, loss = 0.96285684\n",
      "Iteration 3117, loss = 0.96274503\n",
      "Iteration 3118, loss = 0.96263325\n",
      "Iteration 3119, loss = 0.96252157\n",
      "Iteration 3120, loss = 0.96241092\n",
      "Iteration 3121, loss = 0.96229966\n",
      "Iteration 3122, loss = 0.96218901\n",
      "Iteration 3123, loss = 0.96207679\n",
      "Iteration 3124, loss = 0.96196575\n",
      "Iteration 3125, loss = 0.96185447\n",
      "Iteration 3126, loss = 0.96174435\n",
      "Iteration 3127, loss = 0.96163260\n",
      "Iteration 3128, loss = 0.96152119\n",
      "Iteration 3129, loss = 0.96141017\n",
      "Iteration 3130, loss = 0.96129987\n",
      "Iteration 3131, loss = 0.96118748\n",
      "Iteration 3132, loss = 0.96107747\n",
      "Iteration 3133, loss = 0.96096629\n",
      "Iteration 3134, loss = 0.96085474\n",
      "Iteration 3135, loss = 0.96074434\n",
      "Iteration 3136, loss = 0.96063471\n",
      "Iteration 3137, loss = 0.96052316\n",
      "Iteration 3138, loss = 0.96041187\n",
      "Iteration 3139, loss = 0.96030178\n",
      "Iteration 3140, loss = 0.96019169\n",
      "Iteration 3141, loss = 0.96007971\n",
      "Iteration 3142, loss = 0.95996988\n",
      "Iteration 3143, loss = 0.95985962\n",
      "Iteration 3144, loss = 0.95974826\n",
      "Iteration 3145, loss = 0.95963938\n",
      "Iteration 3146, loss = 0.95952802\n",
      "Iteration 3147, loss = 0.95941742\n",
      "Iteration 3148, loss = 0.95930689\n",
      "Iteration 3149, loss = 0.95919884\n",
      "Iteration 3150, loss = 0.95908639\n",
      "Iteration 3151, loss = 0.95897638\n",
      "Iteration 3152, loss = 0.95886621\n",
      "Iteration 3153, loss = 0.95875539\n",
      "Iteration 3154, loss = 0.95864549\n",
      "Iteration 3155, loss = 0.95853513\n",
      "Iteration 3156, loss = 0.95842548\n",
      "Iteration 3157, loss = 0.95831558\n",
      "Iteration 3158, loss = 0.95820575\n",
      "Iteration 3159, loss = 0.95809551\n",
      "Iteration 3160, loss = 0.95798578\n",
      "Iteration 3161, loss = 0.95787467\n",
      "Iteration 3162, loss = 0.95776647\n",
      "Iteration 3163, loss = 0.95765590\n",
      "Iteration 3164, loss = 0.95754640\n",
      "Iteration 3165, loss = 0.95743605\n",
      "Iteration 3166, loss = 0.95732620\n",
      "Iteration 3167, loss = 0.95721688\n",
      "Iteration 3168, loss = 0.95710781\n",
      "Iteration 3169, loss = 0.95699790\n",
      "Iteration 3170, loss = 0.95688755\n",
      "Iteration 3171, loss = 0.95677847\n",
      "Iteration 3172, loss = 0.95666906\n",
      "Iteration 3173, loss = 0.95655915\n",
      "Iteration 3174, loss = 0.95644971\n",
      "Iteration 3175, loss = 0.95634028\n",
      "Iteration 3176, loss = 0.95623029\n",
      "Iteration 3177, loss = 0.95612174\n",
      "Iteration 3178, loss = 0.95601320\n",
      "Iteration 3179, loss = 0.95590329\n",
      "Iteration 3180, loss = 0.95579395\n",
      "Iteration 3181, loss = 0.95568461\n",
      "Iteration 3182, loss = 0.95557587\n",
      "Iteration 3183, loss = 0.95546668\n",
      "Iteration 3184, loss = 0.95535774\n",
      "Iteration 3185, loss = 0.95524852\n",
      "Iteration 3186, loss = 0.95513923\n",
      "Iteration 3187, loss = 0.95503113\n",
      "Iteration 3188, loss = 0.95492127\n",
      "Iteration 3189, loss = 0.95481314\n",
      "Iteration 3190, loss = 0.95470341\n",
      "Iteration 3191, loss = 0.95459479\n",
      "Iteration 3192, loss = 0.95448598\n",
      "Iteration 3193, loss = 0.95437698\n",
      "Iteration 3194, loss = 0.95426854\n",
      "Iteration 3195, loss = 0.95416041\n",
      "Iteration 3196, loss = 0.95405221\n",
      "Iteration 3197, loss = 0.95394312\n",
      "Iteration 3198, loss = 0.95383397\n",
      "Iteration 3199, loss = 0.95372545\n",
      "Iteration 3200, loss = 0.95361656\n",
      "Iteration 3201, loss = 0.95350778\n",
      "Iteration 3202, loss = 0.95339972\n",
      "Iteration 3203, loss = 0.95329130\n",
      "Iteration 3204, loss = 0.95318281\n",
      "Iteration 3205, loss = 0.95307526\n",
      "Iteration 3206, loss = 0.95296660\n",
      "Iteration 3207, loss = 0.95285814\n",
      "Iteration 3208, loss = 0.95275004\n",
      "Iteration 3209, loss = 0.95264243\n",
      "Iteration 3210, loss = 0.95253295\n",
      "Iteration 3211, loss = 0.95242530\n",
      "Iteration 3212, loss = 0.95231774\n",
      "Iteration 3213, loss = 0.95220995\n",
      "Iteration 3214, loss = 0.95210158\n",
      "Iteration 3215, loss = 0.95199366\n",
      "Iteration 3216, loss = 0.95188601\n",
      "Iteration 3217, loss = 0.95177814\n",
      "Iteration 3218, loss = 0.95167003\n",
      "Iteration 3219, loss = 0.95156293\n",
      "Iteration 3220, loss = 0.95145443\n",
      "Iteration 3221, loss = 0.95134706\n",
      "Iteration 3222, loss = 0.95123954\n",
      "Iteration 3223, loss = 0.95113102\n",
      "Iteration 3224, loss = 0.95102442\n",
      "Iteration 3225, loss = 0.95091594\n",
      "Iteration 3226, loss = 0.95080750\n",
      "Iteration 3227, loss = 0.95070128\n",
      "Iteration 3228, loss = 0.95059328\n",
      "Iteration 3229, loss = 0.95048539\n",
      "Iteration 3230, loss = 0.95037891\n",
      "Iteration 3231, loss = 0.95027201\n",
      "Iteration 3232, loss = 0.95016369\n",
      "Iteration 3233, loss = 0.95005572\n",
      "Iteration 3234, loss = 0.94994880\n",
      "Iteration 3235, loss = 0.94984212\n",
      "Iteration 3236, loss = 0.94973363\n",
      "Iteration 3237, loss = 0.94962713\n",
      "Iteration 3238, loss = 0.94951948\n",
      "Iteration 3239, loss = 0.94941291\n",
      "Iteration 3240, loss = 0.94930531\n",
      "Iteration 3241, loss = 0.94919940\n",
      "Iteration 3242, loss = 0.94909142\n",
      "Iteration 3243, loss = 0.94898431\n",
      "Iteration 3244, loss = 0.94887770\n",
      "Iteration 3245, loss = 0.94877068\n",
      "Iteration 3246, loss = 0.94866431\n",
      "Iteration 3247, loss = 0.94855700\n",
      "Iteration 3248, loss = 0.94844993\n",
      "Iteration 3249, loss = 0.94834363\n",
      "Iteration 3250, loss = 0.94823621\n",
      "Iteration 3251, loss = 0.94813039\n",
      "Iteration 3252, loss = 0.94802304\n",
      "Iteration 3253, loss = 0.94791596\n",
      "Iteration 3254, loss = 0.94781060\n",
      "Iteration 3255, loss = 0.94770438\n",
      "Iteration 3256, loss = 0.94759666\n",
      "Iteration 3257, loss = 0.94749043\n",
      "Iteration 3258, loss = 0.94738416\n",
      "Iteration 3259, loss = 0.94727733\n",
      "Iteration 3260, loss = 0.94717134\n",
      "Iteration 3261, loss = 0.94706486\n",
      "Iteration 3262, loss = 0.94695771\n",
      "Iteration 3263, loss = 0.94685267\n",
      "Iteration 3264, loss = 0.94674635\n",
      "Iteration 3265, loss = 0.94663883\n",
      "Iteration 3266, loss = 0.94653452\n",
      "Iteration 3267, loss = 0.94642878\n",
      "Iteration 3268, loss = 0.94632083\n",
      "Iteration 3269, loss = 0.94621478\n",
      "Iteration 3270, loss = 0.94610893\n",
      "Iteration 3271, loss = 0.94600317\n",
      "Iteration 3272, loss = 0.94589630\n",
      "Iteration 3273, loss = 0.94578982\n",
      "Iteration 3274, loss = 0.94568480\n",
      "Iteration 3275, loss = 0.94557774\n",
      "Iteration 3276, loss = 0.94547271\n",
      "Iteration 3277, loss = 0.94536731\n",
      "Iteration 3278, loss = 0.94526152\n",
      "Iteration 3279, loss = 0.94515605\n",
      "Iteration 3280, loss = 0.94505017\n",
      "Iteration 3281, loss = 0.94494295\n",
      "Iteration 3282, loss = 0.94483749\n",
      "Iteration 3283, loss = 0.94473209\n",
      "Iteration 3284, loss = 0.94462694\n",
      "Iteration 3285, loss = 0.94452040\n",
      "Iteration 3286, loss = 0.94441621\n",
      "Iteration 3287, loss = 0.94431016\n",
      "Iteration 3288, loss = 0.94420470\n",
      "Iteration 3289, loss = 0.94409852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3290, loss = 0.94399403\n",
      "Iteration 3291, loss = 0.94388748\n",
      "Iteration 3292, loss = 0.94378298\n",
      "Iteration 3293, loss = 0.94367673\n",
      "Iteration 3294, loss = 0.94357175\n",
      "Iteration 3295, loss = 0.94346616\n",
      "Iteration 3296, loss = 0.94336251\n",
      "Iteration 3297, loss = 0.94325603\n",
      "Iteration 3298, loss = 0.94315064\n",
      "Iteration 3299, loss = 0.94304566\n",
      "Iteration 3300, loss = 0.94294085\n",
      "Iteration 3301, loss = 0.94283602\n",
      "Iteration 3302, loss = 0.94273020\n",
      "Iteration 3303, loss = 0.94262637\n",
      "Iteration 3304, loss = 0.94252052\n",
      "Iteration 3305, loss = 0.94241509\n",
      "Iteration 3306, loss = 0.94231041\n",
      "Iteration 3307, loss = 0.94220597\n",
      "Iteration 3308, loss = 0.94210092\n",
      "Iteration 3309, loss = 0.94199714\n",
      "Iteration 3310, loss = 0.94189184\n",
      "Iteration 3311, loss = 0.94178693\n",
      "Iteration 3312, loss = 0.94168188\n",
      "Iteration 3313, loss = 0.94157732\n",
      "Iteration 3314, loss = 0.94147358\n",
      "Iteration 3315, loss = 0.94136840\n",
      "Iteration 3316, loss = 0.94126336\n",
      "Iteration 3317, loss = 0.94115973\n",
      "Iteration 3318, loss = 0.94105613\n",
      "Iteration 3319, loss = 0.94095009\n",
      "Iteration 3320, loss = 0.94084584\n",
      "Iteration 3321, loss = 0.94074174\n",
      "Iteration 3322, loss = 0.94063615\n",
      "Iteration 3323, loss = 0.94053245\n",
      "Iteration 3324, loss = 0.94042781\n",
      "Iteration 3325, loss = 0.94032509\n",
      "Iteration 3326, loss = 0.94021831\n",
      "Iteration 3327, loss = 0.94011602\n",
      "Iteration 3328, loss = 0.94001173\n",
      "Iteration 3329, loss = 0.93990670\n",
      "Iteration 3330, loss = 0.93980279\n",
      "Iteration 3331, loss = 0.93969867\n",
      "Iteration 3332, loss = 0.93959522\n",
      "Iteration 3333, loss = 0.93949066\n",
      "Iteration 3334, loss = 0.93938680\n",
      "Iteration 3335, loss = 0.93928249\n",
      "Iteration 3336, loss = 0.93917880\n",
      "Iteration 3337, loss = 0.93907555\n",
      "Iteration 3338, loss = 0.93897075\n",
      "Iteration 3339, loss = 0.93886710\n",
      "Iteration 3340, loss = 0.93876213\n",
      "Iteration 3341, loss = 0.93865879\n",
      "Iteration 3342, loss = 0.93855567\n",
      "Iteration 3343, loss = 0.93845171\n",
      "Iteration 3344, loss = 0.93834783\n",
      "Iteration 3345, loss = 0.93824432\n",
      "Iteration 3346, loss = 0.93814081\n",
      "Iteration 3347, loss = 0.93803756\n",
      "Iteration 3348, loss = 0.93793382\n",
      "Iteration 3349, loss = 0.93782974\n",
      "Iteration 3350, loss = 0.93772702\n",
      "Iteration 3351, loss = 0.93762283\n",
      "Iteration 3352, loss = 0.93752066\n",
      "Iteration 3353, loss = 0.93741592\n",
      "Iteration 3354, loss = 0.93731274\n",
      "Iteration 3355, loss = 0.93720939\n",
      "Iteration 3356, loss = 0.93710583\n",
      "Iteration 3357, loss = 0.93700305\n",
      "Iteration 3358, loss = 0.93690027\n",
      "Iteration 3359, loss = 0.93679635\n",
      "Iteration 3360, loss = 0.93669394\n",
      "Iteration 3361, loss = 0.93659085\n",
      "Iteration 3362, loss = 0.93648664\n",
      "Iteration 3363, loss = 0.93638468\n",
      "Iteration 3364, loss = 0.93628043\n",
      "Iteration 3365, loss = 0.93617838\n",
      "Iteration 3366, loss = 0.93607597\n",
      "Iteration 3367, loss = 0.93597262\n",
      "Iteration 3368, loss = 0.93586943\n",
      "Iteration 3369, loss = 0.93576667\n",
      "Iteration 3370, loss = 0.93566336\n",
      "Iteration 3371, loss = 0.93556153\n",
      "Iteration 3372, loss = 0.93545861\n",
      "Iteration 3373, loss = 0.93535574\n",
      "Iteration 3374, loss = 0.93525264\n",
      "Iteration 3375, loss = 0.93515068\n",
      "Iteration 3376, loss = 0.93504771\n",
      "Iteration 3377, loss = 0.93494512\n",
      "Iteration 3378, loss = 0.93484220\n",
      "Iteration 3379, loss = 0.93473953\n",
      "Iteration 3380, loss = 0.93463715\n",
      "Iteration 3381, loss = 0.93453451\n",
      "Iteration 3382, loss = 0.93443205\n",
      "Iteration 3383, loss = 0.93433054\n",
      "Iteration 3384, loss = 0.93422698\n",
      "Iteration 3385, loss = 0.93412481\n",
      "Iteration 3386, loss = 0.93402339\n",
      "Iteration 3387, loss = 0.93392111\n",
      "Iteration 3388, loss = 0.93381941\n",
      "Iteration 3389, loss = 0.93371622\n",
      "Iteration 3390, loss = 0.93361425\n",
      "Iteration 3391, loss = 0.93351196\n",
      "Iteration 3392, loss = 0.93341036\n",
      "Iteration 3393, loss = 0.93330696\n",
      "Iteration 3394, loss = 0.93320546\n",
      "Iteration 3395, loss = 0.93310282\n",
      "Iteration 3396, loss = 0.93300079\n",
      "Iteration 3397, loss = 0.93290031\n",
      "Iteration 3398, loss = 0.93279738\n",
      "Iteration 3399, loss = 0.93269489\n",
      "Iteration 3400, loss = 0.93259426\n",
      "Iteration 3401, loss = 0.93249191\n",
      "Iteration 3402, loss = 0.93239055\n",
      "Iteration 3403, loss = 0.93228777\n",
      "Iteration 3404, loss = 0.93218578\n",
      "Iteration 3405, loss = 0.93208503\n",
      "Iteration 3406, loss = 0.93198219\n",
      "Iteration 3407, loss = 0.93188181\n",
      "Iteration 3408, loss = 0.93178096\n",
      "Iteration 3409, loss = 0.93167795\n",
      "Iteration 3410, loss = 0.93157690\n",
      "Iteration 3411, loss = 0.93147519\n",
      "Iteration 3412, loss = 0.93137430\n",
      "Iteration 3413, loss = 0.93127218\n",
      "Iteration 3414, loss = 0.93117161\n",
      "Iteration 3415, loss = 0.93107031\n",
      "Iteration 3416, loss = 0.93096762\n",
      "Iteration 3417, loss = 0.93086654\n",
      "Iteration 3418, loss = 0.93076522\n",
      "Iteration 3419, loss = 0.93066397\n",
      "Iteration 3420, loss = 0.93056257\n",
      "Iteration 3421, loss = 0.93046148\n",
      "Iteration 3422, loss = 0.93036008\n",
      "Iteration 3423, loss = 0.93025898\n",
      "Iteration 3424, loss = 0.93015824\n",
      "Iteration 3425, loss = 0.93005635\n",
      "Iteration 3426, loss = 0.92995521\n",
      "Iteration 3427, loss = 0.92985426\n",
      "Iteration 3428, loss = 0.92975259\n",
      "Iteration 3429, loss = 0.92965248\n",
      "Iteration 3430, loss = 0.92955173\n",
      "Iteration 3431, loss = 0.92945158\n",
      "Iteration 3432, loss = 0.92934971\n",
      "Iteration 3433, loss = 0.92924869\n",
      "Iteration 3434, loss = 0.92914821\n",
      "Iteration 3435, loss = 0.92904753\n",
      "Iteration 3436, loss = 0.92894602\n",
      "Iteration 3437, loss = 0.92884617\n",
      "Iteration 3438, loss = 0.92874538\n",
      "Iteration 3439, loss = 0.92864442\n",
      "Iteration 3440, loss = 0.92854364\n",
      "Iteration 3441, loss = 0.92844331\n",
      "Iteration 3442, loss = 0.92834277\n",
      "Iteration 3443, loss = 0.92824151\n",
      "Iteration 3444, loss = 0.92814095\n",
      "Iteration 3445, loss = 0.92804120\n",
      "Iteration 3446, loss = 0.92794051\n",
      "Iteration 3447, loss = 0.92783997\n",
      "Iteration 3448, loss = 0.92773922\n",
      "Iteration 3449, loss = 0.92763945\n",
      "Iteration 3450, loss = 0.92753901\n",
      "Iteration 3451, loss = 0.92743808\n",
      "Iteration 3452, loss = 0.92733849\n",
      "Iteration 3453, loss = 0.92723769\n",
      "Iteration 3454, loss = 0.92713829\n",
      "Iteration 3455, loss = 0.92703764\n",
      "Iteration 3456, loss = 0.92693873\n",
      "Iteration 3457, loss = 0.92683854\n",
      "Iteration 3458, loss = 0.92673835\n",
      "Iteration 3459, loss = 0.92663732\n",
      "Iteration 3460, loss = 0.92653774\n",
      "Iteration 3461, loss = 0.92643719\n",
      "Iteration 3462, loss = 0.92633721\n",
      "Iteration 3463, loss = 0.92623710\n",
      "Iteration 3464, loss = 0.92613774\n",
      "Iteration 3465, loss = 0.92603725\n",
      "Iteration 3466, loss = 0.92593870\n",
      "Iteration 3467, loss = 0.92583848\n",
      "Iteration 3468, loss = 0.92573951\n",
      "Iteration 3469, loss = 0.92563905\n",
      "Iteration 3470, loss = 0.92553890\n",
      "Iteration 3471, loss = 0.92543987\n",
      "Iteration 3472, loss = 0.92534019\n",
      "Iteration 3473, loss = 0.92524036\n",
      "Iteration 3474, loss = 0.92514110\n",
      "Iteration 3475, loss = 0.92504154\n",
      "Iteration 3476, loss = 0.92494230\n",
      "Iteration 3477, loss = 0.92484240\n",
      "Iteration 3478, loss = 0.92474268\n",
      "Iteration 3479, loss = 0.92464331\n",
      "Iteration 3480, loss = 0.92454472\n",
      "Iteration 3481, loss = 0.92444404\n",
      "Iteration 3482, loss = 0.92434489\n",
      "Iteration 3483, loss = 0.92424647\n",
      "Iteration 3484, loss = 0.92414701\n",
      "Iteration 3485, loss = 0.92404897\n",
      "Iteration 3486, loss = 0.92394858\n",
      "Iteration 3487, loss = 0.92385028\n",
      "Iteration 3488, loss = 0.92375107\n",
      "Iteration 3489, loss = 0.92365125\n",
      "Iteration 3490, loss = 0.92355162\n",
      "Iteration 3491, loss = 0.92345386\n",
      "Iteration 3492, loss = 0.92335380\n",
      "Iteration 3493, loss = 0.92325587\n",
      "Iteration 3494, loss = 0.92315657\n",
      "Iteration 3495, loss = 0.92305788\n",
      "Iteration 3496, loss = 0.92295829\n",
      "Iteration 3497, loss = 0.92286015\n",
      "Iteration 3498, loss = 0.92276125\n",
      "Iteration 3499, loss = 0.92266288\n",
      "Iteration 3500, loss = 0.92256406\n",
      "Iteration 3501, loss = 0.92246484\n",
      "Iteration 3502, loss = 0.92236625\n",
      "Iteration 3503, loss = 0.92226752\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.895381\n",
      "Training set loss: 0.922268\n",
      "training: inv-scaling with momentum\n",
      "Iteration 1, loss = 1.80716619\n",
      "Iteration 2, loss = 0.68805802\n",
      "Iteration 3, loss = 0.47428264\n",
      "Iteration 4, loss = 0.36684535\n",
      "Iteration 5, loss = 0.33149494\n",
      "Iteration 6, loss = 0.31995923\n",
      "Iteration 7, loss = 0.31330918\n",
      "Iteration 8, loss = 0.30837950\n",
      "Iteration 9, loss = 0.30427265\n",
      "Iteration 10, loss = 0.30138966\n",
      "Iteration 11, loss = 0.29860726\n",
      "Iteration 12, loss = 0.29635282\n",
      "Iteration 13, loss = 0.29423215\n",
      "Iteration 14, loss = 0.29232278\n",
      "Iteration 15, loss = 0.29058362\n",
      "Iteration 16, loss = 0.28889084\n",
      "Iteration 17, loss = 0.28734680\n",
      "Iteration 18, loss = 0.28582486\n",
      "Iteration 19, loss = 0.28447234\n",
      "Iteration 20, loss = 0.28302181\n",
      "Iteration 21, loss = 0.28165825\n",
      "Iteration 22, loss = 0.28043458\n",
      "Iteration 23, loss = 0.27922620\n",
      "Iteration 24, loss = 0.27808806\n",
      "Iteration 25, loss = 0.27700274\n",
      "Iteration 26, loss = 0.27586753\n",
      "Iteration 27, loss = 0.27488050\n",
      "Iteration 28, loss = 0.27386070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.27286720\n",
      "Iteration 30, loss = 0.27188322\n",
      "Iteration 31, loss = 0.27093301\n",
      "Iteration 32, loss = 0.27006159\n",
      "Iteration 33, loss = 0.26920838\n",
      "Iteration 34, loss = 0.26839206\n",
      "Iteration 35, loss = 0.26753582\n",
      "Iteration 36, loss = 0.26667732\n",
      "Iteration 37, loss = 0.26591110\n",
      "Iteration 38, loss = 0.26513661\n",
      "Iteration 39, loss = 0.26436236\n",
      "Iteration 40, loss = 0.26363482\n",
      "Iteration 41, loss = 0.26290252\n",
      "Iteration 42, loss = 0.26220313\n",
      "Iteration 43, loss = 0.26150953\n",
      "Iteration 44, loss = 0.26083187\n",
      "Iteration 45, loss = 0.26019542\n",
      "Iteration 46, loss = 0.25953843\n",
      "Iteration 47, loss = 0.25884908\n",
      "Iteration 48, loss = 0.25828356\n",
      "Iteration 49, loss = 0.25762361\n",
      "Iteration 50, loss = 0.25700892\n",
      "Iteration 51, loss = 0.25644325\n",
      "Iteration 52, loss = 0.25583965\n",
      "Iteration 53, loss = 0.25525115\n",
      "Iteration 54, loss = 0.25472982\n",
      "Iteration 55, loss = 0.25411803\n",
      "Iteration 56, loss = 0.25358920\n",
      "Iteration 57, loss = 0.25307170\n",
      "Iteration 58, loss = 0.25251505\n",
      "Iteration 59, loss = 0.25203965\n",
      "Iteration 60, loss = 0.25148937\n",
      "Iteration 61, loss = 0.25097772\n",
      "Iteration 62, loss = 0.25047680\n",
      "Iteration 63, loss = 0.24998717\n",
      "Iteration 64, loss = 0.24948462\n",
      "Iteration 65, loss = 0.24901780\n",
      "Iteration 66, loss = 0.24856692\n",
      "Iteration 67, loss = 0.24806933\n",
      "Iteration 68, loss = 0.24762462\n",
      "Iteration 69, loss = 0.24718758\n",
      "Iteration 70, loss = 0.24675236\n",
      "Iteration 71, loss = 0.24627741\n",
      "Iteration 72, loss = 0.24584588\n",
      "Iteration 73, loss = 0.24543967\n",
      "Iteration 74, loss = 0.24501185\n",
      "Iteration 75, loss = 0.24458492\n",
      "Iteration 76, loss = 0.24418241\n",
      "Iteration 77, loss = 0.24374778\n",
      "Iteration 78, loss = 0.24333016\n",
      "Iteration 79, loss = 0.24296056\n",
      "Iteration 80, loss = 0.24256060\n",
      "Iteration 81, loss = 0.24216720\n",
      "Iteration 82, loss = 0.24177245\n",
      "Iteration 83, loss = 0.24138718\n",
      "Iteration 84, loss = 0.24100383\n",
      "Iteration 85, loss = 0.24062605\n",
      "Iteration 86, loss = 0.24028276\n",
      "Iteration 87, loss = 0.23989702\n",
      "Iteration 88, loss = 0.23955355\n",
      "Iteration 89, loss = 0.23918882\n",
      "Iteration 90, loss = 0.23886914\n",
      "Iteration 91, loss = 0.23849134\n",
      "Iteration 92, loss = 0.23813797\n",
      "Iteration 93, loss = 0.23781725\n",
      "Iteration 94, loss = 0.23744053\n",
      "Iteration 95, loss = 0.23711774\n",
      "Iteration 96, loss = 0.23679324\n",
      "Iteration 97, loss = 0.23644990\n",
      "Iteration 98, loss = 0.23612711\n",
      "Iteration 99, loss = 0.23578795\n",
      "Iteration 100, loss = 0.23547657\n",
      "Iteration 101, loss = 0.23515741\n",
      "Iteration 102, loss = 0.23484290\n",
      "Iteration 103, loss = 0.23452081\n",
      "Iteration 104, loss = 0.23423684\n",
      "Iteration 105, loss = 0.23393270\n",
      "Iteration 106, loss = 0.23361132\n",
      "Iteration 107, loss = 0.23329990\n",
      "Iteration 108, loss = 0.23299604\n",
      "Iteration 109, loss = 0.23271123\n",
      "Iteration 110, loss = 0.23242101\n",
      "Iteration 111, loss = 0.23213009\n",
      "Iteration 112, loss = 0.23184845\n",
      "Iteration 113, loss = 0.23155564\n",
      "Iteration 114, loss = 0.23126343\n",
      "Iteration 115, loss = 0.23099572\n",
      "Iteration 116, loss = 0.23069527\n",
      "Iteration 117, loss = 0.23042531\n",
      "Iteration 118, loss = 0.23016110\n",
      "Iteration 119, loss = 0.22988932\n",
      "Iteration 120, loss = 0.22960721\n",
      "Iteration 121, loss = 0.22935031\n",
      "Iteration 122, loss = 0.22909077\n",
      "Iteration 123, loss = 0.22881625\n",
      "Iteration 124, loss = 0.22854442\n",
      "Iteration 125, loss = 0.22828442\n",
      "Iteration 126, loss = 0.22803639\n",
      "Iteration 127, loss = 0.22779023\n",
      "Iteration 128, loss = 0.22751666\n",
      "Iteration 129, loss = 0.22728733\n",
      "Iteration 130, loss = 0.22702828\n",
      "Iteration 131, loss = 0.22677925\n",
      "Iteration 132, loss = 0.22652273\n",
      "Iteration 133, loss = 0.22630966\n",
      "Iteration 134, loss = 0.22603555\n",
      "Iteration 135, loss = 0.22577989\n",
      "Iteration 136, loss = 0.22555874\n",
      "Iteration 137, loss = 0.22530402\n",
      "Iteration 138, loss = 0.22510504\n",
      "Iteration 139, loss = 0.22484577\n",
      "Iteration 140, loss = 0.22461483\n",
      "Iteration 141, loss = 0.22438100\n",
      "Iteration 142, loss = 0.22414245\n",
      "Iteration 143, loss = 0.22390176\n",
      "Iteration 144, loss = 0.22369736\n",
      "Iteration 145, loss = 0.22346936\n",
      "Iteration 146, loss = 0.22324001\n",
      "Iteration 147, loss = 0.22302349\n",
      "Iteration 148, loss = 0.22279907\n",
      "Iteration 149, loss = 0.22256237\n",
      "Iteration 150, loss = 0.22234526\n",
      "Iteration 151, loss = 0.22215191\n",
      "Iteration 152, loss = 0.22192931\n",
      "Iteration 153, loss = 0.22169996\n",
      "Iteration 154, loss = 0.22149528\n",
      "Iteration 155, loss = 0.22127429\n",
      "Iteration 156, loss = 0.22107075\n",
      "Iteration 157, loss = 0.22085169\n",
      "Iteration 158, loss = 0.22064689\n",
      "Iteration 159, loss = 0.22044623\n",
      "Iteration 160, loss = 0.22024467\n",
      "Iteration 161, loss = 0.22003817\n",
      "Iteration 162, loss = 0.21982297\n",
      "Iteration 163, loss = 0.21965037\n",
      "Iteration 164, loss = 0.21941995\n",
      "Iteration 165, loss = 0.21921931\n",
      "Iteration 166, loss = 0.21904615\n",
      "Iteration 167, loss = 0.21884203\n",
      "Iteration 168, loss = 0.21864084\n",
      "Iteration 169, loss = 0.21844102\n",
      "Iteration 170, loss = 0.21823799\n",
      "Iteration 171, loss = 0.21805703\n",
      "Iteration 172, loss = 0.21786277\n",
      "Iteration 173, loss = 0.21767364\n",
      "Iteration 174, loss = 0.21749957\n",
      "Iteration 175, loss = 0.21730296\n",
      "Iteration 176, loss = 0.21711041\n",
      "Iteration 177, loss = 0.21694063\n",
      "Iteration 178, loss = 0.21674177\n",
      "Iteration 179, loss = 0.21654600\n",
      "Iteration 180, loss = 0.21637658\n",
      "Iteration 181, loss = 0.21618363\n",
      "Iteration 182, loss = 0.21600080\n",
      "Iteration 183, loss = 0.21583770\n",
      "Iteration 184, loss = 0.21564434\n",
      "Iteration 185, loss = 0.21546825\n",
      "Iteration 186, loss = 0.21529945\n",
      "Iteration 187, loss = 0.21511660\n",
      "Iteration 188, loss = 0.21492975\n",
      "Iteration 189, loss = 0.21478211\n",
      "Iteration 190, loss = 0.21458470\n",
      "Iteration 191, loss = 0.21441650\n",
      "Iteration 192, loss = 0.21424750\n",
      "Iteration 193, loss = 0.21407481\n",
      "Iteration 194, loss = 0.21390987\n",
      "Iteration 195, loss = 0.21373978\n",
      "Iteration 196, loss = 0.21356565\n",
      "Iteration 197, loss = 0.21338865\n",
      "Iteration 198, loss = 0.21322225\n",
      "Iteration 199, loss = 0.21305502\n",
      "Iteration 200, loss = 0.21289990\n",
      "Iteration 201, loss = 0.21272290\n",
      "Iteration 202, loss = 0.21256403\n",
      "Iteration 203, loss = 0.21240761\n",
      "Iteration 204, loss = 0.21223723\n",
      "Iteration 205, loss = 0.21207791\n",
      "Iteration 206, loss = 0.21192525\n",
      "Iteration 207, loss = 0.21176300\n",
      "Iteration 208, loss = 0.21160067\n",
      "Iteration 209, loss = 0.21145199\n",
      "Iteration 210, loss = 0.21127689\n",
      "Iteration 211, loss = 0.21114581\n",
      "Iteration 212, loss = 0.21097644\n",
      "Iteration 213, loss = 0.21081795\n",
      "Iteration 214, loss = 0.21066688\n",
      "Iteration 215, loss = 0.21050746\n",
      "Iteration 216, loss = 0.21035338\n",
      "Iteration 217, loss = 0.21020257\n",
      "Iteration 218, loss = 0.21004174\n",
      "Iteration 219, loss = 0.20990111\n",
      "Iteration 220, loss = 0.20975524\n",
      "Iteration 221, loss = 0.20959398\n",
      "Iteration 222, loss = 0.20945321\n",
      "Iteration 223, loss = 0.20929085\n",
      "Iteration 224, loss = 0.20915024\n",
      "Iteration 225, loss = 0.20900140\n",
      "Iteration 226, loss = 0.20885511\n",
      "Iteration 227, loss = 0.20872620\n",
      "Iteration 228, loss = 0.20856864\n",
      "Iteration 229, loss = 0.20842288\n",
      "Iteration 230, loss = 0.20827085\n",
      "Iteration 231, loss = 0.20814131\n",
      "Iteration 232, loss = 0.20799960\n",
      "Iteration 233, loss = 0.20786078\n",
      "Iteration 234, loss = 0.20770092\n",
      "Iteration 235, loss = 0.20756903\n",
      "Iteration 236, loss = 0.20742251\n",
      "Iteration 237, loss = 0.20728322\n",
      "Iteration 238, loss = 0.20715312\n",
      "Iteration 239, loss = 0.20700213\n",
      "Iteration 240, loss = 0.20686248\n",
      "Iteration 241, loss = 0.20672893\n",
      "Iteration 242, loss = 0.20659188\n",
      "Iteration 243, loss = 0.20645748\n",
      "Iteration 244, loss = 0.20631165\n",
      "Iteration 245, loss = 0.20617744\n",
      "Iteration 246, loss = 0.20605690\n",
      "Iteration 247, loss = 0.20590555\n",
      "Iteration 248, loss = 0.20576987\n",
      "Iteration 249, loss = 0.20564303\n",
      "Iteration 250, loss = 0.20550542\n",
      "Iteration 251, loss = 0.20537351\n",
      "Iteration 252, loss = 0.20526696\n",
      "Iteration 253, loss = 0.20511170\n",
      "Iteration 254, loss = 0.20497806\n",
      "Iteration 255, loss = 0.20486070\n",
      "Iteration 256, loss = 0.20473176\n",
      "Iteration 257, loss = 0.20459047\n",
      "Iteration 258, loss = 0.20446182\n",
      "Iteration 259, loss = 0.20434648\n",
      "Iteration 260, loss = 0.20420773\n",
      "Iteration 261, loss = 0.20407636\n",
      "Iteration 262, loss = 0.20394792\n",
      "Iteration 263, loss = 0.20382434\n",
      "Iteration 264, loss = 0.20370880\n",
      "Iteration 265, loss = 0.20356974\n",
      "Iteration 266, loss = 0.20344335\n",
      "Iteration 267, loss = 0.20331917\n",
      "Iteration 268, loss = 0.20320791\n",
      "Iteration 269, loss = 0.20309148\n",
      "Iteration 270, loss = 0.20295164\n",
      "Iteration 271, loss = 0.20282884\n",
      "Iteration 272, loss = 0.20270806\n",
      "Iteration 273, loss = 0.20259152\n",
      "Iteration 274, loss = 0.20246017\n",
      "Iteration 275, loss = 0.20234896\n",
      "Iteration 276, loss = 0.20222312\n",
      "Iteration 277, loss = 0.20210741\n",
      "Iteration 278, loss = 0.20198675\n",
      "Iteration 279, loss = 0.20186556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 280, loss = 0.20175395\n",
      "Iteration 281, loss = 0.20162413\n",
      "Iteration 282, loss = 0.20152487\n",
      "Iteration 283, loss = 0.20140914\n",
      "Iteration 284, loss = 0.20127137\n",
      "Iteration 285, loss = 0.20116388\n",
      "Iteration 286, loss = 0.20104365\n",
      "Iteration 287, loss = 0.20092760\n",
      "Iteration 288, loss = 0.20081718\n",
      "Iteration 289, loss = 0.20070087\n",
      "Iteration 290, loss = 0.20057559\n",
      "Iteration 291, loss = 0.20046124\n",
      "Iteration 292, loss = 0.20035224\n",
      "Iteration 293, loss = 0.20022839\n",
      "Iteration 294, loss = 0.20013142\n",
      "Iteration 295, loss = 0.20002684\n",
      "Iteration 296, loss = 0.19989589\n",
      "Iteration 297, loss = 0.19979226\n",
      "Iteration 298, loss = 0.19969273\n",
      "Iteration 299, loss = 0.19957269\n",
      "Iteration 300, loss = 0.19944681\n",
      "Iteration 301, loss = 0.19934128\n",
      "Iteration 302, loss = 0.19924064\n",
      "Iteration 303, loss = 0.19913434\n",
      "Iteration 304, loss = 0.19900989\n",
      "Iteration 305, loss = 0.19890641\n",
      "Iteration 306, loss = 0.19879216\n",
      "Iteration 307, loss = 0.19868242\n",
      "Iteration 308, loss = 0.19857882\n",
      "Iteration 309, loss = 0.19847150\n",
      "Iteration 310, loss = 0.19835451\n",
      "Iteration 311, loss = 0.19825269\n",
      "Iteration 312, loss = 0.19815688\n",
      "Iteration 313, loss = 0.19803782\n",
      "Iteration 314, loss = 0.19792533\n",
      "Iteration 315, loss = 0.19782646\n",
      "Iteration 316, loss = 0.19772840\n",
      "Iteration 317, loss = 0.19762035\n",
      "Iteration 318, loss = 0.19751808\n",
      "Iteration 319, loss = 0.19740472\n",
      "Iteration 320, loss = 0.19729758\n",
      "Iteration 321, loss = 0.19719646\n",
      "Iteration 322, loss = 0.19710031\n",
      "Iteration 323, loss = 0.19700832\n",
      "Iteration 324, loss = 0.19688411\n",
      "Iteration 325, loss = 0.19677984\n",
      "Iteration 326, loss = 0.19669412\n",
      "Iteration 327, loss = 0.19658463\n",
      "Iteration 328, loss = 0.19647780\n",
      "Iteration 329, loss = 0.19637494\n",
      "Iteration 330, loss = 0.19627573\n",
      "Iteration 331, loss = 0.19618061\n",
      "Iteration 332, loss = 0.19606877\n",
      "Iteration 333, loss = 0.19597205\n",
      "Iteration 334, loss = 0.19588269\n",
      "Iteration 335, loss = 0.19577933\n",
      "Iteration 336, loss = 0.19567944\n",
      "Iteration 337, loss = 0.19558614\n",
      "Iteration 338, loss = 0.19548073\n",
      "Iteration 339, loss = 0.19538901\n",
      "Iteration 340, loss = 0.19527809\n",
      "Iteration 341, loss = 0.19518206\n",
      "Iteration 342, loss = 0.19508178\n",
      "Iteration 343, loss = 0.19499930\n",
      "Iteration 344, loss = 0.19489059\n",
      "Iteration 345, loss = 0.19479364\n",
      "Iteration 346, loss = 0.19469143\n",
      "Iteration 347, loss = 0.19460072\n",
      "Iteration 348, loss = 0.19450305\n",
      "Iteration 349, loss = 0.19442431\n",
      "Iteration 350, loss = 0.19432213\n",
      "Iteration 351, loss = 0.19421837\n",
      "Iteration 352, loss = 0.19412108\n",
      "Iteration 353, loss = 0.19402418\n",
      "Iteration 354, loss = 0.19394746\n",
      "Iteration 355, loss = 0.19383594\n",
      "Iteration 356, loss = 0.19375189\n",
      "Iteration 357, loss = 0.19364839\n",
      "Iteration 358, loss = 0.19355929\n",
      "Iteration 359, loss = 0.19346942\n",
      "Iteration 360, loss = 0.19337805\n",
      "Iteration 361, loss = 0.19327834\n",
      "Iteration 362, loss = 0.19319188\n",
      "Iteration 363, loss = 0.19309932\n",
      "Iteration 364, loss = 0.19300661\n",
      "Iteration 365, loss = 0.19291816\n",
      "Iteration 366, loss = 0.19283173\n",
      "Iteration 367, loss = 0.19272893\n",
      "Iteration 368, loss = 0.19264748\n",
      "Iteration 369, loss = 0.19255101\n",
      "Iteration 370, loss = 0.19245816\n",
      "Iteration 371, loss = 0.19236919\n",
      "Iteration 372, loss = 0.19228206\n",
      "Iteration 373, loss = 0.19219763\n",
      "Iteration 374, loss = 0.19209959\n",
      "Iteration 375, loss = 0.19200505\n",
      "Iteration 376, loss = 0.19192884\n",
      "Iteration 377, loss = 0.19183447\n",
      "Iteration 378, loss = 0.19175455\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.948247\n",
      "Training set loss: 0.191755\n",
      "training: inv-scaling with Nesterov's momentum\n",
      "Iteration 1, loss = 1.94238221\n",
      "Iteration 2, loss = 1.02966209\n",
      "Iteration 3, loss = 0.65086024\n",
      "Iteration 4, loss = 0.40808694\n",
      "Iteration 5, loss = 0.37051408\n",
      "Iteration 6, loss = 0.36274491\n",
      "Iteration 7, loss = 0.35222370\n",
      "Iteration 8, loss = 0.34292204\n",
      "Iteration 9, loss = 0.33616468\n",
      "Iteration 10, loss = 0.33181677\n",
      "Iteration 11, loss = 0.32827739\n",
      "Iteration 12, loss = 0.32538427\n",
      "Iteration 13, loss = 0.32278729\n",
      "Iteration 14, loss = 0.32053006\n",
      "Iteration 15, loss = 0.31838693\n",
      "Iteration 16, loss = 0.31635300\n",
      "Iteration 17, loss = 0.31447224\n",
      "Iteration 18, loss = 0.31263545\n",
      "Iteration 19, loss = 0.31101187\n",
      "Iteration 20, loss = 0.30928120\n",
      "Iteration 21, loss = 0.30765133\n",
      "Iteration 22, loss = 0.30616853\n",
      "Iteration 23, loss = 0.30469398\n",
      "Iteration 24, loss = 0.30331228\n",
      "Iteration 25, loss = 0.30200013\n",
      "Iteration 26, loss = 0.30063288\n",
      "Iteration 27, loss = 0.29942739\n",
      "Iteration 28, loss = 0.29819457\n",
      "Iteration 29, loss = 0.29699021\n",
      "Iteration 30, loss = 0.29580395\n",
      "Iteration 31, loss = 0.29466031\n",
      "Iteration 32, loss = 0.29359904\n",
      "Iteration 33, loss = 0.29254924\n",
      "Iteration 34, loss = 0.29156222\n",
      "Iteration 35, loss = 0.29052087\n",
      "Iteration 36, loss = 0.28950587\n",
      "Iteration 37, loss = 0.28857824\n",
      "Iteration 38, loss = 0.28763730\n",
      "Iteration 39, loss = 0.28670694\n",
      "Iteration 40, loss = 0.28581995\n",
      "Iteration 41, loss = 0.28493580\n",
      "Iteration 42, loss = 0.28409004\n",
      "Iteration 43, loss = 0.28325880\n",
      "Iteration 44, loss = 0.28244846\n",
      "Iteration 45, loss = 0.28168483\n",
      "Iteration 46, loss = 0.28089006\n",
      "Iteration 47, loss = 0.28006964\n",
      "Iteration 48, loss = 0.27938992\n",
      "Iteration 49, loss = 0.27859873\n",
      "Iteration 50, loss = 0.27785976\n",
      "Iteration 51, loss = 0.27718421\n",
      "Iteration 52, loss = 0.27646166\n",
      "Iteration 53, loss = 0.27575224\n",
      "Iteration 54, loss = 0.27512075\n",
      "Iteration 55, loss = 0.27439791\n",
      "Iteration 56, loss = 0.27376673\n",
      "Iteration 57, loss = 0.27314275\n",
      "Iteration 58, loss = 0.27248081\n",
      "Iteration 59, loss = 0.27190288\n",
      "Iteration 60, loss = 0.27124896\n",
      "Iteration 61, loss = 0.27064347\n",
      "Iteration 62, loss = 0.27004518\n",
      "Iteration 63, loss = 0.26946296\n",
      "Iteration 64, loss = 0.26885479\n",
      "Iteration 65, loss = 0.26830383\n",
      "Iteration 66, loss = 0.26775641\n",
      "Iteration 67, loss = 0.26717565\n",
      "Iteration 68, loss = 0.26664512\n",
      "Iteration 69, loss = 0.26611589\n",
      "Iteration 70, loss = 0.26559443\n",
      "Iteration 71, loss = 0.26502838\n",
      "Iteration 72, loss = 0.26451924\n",
      "Iteration 73, loss = 0.26403331\n",
      "Iteration 74, loss = 0.26352652\n",
      "Iteration 75, loss = 0.26302194\n",
      "Iteration 76, loss = 0.26253615\n",
      "Iteration 77, loss = 0.26201942\n",
      "Iteration 78, loss = 0.26152466\n",
      "Iteration 79, loss = 0.26108939\n",
      "Iteration 80, loss = 0.26060617\n",
      "Iteration 81, loss = 0.26014397\n",
      "Iteration 82, loss = 0.25967421\n",
      "Iteration 83, loss = 0.25921572\n",
      "Iteration 84, loss = 0.25876090\n",
      "Iteration 85, loss = 0.25831743\n",
      "Iteration 86, loss = 0.25790983\n",
      "Iteration 87, loss = 0.25744874\n",
      "Iteration 88, loss = 0.25704228\n",
      "Iteration 89, loss = 0.25660723\n",
      "Iteration 90, loss = 0.25622982\n",
      "Iteration 91, loss = 0.25577611\n",
      "Iteration 92, loss = 0.25536648\n",
      "Iteration 93, loss = 0.25497528\n",
      "Iteration 94, loss = 0.25453856\n",
      "Iteration 95, loss = 0.25415287\n",
      "Iteration 96, loss = 0.25377211\n",
      "Iteration 97, loss = 0.25336228\n",
      "Iteration 98, loss = 0.25298445\n",
      "Iteration 99, loss = 0.25258232\n",
      "Iteration 100, loss = 0.25221367\n",
      "Iteration 101, loss = 0.25183794\n",
      "Iteration 102, loss = 0.25146088\n",
      "Iteration 103, loss = 0.25108646\n",
      "Iteration 104, loss = 0.25074317\n",
      "Iteration 105, loss = 0.25038219\n",
      "Iteration 106, loss = 0.25000328\n",
      "Iteration 107, loss = 0.24963415\n",
      "Iteration 108, loss = 0.24928611\n",
      "Iteration 109, loss = 0.24894302\n",
      "Iteration 110, loss = 0.24859603\n",
      "Iteration 111, loss = 0.24825542\n",
      "Iteration 112, loss = 0.24791904\n",
      "Iteration 113, loss = 0.24757186\n",
      "Iteration 114, loss = 0.24722333\n",
      "Iteration 115, loss = 0.24691007\n",
      "Iteration 116, loss = 0.24655773\n",
      "Iteration 117, loss = 0.24623629\n",
      "Iteration 118, loss = 0.24592382\n",
      "Iteration 119, loss = 0.24559769\n",
      "Iteration 120, loss = 0.24527066\n",
      "Iteration 121, loss = 0.24496682\n",
      "Iteration 122, loss = 0.24465773\n",
      "Iteration 123, loss = 0.24433663\n",
      "Iteration 124, loss = 0.24401814\n",
      "Iteration 125, loss = 0.24371026\n",
      "Iteration 126, loss = 0.24341098\n",
      "Iteration 127, loss = 0.24312440\n",
      "Iteration 128, loss = 0.24280502\n",
      "Iteration 129, loss = 0.24253599\n",
      "Iteration 130, loss = 0.24222767\n",
      "Iteration 131, loss = 0.24193030\n",
      "Iteration 132, loss = 0.24162911\n",
      "Iteration 133, loss = 0.24137703\n",
      "Iteration 134, loss = 0.24105711\n",
      "Iteration 135, loss = 0.24075612\n",
      "Iteration 136, loss = 0.24049718\n",
      "Iteration 137, loss = 0.24019686\n",
      "Iteration 138, loss = 0.23995652\n",
      "Iteration 139, loss = 0.23965091\n",
      "Iteration 140, loss = 0.23938242\n",
      "Iteration 141, loss = 0.23910623\n",
      "Iteration 142, loss = 0.23882344\n",
      "Iteration 143, loss = 0.23854427\n",
      "Iteration 144, loss = 0.23829712\n",
      "Iteration 145, loss = 0.23803212\n",
      "Iteration 146, loss = 0.23776090\n",
      "Iteration 147, loss = 0.23750083\n",
      "Iteration 148, loss = 0.23723941\n",
      "Iteration 149, loss = 0.23696038\n",
      "Iteration 150, loss = 0.23670291\n",
      "Iteration 151, loss = 0.23646901\n",
      "Iteration 152, loss = 0.23620374\n",
      "Iteration 153, loss = 0.23593600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 154, loss = 0.23569252\n",
      "Iteration 155, loss = 0.23543512\n",
      "Iteration 156, loss = 0.23519628\n",
      "Iteration 157, loss = 0.23493675\n",
      "Iteration 158, loss = 0.23469303\n",
      "Iteration 159, loss = 0.23445294\n",
      "Iteration 160, loss = 0.23421757\n",
      "Iteration 161, loss = 0.23397152\n",
      "Iteration 162, loss = 0.23371793\n",
      "Iteration 163, loss = 0.23350957\n",
      "Iteration 164, loss = 0.23324198\n",
      "Iteration 165, loss = 0.23300611\n",
      "Iteration 166, loss = 0.23279640\n",
      "Iteration 167, loss = 0.23256057\n",
      "Iteration 168, loss = 0.23232052\n",
      "Iteration 169, loss = 0.23208580\n",
      "Iteration 170, loss = 0.23184738\n",
      "Iteration 171, loss = 0.23163013\n",
      "Iteration 172, loss = 0.23140037\n",
      "Iteration 173, loss = 0.23117808\n",
      "Iteration 174, loss = 0.23096785\n",
      "Iteration 175, loss = 0.23074342\n",
      "Iteration 176, loss = 0.23051676\n",
      "Iteration 177, loss = 0.23031034\n",
      "Iteration 178, loss = 0.23007988\n",
      "Iteration 179, loss = 0.22985097\n",
      "Iteration 180, loss = 0.22965055\n",
      "Iteration 181, loss = 0.22942381\n",
      "Iteration 182, loss = 0.22920799\n",
      "Iteration 183, loss = 0.22901406\n",
      "Iteration 184, loss = 0.22878523\n",
      "Iteration 185, loss = 0.22857968\n",
      "Iteration 186, loss = 0.22838086\n",
      "Iteration 187, loss = 0.22816648\n",
      "Iteration 188, loss = 0.22794771\n",
      "Iteration 189, loss = 0.22777158\n",
      "Iteration 190, loss = 0.22754146\n",
      "Iteration 191, loss = 0.22734427\n",
      "Iteration 192, loss = 0.22714771\n",
      "Iteration 193, loss = 0.22694550\n",
      "Iteration 194, loss = 0.22675119\n",
      "Iteration 195, loss = 0.22654963\n",
      "Iteration 196, loss = 0.22634561\n",
      "Iteration 197, loss = 0.22614061\n",
      "Iteration 198, loss = 0.22594638\n",
      "Iteration 199, loss = 0.22575143\n",
      "Iteration 200, loss = 0.22556787\n",
      "Iteration 201, loss = 0.22535859\n",
      "Iteration 202, loss = 0.22517565\n",
      "Iteration 203, loss = 0.22499188\n",
      "Iteration 204, loss = 0.22478806\n",
      "Iteration 205, loss = 0.22460399\n",
      "Iteration 206, loss = 0.22441952\n",
      "Iteration 207, loss = 0.22423296\n",
      "Iteration 208, loss = 0.22404368\n",
      "Iteration 209, loss = 0.22386724\n",
      "Iteration 210, loss = 0.22366539\n",
      "Iteration 211, loss = 0.22350811\n",
      "Iteration 212, loss = 0.22330668\n",
      "Iteration 213, loss = 0.22312557\n",
      "Iteration 214, loss = 0.22294702\n",
      "Iteration 215, loss = 0.22276064\n",
      "Iteration 216, loss = 0.22257927\n",
      "Iteration 217, loss = 0.22240084\n",
      "Iteration 218, loss = 0.22221340\n",
      "Iteration 219, loss = 0.22204599\n",
      "Iteration 220, loss = 0.22187820\n",
      "Iteration 221, loss = 0.22169086\n",
      "Iteration 222, loss = 0.22152267\n",
      "Iteration 223, loss = 0.22133421\n",
      "Iteration 224, loss = 0.22116844\n",
      "Iteration 225, loss = 0.22099675\n",
      "Iteration 226, loss = 0.22082474\n",
      "Iteration 227, loss = 0.22067245\n",
      "Iteration 228, loss = 0.22048959\n",
      "Iteration 229, loss = 0.22031852\n",
      "Iteration 230, loss = 0.22013998\n",
      "Iteration 231, loss = 0.21999002\n",
      "Iteration 232, loss = 0.21982115\n",
      "Iteration 233, loss = 0.21966121\n",
      "Iteration 234, loss = 0.21947518\n",
      "Iteration 235, loss = 0.21931742\n",
      "Iteration 236, loss = 0.21914894\n",
      "Iteration 237, loss = 0.21898470\n",
      "Iteration 238, loss = 0.21883374\n",
      "Iteration 239, loss = 0.21865617\n",
      "Iteration 240, loss = 0.21849299\n",
      "Iteration 241, loss = 0.21833541\n",
      "Iteration 242, loss = 0.21817655\n",
      "Iteration 243, loss = 0.21801900\n",
      "Iteration 244, loss = 0.21784986\n",
      "Iteration 245, loss = 0.21769308\n",
      "Iteration 246, loss = 0.21755070\n",
      "Iteration 247, loss = 0.21737514\n",
      "Iteration 248, loss = 0.21721877\n",
      "Iteration 249, loss = 0.21706778\n",
      "Iteration 250, loss = 0.21690700\n",
      "Iteration 251, loss = 0.21675404\n",
      "Iteration 252, loss = 0.21662619\n",
      "Iteration 253, loss = 0.21645060\n",
      "Iteration 254, loss = 0.21629392\n",
      "Iteration 255, loss = 0.21615371\n",
      "Iteration 256, loss = 0.21600277\n",
      "Iteration 257, loss = 0.21583927\n",
      "Iteration 258, loss = 0.21569179\n",
      "Iteration 259, loss = 0.21555412\n",
      "Iteration 260, loss = 0.21539571\n",
      "Iteration 261, loss = 0.21524234\n",
      "Iteration 262, loss = 0.21509218\n",
      "Iteration 263, loss = 0.21494812\n",
      "Iteration 264, loss = 0.21481270\n",
      "Iteration 265, loss = 0.21465317\n",
      "Iteration 266, loss = 0.21450477\n",
      "Iteration 267, loss = 0.21436065\n",
      "Iteration 268, loss = 0.21422772\n",
      "Iteration 269, loss = 0.21409304\n",
      "Iteration 270, loss = 0.21393064\n",
      "Iteration 271, loss = 0.21378857\n",
      "Iteration 272, loss = 0.21364912\n",
      "Iteration 273, loss = 0.21351006\n",
      "Iteration 274, loss = 0.21336035\n",
      "Iteration 275, loss = 0.21322761\n",
      "Iteration 276, loss = 0.21308005\n",
      "Iteration 277, loss = 0.21294719\n",
      "Iteration 278, loss = 0.21280603\n",
      "Iteration 279, loss = 0.21266317\n",
      "Iteration 280, loss = 0.21253328\n",
      "Iteration 281, loss = 0.21238295\n",
      "Iteration 282, loss = 0.21226638\n",
      "Iteration 283, loss = 0.21213119\n",
      "Iteration 284, loss = 0.21197237\n",
      "Iteration 285, loss = 0.21184606\n",
      "Iteration 286, loss = 0.21170736\n",
      "Iteration 287, loss = 0.21156972\n",
      "Iteration 288, loss = 0.21144333\n",
      "Iteration 289, loss = 0.21130897\n",
      "Iteration 290, loss = 0.21116390\n",
      "Iteration 291, loss = 0.21102951\n",
      "Iteration 292, loss = 0.21090233\n",
      "Iteration 293, loss = 0.21076067\n",
      "Iteration 294, loss = 0.21064871\n",
      "Iteration 295, loss = 0.21052177\n",
      "Iteration 296, loss = 0.21037240\n",
      "Iteration 297, loss = 0.21025115\n",
      "Iteration 298, loss = 0.21013371\n",
      "Iteration 299, loss = 0.20999615\n",
      "Iteration 300, loss = 0.20985237\n",
      "Iteration 301, loss = 0.20972774\n",
      "Iteration 302, loss = 0.20960990\n",
      "Iteration 303, loss = 0.20948521\n",
      "Iteration 304, loss = 0.20934241\n",
      "Iteration 305, loss = 0.20922313\n",
      "Iteration 306, loss = 0.20909040\n",
      "Iteration 307, loss = 0.20896435\n",
      "Iteration 308, loss = 0.20884587\n",
      "Iteration 309, loss = 0.20871577\n",
      "Iteration 310, loss = 0.20858468\n",
      "Iteration 311, loss = 0.20846421\n",
      "Iteration 312, loss = 0.20835192\n",
      "Iteration 313, loss = 0.20821580\n",
      "Iteration 314, loss = 0.20808690\n",
      "Iteration 315, loss = 0.20797090\n",
      "Iteration 316, loss = 0.20785561\n",
      "Iteration 317, loss = 0.20773239\n",
      "Iteration 318, loss = 0.20761203\n",
      "Iteration 319, loss = 0.20748222\n",
      "Iteration 320, loss = 0.20735893\n",
      "Iteration 321, loss = 0.20724082\n",
      "Iteration 322, loss = 0.20712854\n",
      "Iteration 323, loss = 0.20702238\n",
      "Iteration 324, loss = 0.20687943\n",
      "Iteration 325, loss = 0.20676003\n",
      "Iteration 326, loss = 0.20665975\n",
      "Iteration 327, loss = 0.20653293\n",
      "Iteration 328, loss = 0.20641063\n",
      "Iteration 329, loss = 0.20629069\n",
      "Iteration 330, loss = 0.20617730\n",
      "Iteration 331, loss = 0.20606351\n",
      "Iteration 332, loss = 0.20593911\n",
      "Iteration 333, loss = 0.20582650\n",
      "Iteration 334, loss = 0.20572157\n",
      "Iteration 335, loss = 0.20560535\n",
      "Iteration 336, loss = 0.20548871\n",
      "Iteration 337, loss = 0.20538089\n",
      "Iteration 338, loss = 0.20526119\n",
      "Iteration 339, loss = 0.20515377\n",
      "Iteration 340, loss = 0.20502548\n",
      "Iteration 341, loss = 0.20491511\n",
      "Iteration 342, loss = 0.20479944\n",
      "Iteration 343, loss = 0.20470360\n",
      "Iteration 344, loss = 0.20457763\n",
      "Iteration 345, loss = 0.20446715\n",
      "Iteration 346, loss = 0.20435066\n",
      "Iteration 347, loss = 0.20424455\n",
      "Iteration 348, loss = 0.20413413\n",
      "Iteration 349, loss = 0.20404036\n",
      "Iteration 350, loss = 0.20392398\n",
      "Iteration 351, loss = 0.20380487\n",
      "Iteration 352, loss = 0.20369274\n",
      "Iteration 353, loss = 0.20358038\n",
      "Iteration 354, loss = 0.20349170\n",
      "Iteration 355, loss = 0.20336401\n",
      "Iteration 356, loss = 0.20326513\n",
      "Iteration 357, loss = 0.20314897\n",
      "Iteration 358, loss = 0.20304538\n",
      "Iteration 359, loss = 0.20293978\n",
      "Iteration 360, loss = 0.20283614\n",
      "Iteration 361, loss = 0.20272210\n",
      "Iteration 362, loss = 0.20262347\n",
      "Iteration 363, loss = 0.20251367\n",
      "Iteration 364, loss = 0.20240959\n",
      "Iteration 365, loss = 0.20230503\n",
      "Iteration 366, loss = 0.20220767\n",
      "Iteration 367, loss = 0.20209152\n",
      "Iteration 368, loss = 0.20199359\n",
      "Iteration 369, loss = 0.20188436\n",
      "Iteration 370, loss = 0.20177910\n",
      "Iteration 371, loss = 0.20167565\n",
      "Iteration 372, loss = 0.20157571\n",
      "Iteration 373, loss = 0.20147814\n",
      "Iteration 374, loss = 0.20136763\n",
      "Iteration 375, loss = 0.20125870\n",
      "Iteration 376, loss = 0.20116857\n",
      "Iteration 377, loss = 0.20106109\n",
      "Iteration 378, loss = 0.20096739\n",
      "Iteration 379, loss = 0.20086950\n",
      "Iteration 380, loss = 0.20075762\n",
      "Iteration 381, loss = 0.20066101\n",
      "Iteration 382, loss = 0.20055546\n",
      "Iteration 383, loss = 0.20045686\n",
      "Iteration 384, loss = 0.20036182\n",
      "Iteration 385, loss = 0.20026581\n",
      "Iteration 386, loss = 0.20015733\n",
      "Iteration 387, loss = 0.20006764\n",
      "Iteration 388, loss = 0.19996861\n",
      "Iteration 389, loss = 0.19985968\n",
      "Iteration 390, loss = 0.19976860\n",
      "Iteration 391, loss = 0.19966623\n",
      "Iteration 392, loss = 0.19958406\n",
      "Iteration 393, loss = 0.19947398\n",
      "Iteration 394, loss = 0.19937605\n",
      "Iteration 395, loss = 0.19927792\n",
      "Iteration 396, loss = 0.19919214\n",
      "Iteration 397, loss = 0.19909603\n",
      "Iteration 398, loss = 0.19900290\n",
      "Iteration 399, loss = 0.19888659\n",
      "Iteration 400, loss = 0.19879679\n",
      "Iteration 401, loss = 0.19870125\n",
      "Iteration 402, loss = 0.19861544\n",
      "Iteration 403, loss = 0.19850817\n",
      "Iteration 404, loss = 0.19841259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 405, loss = 0.19832692\n",
      "Iteration 406, loss = 0.19822906\n",
      "Iteration 407, loss = 0.19813694\n",
      "Iteration 408, loss = 0.19804331\n",
      "Iteration 409, loss = 0.19794230\n",
      "Iteration 410, loss = 0.19786765\n",
      "Iteration 411, loss = 0.19775887\n",
      "Iteration 412, loss = 0.19767228\n",
      "Iteration 413, loss = 0.19757364\n",
      "Iteration 414, loss = 0.19747871\n",
      "Iteration 415, loss = 0.19740062\n",
      "Iteration 416, loss = 0.19729880\n",
      "Iteration 417, loss = 0.19720289\n",
      "Iteration 418, loss = 0.19712400\n",
      "Iteration 419, loss = 0.19702327\n",
      "Iteration 420, loss = 0.19693920\n",
      "Iteration 421, loss = 0.19684363\n",
      "Iteration 422, loss = 0.19675966\n",
      "Iteration 423, loss = 0.19666409\n",
      "Iteration 424, loss = 0.19658305\n",
      "Iteration 425, loss = 0.19649642\n",
      "Iteration 426, loss = 0.19639219\n",
      "Iteration 427, loss = 0.19630618\n",
      "Iteration 428, loss = 0.19621343\n",
      "Iteration 429, loss = 0.19612545\n",
      "Iteration 430, loss = 0.19603537\n",
      "Iteration 431, loss = 0.19594969\n",
      "Iteration 432, loss = 0.19586322\n",
      "Iteration 433, loss = 0.19577256\n",
      "Iteration 434, loss = 0.19568572\n",
      "Iteration 435, loss = 0.19559924\n",
      "Iteration 436, loss = 0.19552333\n",
      "Iteration 437, loss = 0.19542346\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.949917\n",
      "Training set loss: 0.195423\n",
      "training: adam\n",
      "Iteration 1, loss = 1.79299026\n",
      "Iteration 2, loss = 0.70576785\n",
      "Iteration 3, loss = 0.32082894\n",
      "Iteration 4, loss = 0.20531281\n",
      "Iteration 5, loss = 0.15483935\n",
      "Iteration 6, loss = 0.13064054\n",
      "Iteration 7, loss = 0.11579721\n",
      "Iteration 8, loss = 0.09939453\n",
      "Iteration 9, loss = 0.08412694\n",
      "Iteration 10, loss = 0.07732560\n",
      "Iteration 11, loss = 0.07169006\n",
      "Iteration 12, loss = 0.06331308\n",
      "Iteration 13, loss = 0.05730114\n",
      "Iteration 14, loss = 0.05329983\n",
      "Iteration 15, loss = 0.04593450\n",
      "Iteration 16, loss = 0.04249543\n",
      "Iteration 17, loss = 0.03997710\n",
      "Iteration 18, loss = 0.03626572\n",
      "Iteration 19, loss = 0.03605921\n",
      "Iteration 20, loss = 0.03171875\n",
      "Iteration 21, loss = 0.02947410\n",
      "Iteration 22, loss = 0.02660658\n",
      "Iteration 23, loss = 0.02429873\n",
      "Iteration 24, loss = 0.02011003\n",
      "Iteration 25, loss = 0.01864453\n",
      "Iteration 26, loss = 0.01749772\n",
      "Iteration 27, loss = 0.01622076\n",
      "Iteration 28, loss = 0.01607448\n",
      "Iteration 29, loss = 0.01590500\n",
      "Iteration 30, loss = 0.01473511\n",
      "Iteration 31, loss = 0.01274196\n",
      "Iteration 32, loss = 0.01098187\n",
      "Iteration 33, loss = 0.01040021\n",
      "Iteration 34, loss = 0.01023764\n",
      "Iteration 35, loss = 0.00897877\n",
      "Iteration 36, loss = 0.00872924\n",
      "Iteration 37, loss = 0.00811416\n",
      "Iteration 38, loss = 0.00779988\n",
      "Iteration 39, loss = 0.00740746\n",
      "Iteration 40, loss = 0.00695684\n",
      "Iteration 41, loss = 0.00675809\n",
      "Iteration 42, loss = 0.00678919\n",
      "Iteration 43, loss = 0.00588270\n",
      "Iteration 44, loss = 0.00578085\n",
      "Iteration 45, loss = 0.00556886\n",
      "Iteration 46, loss = 0.00545432\n",
      "Iteration 47, loss = 0.00497094\n",
      "Iteration 48, loss = 0.00462583\n",
      "Iteration 49, loss = 0.00430916\n",
      "Iteration 50, loss = 0.00409276\n",
      "Iteration 51, loss = 0.00391729\n",
      "Iteration 52, loss = 0.00376940\n",
      "Iteration 53, loss = 0.00349451\n",
      "Iteration 54, loss = 0.00367591\n",
      "Iteration 55, loss = 0.00336975\n",
      "Iteration 56, loss = 0.00324305\n",
      "Iteration 57, loss = 0.00318373\n",
      "Iteration 58, loss = 0.00317282\n",
      "Iteration 59, loss = 0.00295666\n",
      "Iteration 60, loss = 0.00279235\n",
      "Iteration 61, loss = 0.00269615\n",
      "Iteration 62, loss = 0.00254248\n",
      "Iteration 63, loss = 0.00251892\n",
      "Iteration 64, loss = 0.00255487\n",
      "Iteration 65, loss = 0.00237797\n",
      "Iteration 66, loss = 0.00225238\n",
      "Iteration 67, loss = 0.00218249\n",
      "Iteration 68, loss = 0.00210973\n",
      "Iteration 69, loss = 0.00206163\n",
      "Iteration 70, loss = 0.00215144\n",
      "Iteration 71, loss = 0.00205416\n",
      "Iteration 72, loss = 0.00195937\n",
      "Iteration 73, loss = 0.00186411\n",
      "Iteration 74, loss = 0.00180556\n",
      "Iteration 75, loss = 0.00178158\n",
      "Iteration 76, loss = 0.00173693\n",
      "Iteration 77, loss = 0.00172891\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 1.000000\n",
      "Training set loss: 0.001729\n",
      "\n",
      "learning on dataset circles\n",
      "training: constant learning-rate\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.69336674\n",
      "Iteration 3, loss = 0.69183412\n",
      "Iteration 4, loss = 0.69072503\n",
      "Iteration 5, loss = 0.68991576\n",
      "Iteration 6, loss = 0.68928297\n",
      "Iteration 7, loss = 0.68877282\n",
      "Iteration 8, loss = 0.68834795\n",
      "Iteration 9, loss = 0.68799632\n",
      "Iteration 10, loss = 0.68769577\n",
      "Iteration 11, loss = 0.68742717\n",
      "Iteration 12, loss = 0.68718825\n",
      "Iteration 13, loss = 0.68696762\n",
      "Iteration 14, loss = 0.68675326\n",
      "Iteration 15, loss = 0.68653513\n",
      "Iteration 16, loss = 0.68631577\n",
      "Iteration 17, loss = 0.68610482\n",
      "Iteration 18, loss = 0.68588712\n",
      "Iteration 19, loss = 0.68566580\n",
      "Iteration 20, loss = 0.68543962\n",
      "Iteration 21, loss = 0.68522084\n",
      "Iteration 22, loss = 0.68501041\n",
      "Iteration 23, loss = 0.68481008\n",
      "Iteration 24, loss = 0.68461552\n",
      "Iteration 25, loss = 0.68441030\n",
      "Iteration 26, loss = 0.68421279\n",
      "Iteration 27, loss = 0.68402724\n",
      "Iteration 28, loss = 0.68386305\n",
      "Iteration 29, loss = 0.68371924\n",
      "Iteration 30, loss = 0.68358544\n",
      "Iteration 31, loss = 0.68345139\n",
      "Iteration 32, loss = 0.68332397\n",
      "Iteration 33, loss = 0.68319834\n",
      "Iteration 34, loss = 0.68307718\n",
      "Iteration 35, loss = 0.68295747\n",
      "Iteration 36, loss = 0.68284132\n",
      "Iteration 37, loss = 0.68273132\n",
      "Iteration 38, loss = 0.68262330\n",
      "Iteration 39, loss = 0.68251852\n",
      "Iteration 40, loss = 0.68241398\n",
      "Iteration 41, loss = 0.68231056\n",
      "Iteration 42, loss = 0.68220776\n",
      "Iteration 43, loss = 0.68210447\n",
      "Iteration 44, loss = 0.68200034\n",
      "Iteration 45, loss = 0.68189766\n",
      "Iteration 46, loss = 0.68179759\n",
      "Iteration 47, loss = 0.68169821\n",
      "Iteration 48, loss = 0.68159827\n",
      "Iteration 49, loss = 0.68149842\n",
      "Iteration 50, loss = 0.68139856\n",
      "Iteration 51, loss = 0.68129925\n",
      "Iteration 52, loss = 0.68119983\n",
      "Iteration 53, loss = 0.68109923\n",
      "Iteration 54, loss = 0.68099749\n",
      "Iteration 55, loss = 0.68089509\n",
      "Iteration 56, loss = 0.68079308\n",
      "Iteration 57, loss = 0.68069103\n",
      "Iteration 58, loss = 0.68058818\n",
      "Iteration 59, loss = 0.68048382\n",
      "Iteration 60, loss = 0.68037928\n",
      "Iteration 61, loss = 0.68027431\n",
      "Iteration 62, loss = 0.68016889\n",
      "Iteration 63, loss = 0.68006299\n",
      "Iteration 64, loss = 0.67995854\n",
      "Iteration 65, loss = 0.67985320\n",
      "Iteration 66, loss = 0.67974814\n",
      "Iteration 67, loss = 0.67964317\n",
      "Iteration 68, loss = 0.67953760\n",
      "Iteration 69, loss = 0.67943116\n",
      "Iteration 70, loss = 0.67932354\n",
      "Iteration 71, loss = 0.67921457\n",
      "Iteration 72, loss = 0.67910528\n",
      "Iteration 73, loss = 0.67899456\n",
      "Iteration 74, loss = 0.67888263\n",
      "Iteration 75, loss = 0.67877001\n",
      "Iteration 76, loss = 0.67865730\n",
      "Iteration 77, loss = 0.67854484\n",
      "Iteration 78, loss = 0.67843164\n",
      "Iteration 79, loss = 0.67831836\n",
      "Iteration 80, loss = 0.67820403\n",
      "Iteration 81, loss = 0.67809000\n",
      "Iteration 82, loss = 0.67797463\n",
      "Iteration 83, loss = 0.67785920\n",
      "Iteration 84, loss = 0.67774169\n",
      "Iteration 85, loss = 0.67762228\n",
      "Iteration 86, loss = 0.67750156\n",
      "Iteration 87, loss = 0.67737882\n",
      "Iteration 88, loss = 0.67725328\n",
      "Iteration 89, loss = 0.67712794\n",
      "Iteration 90, loss = 0.67700293\n",
      "Iteration 91, loss = 0.67687813\n",
      "Iteration 92, loss = 0.67675278\n",
      "Iteration 93, loss = 0.67662632\n",
      "Iteration 94, loss = 0.67649915\n",
      "Iteration 95, loss = 0.67637199\n",
      "Iteration 96, loss = 0.67624150\n",
      "Iteration 97, loss = 0.67610839\n",
      "Iteration 98, loss = 0.67597534\n",
      "Iteration 99, loss = 0.67584136\n",
      "Iteration 100, loss = 0.67570553\n",
      "Iteration 101, loss = 0.67556876\n",
      "Iteration 102, loss = 0.67542436\n",
      "Iteration 103, loss = 0.67527494\n",
      "Iteration 104, loss = 0.67512423\n",
      "Iteration 105, loss = 0.67497003\n",
      "Iteration 106, loss = 0.67479976\n",
      "Iteration 107, loss = 0.67461642\n",
      "Iteration 108, loss = 0.67443003\n",
      "Iteration 109, loss = 0.67424585\n",
      "Iteration 110, loss = 0.67407829\n",
      "Iteration 111, loss = 0.67392181\n",
      "Iteration 112, loss = 0.67376151\n",
      "Iteration 113, loss = 0.67359601\n",
      "Iteration 114, loss = 0.67343570\n",
      "Iteration 115, loss = 0.67328177\n",
      "Iteration 116, loss = 0.67312572\n",
      "Iteration 117, loss = 0.67296889\n",
      "Iteration 118, loss = 0.67281165\n",
      "Iteration 119, loss = 0.67265954\n",
      "Iteration 120, loss = 0.67251258\n",
      "Iteration 121, loss = 0.67236702\n",
      "Iteration 122, loss = 0.67222041\n",
      "Iteration 123, loss = 0.67207168\n",
      "Iteration 124, loss = 0.67192025\n",
      "Iteration 125, loss = 0.67176919\n",
      "Iteration 126, loss = 0.67161748\n",
      "Iteration 127, loss = 0.67146679\n",
      "Iteration 128, loss = 0.67131746\n",
      "Iteration 129, loss = 0.67116793\n",
      "Iteration 130, loss = 0.67101909\n",
      "Iteration 131, loss = 0.67087130\n",
      "Iteration 132, loss = 0.67072382\n",
      "Iteration 133, loss = 0.67057633\n",
      "Iteration 134, loss = 0.67042891\n",
      "Iteration 135, loss = 0.67028230\n",
      "Iteration 136, loss = 0.67013585\n",
      "Iteration 137, loss = 0.66998853\n",
      "Iteration 138, loss = 0.66984043\n",
      "Iteration 139, loss = 0.66969217\n",
      "Iteration 140, loss = 0.66954345\n",
      "Iteration 141, loss = 0.66939493\n",
      "Iteration 142, loss = 0.66924573\n",
      "Iteration 143, loss = 0.66909547\n",
      "Iteration 144, loss = 0.66894460\n",
      "Iteration 145, loss = 0.66879302\n",
      "Iteration 146, loss = 0.66864094\n",
      "Iteration 147, loss = 0.66848911\n",
      "Iteration 148, loss = 0.66833638\n",
      "Iteration 149, loss = 0.66818271\n",
      "Iteration 150, loss = 0.66802794\n",
      "Iteration 151, loss = 0.66787277\n",
      "Iteration 152, loss = 0.66771682\n",
      "Iteration 153, loss = 0.66756019\n",
      "Iteration 154, loss = 0.66740353\n",
      "Iteration 155, loss = 0.66724618\n",
      "Iteration 156, loss = 0.66708836\n",
      "Iteration 157, loss = 0.66692973\n",
      "Iteration 158, loss = 0.66677085\n",
      "Iteration 159, loss = 0.66661128\n",
      "Iteration 160, loss = 0.66645182\n",
      "Iteration 161, loss = 0.66629119\n",
      "Iteration 162, loss = 0.66612963\n",
      "Iteration 163, loss = 0.66596748\n",
      "Iteration 164, loss = 0.66580493\n",
      "Iteration 165, loss = 0.66564160\n",
      "Iteration 166, loss = 0.66547769\n",
      "Iteration 167, loss = 0.66531304\n",
      "Iteration 168, loss = 0.66514670\n",
      "Iteration 169, loss = 0.66497947\n",
      "Iteration 170, loss = 0.66481217\n",
      "Iteration 171, loss = 0.66464341\n",
      "Iteration 172, loss = 0.66447480\n",
      "Iteration 173, loss = 0.66430521\n",
      "Iteration 174, loss = 0.66413515\n",
      "Iteration 175, loss = 0.66396445\n",
      "Iteration 176, loss = 0.66379254\n",
      "Iteration 177, loss = 0.66361952\n",
      "Iteration 178, loss = 0.66344692\n",
      "Iteration 179, loss = 0.66327248\n",
      "Iteration 180, loss = 0.66309799\n",
      "Iteration 181, loss = 0.66292225\n",
      "Iteration 182, loss = 0.66274593\n",
      "Iteration 183, loss = 0.66256884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 184, loss = 0.66239176\n",
      "Iteration 185, loss = 0.66221310\n",
      "Iteration 186, loss = 0.66203470\n",
      "Iteration 187, loss = 0.66185483\n",
      "Iteration 188, loss = 0.66167497\n",
      "Iteration 189, loss = 0.66149367\n",
      "Iteration 190, loss = 0.66131137\n",
      "Iteration 191, loss = 0.66112911\n",
      "Iteration 192, loss = 0.66094547\n",
      "Iteration 193, loss = 0.66076158\n",
      "Iteration 194, loss = 0.66057661\n",
      "Iteration 195, loss = 0.66039145\n",
      "Iteration 196, loss = 0.66020504\n",
      "Iteration 197, loss = 0.66001724\n",
      "Iteration 198, loss = 0.65982910\n",
      "Iteration 199, loss = 0.65963918\n",
      "Iteration 200, loss = 0.65944976\n",
      "Iteration 201, loss = 0.65925796\n",
      "Iteration 202, loss = 0.65906532\n",
      "Iteration 203, loss = 0.65887273\n",
      "Iteration 204, loss = 0.65867780\n",
      "Iteration 205, loss = 0.65848197\n",
      "Iteration 206, loss = 0.65828588\n",
      "Iteration 207, loss = 0.65808854\n",
      "Iteration 208, loss = 0.65788997\n",
      "Iteration 209, loss = 0.65769167\n",
      "Iteration 210, loss = 0.65749202\n",
      "Iteration 211, loss = 0.65729139\n",
      "Iteration 212, loss = 0.65709098\n",
      "Iteration 213, loss = 0.65688834\n",
      "Iteration 214, loss = 0.65668453\n",
      "Iteration 215, loss = 0.65647900\n",
      "Iteration 216, loss = 0.65627233\n",
      "Iteration 217, loss = 0.65606466\n",
      "Iteration 218, loss = 0.65585671\n",
      "Iteration 219, loss = 0.65564701\n",
      "Iteration 220, loss = 0.65543712\n",
      "Iteration 221, loss = 0.65522491\n",
      "Iteration 222, loss = 0.65501380\n",
      "Iteration 223, loss = 0.65480029\n",
      "Iteration 224, loss = 0.65458608\n",
      "Iteration 225, loss = 0.65437029\n",
      "Iteration 226, loss = 0.65415502\n",
      "Iteration 227, loss = 0.65393795\n",
      "Iteration 228, loss = 0.65371971\n",
      "Iteration 229, loss = 0.65350065\n",
      "Iteration 230, loss = 0.65328055\n",
      "Iteration 231, loss = 0.65306085\n",
      "Iteration 232, loss = 0.65283963\n",
      "Iteration 233, loss = 0.65261795\n",
      "Iteration 234, loss = 0.65239656\n",
      "Iteration 235, loss = 0.65217187\n",
      "Iteration 236, loss = 0.65194770\n",
      "Iteration 237, loss = 0.65172085\n",
      "Iteration 238, loss = 0.65149422\n",
      "Iteration 239, loss = 0.65126587\n",
      "Iteration 240, loss = 0.65103619\n",
      "Iteration 241, loss = 0.65080725\n",
      "Iteration 242, loss = 0.65057744\n",
      "Iteration 243, loss = 0.65034748\n",
      "Iteration 244, loss = 0.65011605\n",
      "Iteration 245, loss = 0.64988332\n",
      "Iteration 246, loss = 0.64965013\n",
      "Iteration 247, loss = 0.64941616\n",
      "Iteration 248, loss = 0.64918124\n",
      "Iteration 249, loss = 0.64894573\n",
      "Iteration 250, loss = 0.64870981\n",
      "Iteration 251, loss = 0.64847309\n",
      "Iteration 252, loss = 0.64823471\n",
      "Iteration 253, loss = 0.64799528\n",
      "Iteration 254, loss = 0.64775456\n",
      "Iteration 255, loss = 0.64751345\n",
      "Iteration 256, loss = 0.64727121\n",
      "Iteration 257, loss = 0.64702861\n",
      "Iteration 258, loss = 0.64678506\n",
      "Iteration 259, loss = 0.64654020\n",
      "Iteration 260, loss = 0.64629520\n",
      "Iteration 261, loss = 0.64604925\n",
      "Iteration 262, loss = 0.64580229\n",
      "Iteration 263, loss = 0.64555337\n",
      "Iteration 264, loss = 0.64530414\n",
      "Iteration 265, loss = 0.64505481\n",
      "Iteration 266, loss = 0.64480446\n",
      "Iteration 267, loss = 0.64455243\n",
      "Iteration 268, loss = 0.64429991\n",
      "Iteration 269, loss = 0.64404663\n",
      "Iteration 270, loss = 0.64379195\n",
      "Iteration 271, loss = 0.64353698\n",
      "Iteration 272, loss = 0.64328052\n",
      "Iteration 273, loss = 0.64302248\n",
      "Iteration 274, loss = 0.64276356\n",
      "Iteration 275, loss = 0.64250364\n",
      "Iteration 276, loss = 0.64224335\n",
      "Iteration 277, loss = 0.64198264\n",
      "Iteration 278, loss = 0.64171991\n",
      "Iteration 279, loss = 0.64145561\n",
      "Iteration 280, loss = 0.64119071\n",
      "Iteration 281, loss = 0.64092541\n",
      "Iteration 282, loss = 0.64065934\n",
      "Iteration 283, loss = 0.64039207\n",
      "Iteration 284, loss = 0.64012369\n",
      "Iteration 285, loss = 0.63985227\n",
      "Iteration 286, loss = 0.63958121\n",
      "Iteration 287, loss = 0.63930693\n",
      "Iteration 288, loss = 0.63903208\n",
      "Iteration 289, loss = 0.63875524\n",
      "Iteration 290, loss = 0.63847841\n",
      "Iteration 291, loss = 0.63820035\n",
      "Iteration 292, loss = 0.63791930\n",
      "Iteration 293, loss = 0.63763370\n",
      "Iteration 294, loss = 0.63734735\n",
      "Iteration 295, loss = 0.63706057\n",
      "Iteration 296, loss = 0.63677124\n",
      "Iteration 297, loss = 0.63647863\n",
      "Iteration 298, loss = 0.63618362\n",
      "Iteration 299, loss = 0.63588869\n",
      "Iteration 300, loss = 0.63559695\n",
      "Iteration 301, loss = 0.63530377\n",
      "Iteration 302, loss = 0.63500902\n",
      "Iteration 303, loss = 0.63471674\n",
      "Iteration 304, loss = 0.63442419\n",
      "Iteration 305, loss = 0.63413151\n",
      "Iteration 306, loss = 0.63383713\n",
      "Iteration 307, loss = 0.63354507\n",
      "Iteration 308, loss = 0.63325019\n",
      "Iteration 309, loss = 0.63295478\n",
      "Iteration 310, loss = 0.63266071\n",
      "Iteration 311, loss = 0.63236383\n",
      "Iteration 312, loss = 0.63206507\n",
      "Iteration 313, loss = 0.63176624\n",
      "Iteration 314, loss = 0.63146668\n",
      "Iteration 315, loss = 0.63116759\n",
      "Iteration 316, loss = 0.63086582\n",
      "Iteration 317, loss = 0.63056247\n",
      "Iteration 318, loss = 0.63025789\n",
      "Iteration 319, loss = 0.62995237\n",
      "Iteration 320, loss = 0.62964756\n",
      "Iteration 321, loss = 0.62933982\n",
      "Iteration 322, loss = 0.62903003\n",
      "Iteration 323, loss = 0.62872065\n",
      "Iteration 324, loss = 0.62841128\n",
      "Iteration 325, loss = 0.62809894\n",
      "Iteration 326, loss = 0.62778441\n",
      "Iteration 327, loss = 0.62747196\n",
      "Iteration 328, loss = 0.62715585\n",
      "Iteration 329, loss = 0.62683975\n",
      "Iteration 330, loss = 0.62652263\n",
      "Iteration 331, loss = 0.62620538\n",
      "Iteration 332, loss = 0.62588579\n",
      "Iteration 333, loss = 0.62556674\n",
      "Iteration 334, loss = 0.62524436\n",
      "Iteration 335, loss = 0.62492199\n",
      "Iteration 336, loss = 0.62460003\n",
      "Iteration 337, loss = 0.62427488\n",
      "Iteration 338, loss = 0.62394908\n",
      "Iteration 339, loss = 0.62362253\n",
      "Iteration 340, loss = 0.62329624\n",
      "Iteration 341, loss = 0.62296701\n",
      "Iteration 342, loss = 0.62263713\n",
      "Iteration 343, loss = 0.62230767\n",
      "Iteration 344, loss = 0.62197310\n",
      "Iteration 345, loss = 0.62163906\n",
      "Iteration 346, loss = 0.62130258\n",
      "Iteration 347, loss = 0.62096656\n",
      "Iteration 348, loss = 0.62062713\n",
      "Iteration 349, loss = 0.62028458\n",
      "Iteration 350, loss = 0.61994364\n",
      "Iteration 351, loss = 0.61960456\n",
      "Iteration 352, loss = 0.61926305\n",
      "Iteration 353, loss = 0.61891803\n",
      "Iteration 354, loss = 0.61857341\n",
      "Iteration 355, loss = 0.61822990\n",
      "Iteration 356, loss = 0.61788167\n",
      "Iteration 357, loss = 0.61753778\n",
      "Iteration 358, loss = 0.61718789\n",
      "Iteration 359, loss = 0.61683758\n",
      "Iteration 360, loss = 0.61648568\n",
      "Iteration 361, loss = 0.61613553\n",
      "Iteration 362, loss = 0.61578121\n",
      "Iteration 363, loss = 0.61542492\n",
      "Iteration 364, loss = 0.61507036\n",
      "Iteration 365, loss = 0.61471248\n",
      "Iteration 366, loss = 0.61435195\n",
      "Iteration 367, loss = 0.61399451\n",
      "Iteration 368, loss = 0.61363053\n",
      "Iteration 369, loss = 0.61326691\n",
      "Iteration 370, loss = 0.61291037\n",
      "Iteration 371, loss = 0.61254049\n",
      "Iteration 372, loss = 0.61217638\n",
      "Iteration 373, loss = 0.61180840\n",
      "Iteration 374, loss = 0.61144489\n",
      "Iteration 375, loss = 0.61107237\n",
      "Iteration 376, loss = 0.61070317\n",
      "Iteration 377, loss = 0.61033322\n",
      "Iteration 378, loss = 0.60995840\n",
      "Iteration 379, loss = 0.60958640\n",
      "Iteration 380, loss = 0.60920880\n",
      "Iteration 381, loss = 0.60883242\n",
      "Iteration 382, loss = 0.60845326\n",
      "Iteration 383, loss = 0.60807729\n",
      "Iteration 384, loss = 0.60769494\n",
      "Iteration 385, loss = 0.60731509\n",
      "Iteration 386, loss = 0.60692876\n",
      "Iteration 387, loss = 0.60654677\n",
      "Iteration 388, loss = 0.60615334\n",
      "Iteration 389, loss = 0.60575936\n",
      "Iteration 390, loss = 0.60535294\n",
      "Iteration 391, loss = 0.60494525\n",
      "Iteration 392, loss = 0.60452614\n",
      "Iteration 393, loss = 0.60411030\n",
      "Iteration 394, loss = 0.60369268\n",
      "Iteration 395, loss = 0.60326536\n",
      "Iteration 396, loss = 0.60283216\n",
      "Iteration 397, loss = 0.60238757\n",
      "Iteration 398, loss = 0.60194306\n",
      "Iteration 399, loss = 0.60148734\n",
      "Iteration 400, loss = 0.60105195\n",
      "Iteration 401, loss = 0.60061484\n",
      "Iteration 402, loss = 0.60017402\n",
      "Iteration 403, loss = 0.59973515\n",
      "Iteration 404, loss = 0.59930263\n",
      "Iteration 405, loss = 0.59886010\n",
      "Iteration 406, loss = 0.59842131\n",
      "Iteration 407, loss = 0.59796830\n",
      "Iteration 408, loss = 0.59752687\n",
      "Iteration 409, loss = 0.59707316\n",
      "Iteration 410, loss = 0.59662845\n",
      "Iteration 411, loss = 0.59618560\n",
      "Iteration 412, loss = 0.59575501\n",
      "Iteration 413, loss = 0.59532494\n",
      "Iteration 414, loss = 0.59489888\n",
      "Iteration 415, loss = 0.59447213\n",
      "Iteration 416, loss = 0.59405507\n",
      "Iteration 417, loss = 0.59363322\n",
      "Iteration 418, loss = 0.59321520\n",
      "Iteration 419, loss = 0.59279363\n",
      "Iteration 420, loss = 0.59237327\n",
      "Iteration 421, loss = 0.59194985\n",
      "Iteration 422, loss = 0.59152833\n",
      "Iteration 423, loss = 0.59110193\n",
      "Iteration 424, loss = 0.59067570\n",
      "Iteration 425, loss = 0.59024873\n",
      "Iteration 426, loss = 0.58982007\n",
      "Iteration 427, loss = 0.58939282\n",
      "Iteration 428, loss = 0.58896147\n",
      "Iteration 429, loss = 0.58852942\n",
      "Iteration 430, loss = 0.58809728\n",
      "Iteration 431, loss = 0.58766407\n",
      "Iteration 432, loss = 0.58722884\n",
      "Iteration 433, loss = 0.58679239\n",
      "Iteration 434, loss = 0.58635649\n",
      "Iteration 435, loss = 0.58591859\n",
      "Iteration 436, loss = 0.58547688\n",
      "Iteration 437, loss = 0.58503564\n",
      "Iteration 438, loss = 0.58459382\n",
      "Iteration 439, loss = 0.58415156\n",
      "Iteration 440, loss = 0.58370757\n",
      "Iteration 441, loss = 0.58326158\n",
      "Iteration 442, loss = 0.58281597\n",
      "Iteration 443, loss = 0.58237288\n",
      "Iteration 444, loss = 0.58192443\n",
      "Iteration 445, loss = 0.58147507\n",
      "Iteration 446, loss = 0.58102742\n",
      "Iteration 447, loss = 0.58057754\n",
      "Iteration 448, loss = 0.58012823\n",
      "Iteration 449, loss = 0.57967666\n",
      "Iteration 450, loss = 0.57922281\n",
      "Iteration 451, loss = 0.57876841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 452, loss = 0.57831600\n",
      "Iteration 453, loss = 0.57785893\n",
      "Iteration 454, loss = 0.57740283\n",
      "Iteration 455, loss = 0.57694543\n",
      "Iteration 456, loss = 0.57648804\n",
      "Iteration 457, loss = 0.57602664\n",
      "Iteration 458, loss = 0.57556530\n",
      "Iteration 459, loss = 0.57510259\n",
      "Iteration 460, loss = 0.57463837\n",
      "Iteration 461, loss = 0.57417339\n",
      "Iteration 462, loss = 0.57370743\n",
      "Iteration 463, loss = 0.57324287\n",
      "Iteration 464, loss = 0.57277656\n",
      "Iteration 465, loss = 0.57230500\n",
      "Iteration 466, loss = 0.57183882\n",
      "Iteration 467, loss = 0.57136518\n",
      "Iteration 468, loss = 0.57089137\n",
      "Iteration 469, loss = 0.57042037\n",
      "Iteration 470, loss = 0.56994859\n",
      "Iteration 471, loss = 0.56947441\n",
      "Iteration 472, loss = 0.56899846\n",
      "Iteration 473, loss = 0.56852202\n",
      "Iteration 474, loss = 0.56804453\n",
      "Iteration 475, loss = 0.56756836\n",
      "Iteration 476, loss = 0.56709018\n",
      "Iteration 477, loss = 0.56660983\n",
      "Iteration 478, loss = 0.56612751\n",
      "Iteration 479, loss = 0.56564331\n",
      "Iteration 480, loss = 0.56516185\n",
      "Iteration 481, loss = 0.56467833\n",
      "Iteration 482, loss = 0.56419195\n",
      "Iteration 483, loss = 0.56370471\n",
      "Iteration 484, loss = 0.56321779\n",
      "Iteration 485, loss = 0.56273236\n",
      "Iteration 486, loss = 0.56224129\n",
      "Iteration 487, loss = 0.56175139\n",
      "Iteration 488, loss = 0.56126015\n",
      "Iteration 489, loss = 0.56076376\n",
      "Iteration 490, loss = 0.56026865\n",
      "Iteration 491, loss = 0.55977301\n",
      "Iteration 492, loss = 0.55926949\n",
      "Iteration 493, loss = 0.55877310\n",
      "Iteration 494, loss = 0.55827595\n",
      "Iteration 495, loss = 0.55777875\n",
      "Iteration 496, loss = 0.55727892\n",
      "Iteration 497, loss = 0.55677915\n",
      "Iteration 498, loss = 0.55628021\n",
      "Iteration 499, loss = 0.55577825\n",
      "Iteration 500, loss = 0.55527098\n",
      "Iteration 501, loss = 0.55476818\n",
      "Iteration 502, loss = 0.55426420\n",
      "Iteration 503, loss = 0.55375737\n",
      "Iteration 504, loss = 0.55325159\n",
      "Iteration 505, loss = 0.55274503\n",
      "Iteration 506, loss = 0.55223922\n",
      "Iteration 507, loss = 0.55173124\n",
      "Iteration 508, loss = 0.55122360\n",
      "Iteration 509, loss = 0.55071253\n",
      "Iteration 510, loss = 0.55020025\n",
      "Iteration 511, loss = 0.54968726\n",
      "Iteration 512, loss = 0.54917595\n",
      "Iteration 513, loss = 0.54866326\n",
      "Iteration 514, loss = 0.54815124\n",
      "Iteration 515, loss = 0.54763477\n",
      "Iteration 516, loss = 0.54712253\n",
      "Iteration 517, loss = 0.54660580\n",
      "Iteration 518, loss = 0.54609388\n",
      "Iteration 519, loss = 0.54557839\n",
      "Iteration 520, loss = 0.54505966\n",
      "Iteration 521, loss = 0.54454879\n",
      "Iteration 522, loss = 0.54403164\n",
      "Iteration 523, loss = 0.54351554\n",
      "Iteration 524, loss = 0.54300086\n",
      "Iteration 525, loss = 0.54248219\n",
      "Iteration 526, loss = 0.54196579\n",
      "Iteration 527, loss = 0.54144709\n",
      "Iteration 528, loss = 0.54093030\n",
      "Iteration 529, loss = 0.54041037\n",
      "Iteration 530, loss = 0.53989065\n",
      "Iteration 531, loss = 0.53937198\n",
      "Iteration 532, loss = 0.53885106\n",
      "Iteration 533, loss = 0.53832965\n",
      "Iteration 534, loss = 0.53780688\n",
      "Iteration 535, loss = 0.53728538\n",
      "Iteration 536, loss = 0.53676289\n",
      "Iteration 537, loss = 0.53623812\n",
      "Iteration 538, loss = 0.53571369\n",
      "Iteration 539, loss = 0.53518930\n",
      "Iteration 540, loss = 0.53466480\n",
      "Iteration 541, loss = 0.53414105\n",
      "Iteration 542, loss = 0.53361130\n",
      "Iteration 543, loss = 0.53308879\n",
      "Iteration 544, loss = 0.53255931\n",
      "Iteration 545, loss = 0.53203587\n",
      "Iteration 546, loss = 0.53150372\n",
      "Iteration 547, loss = 0.53098099\n",
      "Iteration 548, loss = 0.53045193\n",
      "Iteration 549, loss = 0.52992196\n",
      "Iteration 550, loss = 0.52939240\n",
      "Iteration 551, loss = 0.52886372\n",
      "Iteration 552, loss = 0.52833273\n",
      "Iteration 553, loss = 0.52780411\n",
      "Iteration 554, loss = 0.52727201\n",
      "Iteration 555, loss = 0.52674153\n",
      "Iteration 556, loss = 0.52621023\n",
      "Iteration 557, loss = 0.52567838\n",
      "Iteration 558, loss = 0.52514517\n",
      "Iteration 559, loss = 0.52460959\n",
      "Iteration 560, loss = 0.52408041\n",
      "Iteration 561, loss = 0.52354245\n",
      "Iteration 562, loss = 0.52301217\n",
      "Iteration 563, loss = 0.52247558\n",
      "Iteration 564, loss = 0.52193846\n",
      "Iteration 565, loss = 0.52140881\n",
      "Iteration 566, loss = 0.52086767\n",
      "Iteration 567, loss = 0.52033444\n",
      "Iteration 568, loss = 0.51979820\n",
      "Iteration 569, loss = 0.51926177\n",
      "Iteration 570, loss = 0.51872811\n",
      "Iteration 571, loss = 0.51818718\n",
      "Iteration 572, loss = 0.51765145\n",
      "Iteration 573, loss = 0.51711472\n",
      "Iteration 574, loss = 0.51657407\n",
      "Iteration 575, loss = 0.51603925\n",
      "Iteration 576, loss = 0.51550160\n",
      "Iteration 577, loss = 0.51496134\n",
      "Iteration 578, loss = 0.51442064\n",
      "Iteration 579, loss = 0.51388284\n",
      "Iteration 580, loss = 0.51334296\n",
      "Iteration 581, loss = 0.51280201\n",
      "Iteration 582, loss = 0.51226330\n",
      "Iteration 583, loss = 0.51172160\n",
      "Iteration 584, loss = 0.51117699\n",
      "Iteration 585, loss = 0.51063797\n",
      "Iteration 586, loss = 0.51009588\n",
      "Iteration 587, loss = 0.50955017\n",
      "Iteration 588, loss = 0.50900989\n",
      "Iteration 589, loss = 0.50846735\n",
      "Iteration 590, loss = 0.50792153\n",
      "Iteration 591, loss = 0.50737905\n",
      "Iteration 592, loss = 0.50683347\n",
      "Iteration 593, loss = 0.50628014\n",
      "Iteration 594, loss = 0.50573201\n",
      "Iteration 595, loss = 0.50518670\n",
      "Iteration 596, loss = 0.50463749\n",
      "Iteration 597, loss = 0.50408993\n",
      "Iteration 598, loss = 0.50354487\n",
      "Iteration 599, loss = 0.50299695\n",
      "Iteration 600, loss = 0.50244778\n",
      "Iteration 601, loss = 0.50190269\n",
      "Iteration 602, loss = 0.50135993\n",
      "Iteration 603, loss = 0.50081097\n",
      "Iteration 604, loss = 0.50026223\n",
      "Iteration 605, loss = 0.49971142\n",
      "Iteration 606, loss = 0.49915989\n",
      "Iteration 607, loss = 0.49861050\n",
      "Iteration 608, loss = 0.49806318\n",
      "Iteration 609, loss = 0.49751138\n",
      "Iteration 610, loss = 0.49696156\n",
      "Iteration 611, loss = 0.49641562\n",
      "Iteration 612, loss = 0.49587242\n",
      "Iteration 613, loss = 0.49532539\n",
      "Iteration 614, loss = 0.49477178\n",
      "Iteration 615, loss = 0.49422806\n",
      "Iteration 616, loss = 0.49368285\n",
      "Iteration 617, loss = 0.49313855\n",
      "Iteration 618, loss = 0.49259113\n",
      "Iteration 619, loss = 0.49204508\n",
      "Iteration 620, loss = 0.49149802\n",
      "Iteration 621, loss = 0.49095158\n",
      "Iteration 622, loss = 0.49040404\n",
      "Iteration 623, loss = 0.48985978\n",
      "Iteration 624, loss = 0.48931300\n",
      "Iteration 625, loss = 0.48877079\n",
      "Iteration 626, loss = 0.48822103\n",
      "Iteration 627, loss = 0.48767585\n",
      "Iteration 628, loss = 0.48713087\n",
      "Iteration 629, loss = 0.48658701\n",
      "Iteration 630, loss = 0.48603916\n",
      "Iteration 631, loss = 0.48549164\n",
      "Iteration 632, loss = 0.48495059\n",
      "Iteration 633, loss = 0.48440359\n",
      "Iteration 634, loss = 0.48385605\n",
      "Iteration 635, loss = 0.48331230\n",
      "Iteration 636, loss = 0.48276673\n",
      "Iteration 637, loss = 0.48222228\n",
      "Iteration 638, loss = 0.48166999\n",
      "Iteration 639, loss = 0.48111876\n",
      "Iteration 640, loss = 0.48056503\n",
      "Iteration 641, loss = 0.48001777\n",
      "Iteration 642, loss = 0.47947187\n",
      "Iteration 643, loss = 0.47892633\n",
      "Iteration 644, loss = 0.47838465\n",
      "Iteration 645, loss = 0.47783582\n",
      "Iteration 646, loss = 0.47729502\n",
      "Iteration 647, loss = 0.47675133\n",
      "Iteration 648, loss = 0.47621089\n",
      "Iteration 649, loss = 0.47566459\n",
      "Iteration 650, loss = 0.47512040\n",
      "Iteration 651, loss = 0.47458170\n",
      "Iteration 652, loss = 0.47403778\n",
      "Iteration 653, loss = 0.47349450\n",
      "Iteration 654, loss = 0.47294707\n",
      "Iteration 655, loss = 0.47240972\n",
      "Iteration 656, loss = 0.47186583\n",
      "Iteration 657, loss = 0.47132262\n",
      "Iteration 658, loss = 0.47078116\n",
      "Iteration 659, loss = 0.47023754\n",
      "Iteration 660, loss = 0.46969948\n",
      "Iteration 661, loss = 0.46915927\n",
      "Iteration 662, loss = 0.46861586\n",
      "Iteration 663, loss = 0.46808001\n",
      "Iteration 664, loss = 0.46753904\n",
      "Iteration 665, loss = 0.46700105\n",
      "Iteration 666, loss = 0.46645744\n",
      "Iteration 667, loss = 0.46592038\n",
      "Iteration 668, loss = 0.46538286\n",
      "Iteration 669, loss = 0.46484268\n",
      "Iteration 670, loss = 0.46430648\n",
      "Iteration 671, loss = 0.46376584\n",
      "Iteration 672, loss = 0.46322837\n",
      "Iteration 673, loss = 0.46269142\n",
      "Iteration 674, loss = 0.46215264\n",
      "Iteration 675, loss = 0.46161808\n",
      "Iteration 676, loss = 0.46107744\n",
      "Iteration 677, loss = 0.46053823\n",
      "Iteration 678, loss = 0.46000683\n",
      "Iteration 679, loss = 0.45947095\n",
      "Iteration 680, loss = 0.45893112\n",
      "Iteration 681, loss = 0.45839303\n",
      "Iteration 682, loss = 0.45786578\n",
      "Iteration 683, loss = 0.45732603\n",
      "Iteration 684, loss = 0.45678977\n",
      "Iteration 685, loss = 0.45625432\n",
      "Iteration 686, loss = 0.45571643\n",
      "Iteration 687, loss = 0.45518751\n",
      "Iteration 688, loss = 0.45465213\n",
      "Iteration 689, loss = 0.45411459\n",
      "Iteration 690, loss = 0.45358270\n",
      "Iteration 691, loss = 0.45305009\n",
      "Iteration 692, loss = 0.45251377\n",
      "Iteration 693, loss = 0.45197766\n",
      "Iteration 694, loss = 0.45144939\n",
      "Iteration 695, loss = 0.45091231\n",
      "Iteration 696, loss = 0.45037778\n",
      "Iteration 697, loss = 0.44984669\n",
      "Iteration 698, loss = 0.44931016\n",
      "Iteration 699, loss = 0.44878523\n",
      "Iteration 700, loss = 0.44824980\n",
      "Iteration 701, loss = 0.44771190\n",
      "Iteration 702, loss = 0.44718311\n",
      "Iteration 703, loss = 0.44664793\n",
      "Iteration 704, loss = 0.44611055\n",
      "Iteration 705, loss = 0.44557685\n",
      "Iteration 706, loss = 0.44504668\n",
      "Iteration 707, loss = 0.44451110\n",
      "Iteration 708, loss = 0.44397669\n",
      "Iteration 709, loss = 0.44345137\n",
      "Iteration 710, loss = 0.44292169\n",
      "Iteration 711, loss = 0.44239731\n",
      "Iteration 712, loss = 0.44186840\n",
      "Iteration 713, loss = 0.44134327\n",
      "Iteration 714, loss = 0.44082231\n",
      "Iteration 715, loss = 0.44029810\n",
      "Iteration 716, loss = 0.43977111\n",
      "Iteration 717, loss = 0.43924896\n",
      "Iteration 718, loss = 0.43872606\n",
      "Iteration 719, loss = 0.43821374\n",
      "Iteration 720, loss = 0.43768685\n",
      "Iteration 721, loss = 0.43716807\n",
      "Iteration 722, loss = 0.43664416\n",
      "Iteration 723, loss = 0.43612967\n",
      "Iteration 724, loss = 0.43561554\n",
      "Iteration 725, loss = 0.43509300\n",
      "Iteration 726, loss = 0.43457592\n",
      "Iteration 727, loss = 0.43406243\n",
      "Iteration 728, loss = 0.43354759\n",
      "Iteration 729, loss = 0.43303111\n",
      "Iteration 730, loss = 0.43251811\n",
      "Iteration 731, loss = 0.43199909\n",
      "Iteration 732, loss = 0.43149053\n",
      "Iteration 733, loss = 0.43098069\n",
      "Iteration 734, loss = 0.43046134\n",
      "Iteration 735, loss = 0.42995035\n",
      "Iteration 736, loss = 0.42943681\n",
      "Iteration 737, loss = 0.42892821\n",
      "Iteration 738, loss = 0.42841653\n",
      "Iteration 739, loss = 0.42790211\n",
      "Iteration 740, loss = 0.42739035\n",
      "Iteration 741, loss = 0.42688819\n",
      "Iteration 742, loss = 0.42637806\n",
      "Iteration 743, loss = 0.42586912\n",
      "Iteration 744, loss = 0.42535774\n",
      "Iteration 745, loss = 0.42485173\n",
      "Iteration 746, loss = 0.42435350\n",
      "Iteration 747, loss = 0.42384314\n",
      "Iteration 748, loss = 0.42333592\n",
      "Iteration 749, loss = 0.42283144\n",
      "Iteration 750, loss = 0.42232812\n",
      "Iteration 751, loss = 0.42182467\n",
      "Iteration 752, loss = 0.42132795\n",
      "Iteration 753, loss = 0.42082034\n",
      "Iteration 754, loss = 0.42032025\n",
      "Iteration 755, loss = 0.41982439\n",
      "Iteration 756, loss = 0.41933063\n",
      "Iteration 757, loss = 0.41882643\n",
      "Iteration 758, loss = 0.41832692\n",
      "Iteration 759, loss = 0.41782620\n",
      "Iteration 760, loss = 0.41734239\n",
      "Iteration 761, loss = 0.41683885\n",
      "Iteration 762, loss = 0.41634244\n",
      "Iteration 763, loss = 0.41584367\n",
      "Iteration 764, loss = 0.41535530\n",
      "Iteration 765, loss = 0.41486574\n",
      "Iteration 766, loss = 0.41436583\n",
      "Iteration 767, loss = 0.41387616\n",
      "Iteration 768, loss = 0.41338229\n",
      "Iteration 769, loss = 0.41289825\n",
      "Iteration 770, loss = 0.41240675\n",
      "Iteration 771, loss = 0.41191671\n",
      "Iteration 772, loss = 0.41142749\n",
      "Iteration 773, loss = 0.41093805\n",
      "Iteration 774, loss = 0.41045349\n",
      "Iteration 775, loss = 0.40996786\n",
      "Iteration 776, loss = 0.40948532\n",
      "Iteration 777, loss = 0.40899295\n",
      "Iteration 778, loss = 0.40851006\n",
      "Iteration 779, loss = 0.40803350\n",
      "Iteration 780, loss = 0.40755362\n",
      "Iteration 781, loss = 0.40706753\n",
      "Iteration 782, loss = 0.40658299\n",
      "Iteration 783, loss = 0.40609962\n",
      "Iteration 784, loss = 0.40562404\n",
      "Iteration 785, loss = 0.40514941\n",
      "Iteration 786, loss = 0.40466920\n",
      "Iteration 787, loss = 0.40418823\n",
      "Iteration 788, loss = 0.40371280\n",
      "Iteration 789, loss = 0.40324079\n",
      "Iteration 790, loss = 0.40276694\n",
      "Iteration 791, loss = 0.40229309\n",
      "Iteration 792, loss = 0.40181250\n",
      "Iteration 793, loss = 0.40134192\n",
      "Iteration 794, loss = 0.40087166\n",
      "Iteration 795, loss = 0.40040223\n",
      "Iteration 796, loss = 0.39992713\n",
      "Iteration 797, loss = 0.39945583\n",
      "Iteration 798, loss = 0.39898695\n",
      "Iteration 799, loss = 0.39852193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 800, loss = 0.39806336\n",
      "Iteration 801, loss = 0.39759070\n",
      "Iteration 802, loss = 0.39712059\n",
      "Iteration 803, loss = 0.39666298\n",
      "Iteration 804, loss = 0.39619150\n",
      "Iteration 805, loss = 0.39573049\n",
      "Iteration 806, loss = 0.39526898\n",
      "Iteration 807, loss = 0.39480927\n",
      "Iteration 808, loss = 0.39434119\n",
      "Iteration 809, loss = 0.39389516\n",
      "Iteration 810, loss = 0.39343354\n",
      "Iteration 811, loss = 0.39297287\n",
      "Iteration 812, loss = 0.39250791\n",
      "Iteration 813, loss = 0.39205779\n",
      "Iteration 814, loss = 0.39159902\n",
      "Iteration 815, loss = 0.39114205\n",
      "Iteration 816, loss = 0.39068943\n",
      "Iteration 817, loss = 0.39023284\n",
      "Iteration 818, loss = 0.38977909\n",
      "Iteration 819, loss = 0.38932907\n",
      "Iteration 820, loss = 0.38887696\n",
      "Iteration 821, loss = 0.38842331\n",
      "Iteration 822, loss = 0.38797327\n",
      "Iteration 823, loss = 0.38752325\n",
      "Iteration 824, loss = 0.38708184\n",
      "Iteration 825, loss = 0.38662825\n",
      "Iteration 826, loss = 0.38618374\n",
      "Iteration 827, loss = 0.38573806\n",
      "Iteration 828, loss = 0.38529333\n",
      "Iteration 829, loss = 0.38485141\n",
      "Iteration 830, loss = 0.38440608\n",
      "Iteration 831, loss = 0.38396462\n",
      "Iteration 832, loss = 0.38351694\n",
      "Iteration 833, loss = 0.38308302\n",
      "Iteration 834, loss = 0.38264895\n",
      "Iteration 835, loss = 0.38220591\n",
      "Iteration 836, loss = 0.38176492\n",
      "Iteration 837, loss = 0.38132414\n",
      "Iteration 838, loss = 0.38089636\n",
      "Iteration 839, loss = 0.38045717\n",
      "Iteration 840, loss = 0.38001171\n",
      "Iteration 841, loss = 0.37957555\n",
      "Iteration 842, loss = 0.37914113\n",
      "Iteration 843, loss = 0.37870172\n",
      "Iteration 844, loss = 0.37826542\n",
      "Iteration 845, loss = 0.37782781\n",
      "Iteration 846, loss = 0.37739486\n",
      "Iteration 847, loss = 0.37696834\n",
      "Iteration 848, loss = 0.37653478\n",
      "Iteration 849, loss = 0.37610090\n",
      "Iteration 850, loss = 0.37567356\n",
      "Iteration 851, loss = 0.37524845\n",
      "Iteration 852, loss = 0.37481574\n",
      "Iteration 853, loss = 0.37438662\n",
      "Iteration 854, loss = 0.37396769\n",
      "Iteration 855, loss = 0.37354264\n",
      "Iteration 856, loss = 0.37311824\n",
      "Iteration 857, loss = 0.37267424\n",
      "Iteration 858, loss = 0.37224666\n",
      "Iteration 859, loss = 0.37182267\n",
      "Iteration 860, loss = 0.37138692\n",
      "Iteration 861, loss = 0.37096377\n",
      "Iteration 862, loss = 0.37053700\n",
      "Iteration 863, loss = 0.37012087\n",
      "Iteration 864, loss = 0.36970342\n",
      "Iteration 865, loss = 0.36927688\n",
      "Iteration 866, loss = 0.36885852\n",
      "Iteration 867, loss = 0.36844114\n",
      "Iteration 868, loss = 0.36802946\n",
      "Iteration 869, loss = 0.36761744\n",
      "Iteration 870, loss = 0.36719742\n",
      "Iteration 871, loss = 0.36678406\n",
      "Iteration 872, loss = 0.36637673\n",
      "Iteration 873, loss = 0.36597537\n",
      "Iteration 874, loss = 0.36555155\n",
      "Iteration 875, loss = 0.36514086\n",
      "Iteration 876, loss = 0.36472302\n",
      "Iteration 877, loss = 0.36431042\n",
      "Iteration 878, loss = 0.36390844\n",
      "Iteration 879, loss = 0.36349909\n",
      "Iteration 880, loss = 0.36308544\n",
      "Iteration 881, loss = 0.36267477\n",
      "Iteration 882, loss = 0.36226695\n",
      "Iteration 883, loss = 0.36186362\n",
      "Iteration 884, loss = 0.36146584\n",
      "Iteration 885, loss = 0.36105189\n",
      "Iteration 886, loss = 0.36064625\n",
      "Iteration 887, loss = 0.36024660\n",
      "Iteration 888, loss = 0.35984894\n",
      "Iteration 889, loss = 0.35944130\n",
      "Iteration 890, loss = 0.35903567\n",
      "Iteration 891, loss = 0.35863583\n",
      "Iteration 892, loss = 0.35823228\n",
      "Iteration 893, loss = 0.35783917\n",
      "Iteration 894, loss = 0.35742888\n",
      "Iteration 895, loss = 0.35703688\n",
      "Iteration 896, loss = 0.35662788\n",
      "Iteration 897, loss = 0.35623422\n",
      "Iteration 898, loss = 0.35585223\n",
      "Iteration 899, loss = 0.35545069\n",
      "Iteration 900, loss = 0.35505773\n",
      "Iteration 901, loss = 0.35466009\n",
      "Iteration 902, loss = 0.35427157\n",
      "Iteration 903, loss = 0.35388843\n",
      "Iteration 904, loss = 0.35349547\n",
      "Iteration 905, loss = 0.35311177\n",
      "Iteration 906, loss = 0.35271336\n",
      "Iteration 907, loss = 0.35233808\n",
      "Iteration 908, loss = 0.35195622\n",
      "Iteration 909, loss = 0.35156636\n",
      "Iteration 910, loss = 0.35118126\n",
      "Iteration 911, loss = 0.35079342\n",
      "Iteration 912, loss = 0.35041659\n",
      "Iteration 913, loss = 0.35004060\n",
      "Iteration 914, loss = 0.34966609\n",
      "Iteration 915, loss = 0.34928828\n",
      "Iteration 916, loss = 0.34890294\n",
      "Iteration 917, loss = 0.34852677\n",
      "Iteration 918, loss = 0.34815772\n",
      "Iteration 919, loss = 0.34777995\n",
      "Iteration 920, loss = 0.34740142\n",
      "Iteration 921, loss = 0.34703313\n",
      "Iteration 922, loss = 0.34665792\n",
      "Iteration 923, loss = 0.34628703\n",
      "Iteration 924, loss = 0.34592650\n",
      "Iteration 925, loss = 0.34555455\n",
      "Iteration 926, loss = 0.34519137\n",
      "Iteration 927, loss = 0.34481161\n",
      "Iteration 928, loss = 0.34445142\n",
      "Iteration 929, loss = 0.34408914\n",
      "Iteration 930, loss = 0.34372466\n",
      "Iteration 931, loss = 0.34335876\n",
      "Iteration 932, loss = 0.34298737\n",
      "Iteration 933, loss = 0.34263074\n",
      "Iteration 934, loss = 0.34226320\n",
      "Iteration 935, loss = 0.34191113\n",
      "Iteration 936, loss = 0.34155536\n",
      "Iteration 937, loss = 0.34118847\n",
      "Iteration 938, loss = 0.34083460\n",
      "Iteration 939, loss = 0.34046724\n",
      "Iteration 940, loss = 0.34011525\n",
      "Iteration 941, loss = 0.33976288\n",
      "Iteration 942, loss = 0.33940839\n",
      "Iteration 943, loss = 0.33905327\n",
      "Iteration 944, loss = 0.33869821\n",
      "Iteration 945, loss = 0.33834404\n",
      "Iteration 946, loss = 0.33798271\n",
      "Iteration 947, loss = 0.33763475\n",
      "Iteration 948, loss = 0.33727877\n",
      "Iteration 949, loss = 0.33692014\n",
      "Iteration 950, loss = 0.33656306\n",
      "Iteration 951, loss = 0.33620837\n",
      "Iteration 952, loss = 0.33586171\n",
      "Iteration 953, loss = 0.33551918\n",
      "Iteration 954, loss = 0.33515743\n",
      "Iteration 955, loss = 0.33480744\n",
      "Iteration 956, loss = 0.33446786\n",
      "Iteration 957, loss = 0.33411039\n",
      "Iteration 958, loss = 0.33376448\n",
      "Iteration 959, loss = 0.33342441\n",
      "Iteration 960, loss = 0.33307895\n",
      "Iteration 961, loss = 0.33273220\n",
      "Iteration 962, loss = 0.33239021\n",
      "Iteration 963, loss = 0.33205362\n",
      "Iteration 964, loss = 0.33171277\n",
      "Iteration 965, loss = 0.33137514\n",
      "Iteration 966, loss = 0.33102374\n",
      "Iteration 967, loss = 0.33068928\n",
      "Iteration 968, loss = 0.33034544\n",
      "Iteration 969, loss = 0.33000082\n",
      "Iteration 970, loss = 0.32967245\n",
      "Iteration 971, loss = 0.32933929\n",
      "Iteration 972, loss = 0.32899851\n",
      "Iteration 973, loss = 0.32866937\n",
      "Iteration 974, loss = 0.32832162\n",
      "Iteration 975, loss = 0.32801229\n",
      "Iteration 976, loss = 0.32767150\n",
      "Iteration 977, loss = 0.32733172\n",
      "Iteration 978, loss = 0.32700285\n",
      "Iteration 979, loss = 0.32666791\n",
      "Iteration 980, loss = 0.32634637\n",
      "Iteration 981, loss = 0.32602813\n",
      "Iteration 982, loss = 0.32568643\n",
      "Iteration 983, loss = 0.32536233\n",
      "Iteration 984, loss = 0.32504193\n",
      "Iteration 985, loss = 0.32470785\n",
      "Iteration 986, loss = 0.32438845\n",
      "Iteration 987, loss = 0.32406069\n",
      "Iteration 988, loss = 0.32373896\n",
      "Iteration 989, loss = 0.32341623\n",
      "Iteration 990, loss = 0.32309452\n",
      "Iteration 991, loss = 0.32278510\n",
      "Iteration 992, loss = 0.32245664\n",
      "Iteration 993, loss = 0.32214337\n",
      "Iteration 994, loss = 0.32180890\n",
      "Iteration 995, loss = 0.32149844\n",
      "Iteration 996, loss = 0.32117796\n",
      "Iteration 997, loss = 0.32087293\n",
      "Iteration 998, loss = 0.32055552\n",
      "Iteration 999, loss = 0.32024473\n",
      "Iteration 1000, loss = 0.31992282\n",
      "Iteration 1001, loss = 0.31961299\n",
      "Iteration 1002, loss = 0.31929687\n",
      "Iteration 1003, loss = 0.31900778\n",
      "Iteration 1004, loss = 0.31867118\n",
      "Iteration 1005, loss = 0.31837659\n",
      "Iteration 1006, loss = 0.31805390\n",
      "Iteration 1007, loss = 0.31774217\n",
      "Iteration 1008, loss = 0.31744306\n",
      "Iteration 1009, loss = 0.31714398\n",
      "Iteration 1010, loss = 0.31682434\n",
      "Iteration 1011, loss = 0.31652534\n",
      "Iteration 1012, loss = 0.31621677\n",
      "Iteration 1013, loss = 0.31591221\n",
      "Iteration 1014, loss = 0.31560687\n",
      "Iteration 1015, loss = 0.31531233\n",
      "Iteration 1016, loss = 0.31500497\n",
      "Iteration 1017, loss = 0.31470149\n",
      "Iteration 1018, loss = 0.31440511\n",
      "Iteration 1019, loss = 0.31409324\n",
      "Iteration 1020, loss = 0.31382050\n",
      "Iteration 1021, loss = 0.31351312\n",
      "Iteration 1022, loss = 0.31321046\n",
      "Iteration 1023, loss = 0.31290637\n",
      "Iteration 1024, loss = 0.31260308\n",
      "Iteration 1025, loss = 0.31233341\n",
      "Iteration 1026, loss = 0.31203216\n",
      "Iteration 1027, loss = 0.31172837\n",
      "Iteration 1028, loss = 0.31144423\n",
      "Iteration 1029, loss = 0.31116594\n",
      "Iteration 1030, loss = 0.31089214\n",
      "Iteration 1031, loss = 0.31056531\n",
      "Iteration 1032, loss = 0.31028827\n",
      "Iteration 1033, loss = 0.30999527\n",
      "Iteration 1034, loss = 0.30973406\n",
      "Iteration 1035, loss = 0.30942071\n",
      "Iteration 1036, loss = 0.30913809\n",
      "Iteration 1037, loss = 0.30885697\n",
      "Iteration 1038, loss = 0.30858262\n",
      "Iteration 1039, loss = 0.30829849\n",
      "Iteration 1040, loss = 0.30799528\n",
      "Iteration 1041, loss = 0.30772806\n",
      "Iteration 1042, loss = 0.30745665\n",
      "Iteration 1043, loss = 0.30716332\n",
      "Iteration 1044, loss = 0.30687281\n",
      "Iteration 1045, loss = 0.30660616\n",
      "Iteration 1046, loss = 0.30632082\n",
      "Iteration 1047, loss = 0.30604131\n",
      "Iteration 1048, loss = 0.30578650\n",
      "Iteration 1049, loss = 0.30549486\n",
      "Iteration 1050, loss = 0.30521030\n",
      "Iteration 1051, loss = 0.30495263\n",
      "Iteration 1052, loss = 0.30468373\n",
      "Iteration 1053, loss = 0.30439929\n",
      "Iteration 1054, loss = 0.30411166\n",
      "Iteration 1055, loss = 0.30384665\n",
      "Iteration 1056, loss = 0.30356425\n",
      "Iteration 1057, loss = 0.30329971\n",
      "Iteration 1058, loss = 0.30305044\n",
      "Iteration 1059, loss = 0.30276841\n",
      "Iteration 1060, loss = 0.30247794\n",
      "Iteration 1061, loss = 0.30223417\n",
      "Iteration 1062, loss = 0.30194148\n",
      "Iteration 1063, loss = 0.30167195\n",
      "Iteration 1064, loss = 0.30140977\n",
      "Iteration 1065, loss = 0.30116482\n",
      "Iteration 1066, loss = 0.30088251\n",
      "Iteration 1067, loss = 0.30062149\n",
      "Iteration 1068, loss = 0.30036338\n",
      "Iteration 1069, loss = 0.30008035\n",
      "Iteration 1070, loss = 0.29982983\n",
      "Iteration 1071, loss = 0.29958377\n",
      "Iteration 1072, loss = 0.29929900\n",
      "Iteration 1073, loss = 0.29902936\n",
      "Iteration 1074, loss = 0.29880989\n",
      "Iteration 1075, loss = 0.29851565\n",
      "Iteration 1076, loss = 0.29826328\n",
      "Iteration 1077, loss = 0.29799268\n",
      "Iteration 1078, loss = 0.29773394\n",
      "Iteration 1079, loss = 0.29747717\n",
      "Iteration 1080, loss = 0.29722680\n",
      "Iteration 1081, loss = 0.29696109\n",
      "Iteration 1082, loss = 0.29672650\n",
      "Iteration 1083, loss = 0.29644691\n",
      "Iteration 1084, loss = 0.29621796\n",
      "Iteration 1085, loss = 0.29593741\n",
      "Iteration 1086, loss = 0.29570683\n",
      "Iteration 1087, loss = 0.29544801\n",
      "Iteration 1088, loss = 0.29518237\n",
      "Iteration 1089, loss = 0.29494530\n",
      "Iteration 1090, loss = 0.29469297\n",
      "Iteration 1091, loss = 0.29441863\n",
      "Iteration 1092, loss = 0.29419293\n",
      "Iteration 1093, loss = 0.29393882\n",
      "Iteration 1094, loss = 0.29368760\n",
      "Iteration 1095, loss = 0.29344795\n",
      "Iteration 1096, loss = 0.29320744\n",
      "Iteration 1097, loss = 0.29293321\n",
      "Iteration 1098, loss = 0.29270702\n",
      "Iteration 1099, loss = 0.29245649\n",
      "Iteration 1100, loss = 0.29219860\n",
      "Iteration 1101, loss = 0.29196468\n",
      "Iteration 1102, loss = 0.29173182\n",
      "Iteration 1103, loss = 0.29147826\n",
      "Iteration 1104, loss = 0.29123600\n",
      "Iteration 1105, loss = 0.29099095\n",
      "Iteration 1106, loss = 0.29075779\n",
      "Iteration 1107, loss = 0.29050341\n",
      "Iteration 1108, loss = 0.29026343\n",
      "Iteration 1109, loss = 0.29002800\n",
      "Iteration 1110, loss = 0.28979436\n",
      "Iteration 1111, loss = 0.28956252\n",
      "Iteration 1112, loss = 0.28932156\n",
      "Iteration 1113, loss = 0.28907263\n",
      "Iteration 1114, loss = 0.28885083\n",
      "Iteration 1115, loss = 0.28860191\n",
      "Iteration 1116, loss = 0.28836180\n",
      "Iteration 1117, loss = 0.28813663\n",
      "Iteration 1118, loss = 0.28791552\n",
      "Iteration 1119, loss = 0.28766792\n",
      "Iteration 1120, loss = 0.28744377\n",
      "Iteration 1121, loss = 0.28720739\n",
      "Iteration 1122, loss = 0.28696252\n",
      "Iteration 1123, loss = 0.28674486\n",
      "Iteration 1124, loss = 0.28652810\n",
      "Iteration 1125, loss = 0.28626624\n",
      "Iteration 1126, loss = 0.28605738\n",
      "Iteration 1127, loss = 0.28581609\n",
      "Iteration 1128, loss = 0.28559813\n",
      "Iteration 1129, loss = 0.28537344\n",
      "Iteration 1130, loss = 0.28513915\n",
      "Iteration 1131, loss = 0.28490112\n",
      "Iteration 1132, loss = 0.28468319\n",
      "Iteration 1133, loss = 0.28447146\n",
      "Iteration 1134, loss = 0.28423714\n",
      "Iteration 1135, loss = 0.28401809\n",
      "Iteration 1136, loss = 0.28376678\n",
      "Iteration 1137, loss = 0.28356580\n",
      "Iteration 1138, loss = 0.28333670\n",
      "Iteration 1139, loss = 0.28310943\n",
      "Iteration 1140, loss = 0.28290569\n",
      "Iteration 1141, loss = 0.28266918\n",
      "Iteration 1142, loss = 0.28245458\n",
      "Iteration 1143, loss = 0.28222242\n",
      "Iteration 1144, loss = 0.28202620\n",
      "Iteration 1145, loss = 0.28179236\n",
      "Iteration 1146, loss = 0.28157029\n",
      "Iteration 1147, loss = 0.28135721\n",
      "Iteration 1148, loss = 0.28112159\n",
      "Iteration 1149, loss = 0.28094230\n",
      "Iteration 1150, loss = 0.28070465\n",
      "Iteration 1151, loss = 0.28047805\n",
      "Iteration 1152, loss = 0.28027918\n",
      "Iteration 1153, loss = 0.28004875\n",
      "Iteration 1154, loss = 0.27985032\n",
      "Iteration 1155, loss = 0.27964210\n",
      "Iteration 1156, loss = 0.27942846\n",
      "Iteration 1157, loss = 0.27919446\n",
      "Iteration 1158, loss = 0.27899131\n",
      "Iteration 1159, loss = 0.27877354\n",
      "Iteration 1160, loss = 0.27856667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1161, loss = 0.27835804\n",
      "Iteration 1162, loss = 0.27813943\n",
      "Iteration 1163, loss = 0.27792907\n",
      "Iteration 1164, loss = 0.27771447\n",
      "Iteration 1165, loss = 0.27752429\n",
      "Iteration 1166, loss = 0.27729372\n",
      "Iteration 1167, loss = 0.27710723\n",
      "Iteration 1168, loss = 0.27689568\n",
      "Iteration 1169, loss = 0.27667010\n",
      "Iteration 1170, loss = 0.27647806\n",
      "Iteration 1171, loss = 0.27626030\n",
      "Iteration 1172, loss = 0.27606428\n",
      "Iteration 1173, loss = 0.27587202\n",
      "Iteration 1174, loss = 0.27566662\n",
      "Iteration 1175, loss = 0.27545230\n",
      "Iteration 1176, loss = 0.27523368\n",
      "Iteration 1177, loss = 0.27504573\n",
      "Iteration 1178, loss = 0.27482883\n",
      "Iteration 1179, loss = 0.27462224\n",
      "Iteration 1180, loss = 0.27444537\n",
      "Iteration 1181, loss = 0.27421338\n",
      "Iteration 1182, loss = 0.27403005\n",
      "Iteration 1183, loss = 0.27383361\n",
      "Iteration 1184, loss = 0.27361680\n",
      "Iteration 1185, loss = 0.27343455\n",
      "Iteration 1186, loss = 0.27322103\n",
      "Iteration 1187, loss = 0.27302861\n",
      "Iteration 1188, loss = 0.27283220\n",
      "Iteration 1189, loss = 0.27263785\n",
      "Iteration 1190, loss = 0.27242762\n",
      "Iteration 1191, loss = 0.27222807\n",
      "Iteration 1192, loss = 0.27205879\n",
      "Iteration 1193, loss = 0.27183012\n",
      "Iteration 1194, loss = 0.27163860\n",
      "Iteration 1195, loss = 0.27146831\n",
      "Iteration 1196, loss = 0.27123943\n",
      "Iteration 1197, loss = 0.27105780\n",
      "Iteration 1198, loss = 0.27086593\n",
      "Iteration 1199, loss = 0.27067734\n",
      "Iteration 1200, loss = 0.27048503\n",
      "Iteration 1201, loss = 0.27028200\n",
      "Iteration 1202, loss = 0.27009807\n",
      "Iteration 1203, loss = 0.26989985\n",
      "Iteration 1204, loss = 0.26970545\n",
      "Iteration 1205, loss = 0.26951222\n",
      "Iteration 1206, loss = 0.26933481\n",
      "Iteration 1207, loss = 0.26912595\n",
      "Iteration 1208, loss = 0.26894957\n",
      "Iteration 1209, loss = 0.26876089\n",
      "Iteration 1210, loss = 0.26854907\n",
      "Iteration 1211, loss = 0.26838272\n",
      "Iteration 1212, loss = 0.26819347\n",
      "Iteration 1213, loss = 0.26798898\n",
      "Iteration 1214, loss = 0.26781183\n",
      "Iteration 1215, loss = 0.26764013\n",
      "Iteration 1216, loss = 0.26742411\n",
      "Iteration 1217, loss = 0.26725007\n",
      "Iteration 1218, loss = 0.26707874\n",
      "Iteration 1219, loss = 0.26687259\n",
      "Iteration 1220, loss = 0.26668627\n",
      "Iteration 1221, loss = 0.26653221\n",
      "Iteration 1222, loss = 0.26631019\n",
      "Iteration 1223, loss = 0.26614172\n",
      "Iteration 1224, loss = 0.26597325\n",
      "Iteration 1225, loss = 0.26577322\n",
      "Iteration 1226, loss = 0.26558248\n",
      "Iteration 1227, loss = 0.26542991\n",
      "Iteration 1228, loss = 0.26523142\n",
      "Iteration 1229, loss = 0.26503327\n",
      "Iteration 1230, loss = 0.26487825\n",
      "Iteration 1231, loss = 0.26468931\n",
      "Iteration 1232, loss = 0.26450335\n",
      "Iteration 1233, loss = 0.26432809\n",
      "Iteration 1234, loss = 0.26414416\n",
      "Iteration 1235, loss = 0.26398734\n",
      "Iteration 1236, loss = 0.26379693\n",
      "Iteration 1237, loss = 0.26359324\n",
      "Iteration 1238, loss = 0.26344465\n",
      "Iteration 1239, loss = 0.26325545\n",
      "Iteration 1240, loss = 0.26307198\n",
      "Iteration 1241, loss = 0.26290169\n",
      "Iteration 1242, loss = 0.26275450\n",
      "Iteration 1243, loss = 0.26253777\n",
      "Iteration 1244, loss = 0.26237545\n",
      "Iteration 1245, loss = 0.26221191\n",
      "Iteration 1246, loss = 0.26202422\n",
      "Iteration 1247, loss = 0.26184731\n",
      "Iteration 1248, loss = 0.26166127\n",
      "Iteration 1249, loss = 0.26151090\n",
      "Iteration 1250, loss = 0.26132430\n",
      "Iteration 1251, loss = 0.26114807\n",
      "Iteration 1252, loss = 0.26097871\n",
      "Iteration 1253, loss = 0.26079800\n",
      "Iteration 1254, loss = 0.26065732\n",
      "Iteration 1255, loss = 0.26047961\n",
      "Iteration 1256, loss = 0.26028650\n",
      "Iteration 1257, loss = 0.26012116\n",
      "Iteration 1258, loss = 0.25996964\n",
      "Iteration 1259, loss = 0.25976599\n",
      "Iteration 1260, loss = 0.25959482\n",
      "Iteration 1261, loss = 0.25947867\n",
      "Iteration 1262, loss = 0.25927422\n",
      "Iteration 1263, loss = 0.25909419\n",
      "Iteration 1264, loss = 0.25894048\n",
      "Iteration 1265, loss = 0.25878302\n",
      "Iteration 1266, loss = 0.25862271\n",
      "Iteration 1267, loss = 0.25841980\n",
      "Iteration 1268, loss = 0.25828429\n",
      "Iteration 1269, loss = 0.25809702\n",
      "Iteration 1270, loss = 0.25794672\n",
      "Iteration 1271, loss = 0.25778820\n",
      "Iteration 1272, loss = 0.25760899\n",
      "Iteration 1273, loss = 0.25745517\n",
      "Iteration 1274, loss = 0.25728836\n",
      "Iteration 1275, loss = 0.25710511\n",
      "Iteration 1276, loss = 0.25694881\n",
      "Iteration 1277, loss = 0.25681896\n",
      "Iteration 1278, loss = 0.25662742\n",
      "Iteration 1279, loss = 0.25646040\n",
      "Iteration 1280, loss = 0.25630890\n",
      "Iteration 1281, loss = 0.25614787\n",
      "Iteration 1282, loss = 0.25597744\n",
      "Iteration 1283, loss = 0.25582725\n",
      "Iteration 1284, loss = 0.25567880\n",
      "Iteration 1285, loss = 0.25550795\n",
      "Iteration 1286, loss = 0.25533863\n",
      "Iteration 1287, loss = 0.25517894\n",
      "Iteration 1288, loss = 0.25502609\n",
      "Iteration 1289, loss = 0.25486932\n",
      "Iteration 1290, loss = 0.25471857\n",
      "Iteration 1291, loss = 0.25453676\n",
      "Iteration 1292, loss = 0.25440230\n",
      "Iteration 1293, loss = 0.25421898\n",
      "Iteration 1294, loss = 0.25408015\n",
      "Iteration 1295, loss = 0.25392376\n",
      "Iteration 1296, loss = 0.25374875\n",
      "Iteration 1297, loss = 0.25362162\n",
      "Iteration 1298, loss = 0.25345922\n",
      "Iteration 1299, loss = 0.25329900\n",
      "Iteration 1300, loss = 0.25314205\n",
      "Iteration 1301, loss = 0.25296387\n",
      "Iteration 1302, loss = 0.25283446\n",
      "Iteration 1303, loss = 0.25271037\n",
      "Iteration 1304, loss = 0.25252722\n",
      "Iteration 1305, loss = 0.25235173\n",
      "Iteration 1306, loss = 0.25221801\n",
      "Iteration 1307, loss = 0.25207587\n",
      "Iteration 1308, loss = 0.25191723\n",
      "Iteration 1309, loss = 0.25175295\n",
      "Iteration 1310, loss = 0.25160896\n",
      "Iteration 1311, loss = 0.25145110\n",
      "Iteration 1312, loss = 0.25129332\n",
      "Iteration 1313, loss = 0.25111192\n",
      "Iteration 1314, loss = 0.25095693\n",
      "Iteration 1315, loss = 0.25079633\n",
      "Iteration 1316, loss = 0.25062112\n",
      "Iteration 1317, loss = 0.25044859\n",
      "Iteration 1318, loss = 0.25029758\n",
      "Iteration 1319, loss = 0.25013834\n",
      "Iteration 1320, loss = 0.24996789\n",
      "Iteration 1321, loss = 0.24978609\n",
      "Iteration 1322, loss = 0.24962216\n",
      "Iteration 1323, loss = 0.24947237\n",
      "Iteration 1324, loss = 0.24927875\n",
      "Iteration 1325, loss = 0.24913720\n",
      "Iteration 1326, loss = 0.24897039\n",
      "Iteration 1327, loss = 0.24881104\n",
      "Iteration 1328, loss = 0.24866179\n",
      "Iteration 1329, loss = 0.24848422\n",
      "Iteration 1330, loss = 0.24832780\n",
      "Iteration 1331, loss = 0.24816990\n",
      "Iteration 1332, loss = 0.24800139\n",
      "Iteration 1333, loss = 0.24784795\n",
      "Iteration 1334, loss = 0.24768100\n",
      "Iteration 1335, loss = 0.24754158\n",
      "Iteration 1336, loss = 0.24737814\n",
      "Iteration 1337, loss = 0.24721959\n",
      "Iteration 1338, loss = 0.24705282\n",
      "Iteration 1339, loss = 0.24691217\n",
      "Iteration 1340, loss = 0.24675534\n",
      "Iteration 1341, loss = 0.24658682\n",
      "Iteration 1342, loss = 0.24645116\n",
      "Iteration 1343, loss = 0.24626655\n",
      "Iteration 1344, loss = 0.24614269\n",
      "Iteration 1345, loss = 0.24597760\n",
      "Iteration 1346, loss = 0.24582648\n",
      "Iteration 1347, loss = 0.24566486\n",
      "Iteration 1348, loss = 0.24552154\n",
      "Iteration 1349, loss = 0.24537291\n",
      "Iteration 1350, loss = 0.24520991\n",
      "Iteration 1351, loss = 0.24505143\n",
      "Iteration 1352, loss = 0.24491811\n",
      "Iteration 1353, loss = 0.24476798\n",
      "Iteration 1354, loss = 0.24462788\n",
      "Iteration 1355, loss = 0.24444147\n",
      "Iteration 1356, loss = 0.24432013\n",
      "Iteration 1357, loss = 0.24416282\n",
      "Iteration 1358, loss = 0.24399818\n",
      "Iteration 1359, loss = 0.24386411\n",
      "Iteration 1360, loss = 0.24368578\n",
      "Iteration 1361, loss = 0.24357221\n",
      "Iteration 1362, loss = 0.24338989\n",
      "Iteration 1363, loss = 0.24326614\n",
      "Iteration 1364, loss = 0.24309341\n",
      "Iteration 1365, loss = 0.24296578\n",
      "Iteration 1366, loss = 0.24281324\n",
      "Iteration 1367, loss = 0.24264811\n",
      "Iteration 1368, loss = 0.24252170\n",
      "Iteration 1369, loss = 0.24237213\n",
      "Iteration 1370, loss = 0.24222219\n",
      "Iteration 1371, loss = 0.24206516\n",
      "Iteration 1372, loss = 0.24193365\n",
      "Iteration 1373, loss = 0.24179827\n",
      "Iteration 1374, loss = 0.24162863\n",
      "Iteration 1375, loss = 0.24149585\n",
      "Iteration 1376, loss = 0.24134962\n",
      "Iteration 1377, loss = 0.24121256\n",
      "Iteration 1378, loss = 0.24104711\n",
      "Iteration 1379, loss = 0.24091380\n",
      "Iteration 1380, loss = 0.24076728\n",
      "Iteration 1381, loss = 0.24061318\n",
      "Iteration 1382, loss = 0.24048689\n",
      "Iteration 1383, loss = 0.24032301\n",
      "Iteration 1384, loss = 0.24019274\n",
      "Iteration 1385, loss = 0.24004704\n",
      "Iteration 1386, loss = 0.23991609\n",
      "Iteration 1387, loss = 0.23976657\n",
      "Iteration 1388, loss = 0.23963942\n",
      "Iteration 1389, loss = 0.23948328\n",
      "Iteration 1390, loss = 0.23933975\n",
      "Iteration 1391, loss = 0.23918371\n",
      "Iteration 1392, loss = 0.23906665\n",
      "Iteration 1393, loss = 0.23891402\n",
      "Iteration 1394, loss = 0.23875275\n",
      "Iteration 1395, loss = 0.23863796\n",
      "Iteration 1396, loss = 0.23849166\n",
      "Iteration 1397, loss = 0.23835887\n",
      "Iteration 1398, loss = 0.23819694\n",
      "Iteration 1399, loss = 0.23809447\n",
      "Iteration 1400, loss = 0.23792155\n",
      "Iteration 1401, loss = 0.23779315\n",
      "Iteration 1402, loss = 0.23765595\n",
      "Iteration 1403, loss = 0.23750640\n",
      "Iteration 1404, loss = 0.23739991\n",
      "Iteration 1405, loss = 0.23722218\n",
      "Iteration 1406, loss = 0.23708792\n",
      "Iteration 1407, loss = 0.23697779\n",
      "Iteration 1408, loss = 0.23681701\n",
      "Iteration 1409, loss = 0.23668882\n",
      "Iteration 1410, loss = 0.23654319\n",
      "Iteration 1411, loss = 0.23644358\n",
      "Iteration 1412, loss = 0.23627285\n",
      "Iteration 1413, loss = 0.23613603\n",
      "Iteration 1414, loss = 0.23602591\n",
      "Iteration 1415, loss = 0.23586491\n",
      "Iteration 1416, loss = 0.23572778\n",
      "Iteration 1417, loss = 0.23558689\n",
      "Iteration 1418, loss = 0.23545366\n",
      "Iteration 1419, loss = 0.23530746\n",
      "Iteration 1420, loss = 0.23519557\n",
      "Iteration 1421, loss = 0.23502936\n",
      "Iteration 1422, loss = 0.23494331\n",
      "Iteration 1423, loss = 0.23476552\n",
      "Iteration 1424, loss = 0.23464957\n",
      "Iteration 1425, loss = 0.23453745\n",
      "Iteration 1426, loss = 0.23436960\n",
      "Iteration 1427, loss = 0.23424509\n",
      "Iteration 1428, loss = 0.23412881\n",
      "Iteration 1429, loss = 0.23397188\n",
      "Iteration 1430, loss = 0.23385799\n",
      "Iteration 1431, loss = 0.23371489\n",
      "Iteration 1432, loss = 0.23359332\n",
      "Iteration 1433, loss = 0.23345399\n",
      "Iteration 1434, loss = 0.23330393\n",
      "Iteration 1435, loss = 0.23322618\n",
      "Iteration 1436, loss = 0.23304930\n",
      "Iteration 1437, loss = 0.23293828\n",
      "Iteration 1438, loss = 0.23282452\n",
      "Iteration 1439, loss = 0.23269091\n",
      "Iteration 1440, loss = 0.23255990\n",
      "Iteration 1441, loss = 0.23242263\n",
      "Iteration 1442, loss = 0.23231379\n",
      "Iteration 1443, loss = 0.23219424\n",
      "Iteration 1444, loss = 0.23204919\n",
      "Iteration 1445, loss = 0.23193281\n",
      "Iteration 1446, loss = 0.23182060\n",
      "Iteration 1447, loss = 0.23169217\n",
      "Iteration 1448, loss = 0.23155120\n",
      "Iteration 1449, loss = 0.23145164\n",
      "Iteration 1450, loss = 0.23133214\n",
      "Iteration 1451, loss = 0.23122643\n",
      "Iteration 1452, loss = 0.23106968\n",
      "Iteration 1453, loss = 0.23099153\n",
      "Iteration 1454, loss = 0.23084033\n",
      "Iteration 1455, loss = 0.23075269\n",
      "Iteration 1456, loss = 0.23062077\n",
      "Iteration 1457, loss = 0.23053077\n",
      "Iteration 1458, loss = 0.23036991\n",
      "Iteration 1459, loss = 0.23025755\n",
      "Iteration 1460, loss = 0.23018258\n",
      "Iteration 1461, loss = 0.23005234\n",
      "Iteration 1462, loss = 0.22990613\n",
      "Iteration 1463, loss = 0.22987260\n",
      "Iteration 1464, loss = 0.22969900\n",
      "Iteration 1465, loss = 0.22957106\n",
      "Iteration 1466, loss = 0.22951299\n",
      "Iteration 1467, loss = 0.22935255\n",
      "Iteration 1468, loss = 0.22925671\n",
      "Iteration 1469, loss = 0.22917655\n",
      "Iteration 1470, loss = 0.22899906\n",
      "Iteration 1471, loss = 0.22890888\n",
      "Iteration 1472, loss = 0.22880438\n",
      "Iteration 1473, loss = 0.22872901\n",
      "Iteration 1474, loss = 0.22857228\n",
      "Iteration 1475, loss = 0.22845000\n",
      "Iteration 1476, loss = 0.22838757\n",
      "Iteration 1477, loss = 0.22824228\n",
      "Iteration 1478, loss = 0.22814988\n",
      "Iteration 1479, loss = 0.22806352\n",
      "Iteration 1480, loss = 0.22791274\n",
      "Iteration 1481, loss = 0.22779224\n",
      "Iteration 1482, loss = 0.22770644\n",
      "Iteration 1483, loss = 0.22759022\n",
      "Iteration 1484, loss = 0.22750084\n",
      "Iteration 1485, loss = 0.22739224\n",
      "Iteration 1486, loss = 0.22727922\n",
      "Iteration 1487, loss = 0.22718911\n",
      "Iteration 1488, loss = 0.22706788\n",
      "Iteration 1489, loss = 0.22703005\n",
      "Iteration 1490, loss = 0.22685246\n",
      "Iteration 1491, loss = 0.22674666\n",
      "Iteration 1492, loss = 0.22662273\n",
      "Iteration 1493, loss = 0.22653381\n",
      "Iteration 1494, loss = 0.22642008\n",
      "Iteration 1495, loss = 0.22633841\n",
      "Iteration 1496, loss = 0.22621575\n",
      "Iteration 1497, loss = 0.22607840\n",
      "Iteration 1498, loss = 0.22599076\n",
      "Iteration 1499, loss = 0.22588928\n",
      "Iteration 1500, loss = 0.22578493\n",
      "Iteration 1501, loss = 0.22568924\n",
      "Iteration 1502, loss = 0.22557966\n",
      "Iteration 1503, loss = 0.22546926\n",
      "Iteration 1504, loss = 0.22539289\n",
      "Iteration 1505, loss = 0.22523246\n",
      "Iteration 1506, loss = 0.22517232\n",
      "Iteration 1507, loss = 0.22506632\n",
      "Iteration 1508, loss = 0.22494943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1509, loss = 0.22483308\n",
      "Iteration 1510, loss = 0.22479707\n",
      "Iteration 1511, loss = 0.22463909\n",
      "Iteration 1512, loss = 0.22453401\n",
      "Iteration 1513, loss = 0.22446633\n",
      "Iteration 1514, loss = 0.22432417\n",
      "Iteration 1515, loss = 0.22425986\n",
      "Iteration 1516, loss = 0.22415940\n",
      "Iteration 1517, loss = 0.22403728\n",
      "Iteration 1518, loss = 0.22393196\n",
      "Iteration 1519, loss = 0.22386017\n",
      "Iteration 1520, loss = 0.22372844\n",
      "Iteration 1521, loss = 0.22365078\n",
      "Iteration 1522, loss = 0.22359371\n",
      "Iteration 1523, loss = 0.22343965\n",
      "Iteration 1524, loss = 0.22336677\n",
      "Iteration 1525, loss = 0.22323743\n",
      "Iteration 1526, loss = 0.22314281\n",
      "Iteration 1527, loss = 0.22305027\n",
      "Iteration 1528, loss = 0.22299178\n",
      "Iteration 1529, loss = 0.22286147\n",
      "Iteration 1530, loss = 0.22274741\n",
      "Iteration 1531, loss = 0.22267822\n",
      "Iteration 1532, loss = 0.22255609\n",
      "Iteration 1533, loss = 0.22248013\n",
      "Iteration 1534, loss = 0.22241093\n",
      "Iteration 1535, loss = 0.22227473\n",
      "Iteration 1536, loss = 0.22220224\n",
      "Iteration 1537, loss = 0.22207055\n",
      "Iteration 1538, loss = 0.22197320\n",
      "Iteration 1539, loss = 0.22187745\n",
      "Iteration 1540, loss = 0.22178167\n",
      "Iteration 1541, loss = 0.22168667\n",
      "Iteration 1542, loss = 0.22164105\n",
      "Iteration 1543, loss = 0.22148141\n",
      "Iteration 1544, loss = 0.22143558\n",
      "Iteration 1545, loss = 0.22133424\n",
      "Iteration 1546, loss = 0.22120388\n",
      "Iteration 1547, loss = 0.22117381\n",
      "Iteration 1548, loss = 0.22103168\n",
      "Iteration 1549, loss = 0.22093899\n",
      "Iteration 1550, loss = 0.22083969\n",
      "Iteration 1551, loss = 0.22078408\n",
      "Iteration 1552, loss = 0.22064942\n",
      "Iteration 1553, loss = 0.22058595\n",
      "Iteration 1554, loss = 0.22044964\n",
      "Iteration 1555, loss = 0.22039376\n",
      "Iteration 1556, loss = 0.22030241\n",
      "Iteration 1557, loss = 0.22017601\n",
      "Iteration 1558, loss = 0.22011381\n",
      "Iteration 1559, loss = 0.22000803\n",
      "Iteration 1560, loss = 0.21993338\n",
      "Iteration 1561, loss = 0.21983684\n",
      "Iteration 1562, loss = 0.21969838\n",
      "Iteration 1563, loss = 0.21964167\n",
      "Iteration 1564, loss = 0.21951202\n",
      "Iteration 1565, loss = 0.21944352\n",
      "Iteration 1566, loss = 0.21931081\n",
      "Iteration 1567, loss = 0.21922842\n",
      "Iteration 1568, loss = 0.21911250\n",
      "Iteration 1569, loss = 0.21904071\n",
      "Iteration 1570, loss = 0.21893248\n",
      "Iteration 1571, loss = 0.21883748\n",
      "Iteration 1572, loss = 0.21875549\n",
      "Iteration 1573, loss = 0.21864970\n",
      "Iteration 1574, loss = 0.21854209\n",
      "Iteration 1575, loss = 0.21847028\n",
      "Iteration 1576, loss = 0.21835812\n",
      "Iteration 1577, loss = 0.21826688\n",
      "Iteration 1578, loss = 0.21818483\n",
      "Iteration 1579, loss = 0.21808721\n",
      "Iteration 1580, loss = 0.21800590\n",
      "Iteration 1581, loss = 0.21791861\n",
      "Iteration 1582, loss = 0.21780670\n",
      "Iteration 1583, loss = 0.21771608\n",
      "Iteration 1584, loss = 0.21763824\n",
      "Iteration 1585, loss = 0.21754161\n",
      "Iteration 1586, loss = 0.21747506\n",
      "Iteration 1587, loss = 0.21735312\n",
      "Iteration 1588, loss = 0.21727472\n",
      "Iteration 1589, loss = 0.21717764\n",
      "Iteration 1590, loss = 0.21708520\n",
      "Iteration 1591, loss = 0.21699946\n",
      "Iteration 1592, loss = 0.21691263\n",
      "Iteration 1593, loss = 0.21682041\n",
      "Iteration 1594, loss = 0.21674574\n",
      "Iteration 1595, loss = 0.21665372\n",
      "Iteration 1596, loss = 0.21655530\n",
      "Iteration 1597, loss = 0.21649702\n",
      "Iteration 1598, loss = 0.21639657\n",
      "Iteration 1599, loss = 0.21633147\n",
      "Iteration 1600, loss = 0.21622248\n",
      "Iteration 1601, loss = 0.21614807\n",
      "Iteration 1602, loss = 0.21600201\n",
      "Iteration 1603, loss = 0.21595515\n",
      "Iteration 1604, loss = 0.21585536\n",
      "Iteration 1605, loss = 0.21583409\n",
      "Iteration 1606, loss = 0.21569413\n",
      "Iteration 1607, loss = 0.21562477\n",
      "Iteration 1608, loss = 0.21548817\n",
      "Iteration 1609, loss = 0.21545896\n",
      "Iteration 1610, loss = 0.21535891\n",
      "Iteration 1611, loss = 0.21527200\n",
      "Iteration 1612, loss = 0.21520968\n",
      "Iteration 1613, loss = 0.21513271\n",
      "Iteration 1614, loss = 0.21499192\n",
      "Iteration 1615, loss = 0.21493225\n",
      "Iteration 1616, loss = 0.21482515\n",
      "Iteration 1617, loss = 0.21475104\n",
      "Iteration 1618, loss = 0.21464806\n",
      "Iteration 1619, loss = 0.21458518\n",
      "Iteration 1620, loss = 0.21448830\n",
      "Iteration 1621, loss = 0.21441272\n",
      "Iteration 1622, loss = 0.21433572\n",
      "Iteration 1623, loss = 0.21427826\n",
      "Iteration 1624, loss = 0.21422104\n",
      "Iteration 1625, loss = 0.21410537\n",
      "Iteration 1626, loss = 0.21402084\n",
      "Iteration 1627, loss = 0.21390799\n",
      "Iteration 1628, loss = 0.21384175\n",
      "Iteration 1629, loss = 0.21377537\n",
      "Iteration 1630, loss = 0.21370434\n",
      "Iteration 1631, loss = 0.21359195\n",
      "Iteration 1632, loss = 0.21352835\n",
      "Iteration 1633, loss = 0.21341716\n",
      "Iteration 1634, loss = 0.21336128\n",
      "Iteration 1635, loss = 0.21326747\n",
      "Iteration 1636, loss = 0.21320244\n",
      "Iteration 1637, loss = 0.21311032\n",
      "Iteration 1638, loss = 0.21303375\n",
      "Iteration 1639, loss = 0.21295499\n",
      "Iteration 1640, loss = 0.21286518\n",
      "Iteration 1641, loss = 0.21280720\n",
      "Iteration 1642, loss = 0.21268335\n",
      "Iteration 1643, loss = 0.21265108\n",
      "Iteration 1644, loss = 0.21258632\n",
      "Iteration 1645, loss = 0.21245139\n",
      "Iteration 1646, loss = 0.21237209\n",
      "Iteration 1647, loss = 0.21227101\n",
      "Iteration 1648, loss = 0.21220099\n",
      "Iteration 1649, loss = 0.21213158\n",
      "Iteration 1650, loss = 0.21206773\n",
      "Iteration 1651, loss = 0.21196034\n",
      "Iteration 1652, loss = 0.21190788\n",
      "Iteration 1653, loss = 0.21180749\n",
      "Iteration 1654, loss = 0.21173831\n",
      "Iteration 1655, loss = 0.21166154\n",
      "Iteration 1656, loss = 0.21156922\n",
      "Iteration 1657, loss = 0.21152424\n",
      "Iteration 1658, loss = 0.21142667\n",
      "Iteration 1659, loss = 0.21136877\n",
      "Iteration 1660, loss = 0.21123979\n",
      "Iteration 1661, loss = 0.21119260\n",
      "Iteration 1662, loss = 0.21108804\n",
      "Iteration 1663, loss = 0.21105473\n",
      "Iteration 1664, loss = 0.21093517\n",
      "Iteration 1665, loss = 0.21091376\n",
      "Iteration 1666, loss = 0.21082868\n",
      "Iteration 1667, loss = 0.21074190\n",
      "Iteration 1668, loss = 0.21067443\n",
      "Iteration 1669, loss = 0.21058154\n",
      "Iteration 1670, loss = 0.21051268\n",
      "Iteration 1671, loss = 0.21041299\n",
      "Iteration 1672, loss = 0.21031429\n",
      "Iteration 1673, loss = 0.21027081\n",
      "Iteration 1674, loss = 0.21018505\n",
      "Iteration 1675, loss = 0.21009961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.920000\n",
      "Training set loss: 0.210100\n",
      "training: constant with momentum\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.69336674\n",
      "Iteration 3, loss = 0.69050049\n",
      "Iteration 4, loss = 0.68849635\n",
      "Iteration 5, loss = 0.68795185\n",
      "Iteration 6, loss = 0.68819456\n",
      "Iteration 7, loss = 0.68820704\n",
      "Iteration 8, loss = 0.68739541\n",
      "Iteration 9, loss = 0.68581003\n",
      "Iteration 10, loss = 0.68419661\n",
      "Iteration 11, loss = 0.68328953\n",
      "Iteration 12, loss = 0.68301378\n",
      "Iteration 13, loss = 0.68291225\n",
      "Iteration 14, loss = 0.68245571\n",
      "Iteration 15, loss = 0.68149674\n",
      "Iteration 16, loss = 0.68016390\n",
      "Iteration 17, loss = 0.67885208\n",
      "Iteration 18, loss = 0.67789901\n",
      "Iteration 19, loss = 0.67722329\n",
      "Iteration 20, loss = 0.67651854\n",
      "Iteration 21, loss = 0.67559645\n",
      "Iteration 22, loss = 0.67444870\n",
      "Iteration 23, loss = 0.67315265\n",
      "Iteration 24, loss = 0.67182081\n",
      "Iteration 25, loss = 0.67059247\n",
      "Iteration 26, loss = 0.66941282\n",
      "Iteration 27, loss = 0.66825697\n",
      "Iteration 28, loss = 0.66699810\n",
      "Iteration 29, loss = 0.66559468\n",
      "Iteration 30, loss = 0.66413948\n",
      "Iteration 31, loss = 0.66281419\n",
      "Iteration 32, loss = 0.66144850\n",
      "Iteration 33, loss = 0.65995742\n",
      "Iteration 34, loss = 0.65836491\n",
      "Iteration 35, loss = 0.65660313\n",
      "Iteration 36, loss = 0.65481723\n",
      "Iteration 37, loss = 0.65312243\n",
      "Iteration 38, loss = 0.65136376\n",
      "Iteration 39, loss = 0.64948894\n",
      "Iteration 40, loss = 0.64751310\n",
      "Iteration 41, loss = 0.64541993\n",
      "Iteration 42, loss = 0.64327631\n",
      "Iteration 43, loss = 0.64108932\n",
      "Iteration 44, loss = 0.63882254\n",
      "Iteration 45, loss = 0.63643918\n",
      "Iteration 46, loss = 0.63398411\n",
      "Iteration 47, loss = 0.63148733\n",
      "Iteration 48, loss = 0.62889645\n",
      "Iteration 49, loss = 0.62620723\n",
      "Iteration 50, loss = 0.62345761\n",
      "Iteration 51, loss = 0.62065455\n",
      "Iteration 52, loss = 0.61775549\n",
      "Iteration 53, loss = 0.61473699\n",
      "Iteration 54, loss = 0.61160744\n",
      "Iteration 55, loss = 0.60838924\n",
      "Iteration 56, loss = 0.60513371\n",
      "Iteration 57, loss = 0.60179004\n",
      "Iteration 58, loss = 0.59834439\n",
      "Iteration 59, loss = 0.59482056\n",
      "Iteration 60, loss = 0.59120563\n",
      "Iteration 61, loss = 0.58747978\n",
      "Iteration 62, loss = 0.58364819\n",
      "Iteration 63, loss = 0.57967412\n",
      "Iteration 64, loss = 0.57554549\n",
      "Iteration 65, loss = 0.57132247\n",
      "Iteration 66, loss = 0.56698925\n",
      "Iteration 67, loss = 0.56259344\n",
      "Iteration 68, loss = 0.55822370\n",
      "Iteration 69, loss = 0.55380330\n",
      "Iteration 70, loss = 0.54935323\n",
      "Iteration 71, loss = 0.54483986\n",
      "Iteration 72, loss = 0.54019056\n",
      "Iteration 73, loss = 0.53546909\n",
      "Iteration 74, loss = 0.53072720\n",
      "Iteration 75, loss = 0.52591451\n",
      "Iteration 76, loss = 0.52102082\n",
      "Iteration 77, loss = 0.51604046\n",
      "Iteration 78, loss = 0.51101775\n",
      "Iteration 79, loss = 0.50595539\n",
      "Iteration 80, loss = 0.50082908\n",
      "Iteration 81, loss = 0.49569104\n",
      "Iteration 82, loss = 0.49057194\n",
      "Iteration 83, loss = 0.48545921\n",
      "Iteration 84, loss = 0.48030221\n",
      "Iteration 85, loss = 0.47512757\n",
      "Iteration 86, loss = 0.46996549\n",
      "Iteration 87, loss = 0.46480063\n",
      "Iteration 88, loss = 0.45961852\n",
      "Iteration 89, loss = 0.45443574\n",
      "Iteration 90, loss = 0.44927808\n",
      "Iteration 91, loss = 0.44413810\n",
      "Iteration 92, loss = 0.43901322\n",
      "Iteration 93, loss = 0.43393955\n",
      "Iteration 94, loss = 0.42887850\n",
      "Iteration 95, loss = 0.42386243\n",
      "Iteration 96, loss = 0.41889424\n",
      "Iteration 97, loss = 0.41397601\n",
      "Iteration 98, loss = 0.40911526\n",
      "Iteration 99, loss = 0.40432553\n",
      "Iteration 100, loss = 0.39956281\n",
      "Iteration 101, loss = 0.39482484\n",
      "Iteration 102, loss = 0.39013901\n",
      "Iteration 103, loss = 0.38549478\n",
      "Iteration 104, loss = 0.38091267\n",
      "Iteration 105, loss = 0.37642805\n",
      "Iteration 106, loss = 0.37202747\n",
      "Iteration 107, loss = 0.36771540\n",
      "Iteration 108, loss = 0.36347798\n",
      "Iteration 109, loss = 0.35925485\n",
      "Iteration 110, loss = 0.35509049\n",
      "Iteration 111, loss = 0.35103345\n",
      "Iteration 112, loss = 0.34701626\n",
      "Iteration 113, loss = 0.34307445\n",
      "Iteration 114, loss = 0.33918571\n",
      "Iteration 115, loss = 0.33544066\n",
      "Iteration 116, loss = 0.33176797\n",
      "Iteration 117, loss = 0.32819243\n",
      "Iteration 118, loss = 0.32467497\n",
      "Iteration 119, loss = 0.32121424\n",
      "Iteration 120, loss = 0.31783775\n",
      "Iteration 121, loss = 0.31450414\n",
      "Iteration 122, loss = 0.31124349\n",
      "Iteration 123, loss = 0.30801618\n",
      "Iteration 124, loss = 0.30487620\n",
      "Iteration 125, loss = 0.30186396\n",
      "Iteration 126, loss = 0.29893102\n",
      "Iteration 127, loss = 0.29607370\n",
      "Iteration 128, loss = 0.29332068\n",
      "Iteration 129, loss = 0.29065415\n",
      "Iteration 130, loss = 0.28802291\n",
      "Iteration 131, loss = 0.28544242\n",
      "Iteration 132, loss = 0.28289621\n",
      "Iteration 133, loss = 0.28044040\n",
      "Iteration 134, loss = 0.27810792\n",
      "Iteration 135, loss = 0.27576380\n",
      "Iteration 136, loss = 0.27348504\n",
      "Iteration 137, loss = 0.27128798\n",
      "Iteration 138, loss = 0.26913765\n",
      "Iteration 139, loss = 0.26707611\n",
      "Iteration 140, loss = 0.26505059\n",
      "Iteration 141, loss = 0.26304724\n",
      "Iteration 142, loss = 0.26110966\n",
      "Iteration 143, loss = 0.25925186\n",
      "Iteration 144, loss = 0.25738433\n",
      "Iteration 145, loss = 0.25552856\n",
      "Iteration 146, loss = 0.25375440\n",
      "Iteration 147, loss = 0.25202459\n",
      "Iteration 148, loss = 0.25027344\n",
      "Iteration 149, loss = 0.24863267\n",
      "Iteration 150, loss = 0.24695851\n",
      "Iteration 151, loss = 0.24534471\n",
      "Iteration 152, loss = 0.24387756\n",
      "Iteration 153, loss = 0.24240937\n",
      "Iteration 154, loss = 0.24094750\n",
      "Iteration 155, loss = 0.23951251\n",
      "Iteration 156, loss = 0.23816486\n",
      "Iteration 157, loss = 0.23682062\n",
      "Iteration 158, loss = 0.23539098\n",
      "Iteration 159, loss = 0.23406729\n",
      "Iteration 160, loss = 0.23279957\n",
      "Iteration 161, loss = 0.23149972\n",
      "Iteration 162, loss = 0.23022656\n",
      "Iteration 163, loss = 0.22894661\n",
      "Iteration 164, loss = 0.22766336\n",
      "Iteration 165, loss = 0.22642017\n",
      "Iteration 166, loss = 0.22519327\n",
      "Iteration 167, loss = 0.22395392\n",
      "Iteration 168, loss = 0.22272423\n",
      "Iteration 169, loss = 0.22151913\n",
      "Iteration 170, loss = 0.22038445\n",
      "Iteration 171, loss = 0.21918804\n",
      "Iteration 172, loss = 0.21801609\n",
      "Iteration 173, loss = 0.21697552\n",
      "Iteration 174, loss = 0.21588546\n",
      "Iteration 175, loss = 0.21474280\n",
      "Iteration 176, loss = 0.21377459\n",
      "Iteration 177, loss = 0.21279945\n",
      "Iteration 178, loss = 0.21178227\n",
      "Iteration 179, loss = 0.21080474\n",
      "Iteration 180, loss = 0.20998232\n",
      "Iteration 181, loss = 0.20911406\n",
      "Iteration 182, loss = 0.20825184\n",
      "Iteration 183, loss = 0.20749144\n",
      "Iteration 184, loss = 0.20666216\n",
      "Iteration 185, loss = 0.20591701\n",
      "Iteration 186, loss = 0.20519976\n",
      "Iteration 187, loss = 0.20432476\n",
      "Iteration 188, loss = 0.20370510\n",
      "Iteration 189, loss = 0.20291695\n",
      "Iteration 190, loss = 0.20222703\n",
      "Iteration 191, loss = 0.20156056\n",
      "Iteration 192, loss = 0.20086734\n",
      "Iteration 193, loss = 0.20027739\n",
      "Iteration 194, loss = 0.19955499\n",
      "Iteration 195, loss = 0.19899269\n",
      "Iteration 196, loss = 0.19835804\n",
      "Iteration 197, loss = 0.19775333\n",
      "Iteration 198, loss = 0.19712513\n",
      "Iteration 199, loss = 0.19651181\n",
      "Iteration 200, loss = 0.19600130\n",
      "Iteration 201, loss = 0.19550534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 202, loss = 0.19485036\n",
      "Iteration 203, loss = 0.19431949\n",
      "Iteration 204, loss = 0.19378136\n",
      "Iteration 205, loss = 0.19324504\n",
      "Iteration 206, loss = 0.19282020\n",
      "Iteration 207, loss = 0.19229325\n",
      "Iteration 208, loss = 0.19176096\n",
      "Iteration 209, loss = 0.19131950\n",
      "Iteration 210, loss = 0.19082673\n",
      "Iteration 211, loss = 0.19036693\n",
      "Iteration 212, loss = 0.18992192\n",
      "Iteration 213, loss = 0.18948607\n",
      "Iteration 214, loss = 0.18904405\n",
      "Iteration 215, loss = 0.18868328\n",
      "Iteration 216, loss = 0.18818885\n",
      "Iteration 217, loss = 0.18770193\n",
      "Iteration 218, loss = 0.18740346\n",
      "Iteration 219, loss = 0.18694114\n",
      "Iteration 220, loss = 0.18656569\n",
      "Iteration 221, loss = 0.18617213\n",
      "Iteration 222, loss = 0.18579575\n",
      "Iteration 223, loss = 0.18536059\n",
      "Iteration 224, loss = 0.18501182\n",
      "Iteration 225, loss = 0.18472561\n",
      "Iteration 226, loss = 0.18434603\n",
      "Iteration 227, loss = 0.18394086\n",
      "Iteration 228, loss = 0.18358713\n",
      "Iteration 229, loss = 0.18321983\n",
      "Iteration 230, loss = 0.18294638\n",
      "Iteration 231, loss = 0.18257737\n",
      "Iteration 232, loss = 0.18221489\n",
      "Iteration 233, loss = 0.18188928\n",
      "Iteration 234, loss = 0.18159280\n",
      "Iteration 235, loss = 0.18126245\n",
      "Iteration 236, loss = 0.18097205\n",
      "Iteration 237, loss = 0.18064779\n",
      "Iteration 238, loss = 0.18039435\n",
      "Iteration 239, loss = 0.18006490\n",
      "Iteration 240, loss = 0.17964327\n",
      "Iteration 241, loss = 0.17932924\n",
      "Iteration 242, loss = 0.17907227\n",
      "Iteration 243, loss = 0.17872614\n",
      "Iteration 244, loss = 0.17833974\n",
      "Iteration 245, loss = 0.17797180\n",
      "Iteration 246, loss = 0.17768946\n",
      "Iteration 247, loss = 0.17736879\n",
      "Iteration 248, loss = 0.17700089\n",
      "Iteration 249, loss = 0.17669452\n",
      "Iteration 250, loss = 0.17627829\n",
      "Iteration 251, loss = 0.17596699\n",
      "Iteration 252, loss = 0.17574671\n",
      "Iteration 253, loss = 0.17543146\n",
      "Iteration 254, loss = 0.17512305\n",
      "Iteration 255, loss = 0.17486619\n",
      "Iteration 256, loss = 0.17460730\n",
      "Iteration 257, loss = 0.17434431\n",
      "Iteration 258, loss = 0.17410463\n",
      "Iteration 259, loss = 0.17379034\n",
      "Iteration 260, loss = 0.17352917\n",
      "Iteration 261, loss = 0.17325005\n",
      "Iteration 262, loss = 0.17300125\n",
      "Iteration 263, loss = 0.17274118\n",
      "Iteration 264, loss = 0.17252946\n",
      "Iteration 265, loss = 0.17230241\n",
      "Iteration 266, loss = 0.17204870\n",
      "Iteration 267, loss = 0.17174400\n",
      "Iteration 268, loss = 0.17161954\n",
      "Iteration 269, loss = 0.17138927\n",
      "Iteration 270, loss = 0.17107810\n",
      "Iteration 271, loss = 0.17083766\n",
      "Iteration 272, loss = 0.17067018\n",
      "Iteration 273, loss = 0.17049788\n",
      "Iteration 274, loss = 0.17034903\n",
      "Iteration 275, loss = 0.17015113\n",
      "Iteration 276, loss = 0.16987316\n",
      "Iteration 277, loss = 0.16974132\n",
      "Iteration 278, loss = 0.16949961\n",
      "Iteration 279, loss = 0.16930550\n",
      "Iteration 280, loss = 0.16919952\n",
      "Iteration 281, loss = 0.16915605\n",
      "Iteration 282, loss = 0.16889741\n",
      "Iteration 283, loss = 0.16874532\n",
      "Iteration 284, loss = 0.16851245\n",
      "Iteration 285, loss = 0.16832745\n",
      "Iteration 286, loss = 0.16824121\n",
      "Iteration 287, loss = 0.16810292\n",
      "Iteration 288, loss = 0.16775526\n",
      "Iteration 289, loss = 0.16742220\n",
      "Iteration 290, loss = 0.16742340\n",
      "Iteration 291, loss = 0.16742157\n",
      "Iteration 292, loss = 0.16716107\n",
      "Iteration 293, loss = 0.16694617\n",
      "Iteration 294, loss = 0.16679434\n",
      "Iteration 295, loss = 0.16675754\n",
      "Iteration 296, loss = 0.16666775\n",
      "Iteration 297, loss = 0.16631959\n",
      "Iteration 298, loss = 0.16634222\n",
      "Iteration 299, loss = 0.16618307\n",
      "Iteration 300, loss = 0.16587963\n",
      "Iteration 301, loss = 0.16574624\n",
      "Iteration 302, loss = 0.16567308\n",
      "Iteration 303, loss = 0.16557514\n",
      "Iteration 304, loss = 0.16543178\n",
      "Iteration 305, loss = 0.16521150\n",
      "Iteration 306, loss = 0.16507163\n",
      "Iteration 307, loss = 0.16492145\n",
      "Iteration 308, loss = 0.16494408\n",
      "Iteration 309, loss = 0.16464309\n",
      "Iteration 310, loss = 0.16450766\n",
      "Iteration 311, loss = 0.16431462\n",
      "Iteration 312, loss = 0.16413360\n",
      "Iteration 313, loss = 0.16412182\n",
      "Iteration 314, loss = 0.16403041\n",
      "Iteration 315, loss = 0.16386670\n",
      "Iteration 316, loss = 0.16370134\n",
      "Iteration 317, loss = 0.16364858\n",
      "Iteration 318, loss = 0.16375653\n",
      "Iteration 319, loss = 0.16371246\n",
      "Iteration 320, loss = 0.16351446\n",
      "Iteration 321, loss = 0.16318700\n",
      "Iteration 322, loss = 0.16336627\n",
      "Iteration 323, loss = 0.16319997\n",
      "Iteration 324, loss = 0.16294068\n",
      "Iteration 325, loss = 0.16292187\n",
      "Iteration 326, loss = 0.16295990\n",
      "Iteration 327, loss = 0.16277295\n",
      "Iteration 328, loss = 0.16252265\n",
      "Iteration 329, loss = 0.16252502\n",
      "Iteration 330, loss = 0.16267718\n",
      "Iteration 331, loss = 0.16239195\n",
      "Iteration 332, loss = 0.16223694\n",
      "Iteration 333, loss = 0.16211621\n",
      "Iteration 334, loss = 0.16213428\n",
      "Iteration 335, loss = 0.16196941\n",
      "Iteration 336, loss = 0.16181165\n",
      "Iteration 337, loss = 0.16180387\n",
      "Iteration 338, loss = 0.16166858\n",
      "Iteration 339, loss = 0.16142100\n",
      "Iteration 340, loss = 0.16148653\n",
      "Iteration 341, loss = 0.16161038\n",
      "Iteration 342, loss = 0.16130338\n",
      "Iteration 343, loss = 0.16122745\n",
      "Iteration 344, loss = 0.16125577\n",
      "Iteration 345, loss = 0.16118455\n",
      "Iteration 346, loss = 0.16109147\n",
      "Iteration 347, loss = 0.16093957\n",
      "Iteration 348, loss = 0.16073295\n",
      "Iteration 349, loss = 0.16073498\n",
      "Iteration 350, loss = 0.16058260\n",
      "Iteration 351, loss = 0.16037394\n",
      "Iteration 352, loss = 0.16043855\n",
      "Iteration 353, loss = 0.16036075\n",
      "Iteration 354, loss = 0.16036446\n",
      "Iteration 355, loss = 0.16012313\n",
      "Iteration 356, loss = 0.16007403\n",
      "Iteration 357, loss = 0.16005055\n",
      "Iteration 358, loss = 0.16001127\n",
      "Iteration 359, loss = 0.15991940\n",
      "Iteration 360, loss = 0.15992985\n",
      "Iteration 361, loss = 0.15979475\n",
      "Iteration 362, loss = 0.15966270\n",
      "Iteration 363, loss = 0.15944425\n",
      "Iteration 364, loss = 0.15958156\n",
      "Iteration 365, loss = 0.15942672\n",
      "Iteration 366, loss = 0.15931462\n",
      "Iteration 367, loss = 0.15921718\n",
      "Iteration 368, loss = 0.15915407\n",
      "Iteration 369, loss = 0.15910739\n",
      "Iteration 370, loss = 0.15896755\n",
      "Iteration 371, loss = 0.15906309\n",
      "Iteration 372, loss = 0.15892802\n",
      "Iteration 373, loss = 0.15882800\n",
      "Iteration 374, loss = 0.15884709\n",
      "Iteration 375, loss = 0.15877059\n",
      "Iteration 376, loss = 0.15862205\n",
      "Iteration 377, loss = 0.15856635\n",
      "Iteration 378, loss = 0.15870516\n",
      "Iteration 379, loss = 0.15870455\n",
      "Iteration 380, loss = 0.15848839\n",
      "Iteration 381, loss = 0.15845451\n",
      "Iteration 382, loss = 0.15842492\n",
      "Iteration 383, loss = 0.15836215\n",
      "Iteration 384, loss = 0.15820257\n",
      "Iteration 385, loss = 0.15813768\n",
      "Iteration 386, loss = 0.15809109\n",
      "Iteration 387, loss = 0.15804341\n",
      "Iteration 388, loss = 0.15804058\n",
      "Iteration 389, loss = 0.15790980\n",
      "Iteration 390, loss = 0.15779403\n",
      "Iteration 391, loss = 0.15756868\n",
      "Iteration 392, loss = 0.15772811\n",
      "Iteration 393, loss = 0.15774117\n",
      "Iteration 394, loss = 0.15757822\n",
      "Iteration 395, loss = 0.15758018\n",
      "Iteration 396, loss = 0.15734297\n",
      "Iteration 397, loss = 0.15738045\n",
      "Iteration 398, loss = 0.15734690\n",
      "Iteration 399, loss = 0.15725983\n",
      "Iteration 400, loss = 0.15733370\n",
      "Iteration 401, loss = 0.15717100\n",
      "Iteration 402, loss = 0.15704612\n",
      "Iteration 403, loss = 0.15697699\n",
      "Iteration 404, loss = 0.15700878\n",
      "Iteration 405, loss = 0.15694905\n",
      "Iteration 406, loss = 0.15685759\n",
      "Iteration 407, loss = 0.15675167\n",
      "Iteration 408, loss = 0.15703643\n",
      "Iteration 409, loss = 0.15687177\n",
      "Iteration 410, loss = 0.15661230\n",
      "Iteration 411, loss = 0.15681276\n",
      "Iteration 412, loss = 0.15702582\n",
      "Iteration 413, loss = 0.15690811\n",
      "Iteration 414, loss = 0.15673606\n",
      "Iteration 415, loss = 0.15663099\n",
      "Iteration 416, loss = 0.15643758\n",
      "Iteration 417, loss = 0.15607302\n",
      "Iteration 418, loss = 0.15612872\n",
      "Iteration 419, loss = 0.15610344\n",
      "Iteration 420, loss = 0.15592660\n",
      "Iteration 421, loss = 0.15591783\n",
      "Iteration 422, loss = 0.15577653\n",
      "Iteration 423, loss = 0.15577730\n",
      "Iteration 424, loss = 0.15569629\n",
      "Iteration 425, loss = 0.15565312\n",
      "Iteration 426, loss = 0.15556314\n",
      "Iteration 427, loss = 0.15551769\n",
      "Iteration 428, loss = 0.15552692\n",
      "Iteration 429, loss = 0.15564596\n",
      "Iteration 430, loss = 0.15538373\n",
      "Iteration 431, loss = 0.15515230\n",
      "Iteration 432, loss = 0.15528841\n",
      "Iteration 433, loss = 0.15516802\n",
      "Iteration 434, loss = 0.15517448\n",
      "Iteration 435, loss = 0.15510763\n",
      "Iteration 436, loss = 0.15510931\n",
      "Iteration 437, loss = 0.15503967\n",
      "Iteration 438, loss = 0.15502667\n",
      "Iteration 439, loss = 0.15496217\n",
      "Iteration 440, loss = 0.15501248\n",
      "Iteration 441, loss = 0.15496573\n",
      "Iteration 442, loss = 0.15499674\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.940000\n",
      "Training set loss: 0.154997\n",
      "training: constant with Nesterov's momentum\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.69173802\n",
      "Iteration 3, loss = 0.68906067\n",
      "Iteration 4, loss = 0.68782808\n",
      "Iteration 5, loss = 0.68736262\n",
      "Iteration 6, loss = 0.68681760\n",
      "Iteration 7, loss = 0.68599970\n",
      "Iteration 8, loss = 0.68490591\n",
      "Iteration 9, loss = 0.68380315\n",
      "Iteration 10, loss = 0.68295050\n",
      "Iteration 11, loss = 0.68229443\n",
      "Iteration 12, loss = 0.68166351\n",
      "Iteration 13, loss = 0.68097175\n",
      "Iteration 14, loss = 0.68019827\n",
      "Iteration 15, loss = 0.67935126\n",
      "Iteration 16, loss = 0.67848266\n",
      "Iteration 17, loss = 0.67756005\n",
      "Iteration 18, loss = 0.67658273\n",
      "Iteration 19, loss = 0.67558684\n",
      "Iteration 20, loss = 0.67456970\n",
      "Iteration 21, loss = 0.67353667\n",
      "Iteration 22, loss = 0.67241696\n",
      "Iteration 23, loss = 0.67115122\n",
      "Iteration 24, loss = 0.66987799\n",
      "Iteration 25, loss = 0.66859195\n",
      "Iteration 26, loss = 0.66733302\n",
      "Iteration 27, loss = 0.66602275\n",
      "Iteration 28, loss = 0.66468451\n",
      "Iteration 29, loss = 0.66331687\n",
      "Iteration 30, loss = 0.66188298\n",
      "Iteration 31, loss = 0.66037689\n",
      "Iteration 32, loss = 0.65881210\n",
      "Iteration 33, loss = 0.65718414\n",
      "Iteration 34, loss = 0.65549072\n",
      "Iteration 35, loss = 0.65373016\n",
      "Iteration 36, loss = 0.65189545\n",
      "Iteration 37, loss = 0.64999224\n",
      "Iteration 38, loss = 0.64801478\n",
      "Iteration 39, loss = 0.64595046\n",
      "Iteration 40, loss = 0.64381536\n",
      "Iteration 41, loss = 0.64160699\n",
      "Iteration 42, loss = 0.63931247\n",
      "Iteration 43, loss = 0.63694298\n",
      "Iteration 44, loss = 0.63450770\n",
      "Iteration 45, loss = 0.63198530\n",
      "Iteration 46, loss = 0.62937217\n",
      "Iteration 47, loss = 0.62666961\n",
      "Iteration 48, loss = 0.62387639\n",
      "Iteration 49, loss = 0.62099246\n",
      "Iteration 50, loss = 0.61801954\n",
      "Iteration 51, loss = 0.61494151\n",
      "Iteration 52, loss = 0.61177170\n",
      "Iteration 53, loss = 0.60854927\n",
      "Iteration 54, loss = 0.60524125\n",
      "Iteration 55, loss = 0.60183721\n",
      "Iteration 56, loss = 0.59834539\n",
      "Iteration 57, loss = 0.59475664\n",
      "Iteration 58, loss = 0.59109045\n",
      "Iteration 59, loss = 0.58731661\n",
      "Iteration 60, loss = 0.58343229\n",
      "Iteration 61, loss = 0.57941250\n",
      "Iteration 62, loss = 0.57525996\n",
      "Iteration 63, loss = 0.57097848\n",
      "Iteration 64, loss = 0.56662284\n",
      "Iteration 65, loss = 0.56216462\n",
      "Iteration 66, loss = 0.55770116\n",
      "Iteration 67, loss = 0.55323621\n",
      "Iteration 68, loss = 0.54871800\n",
      "Iteration 69, loss = 0.54414745\n",
      "Iteration 70, loss = 0.53952174\n",
      "Iteration 71, loss = 0.53480248\n",
      "Iteration 72, loss = 0.53001095\n",
      "Iteration 73, loss = 0.52515524\n",
      "Iteration 74, loss = 0.52019710\n",
      "Iteration 75, loss = 0.51518168\n",
      "Iteration 76, loss = 0.51011718\n",
      "Iteration 77, loss = 0.50502782\n",
      "Iteration 78, loss = 0.49994509\n",
      "Iteration 79, loss = 0.49485587\n",
      "Iteration 80, loss = 0.48973417\n",
      "Iteration 81, loss = 0.48459318\n",
      "Iteration 82, loss = 0.47945032\n",
      "Iteration 83, loss = 0.47430526\n",
      "Iteration 84, loss = 0.46916010\n",
      "Iteration 85, loss = 0.46401231\n",
      "Iteration 86, loss = 0.45887544\n",
      "Iteration 87, loss = 0.45370061\n",
      "Iteration 88, loss = 0.44847884\n",
      "Iteration 89, loss = 0.44323363\n",
      "Iteration 90, loss = 0.43805752\n",
      "Iteration 91, loss = 0.43288857\n",
      "Iteration 92, loss = 0.42777136\n",
      "Iteration 93, loss = 0.42260491\n",
      "Iteration 94, loss = 0.41752539\n",
      "Iteration 95, loss = 0.41251858\n",
      "Iteration 96, loss = 0.40756751\n",
      "Iteration 97, loss = 0.40266453\n",
      "Iteration 98, loss = 0.39776936\n",
      "Iteration 99, loss = 0.39295812\n",
      "Iteration 100, loss = 0.38823416\n",
      "Iteration 101, loss = 0.38350995\n",
      "Iteration 102, loss = 0.37890033\n",
      "Iteration 103, loss = 0.37441033\n",
      "Iteration 104, loss = 0.36997560\n",
      "Iteration 105, loss = 0.36563508\n",
      "Iteration 106, loss = 0.36134890\n",
      "Iteration 107, loss = 0.35718773\n",
      "Iteration 108, loss = 0.35310259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 109, loss = 0.34908349\n",
      "Iteration 110, loss = 0.34517294\n",
      "Iteration 111, loss = 0.34131316\n",
      "Iteration 112, loss = 0.33755062\n",
      "Iteration 113, loss = 0.33389104\n",
      "Iteration 114, loss = 0.33027592\n",
      "Iteration 115, loss = 0.32676751\n",
      "Iteration 116, loss = 0.32332128\n",
      "Iteration 117, loss = 0.32000193\n",
      "Iteration 118, loss = 0.31670188\n",
      "Iteration 119, loss = 0.31347626\n",
      "Iteration 120, loss = 0.31035311\n",
      "Iteration 121, loss = 0.30727461\n",
      "Iteration 122, loss = 0.30432304\n",
      "Iteration 123, loss = 0.30140117\n",
      "Iteration 124, loss = 0.29855491\n",
      "Iteration 125, loss = 0.29579765\n",
      "Iteration 126, loss = 0.29315624\n",
      "Iteration 127, loss = 0.29051986\n",
      "Iteration 128, loss = 0.28796467\n",
      "Iteration 129, loss = 0.28549494\n",
      "Iteration 130, loss = 0.28305040\n",
      "Iteration 131, loss = 0.28069839\n",
      "Iteration 132, loss = 0.27838564\n",
      "Iteration 133, loss = 0.27612516\n",
      "Iteration 134, loss = 0.27391360\n",
      "Iteration 135, loss = 0.27178094\n",
      "Iteration 136, loss = 0.26971548\n",
      "Iteration 137, loss = 0.26769828\n",
      "Iteration 138, loss = 0.26570435\n",
      "Iteration 139, loss = 0.26379608\n",
      "Iteration 140, loss = 0.26191706\n",
      "Iteration 141, loss = 0.26009916\n",
      "Iteration 142, loss = 0.25835286\n",
      "Iteration 143, loss = 0.25654418\n",
      "Iteration 144, loss = 0.25486990\n",
      "Iteration 145, loss = 0.25317768\n",
      "Iteration 146, loss = 0.25159170\n",
      "Iteration 147, loss = 0.24995495\n",
      "Iteration 148, loss = 0.24838164\n",
      "Iteration 149, loss = 0.24689022\n",
      "Iteration 150, loss = 0.24546711\n",
      "Iteration 151, loss = 0.24400475\n",
      "Iteration 152, loss = 0.24259025\n",
      "Iteration 153, loss = 0.24110862\n",
      "Iteration 154, loss = 0.23971364\n",
      "Iteration 155, loss = 0.23826495\n",
      "Iteration 156, loss = 0.23678748\n",
      "Iteration 157, loss = 0.23536536\n",
      "Iteration 158, loss = 0.23400577\n",
      "Iteration 159, loss = 0.23269808\n",
      "Iteration 160, loss = 0.23130988\n",
      "Iteration 161, loss = 0.23006836\n",
      "Iteration 162, loss = 0.22879904\n",
      "Iteration 163, loss = 0.22751084\n",
      "Iteration 164, loss = 0.22628483\n",
      "Iteration 165, loss = 0.22504035\n",
      "Iteration 166, loss = 0.22383057\n",
      "Iteration 167, loss = 0.22262491\n",
      "Iteration 168, loss = 0.22149707\n",
      "Iteration 169, loss = 0.22030758\n",
      "Iteration 170, loss = 0.21924145\n",
      "Iteration 171, loss = 0.21812587\n",
      "Iteration 172, loss = 0.21713841\n",
      "Iteration 173, loss = 0.21606707\n",
      "Iteration 174, loss = 0.21520247\n",
      "Iteration 175, loss = 0.21431701\n",
      "Iteration 176, loss = 0.21349515\n",
      "Iteration 177, loss = 0.21252514\n",
      "Iteration 178, loss = 0.21160283\n",
      "Iteration 179, loss = 0.21067327\n",
      "Iteration 180, loss = 0.20979881\n",
      "Iteration 181, loss = 0.20895989\n",
      "Iteration 182, loss = 0.20809523\n",
      "Iteration 183, loss = 0.20724136\n",
      "Iteration 184, loss = 0.20646827\n",
      "Iteration 185, loss = 0.20570401\n",
      "Iteration 186, loss = 0.20499483\n",
      "Iteration 187, loss = 0.20415998\n",
      "Iteration 188, loss = 0.20340569\n",
      "Iteration 189, loss = 0.20276711\n",
      "Iteration 190, loss = 0.20213960\n",
      "Iteration 191, loss = 0.20136617\n",
      "Iteration 192, loss = 0.20089424\n",
      "Iteration 193, loss = 0.20011760\n",
      "Iteration 194, loss = 0.19964964\n",
      "Iteration 195, loss = 0.19909254\n",
      "Iteration 196, loss = 0.19827786\n",
      "Iteration 197, loss = 0.19760754\n",
      "Iteration 198, loss = 0.19707071\n",
      "Iteration 199, loss = 0.19635291\n",
      "Iteration 200, loss = 0.19587457\n",
      "Iteration 201, loss = 0.19528503\n",
      "Iteration 202, loss = 0.19493650\n",
      "Iteration 203, loss = 0.19429689\n",
      "Iteration 204, loss = 0.19377511\n",
      "Iteration 205, loss = 0.19312523\n",
      "Iteration 206, loss = 0.19268530\n",
      "Iteration 207, loss = 0.19213861\n",
      "Iteration 208, loss = 0.19170370\n",
      "Iteration 209, loss = 0.19121187\n",
      "Iteration 210, loss = 0.19079601\n",
      "Iteration 211, loss = 0.19028556\n",
      "Iteration 212, loss = 0.18988270\n",
      "Iteration 213, loss = 0.18942631\n",
      "Iteration 214, loss = 0.18931289\n",
      "Iteration 215, loss = 0.18877305\n",
      "Iteration 216, loss = 0.18832419\n",
      "Iteration 217, loss = 0.18771364\n",
      "Iteration 218, loss = 0.18721596\n",
      "Iteration 219, loss = 0.18683538\n",
      "Iteration 220, loss = 0.18636480\n",
      "Iteration 221, loss = 0.18607229\n",
      "Iteration 222, loss = 0.18571599\n",
      "Iteration 223, loss = 0.18527772\n",
      "Iteration 224, loss = 0.18491353\n",
      "Iteration 225, loss = 0.18456052\n",
      "Iteration 226, loss = 0.18428256\n",
      "Iteration 227, loss = 0.18414733\n",
      "Iteration 228, loss = 0.18397518\n",
      "Iteration 229, loss = 0.18336261\n",
      "Iteration 230, loss = 0.18292011\n",
      "Iteration 231, loss = 0.18252757\n",
      "Iteration 232, loss = 0.18215525\n",
      "Iteration 233, loss = 0.18183785\n",
      "Iteration 234, loss = 0.18162209\n",
      "Iteration 235, loss = 0.18124520\n",
      "Iteration 236, loss = 0.18103344\n",
      "Iteration 237, loss = 0.18077650\n",
      "Iteration 238, loss = 0.18060855\n",
      "Iteration 239, loss = 0.18012441\n",
      "Iteration 240, loss = 0.17964988\n",
      "Iteration 241, loss = 0.17949288\n",
      "Iteration 242, loss = 0.17912725\n",
      "Iteration 243, loss = 0.17867515\n",
      "Iteration 244, loss = 0.17835869\n",
      "Iteration 245, loss = 0.17794511\n",
      "Iteration 246, loss = 0.17754737\n",
      "Iteration 247, loss = 0.17727521\n",
      "Iteration 248, loss = 0.17698122\n",
      "Iteration 249, loss = 0.17659984\n",
      "Iteration 250, loss = 0.17645962\n",
      "Iteration 251, loss = 0.17599206\n",
      "Iteration 252, loss = 0.17587993\n",
      "Iteration 253, loss = 0.17545404\n",
      "Iteration 254, loss = 0.17525530\n",
      "Iteration 255, loss = 0.17494266\n",
      "Iteration 256, loss = 0.17470993\n",
      "Iteration 257, loss = 0.17434751\n",
      "Iteration 258, loss = 0.17419585\n",
      "Iteration 259, loss = 0.17387097\n",
      "Iteration 260, loss = 0.17368451\n",
      "Iteration 261, loss = 0.17342704\n",
      "Iteration 262, loss = 0.17322669\n",
      "Iteration 263, loss = 0.17299067\n",
      "Iteration 264, loss = 0.17264461\n",
      "Iteration 265, loss = 0.17236001\n",
      "Iteration 266, loss = 0.17225349\n",
      "Iteration 267, loss = 0.17203361\n",
      "Iteration 268, loss = 0.17189219\n",
      "Iteration 269, loss = 0.17150200\n",
      "Iteration 270, loss = 0.17131737\n",
      "Iteration 271, loss = 0.17101383\n",
      "Iteration 272, loss = 0.17086317\n",
      "Iteration 273, loss = 0.17036227\n",
      "Iteration 274, loss = 0.17015743\n",
      "Iteration 275, loss = 0.16981254\n",
      "Iteration 276, loss = 0.16960982\n",
      "Iteration 277, loss = 0.16932377\n",
      "Iteration 278, loss = 0.16918931\n",
      "Iteration 279, loss = 0.16906647\n",
      "Iteration 280, loss = 0.16883940\n",
      "Iteration 281, loss = 0.16858475\n",
      "Iteration 282, loss = 0.16848918\n",
      "Iteration 283, loss = 0.16817358\n",
      "Iteration 284, loss = 0.16780860\n",
      "Iteration 285, loss = 0.16763283\n",
      "Iteration 286, loss = 0.16751332\n",
      "Iteration 287, loss = 0.16730346\n",
      "Iteration 288, loss = 0.16707045\n",
      "Iteration 289, loss = 0.16688564\n",
      "Iteration 290, loss = 0.16657101\n",
      "Iteration 291, loss = 0.16635597\n",
      "Iteration 292, loss = 0.16604319\n",
      "Iteration 293, loss = 0.16589797\n",
      "Iteration 294, loss = 0.16565564\n",
      "Iteration 295, loss = 0.16539219\n",
      "Iteration 296, loss = 0.16530551\n",
      "Iteration 297, loss = 0.16501025\n",
      "Iteration 298, loss = 0.16474626\n",
      "Iteration 299, loss = 0.16466885\n",
      "Iteration 300, loss = 0.16443101\n",
      "Iteration 301, loss = 0.16423010\n",
      "Iteration 302, loss = 0.16412491\n",
      "Iteration 303, loss = 0.16395920\n",
      "Iteration 304, loss = 0.16388488\n",
      "Iteration 305, loss = 0.16369722\n",
      "Iteration 306, loss = 0.16335329\n",
      "Iteration 307, loss = 0.16327809\n",
      "Iteration 308, loss = 0.16304602\n",
      "Iteration 309, loss = 0.16299264\n",
      "Iteration 310, loss = 0.16314702\n",
      "Iteration 311, loss = 0.16301242\n",
      "Iteration 312, loss = 0.16275554\n",
      "Iteration 313, loss = 0.16254634\n",
      "Iteration 314, loss = 0.16224110\n",
      "Iteration 315, loss = 0.16226044\n",
      "Iteration 316, loss = 0.16253783\n",
      "Iteration 317, loss = 0.16245245\n",
      "Iteration 318, loss = 0.16210045\n",
      "Iteration 319, loss = 0.16174295\n",
      "Iteration 320, loss = 0.16172888\n",
      "Iteration 321, loss = 0.16145982\n",
      "Iteration 322, loss = 0.16161695\n",
      "Iteration 323, loss = 0.16145593\n",
      "Iteration 324, loss = 0.16144188\n",
      "Iteration 325, loss = 0.16127403\n",
      "Iteration 326, loss = 0.16108659\n",
      "Iteration 327, loss = 0.16102223\n",
      "Iteration 328, loss = 0.16098560\n",
      "Iteration 329, loss = 0.16091065\n",
      "Iteration 330, loss = 0.16078031\n",
      "Iteration 331, loss = 0.16073266\n",
      "Iteration 332, loss = 0.16047712\n",
      "Iteration 333, loss = 0.16017375\n",
      "Iteration 334, loss = 0.16015466\n",
      "Iteration 335, loss = 0.16006269\n",
      "Iteration 336, loss = 0.15978348\n",
      "Iteration 337, loss = 0.15981617\n",
      "Iteration 338, loss = 0.15973677\n",
      "Iteration 339, loss = 0.15950787\n",
      "Iteration 340, loss = 0.15950495\n",
      "Iteration 341, loss = 0.15920358\n",
      "Iteration 342, loss = 0.15916684\n",
      "Iteration 343, loss = 0.15929049\n",
      "Iteration 344, loss = 0.15903509\n",
      "Iteration 345, loss = 0.15891386\n",
      "Iteration 346, loss = 0.15883774\n",
      "Iteration 347, loss = 0.15881492\n",
      "Iteration 348, loss = 0.15864655\n",
      "Iteration 349, loss = 0.15879495\n",
      "Iteration 350, loss = 0.15890342\n",
      "Iteration 351, loss = 0.15843890\n",
      "Iteration 352, loss = 0.15855653\n",
      "Iteration 353, loss = 0.15830247\n",
      "Iteration 354, loss = 0.15819351\n",
      "Iteration 355, loss = 0.15810248\n",
      "Iteration 356, loss = 0.15789648\n",
      "Iteration 357, loss = 0.15770931\n",
      "Iteration 358, loss = 0.15755008\n",
      "Iteration 359, loss = 0.15737272\n",
      "Iteration 360, loss = 0.15733105\n",
      "Iteration 361, loss = 0.15724268\n",
      "Iteration 362, loss = 0.15715757\n",
      "Iteration 363, loss = 0.15722102\n",
      "Iteration 364, loss = 0.15708946\n",
      "Iteration 365, loss = 0.15705175\n",
      "Iteration 366, loss = 0.15688825\n",
      "Iteration 367, loss = 0.15718391\n",
      "Iteration 368, loss = 0.15723099\n",
      "Iteration 369, loss = 0.15711337\n",
      "Iteration 370, loss = 0.15687288\n",
      "Iteration 371, loss = 0.15637070\n",
      "Iteration 372, loss = 0.15627848\n",
      "Iteration 373, loss = 0.15640037\n",
      "Iteration 374, loss = 0.15623785\n",
      "Iteration 375, loss = 0.15649189\n",
      "Iteration 376, loss = 0.15638476\n",
      "Iteration 377, loss = 0.15587490\n",
      "Iteration 378, loss = 0.15581593\n",
      "Iteration 379, loss = 0.15579484\n",
      "Iteration 380, loss = 0.15570650\n",
      "Iteration 381, loss = 0.15555498\n",
      "Iteration 382, loss = 0.15556966\n",
      "Iteration 383, loss = 0.15541657\n",
      "Iteration 384, loss = 0.15536625\n",
      "Iteration 385, loss = 0.15528000\n",
      "Iteration 386, loss = 0.15521452\n",
      "Iteration 387, loss = 0.15517159\n",
      "Iteration 388, loss = 0.15508000\n",
      "Iteration 389, loss = 0.15497784\n",
      "Iteration 390, loss = 0.15506279\n",
      "Iteration 391, loss = 0.15485289\n",
      "Iteration 392, loss = 0.15475680\n",
      "Iteration 393, loss = 0.15472825\n",
      "Iteration 394, loss = 0.15468563\n",
      "Iteration 395, loss = 0.15479197\n",
      "Iteration 396, loss = 0.15456967\n",
      "Iteration 397, loss = 0.15460937\n",
      "Iteration 398, loss = 0.15449091\n",
      "Iteration 399, loss = 0.15447440\n",
      "Iteration 400, loss = 0.15445298\n",
      "Iteration 401, loss = 0.15457471\n",
      "Iteration 402, loss = 0.15479725\n",
      "Iteration 403, loss = 0.15472011\n",
      "Iteration 404, loss = 0.15455205\n",
      "Iteration 405, loss = 0.15416715\n",
      "Iteration 406, loss = 0.15412853\n",
      "Iteration 407, loss = 0.15417143\n",
      "Iteration 408, loss = 0.15416945\n",
      "Iteration 409, loss = 0.15398501\n",
      "Iteration 410, loss = 0.15430932\n",
      "Iteration 411, loss = 0.15403809\n",
      "Iteration 412, loss = 0.15406628\n",
      "Iteration 413, loss = 0.15388748\n",
      "Iteration 414, loss = 0.15375729\n",
      "Iteration 415, loss = 0.15378853\n",
      "Iteration 416, loss = 0.15372453\n",
      "Iteration 417, loss = 0.15346981\n",
      "Iteration 418, loss = 0.15370411\n",
      "Iteration 419, loss = 0.15346293\n",
      "Iteration 420, loss = 0.15349659\n",
      "Iteration 421, loss = 0.15345920\n",
      "Iteration 422, loss = 0.15324202\n",
      "Iteration 423, loss = 0.15313755\n",
      "Iteration 424, loss = 0.15327467\n",
      "Iteration 425, loss = 0.15307070\n",
      "Iteration 426, loss = 0.15323286\n",
      "Iteration 427, loss = 0.15338072\n",
      "Iteration 428, loss = 0.15318118\n",
      "Iteration 429, loss = 0.15304240\n",
      "Iteration 430, loss = 0.15315122\n",
      "Iteration 431, loss = 0.15290180\n",
      "Iteration 432, loss = 0.15292630\n",
      "Iteration 433, loss = 0.15275007\n",
      "Iteration 434, loss = 0.15266756\n",
      "Iteration 435, loss = 0.15255054\n",
      "Iteration 436, loss = 0.15261472\n",
      "Iteration 437, loss = 0.15282330\n",
      "Iteration 438, loss = 0.15259034\n",
      "Iteration 439, loss = 0.15272401\n",
      "Iteration 440, loss = 0.15266677\n",
      "Iteration 441, loss = 0.15247047\n",
      "Iteration 442, loss = 0.15253325\n",
      "Iteration 443, loss = 0.15249992\n",
      "Iteration 444, loss = 0.15218305\n",
      "Iteration 445, loss = 0.15212663\n",
      "Iteration 446, loss = 0.15220442\n",
      "Iteration 447, loss = 0.15231078\n",
      "Iteration 448, loss = 0.15211688\n",
      "Iteration 449, loss = 0.15234023\n",
      "Iteration 450, loss = 0.15204521\n",
      "Iteration 451, loss = 0.15204252\n",
      "Iteration 452, loss = 0.15214002\n",
      "Iteration 453, loss = 0.15187156\n",
      "Iteration 454, loss = 0.15199468\n",
      "Iteration 455, loss = 0.15167028\n",
      "Iteration 456, loss = 0.15164226\n",
      "Iteration 457, loss = 0.15179472\n",
      "Iteration 458, loss = 0.15177925\n",
      "Iteration 459, loss = 0.15193631\n",
      "Iteration 460, loss = 0.15173451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 461, loss = 0.15150839\n",
      "Iteration 462, loss = 0.15150750\n",
      "Iteration 463, loss = 0.15142518\n",
      "Iteration 464, loss = 0.15139679\n",
      "Iteration 465, loss = 0.15163926\n",
      "Iteration 466, loss = 0.15143777\n",
      "Iteration 467, loss = 0.15126184\n",
      "Iteration 468, loss = 0.15126558\n",
      "Iteration 469, loss = 0.15147071\n",
      "Iteration 470, loss = 0.15117556\n",
      "Iteration 471, loss = 0.15120016\n",
      "Iteration 472, loss = 0.15126179\n",
      "Iteration 473, loss = 0.15107650\n",
      "Iteration 474, loss = 0.15112809\n",
      "Iteration 475, loss = 0.15108677\n",
      "Iteration 476, loss = 0.15106361\n",
      "Iteration 477, loss = 0.15104602\n",
      "Iteration 478, loss = 0.15094185\n",
      "Iteration 479, loss = 0.15083350\n",
      "Iteration 480, loss = 0.15090645\n",
      "Iteration 481, loss = 0.15074937\n",
      "Iteration 482, loss = 0.15096710\n",
      "Iteration 483, loss = 0.15072933\n",
      "Iteration 484, loss = 0.15084782\n",
      "Iteration 485, loss = 0.15064565\n",
      "Iteration 486, loss = 0.15072982\n",
      "Iteration 487, loss = 0.15070215\n",
      "Iteration 488, loss = 0.15067579\n",
      "Iteration 489, loss = 0.15086284\n",
      "Iteration 490, loss = 0.15063008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.940000\n",
      "Training set loss: 0.150630\n",
      "training: inv-scaling learning-rate\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.69336674\n",
      "Iteration 3, loss = 0.69320289\n",
      "Iteration 4, loss = 0.69309010\n",
      "Iteration 5, loss = 0.69299958\n",
      "Iteration 6, loss = 0.69292251\n",
      "Iteration 7, loss = 0.69285482\n",
      "Iteration 8, loss = 0.69279383\n",
      "Iteration 9, loss = 0.69273791\n",
      "Iteration 10, loss = 0.69268593\n",
      "Iteration 11, loss = 0.69263727\n",
      "Iteration 12, loss = 0.69259187\n",
      "Iteration 13, loss = 0.69254902\n",
      "Iteration 14, loss = 0.69250834\n",
      "Iteration 15, loss = 0.69246955\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.500000\n",
      "Training set loss: 0.692470\n",
      "training: inv-scaling with momentum\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.69173802\n",
      "Iteration 3, loss = 0.69037556\n",
      "Iteration 4, loss = 0.68942003\n",
      "Iteration 5, loss = 0.68880596\n",
      "Iteration 6, loss = 0.68843261\n",
      "Iteration 7, loss = 0.68825649\n",
      "Iteration 8, loss = 0.68821788\n",
      "Iteration 9, loss = 0.68827008\n",
      "Iteration 10, loss = 0.68837637\n",
      "Iteration 11, loss = 0.68850624\n",
      "Iteration 12, loss = 0.68864765\n",
      "Iteration 13, loss = 0.68878599\n",
      "Iteration 14, loss = 0.68890520\n",
      "Iteration 15, loss = 0.68900227\n",
      "Iteration 16, loss = 0.68907489\n",
      "Iteration 17, loss = 0.68912003\n",
      "Iteration 18, loss = 0.68914266\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.500000\n",
      "Training set loss: 0.689143\n",
      "training: inv-scaling with Nesterov's momentum\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.69336674\n",
      "Iteration 3, loss = 0.69160252\n",
      "Iteration 4, loss = 0.69028294\n",
      "Iteration 5, loss = 0.68934700\n",
      "Iteration 6, loss = 0.68875607\n",
      "Iteration 7, loss = 0.68842048\n",
      "Iteration 8, loss = 0.68829201\n",
      "Iteration 9, loss = 0.68830931\n",
      "Iteration 10, loss = 0.68842455\n",
      "Iteration 11, loss = 0.68859263\n",
      "Iteration 12, loss = 0.68878581\n",
      "Iteration 13, loss = 0.68899178\n",
      "Iteration 14, loss = 0.68918736\n",
      "Iteration 15, loss = 0.68936029\n",
      "Iteration 16, loss = 0.68950674\n",
      "Iteration 17, loss = 0.68961951\n",
      "Iteration 18, loss = 0.68970133\n",
      "Iteration 19, loss = 0.68975064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.500000\n",
      "Training set loss: 0.689751\n",
      "training: adam\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.68795907\n",
      "Iteration 3, loss = 0.68701339\n",
      "Iteration 4, loss = 0.68539628\n",
      "Iteration 5, loss = 0.68238095\n",
      "Iteration 6, loss = 0.68001512\n",
      "Iteration 7, loss = 0.67895101\n",
      "Iteration 8, loss = 0.67820336\n",
      "Iteration 9, loss = 0.67669839\n",
      "Iteration 10, loss = 0.67467854\n",
      "Iteration 11, loss = 0.67264650\n",
      "Iteration 12, loss = 0.67092944\n",
      "Iteration 13, loss = 0.66940057\n",
      "Iteration 14, loss = 0.66765192\n",
      "Iteration 15, loss = 0.66549001\n",
      "Iteration 16, loss = 0.66313907\n",
      "Iteration 17, loss = 0.66087438\n",
      "Iteration 18, loss = 0.65867068\n",
      "Iteration 19, loss = 0.65631409\n",
      "Iteration 20, loss = 0.65368460\n",
      "Iteration 21, loss = 0.65087252\n",
      "Iteration 22, loss = 0.64803663\n",
      "Iteration 23, loss = 0.64519251\n",
      "Iteration 24, loss = 0.64222768\n",
      "Iteration 25, loss = 0.63902649\n",
      "Iteration 26, loss = 0.63558805\n",
      "Iteration 27, loss = 0.63211882\n",
      "Iteration 28, loss = 0.62856781\n",
      "Iteration 29, loss = 0.62487401\n",
      "Iteration 30, loss = 0.62101237\n",
      "Iteration 31, loss = 0.61696918\n",
      "Iteration 32, loss = 0.61281824\n",
      "Iteration 33, loss = 0.60848754\n",
      "Iteration 34, loss = 0.60393867\n",
      "Iteration 35, loss = 0.59918875\n",
      "Iteration 36, loss = 0.59435480\n",
      "Iteration 37, loss = 0.58957146\n",
      "Iteration 38, loss = 0.58471825\n",
      "Iteration 39, loss = 0.57974638\n",
      "Iteration 40, loss = 0.57465448\n",
      "Iteration 41, loss = 0.56942429\n",
      "Iteration 42, loss = 0.56408204\n",
      "Iteration 43, loss = 0.55860873\n",
      "Iteration 44, loss = 0.55300221\n",
      "Iteration 45, loss = 0.54729984\n",
      "Iteration 46, loss = 0.54151386\n",
      "Iteration 47, loss = 0.53560947\n",
      "Iteration 48, loss = 0.52959541\n",
      "Iteration 49, loss = 0.52351080\n",
      "Iteration 50, loss = 0.51738085\n",
      "Iteration 51, loss = 0.51124102\n",
      "Iteration 52, loss = 0.50507499\n",
      "Iteration 53, loss = 0.49888673\n",
      "Iteration 54, loss = 0.49269272\n",
      "Iteration 55, loss = 0.48645309\n",
      "Iteration 56, loss = 0.48020065\n",
      "Iteration 57, loss = 0.47393289\n",
      "Iteration 58, loss = 0.46766942\n",
      "Iteration 59, loss = 0.46139991\n",
      "Iteration 60, loss = 0.45512125\n",
      "Iteration 61, loss = 0.44887953\n",
      "Iteration 62, loss = 0.44268392\n",
      "Iteration 63, loss = 0.43656572\n",
      "Iteration 64, loss = 0.43048247\n",
      "Iteration 65, loss = 0.42443623\n",
      "Iteration 66, loss = 0.41842673\n",
      "Iteration 67, loss = 0.41253272\n",
      "Iteration 68, loss = 0.40667597\n",
      "Iteration 69, loss = 0.40089791\n",
      "Iteration 70, loss = 0.39519206\n",
      "Iteration 71, loss = 0.38959107\n",
      "Iteration 72, loss = 0.38405050\n",
      "Iteration 73, loss = 0.37859593\n",
      "Iteration 74, loss = 0.37321082\n",
      "Iteration 75, loss = 0.36790373\n",
      "Iteration 76, loss = 0.36267413\n",
      "Iteration 77, loss = 0.35749738\n",
      "Iteration 78, loss = 0.35246536\n",
      "Iteration 79, loss = 0.34757870\n",
      "Iteration 80, loss = 0.34282694\n",
      "Iteration 81, loss = 0.33812181\n",
      "Iteration 82, loss = 0.33352066\n",
      "Iteration 83, loss = 0.32901617\n",
      "Iteration 84, loss = 0.32459877\n",
      "Iteration 85, loss = 0.32031104\n",
      "Iteration 86, loss = 0.31603872\n",
      "Iteration 87, loss = 0.31180232\n",
      "Iteration 88, loss = 0.30776305\n",
      "Iteration 89, loss = 0.30395738\n",
      "Iteration 90, loss = 0.30030775\n",
      "Iteration 91, loss = 0.29677718\n",
      "Iteration 92, loss = 0.29323544\n",
      "Iteration 93, loss = 0.28979111\n",
      "Iteration 94, loss = 0.28646098\n",
      "Iteration 95, loss = 0.28317303\n",
      "Iteration 96, loss = 0.28003165\n",
      "Iteration 97, loss = 0.27703033\n",
      "Iteration 98, loss = 0.27412458\n",
      "Iteration 99, loss = 0.27133273\n",
      "Iteration 100, loss = 0.26853852\n",
      "Iteration 101, loss = 0.26582938\n",
      "Iteration 102, loss = 0.26325000\n",
      "Iteration 103, loss = 0.26078144\n",
      "Iteration 104, loss = 0.25834108\n",
      "Iteration 105, loss = 0.25594374\n",
      "Iteration 106, loss = 0.25366556\n",
      "Iteration 107, loss = 0.25145191\n",
      "Iteration 108, loss = 0.24933660\n",
      "Iteration 109, loss = 0.24728527\n",
      "Iteration 110, loss = 0.24529232\n",
      "Iteration 111, loss = 0.24334672\n",
      "Iteration 112, loss = 0.24148423\n",
      "Iteration 113, loss = 0.23960658\n",
      "Iteration 114, loss = 0.23779512\n",
      "Iteration 115, loss = 0.23608345\n",
      "Iteration 116, loss = 0.23440067\n",
      "Iteration 117, loss = 0.23271517\n",
      "Iteration 118, loss = 0.23110520\n",
      "Iteration 119, loss = 0.22951459\n",
      "Iteration 120, loss = 0.22805924\n",
      "Iteration 121, loss = 0.22661271\n",
      "Iteration 122, loss = 0.22511679\n",
      "Iteration 123, loss = 0.22370580\n",
      "Iteration 124, loss = 0.22242208\n",
      "Iteration 125, loss = 0.22111086\n",
      "Iteration 126, loss = 0.21981432\n",
      "Iteration 127, loss = 0.21853024\n",
      "Iteration 128, loss = 0.21734259\n",
      "Iteration 129, loss = 0.21617127\n",
      "Iteration 130, loss = 0.21499692\n",
      "Iteration 131, loss = 0.21386934\n",
      "Iteration 132, loss = 0.21280033\n",
      "Iteration 133, loss = 0.21171681\n",
      "Iteration 134, loss = 0.21065176\n",
      "Iteration 135, loss = 0.20961314\n",
      "Iteration 136, loss = 0.20864016\n",
      "Iteration 137, loss = 0.20768427\n",
      "Iteration 138, loss = 0.20684350\n",
      "Iteration 139, loss = 0.20591488\n",
      "Iteration 140, loss = 0.20492691\n",
      "Iteration 141, loss = 0.20412142\n",
      "Iteration 142, loss = 0.20334697\n",
      "Iteration 143, loss = 0.20248233\n",
      "Iteration 144, loss = 0.20164952\n",
      "Iteration 145, loss = 0.20083173\n",
      "Iteration 146, loss = 0.19999978\n",
      "Iteration 147, loss = 0.19922824\n",
      "Iteration 148, loss = 0.19850373\n",
      "Iteration 149, loss = 0.19779634\n",
      "Iteration 150, loss = 0.19707061\n",
      "Iteration 151, loss = 0.19637963\n",
      "Iteration 152, loss = 0.19564752\n",
      "Iteration 153, loss = 0.19500526\n",
      "Iteration 154, loss = 0.19434004\n",
      "Iteration 155, loss = 0.19371348\n",
      "Iteration 156, loss = 0.19307446\n",
      "Iteration 157, loss = 0.19230886\n",
      "Iteration 158, loss = 0.19172809\n",
      "Iteration 159, loss = 0.19115257\n",
      "Iteration 160, loss = 0.19051935\n",
      "Iteration 161, loss = 0.18986040\n",
      "Iteration 162, loss = 0.18920501\n",
      "Iteration 163, loss = 0.18863513\n",
      "Iteration 164, loss = 0.18814632\n",
      "Iteration 165, loss = 0.18750258\n",
      "Iteration 166, loss = 0.18688048\n",
      "Iteration 167, loss = 0.18634485\n",
      "Iteration 168, loss = 0.18583358\n",
      "Iteration 169, loss = 0.18527069\n",
      "Iteration 170, loss = 0.18470148\n",
      "Iteration 171, loss = 0.18412890\n",
      "Iteration 172, loss = 0.18358223\n",
      "Iteration 173, loss = 0.18307446\n",
      "Iteration 174, loss = 0.18253038\n",
      "Iteration 175, loss = 0.18199625\n",
      "Iteration 176, loss = 0.18152342\n",
      "Iteration 177, loss = 0.18111579\n",
      "Iteration 178, loss = 0.18068406\n",
      "Iteration 179, loss = 0.18026546\n",
      "Iteration 180, loss = 0.17988721\n",
      "Iteration 181, loss = 0.17951998\n",
      "Iteration 182, loss = 0.17905216\n",
      "Iteration 183, loss = 0.17867668\n",
      "Iteration 184, loss = 0.17833787\n",
      "Iteration 185, loss = 0.17787040\n",
      "Iteration 186, loss = 0.17749493\n",
      "Iteration 187, loss = 0.17720523\n",
      "Iteration 188, loss = 0.17680050\n",
      "Iteration 189, loss = 0.17649266\n",
      "Iteration 190, loss = 0.17616648\n",
      "Iteration 191, loss = 0.17593006\n",
      "Iteration 192, loss = 0.17562409\n",
      "Iteration 193, loss = 0.17520764\n",
      "Iteration 194, loss = 0.17480198\n",
      "Iteration 195, loss = 0.17452555\n",
      "Iteration 196, loss = 0.17422082\n",
      "Iteration 197, loss = 0.17390409\n",
      "Iteration 198, loss = 0.17361248\n",
      "Iteration 199, loss = 0.17328120\n",
      "Iteration 200, loss = 0.17308927\n",
      "Iteration 201, loss = 0.17278302\n",
      "Iteration 202, loss = 0.17256550\n",
      "Iteration 203, loss = 0.17227969\n",
      "Iteration 204, loss = 0.17189591\n",
      "Iteration 205, loss = 0.17164268\n",
      "Iteration 206, loss = 0.17143812\n",
      "Iteration 207, loss = 0.17118847\n",
      "Iteration 208, loss = 0.17089973\n",
      "Iteration 209, loss = 0.17057277\n",
      "Iteration 210, loss = 0.17038810\n",
      "Iteration 211, loss = 0.17020389\n",
      "Iteration 212, loss = 0.16991964\n",
      "Iteration 213, loss = 0.16962679\n",
      "Iteration 214, loss = 0.16954995\n",
      "Iteration 215, loss = 0.16927867\n",
      "Iteration 216, loss = 0.16890545\n",
      "Iteration 217, loss = 0.16872817\n",
      "Iteration 218, loss = 0.16858220\n",
      "Iteration 219, loss = 0.16838264\n",
      "Iteration 220, loss = 0.16802646\n",
      "Iteration 221, loss = 0.16781166\n",
      "Iteration 222, loss = 0.16766641\n",
      "Iteration 223, loss = 0.16743812\n",
      "Iteration 224, loss = 0.16730703\n",
      "Iteration 225, loss = 0.16712906\n",
      "Iteration 226, loss = 0.16700858\n",
      "Iteration 227, loss = 0.16686994\n",
      "Iteration 228, loss = 0.16664727\n",
      "Iteration 229, loss = 0.16646699\n",
      "Iteration 230, loss = 0.16624586\n",
      "Iteration 231, loss = 0.16606867\n",
      "Iteration 232, loss = 0.16591877\n",
      "Iteration 233, loss = 0.16567776\n",
      "Iteration 234, loss = 0.16551524\n",
      "Iteration 235, loss = 0.16538286\n",
      "Iteration 236, loss = 0.16522373\n",
      "Iteration 237, loss = 0.16499428\n",
      "Iteration 238, loss = 0.16477423\n",
      "Iteration 239, loss = 0.16472041\n",
      "Iteration 240, loss = 0.16450869\n",
      "Iteration 241, loss = 0.16434811\n",
      "Iteration 242, loss = 0.16424797\n",
      "Iteration 243, loss = 0.16407090\n",
      "Iteration 244, loss = 0.16393680\n",
      "Iteration 245, loss = 0.16370543\n",
      "Iteration 246, loss = 0.16364706\n",
      "Iteration 247, loss = 0.16351188\n",
      "Iteration 248, loss = 0.16334136\n",
      "Iteration 249, loss = 0.16312856\n",
      "Iteration 250, loss = 0.16315045\n",
      "Iteration 251, loss = 0.16307818\n",
      "Iteration 252, loss = 0.16285873\n",
      "Iteration 253, loss = 0.16273101\n",
      "Iteration 254, loss = 0.16270689\n",
      "Iteration 255, loss = 0.16268046\n",
      "Iteration 256, loss = 0.16249139\n",
      "Iteration 257, loss = 0.16229965\n",
      "Iteration 258, loss = 0.16213803\n",
      "Iteration 259, loss = 0.16209562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 260, loss = 0.16204044\n",
      "Iteration 261, loss = 0.16185007\n",
      "Iteration 262, loss = 0.16170159\n",
      "Iteration 263, loss = 0.16174021\n",
      "Iteration 264, loss = 0.16148711\n",
      "Iteration 265, loss = 0.16132589\n",
      "Iteration 266, loss = 0.16121277\n",
      "Iteration 267, loss = 0.16105084\n",
      "Iteration 268, loss = 0.16091855\n",
      "Iteration 269, loss = 0.16092187\n",
      "Iteration 270, loss = 0.16070058\n",
      "Iteration 271, loss = 0.16052372\n",
      "Iteration 272, loss = 0.16047888\n",
      "Iteration 273, loss = 0.16039852\n",
      "Iteration 274, loss = 0.16021266\n",
      "Iteration 275, loss = 0.16014344\n",
      "Iteration 276, loss = 0.16008025\n",
      "Iteration 277, loss = 0.15998833\n",
      "Iteration 278, loss = 0.15979132\n",
      "Iteration 279, loss = 0.15968349\n",
      "Iteration 280, loss = 0.15933471\n",
      "Iteration 281, loss = 0.15928555\n",
      "Iteration 282, loss = 0.15918738\n",
      "Iteration 283, loss = 0.15891605\n",
      "Iteration 284, loss = 0.15877286\n",
      "Iteration 285, loss = 0.15870763\n",
      "Iteration 286, loss = 0.15862100\n",
      "Iteration 287, loss = 0.15845317\n",
      "Iteration 288, loss = 0.15827007\n",
      "Iteration 289, loss = 0.15814195\n",
      "Iteration 290, loss = 0.15801809\n",
      "Iteration 291, loss = 0.15788419\n",
      "Iteration 292, loss = 0.15770102\n",
      "Iteration 293, loss = 0.15747740\n",
      "Iteration 294, loss = 0.15758132\n",
      "Iteration 295, loss = 0.15749557\n",
      "Iteration 296, loss = 0.15728609\n",
      "Iteration 297, loss = 0.15734457\n",
      "Iteration 298, loss = 0.15726197\n",
      "Iteration 299, loss = 0.15701341\n",
      "Iteration 300, loss = 0.15686822\n",
      "Iteration 301, loss = 0.15693475\n",
      "Iteration 302, loss = 0.15689024\n",
      "Iteration 303, loss = 0.15667834\n",
      "Iteration 304, loss = 0.15660259\n",
      "Iteration 305, loss = 0.15647269\n",
      "Iteration 306, loss = 0.15648193\n",
      "Iteration 307, loss = 0.15646196\n",
      "Iteration 308, loss = 0.15626355\n",
      "Iteration 309, loss = 0.15613192\n",
      "Iteration 310, loss = 0.15617874\n",
      "Iteration 311, loss = 0.15613319\n",
      "Iteration 312, loss = 0.15593203\n",
      "Iteration 313, loss = 0.15579408\n",
      "Iteration 314, loss = 0.15577531\n",
      "Iteration 315, loss = 0.15568427\n",
      "Iteration 316, loss = 0.15561457\n",
      "Iteration 317, loss = 0.15541288\n",
      "Iteration 318, loss = 0.15531348\n",
      "Iteration 319, loss = 0.15520998\n",
      "Iteration 320, loss = 0.15513885\n",
      "Iteration 321, loss = 0.15501079\n",
      "Iteration 322, loss = 0.15487656\n",
      "Iteration 323, loss = 0.15488549\n",
      "Iteration 324, loss = 0.15465055\n",
      "Iteration 325, loss = 0.15452021\n",
      "Iteration 326, loss = 0.15453807\n",
      "Iteration 327, loss = 0.15445496\n",
      "Iteration 328, loss = 0.15416414\n",
      "Iteration 329, loss = 0.15410189\n",
      "Iteration 330, loss = 0.15404753\n",
      "Iteration 331, loss = 0.15401560\n",
      "Iteration 332, loss = 0.15385070\n",
      "Iteration 333, loss = 0.15368871\n",
      "Iteration 334, loss = 0.15362820\n",
      "Iteration 335, loss = 0.15368866\n",
      "Iteration 336, loss = 0.15369400\n",
      "Iteration 337, loss = 0.15347436\n",
      "Iteration 338, loss = 0.15333059\n",
      "Iteration 339, loss = 0.15337635\n",
      "Iteration 340, loss = 0.15326722\n",
      "Iteration 341, loss = 0.15317888\n",
      "Iteration 342, loss = 0.15320708\n",
      "Iteration 343, loss = 0.15309832\n",
      "Iteration 344, loss = 0.15307198\n",
      "Iteration 345, loss = 0.15291617\n",
      "Iteration 346, loss = 0.15299325\n",
      "Iteration 347, loss = 0.15279094\n",
      "Iteration 348, loss = 0.15266029\n",
      "Iteration 349, loss = 0.15265271\n",
      "Iteration 350, loss = 0.15258555\n",
      "Iteration 351, loss = 0.15257561\n",
      "Iteration 352, loss = 0.15254177\n",
      "Iteration 353, loss = 0.15244568\n",
      "Iteration 354, loss = 0.15244917\n",
      "Iteration 355, loss = 0.15243140\n",
      "Iteration 356, loss = 0.15227026\n",
      "Iteration 357, loss = 0.15218517\n",
      "Iteration 358, loss = 0.15212608\n",
      "Iteration 359, loss = 0.15215261\n",
      "Iteration 360, loss = 0.15208624\n",
      "Iteration 361, loss = 0.15195602\n",
      "Iteration 362, loss = 0.15197737\n",
      "Iteration 363, loss = 0.15192561\n",
      "Iteration 364, loss = 0.15174373\n",
      "Iteration 365, loss = 0.15169830\n",
      "Iteration 366, loss = 0.15172702\n",
      "Iteration 367, loss = 0.15159842\n",
      "Iteration 368, loss = 0.15164086\n",
      "Iteration 369, loss = 0.15179075\n",
      "Iteration 370, loss = 0.15172002\n",
      "Iteration 371, loss = 0.15143885\n",
      "Iteration 372, loss = 0.15147801\n",
      "Iteration 373, loss = 0.15154075\n",
      "Iteration 374, loss = 0.15141120\n",
      "Iteration 375, loss = 0.15117691\n",
      "Iteration 376, loss = 0.15119258\n",
      "Iteration 377, loss = 0.15123721\n",
      "Iteration 378, loss = 0.15107560\n",
      "Iteration 379, loss = 0.15095991\n",
      "Iteration 380, loss = 0.15105004\n",
      "Iteration 381, loss = 0.15102192\n",
      "Iteration 382, loss = 0.15081956\n",
      "Iteration 383, loss = 0.15077400\n",
      "Iteration 384, loss = 0.15074626\n",
      "Iteration 385, loss = 0.15074449\n",
      "Iteration 386, loss = 0.15074832\n",
      "Iteration 387, loss = 0.15066307\n",
      "Iteration 388, loss = 0.15059328\n",
      "Iteration 389, loss = 0.15060471\n",
      "Iteration 390, loss = 0.15061652\n",
      "Iteration 391, loss = 0.15049954\n",
      "Iteration 392, loss = 0.15053544\n",
      "Iteration 393, loss = 0.15052653\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.940000\n",
      "Training set loss: 0.150527\n",
      "\n",
      "learning on dataset moons\n",
      "training: constant learning-rate\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.69564741\n",
      "Iteration 3, loss = 0.68980404\n",
      "Iteration 4, loss = 0.68433863\n",
      "Iteration 5, loss = 0.67912932\n",
      "Iteration 6, loss = 0.67414633\n",
      "Iteration 7, loss = 0.66942320\n",
      "Iteration 8, loss = 0.66485589\n",
      "Iteration 9, loss = 0.66050975\n",
      "Iteration 10, loss = 0.65622883\n",
      "Iteration 11, loss = 0.65196026\n",
      "Iteration 12, loss = 0.64772294\n",
      "Iteration 13, loss = 0.64356352\n",
      "Iteration 14, loss = 0.63946969\n",
      "Iteration 15, loss = 0.63537651\n",
      "Iteration 16, loss = 0.63129319\n",
      "Iteration 17, loss = 0.62726989\n",
      "Iteration 18, loss = 0.62324445\n",
      "Iteration 19, loss = 0.61924561\n",
      "Iteration 20, loss = 0.61525819\n",
      "Iteration 21, loss = 0.61134653\n",
      "Iteration 22, loss = 0.60748732\n",
      "Iteration 23, loss = 0.60366053\n",
      "Iteration 24, loss = 0.59984553\n",
      "Iteration 25, loss = 0.59603630\n",
      "Iteration 26, loss = 0.59223845\n",
      "Iteration 27, loss = 0.58847447\n",
      "Iteration 28, loss = 0.58473976\n",
      "Iteration 29, loss = 0.58100630\n",
      "Iteration 30, loss = 0.57727642\n",
      "Iteration 31, loss = 0.57354392\n",
      "Iteration 32, loss = 0.56982589\n",
      "Iteration 33, loss = 0.56610806\n",
      "Iteration 34, loss = 0.56238563\n",
      "Iteration 35, loss = 0.55866608\n",
      "Iteration 36, loss = 0.55495811\n",
      "Iteration 37, loss = 0.55126928\n",
      "Iteration 38, loss = 0.54758376\n",
      "Iteration 39, loss = 0.54389468\n",
      "Iteration 40, loss = 0.54022557\n",
      "Iteration 41, loss = 0.53657988\n",
      "Iteration 42, loss = 0.53296072\n",
      "Iteration 43, loss = 0.52934749\n",
      "Iteration 44, loss = 0.52576157\n",
      "Iteration 45, loss = 0.52220770\n",
      "Iteration 46, loss = 0.51868031\n",
      "Iteration 47, loss = 0.51516223\n",
      "Iteration 48, loss = 0.51167311\n",
      "Iteration 49, loss = 0.50822951\n",
      "Iteration 50, loss = 0.50481848\n",
      "Iteration 51, loss = 0.50142213\n",
      "Iteration 52, loss = 0.49806778\n",
      "Iteration 53, loss = 0.49474885\n",
      "Iteration 54, loss = 0.49147869\n",
      "Iteration 55, loss = 0.48825265\n",
      "Iteration 56, loss = 0.48507827\n",
      "Iteration 57, loss = 0.48195294\n",
      "Iteration 58, loss = 0.47888562\n",
      "Iteration 59, loss = 0.47587722\n",
      "Iteration 60, loss = 0.47291800\n",
      "Iteration 61, loss = 0.47000215\n",
      "Iteration 62, loss = 0.46713241\n",
      "Iteration 63, loss = 0.46431064\n",
      "Iteration 64, loss = 0.46154153\n",
      "Iteration 65, loss = 0.45882134\n",
      "Iteration 66, loss = 0.45614960\n",
      "Iteration 67, loss = 0.45352633\n",
      "Iteration 68, loss = 0.45095165\n",
      "Iteration 69, loss = 0.44842560\n",
      "Iteration 70, loss = 0.44594758\n",
      "Iteration 71, loss = 0.44351868\n",
      "Iteration 72, loss = 0.44113800\n",
      "Iteration 73, loss = 0.43880463\n",
      "Iteration 74, loss = 0.43651843\n",
      "Iteration 75, loss = 0.43427971\n",
      "Iteration 76, loss = 0.43208979\n",
      "Iteration 77, loss = 0.42994564\n",
      "Iteration 78, loss = 0.42784673\n",
      "Iteration 79, loss = 0.42579230\n",
      "Iteration 80, loss = 0.42378206\n",
      "Iteration 81, loss = 0.42181628\n",
      "Iteration 82, loss = 0.41989404\n",
      "Iteration 83, loss = 0.41801443\n",
      "Iteration 84, loss = 0.41617978\n",
      "Iteration 85, loss = 0.41438736\n",
      "Iteration 86, loss = 0.41263632\n",
      "Iteration 87, loss = 0.41092683\n",
      "Iteration 88, loss = 0.40925808\n",
      "Iteration 89, loss = 0.40762829\n",
      "Iteration 90, loss = 0.40603727\n",
      "Iteration 91, loss = 0.40448387\n",
      "Iteration 92, loss = 0.40296728\n",
      "Iteration 93, loss = 0.40148697\n",
      "Iteration 94, loss = 0.40004303\n",
      "Iteration 95, loss = 0.39863419\n",
      "Iteration 96, loss = 0.39725946\n",
      "Iteration 97, loss = 0.39591813\n",
      "Iteration 98, loss = 0.39460936\n",
      "Iteration 99, loss = 0.39333313\n",
      "Iteration 100, loss = 0.39208820\n",
      "Iteration 101, loss = 0.39087428\n",
      "Iteration 102, loss = 0.38969076\n",
      "Iteration 103, loss = 0.38853614\n",
      "Iteration 104, loss = 0.38740981\n",
      "Iteration 105, loss = 0.38631116\n",
      "Iteration 106, loss = 0.38523954\n",
      "Iteration 107, loss = 0.38419424\n",
      "Iteration 108, loss = 0.38317468\n",
      "Iteration 109, loss = 0.38218029\n",
      "Iteration 110, loss = 0.38121035\n",
      "Iteration 111, loss = 0.38026450\n",
      "Iteration 112, loss = 0.37934204\n",
      "Iteration 113, loss = 0.37844216\n",
      "Iteration 114, loss = 0.37756427\n",
      "Iteration 115, loss = 0.37670786\n",
      "Iteration 116, loss = 0.37587244\n",
      "Iteration 117, loss = 0.37505722\n",
      "Iteration 118, loss = 0.37426154\n",
      "Iteration 119, loss = 0.37348498\n",
      "Iteration 120, loss = 0.37272720\n",
      "Iteration 121, loss = 0.37198768\n",
      "Iteration 122, loss = 0.37126594\n",
      "Iteration 123, loss = 0.37056150\n",
      "Iteration 124, loss = 0.36987377\n",
      "Iteration 125, loss = 0.36920223\n",
      "Iteration 126, loss = 0.36854680\n",
      "Iteration 127, loss = 0.36790689\n",
      "Iteration 128, loss = 0.36728199\n",
      "Iteration 129, loss = 0.36667183\n",
      "Iteration 130, loss = 0.36607587\n",
      "Iteration 131, loss = 0.36549381\n",
      "Iteration 132, loss = 0.36492539\n",
      "Iteration 133, loss = 0.36437006\n",
      "Iteration 134, loss = 0.36382759\n",
      "Iteration 135, loss = 0.36329744\n",
      "Iteration 136, loss = 0.36277946\n",
      "Iteration 137, loss = 0.36227340\n",
      "Iteration 138, loss = 0.36177886\n",
      "Iteration 139, loss = 0.36129553\n",
      "Iteration 140, loss = 0.36082290\n",
      "Iteration 141, loss = 0.36036072\n",
      "Iteration 142, loss = 0.35990860\n",
      "Iteration 143, loss = 0.35946641\n",
      "Iteration 144, loss = 0.35903400\n",
      "Iteration 145, loss = 0.35861215\n",
      "Iteration 146, loss = 0.35819954\n",
      "Iteration 147, loss = 0.35779598\n",
      "Iteration 148, loss = 0.35740107\n",
      "Iteration 149, loss = 0.35701472\n",
      "Iteration 150, loss = 0.35663675\n",
      "Iteration 151, loss = 0.35626687\n",
      "Iteration 152, loss = 0.35590487\n",
      "Iteration 153, loss = 0.35555063\n",
      "Iteration 154, loss = 0.35520425\n",
      "Iteration 155, loss = 0.35486504\n",
      "Iteration 156, loss = 0.35453313\n",
      "Iteration 157, loss = 0.35420812\n",
      "Iteration 158, loss = 0.35388978\n",
      "Iteration 159, loss = 0.35357837\n",
      "Iteration 160, loss = 0.35327330\n",
      "Iteration 161, loss = 0.35297454\n",
      "Iteration 162, loss = 0.35268191\n",
      "Iteration 163, loss = 0.35239522\n",
      "Iteration 164, loss = 0.35211443\n",
      "Iteration 165, loss = 0.35183924\n",
      "Iteration 166, loss = 0.35156968\n",
      "Iteration 167, loss = 0.35130560\n",
      "Iteration 168, loss = 0.35104663\n",
      "Iteration 169, loss = 0.35079291\n",
      "Iteration 170, loss = 0.35054414\n",
      "Iteration 171, loss = 0.35030014\n",
      "Iteration 172, loss = 0.35006110\n",
      "Iteration 173, loss = 0.34982644\n",
      "Iteration 174, loss = 0.34959633\n",
      "Iteration 175, loss = 0.34937081\n",
      "Iteration 176, loss = 0.34914932\n",
      "Iteration 177, loss = 0.34893225\n",
      "Iteration 178, loss = 0.34871941\n",
      "Iteration 179, loss = 0.34851080\n",
      "Iteration 180, loss = 0.34830607\n",
      "Iteration 181, loss = 0.34810531\n",
      "Iteration 182, loss = 0.34790872\n",
      "Iteration 183, loss = 0.34771552\n",
      "Iteration 184, loss = 0.34752621\n",
      "Iteration 185, loss = 0.34734026\n",
      "Iteration 186, loss = 0.34715777\n",
      "Iteration 187, loss = 0.34697871\n",
      "Iteration 188, loss = 0.34680276\n",
      "Iteration 189, loss = 0.34663016\n",
      "Iteration 190, loss = 0.34646053\n",
      "Iteration 191, loss = 0.34629414\n",
      "Iteration 192, loss = 0.34613061\n",
      "Iteration 193, loss = 0.34596991\n",
      "Iteration 194, loss = 0.34581223\n",
      "Iteration 195, loss = 0.34565714\n",
      "Iteration 196, loss = 0.34550502\n",
      "Iteration 197, loss = 0.34535543\n",
      "Iteration 198, loss = 0.34520845\n",
      "Iteration 199, loss = 0.34506400\n",
      "Iteration 200, loss = 0.34492208\n",
      "Iteration 201, loss = 0.34478275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 202, loss = 0.34464567\n",
      "Iteration 203, loss = 0.34451116\n",
      "Iteration 204, loss = 0.34437889\n",
      "Iteration 205, loss = 0.34424876\n",
      "Iteration 206, loss = 0.34412088\n",
      "Iteration 207, loss = 0.34399506\n",
      "Iteration 208, loss = 0.34387175\n",
      "Iteration 209, loss = 0.34375029\n",
      "Iteration 210, loss = 0.34363093\n",
      "Iteration 211, loss = 0.34351352\n",
      "Iteration 212, loss = 0.34339802\n",
      "Iteration 213, loss = 0.34328450\n",
      "Iteration 214, loss = 0.34317280\n",
      "Iteration 215, loss = 0.34306301\n",
      "Iteration 216, loss = 0.34295490\n",
      "Iteration 217, loss = 0.34284862\n",
      "Iteration 218, loss = 0.34274410\n",
      "Iteration 219, loss = 0.34264124\n",
      "Iteration 220, loss = 0.34253989\n",
      "Iteration 221, loss = 0.34244004\n",
      "Iteration 222, loss = 0.34234180\n",
      "Iteration 223, loss = 0.34224511\n",
      "Iteration 224, loss = 0.34214991\n",
      "Iteration 225, loss = 0.34205621\n",
      "Iteration 226, loss = 0.34196404\n",
      "Iteration 227, loss = 0.34187322\n",
      "Iteration 228, loss = 0.34178354\n",
      "Iteration 229, loss = 0.34169507\n",
      "Iteration 230, loss = 0.34160817\n",
      "Iteration 231, loss = 0.34152258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.850000\n",
      "Training set loss: 0.341523\n",
      "training: constant with momentum\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.69564741\n",
      "Iteration 3, loss = 0.68459115\n",
      "Iteration 4, loss = 0.67087272\n",
      "Iteration 5, loss = 0.65602620\n",
      "Iteration 6, loss = 0.64019138\n",
      "Iteration 7, loss = 0.62316597\n",
      "Iteration 8, loss = 0.60471564\n",
      "Iteration 9, loss = 0.58511995\n",
      "Iteration 10, loss = 0.56475645\n",
      "Iteration 11, loss = 0.54386316\n",
      "Iteration 12, loss = 0.52252261\n",
      "Iteration 13, loss = 0.50070217\n",
      "Iteration 14, loss = 0.47869801\n",
      "Iteration 15, loss = 0.45697959\n",
      "Iteration 16, loss = 0.43638625\n",
      "Iteration 17, loss = 0.41773446\n",
      "Iteration 18, loss = 0.40124814\n",
      "Iteration 19, loss = 0.38699357\n",
      "Iteration 20, loss = 0.37507394\n",
      "Iteration 21, loss = 0.36565181\n",
      "Iteration 22, loss = 0.35862838\n",
      "Iteration 23, loss = 0.35356951\n",
      "Iteration 24, loss = 0.34996638\n",
      "Iteration 25, loss = 0.34755402\n",
      "Iteration 26, loss = 0.34618278\n",
      "Iteration 27, loss = 0.34559130\n",
      "Iteration 28, loss = 0.34541583\n",
      "Iteration 29, loss = 0.34539644\n",
      "Iteration 30, loss = 0.34545687\n",
      "Iteration 31, loss = 0.34557996\n",
      "Iteration 32, loss = 0.34567600\n",
      "Iteration 33, loss = 0.34562208\n",
      "Iteration 34, loss = 0.34538192\n",
      "Iteration 35, loss = 0.34501493\n",
      "Iteration 36, loss = 0.34457057\n",
      "Iteration 37, loss = 0.34403134\n",
      "Iteration 38, loss = 0.34337812\n",
      "Iteration 39, loss = 0.34264978\n",
      "Iteration 40, loss = 0.34191643\n",
      "Iteration 41, loss = 0.34121013\n",
      "Iteration 42, loss = 0.34052329\n",
      "Iteration 43, loss = 0.33985266\n",
      "Iteration 44, loss = 0.33923238\n",
      "Iteration 45, loss = 0.33869415\n",
      "Iteration 46, loss = 0.33823561\n",
      "Iteration 47, loss = 0.33784077\n",
      "Iteration 48, loss = 0.33750652\n",
      "Iteration 49, loss = 0.33724353\n",
      "Iteration 50, loss = 0.33704909\n",
      "Iteration 51, loss = 0.33690255\n",
      "Iteration 52, loss = 0.33678730\n",
      "Iteration 53, loss = 0.33670131\n",
      "Iteration 54, loss = 0.33664461\n",
      "Iteration 55, loss = 0.33660551\n",
      "Iteration 56, loss = 0.33656895\n",
      "Iteration 57, loss = 0.33652967\n",
      "Iteration 58, loss = 0.33649238\n",
      "Iteration 59, loss = 0.33645252\n",
      "Iteration 60, loss = 0.33640241\n",
      "Iteration 61, loss = 0.33633909\n",
      "Iteration 62, loss = 0.33626633\n",
      "Iteration 63, loss = 0.33618803\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.850000\n",
      "Training set loss: 0.336188\n",
      "training: constant with Nesterov's momentum\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.69018918\n",
      "Iteration 3, loss = 0.67569838\n",
      "Iteration 4, loss = 0.66004217\n",
      "Iteration 5, loss = 0.64350275\n",
      "Iteration 6, loss = 0.62573147\n",
      "Iteration 7, loss = 0.60671354\n",
      "Iteration 8, loss = 0.58671284\n",
      "Iteration 9, loss = 0.56591368\n",
      "Iteration 10, loss = 0.54439540\n",
      "Iteration 11, loss = 0.52210639\n",
      "Iteration 12, loss = 0.49941114\n",
      "Iteration 13, loss = 0.47689248\n",
      "Iteration 14, loss = 0.45513069\n",
      "Iteration 15, loss = 0.43472066\n",
      "Iteration 16, loss = 0.41619927\n",
      "Iteration 17, loss = 0.39997015\n",
      "Iteration 18, loss = 0.38620807\n",
      "Iteration 19, loss = 0.37490509\n",
      "Iteration 20, loss = 0.36589751\n",
      "Iteration 21, loss = 0.35896386\n",
      "Iteration 22, loss = 0.35380095\n",
      "Iteration 23, loss = 0.35009348\n",
      "Iteration 24, loss = 0.34753605\n",
      "Iteration 25, loss = 0.34584704\n",
      "Iteration 26, loss = 0.34479220\n",
      "Iteration 27, loss = 0.34416466\n",
      "Iteration 28, loss = 0.34380082\n",
      "Iteration 29, loss = 0.34358053\n",
      "Iteration 30, loss = 0.34341337\n",
      "Iteration 31, loss = 0.34324059\n",
      "Iteration 32, loss = 0.34302655\n",
      "Iteration 33, loss = 0.34275101\n",
      "Iteration 34, loss = 0.34240900\n",
      "Iteration 35, loss = 0.34200392\n",
      "Iteration 36, loss = 0.34154741\n",
      "Iteration 37, loss = 0.34105399\n",
      "Iteration 38, loss = 0.34054132\n",
      "Iteration 39, loss = 0.34002318\n",
      "Iteration 40, loss = 0.33951499\n",
      "Iteration 41, loss = 0.33902781\n",
      "Iteration 42, loss = 0.33857234\n",
      "Iteration 43, loss = 0.33815629\n",
      "Iteration 44, loss = 0.33778425\n",
      "Iteration 45, loss = 0.33746270\n",
      "Iteration 46, loss = 0.33718850\n",
      "Iteration 47, loss = 0.33696180\n",
      "Iteration 48, loss = 0.33677621\n",
      "Iteration 49, loss = 0.33662665\n",
      "Iteration 50, loss = 0.33650765\n",
      "Iteration 51, loss = 0.33641390\n",
      "Iteration 52, loss = 0.33634142\n",
      "Iteration 53, loss = 0.33628367\n",
      "Iteration 54, loss = 0.33623536\n",
      "Iteration 55, loss = 0.33619271\n",
      "Iteration 56, loss = 0.33615290\n",
      "Iteration 57, loss = 0.33611230\n",
      "Iteration 58, loss = 0.33606908\n",
      "Iteration 59, loss = 0.33602216\n",
      "Iteration 60, loss = 0.33597249\n",
      "Iteration 61, loss = 0.33591920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.850000\n",
      "Training set loss: 0.335919\n",
      "training: inv-scaling learning-rate\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.69564741\n",
      "Iteration 3, loss = 0.69505668\n",
      "Iteration 4, loss = 0.69464229\n",
      "Iteration 5, loss = 0.69430424\n",
      "Iteration 6, loss = 0.69401243\n",
      "Iteration 7, loss = 0.69375178\n",
      "Iteration 8, loss = 0.69351412\n",
      "Iteration 9, loss = 0.69329505\n",
      "Iteration 10, loss = 0.69308944\n",
      "Iteration 11, loss = 0.69289538\n",
      "Iteration 12, loss = 0.69271158\n",
      "Iteration 13, loss = 0.69253729\n",
      "Iteration 14, loss = 0.69237142\n",
      "Iteration 15, loss = 0.69221255\n",
      "Iteration 16, loss = 0.69205994\n",
      "Iteration 17, loss = 0.69191286\n",
      "Iteration 18, loss = 0.69177098\n",
      "Iteration 19, loss = 0.69163326\n",
      "Iteration 20, loss = 0.69149904\n",
      "Iteration 21, loss = 0.69136818\n",
      "Iteration 22, loss = 0.69124078\n",
      "Iteration 23, loss = 0.69111682\n",
      "Iteration 24, loss = 0.69099583\n",
      "Iteration 25, loss = 0.69087762\n",
      "Iteration 26, loss = 0.69076245\n",
      "Iteration 27, loss = 0.69064977\n",
      "Iteration 28, loss = 0.69053932\n",
      "Iteration 29, loss = 0.69043105\n",
      "Iteration 30, loss = 0.69032497\n",
      "Iteration 31, loss = 0.69022108\n",
      "Iteration 32, loss = 0.69011901\n",
      "Iteration 33, loss = 0.69001872\n",
      "Iteration 34, loss = 0.68992043\n",
      "Iteration 35, loss = 0.68982373\n",
      "Iteration 36, loss = 0.68972853\n",
      "Iteration 37, loss = 0.68963477\n",
      "Iteration 38, loss = 0.68954239\n",
      "Iteration 39, loss = 0.68945138\n",
      "Iteration 40, loss = 0.68936172\n",
      "Iteration 41, loss = 0.68927328\n",
      "Iteration 42, loss = 0.68918600\n",
      "Iteration 43, loss = 0.68909984\n",
      "Iteration 44, loss = 0.68901475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.500000\n",
      "Training set loss: 0.689015\n",
      "training: inv-scaling with momentum\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.69018918\n",
      "Iteration 3, loss = 0.68449881\n",
      "Iteration 4, loss = 0.67946035\n",
      "Iteration 5, loss = 0.67497597\n",
      "Iteration 6, loss = 0.67095941\n",
      "Iteration 7, loss = 0.66734698\n",
      "Iteration 8, loss = 0.66406503\n",
      "Iteration 9, loss = 0.66110121\n",
      "Iteration 10, loss = 0.65836499\n",
      "Iteration 11, loss = 0.65581703\n",
      "Iteration 12, loss = 0.65343656\n",
      "Iteration 13, loss = 0.65122213\n",
      "Iteration 14, loss = 0.64915191\n",
      "Iteration 15, loss = 0.64720476\n",
      "Iteration 16, loss = 0.64536180\n",
      "Iteration 17, loss = 0.64361420\n",
      "Iteration 18, loss = 0.64194657\n",
      "Iteration 19, loss = 0.64034956\n",
      "Iteration 20, loss = 0.63881861\n",
      "Iteration 21, loss = 0.63735387\n",
      "Iteration 22, loss = 0.63595077\n",
      "Iteration 23, loss = 0.63460296\n",
      "Iteration 24, loss = 0.63330252\n",
      "Iteration 25, loss = 0.63205258\n",
      "Iteration 26, loss = 0.63084934\n",
      "Iteration 27, loss = 0.62969016\n",
      "Iteration 28, loss = 0.62857102\n",
      "Iteration 29, loss = 0.62749161\n",
      "Iteration 30, loss = 0.62644515\n",
      "Iteration 31, loss = 0.62543284\n",
      "Iteration 32, loss = 0.62445348\n",
      "Iteration 33, loss = 0.62350210\n",
      "Iteration 34, loss = 0.62257528\n",
      "Iteration 35, loss = 0.62167397\n",
      "Iteration 36, loss = 0.62079887\n",
      "Iteration 37, loss = 0.61994781\n",
      "Iteration 38, loss = 0.61911877\n",
      "Iteration 39, loss = 0.61830962\n",
      "Iteration 40, loss = 0.61752209\n",
      "Iteration 41, loss = 0.61675369\n",
      "Iteration 42, loss = 0.61600273\n",
      "Iteration 43, loss = 0.61526775\n",
      "Iteration 44, loss = 0.61454779\n",
      "Iteration 45, loss = 0.61384241\n",
      "Iteration 46, loss = 0.61315206\n",
      "Iteration 47, loss = 0.61247528\n",
      "Iteration 48, loss = 0.61181200\n",
      "Iteration 49, loss = 0.61116209\n",
      "Iteration 50, loss = 0.61052408\n",
      "Iteration 51, loss = 0.60989655\n",
      "Iteration 52, loss = 0.60927887\n",
      "Iteration 53, loss = 0.60867019\n",
      "Iteration 54, loss = 0.60807114\n",
      "Iteration 55, loss = 0.60748070\n",
      "Iteration 56, loss = 0.60689821\n",
      "Iteration 57, loss = 0.60632440\n",
      "Iteration 58, loss = 0.60575826\n",
      "Iteration 59, loss = 0.60519948\n",
      "Iteration 60, loss = 0.60464775\n",
      "Iteration 61, loss = 0.60410191\n",
      "Iteration 62, loss = 0.60356261\n",
      "Iteration 63, loss = 0.60302945\n",
      "Iteration 64, loss = 0.60250236\n",
      "Iteration 65, loss = 0.60198140\n",
      "Iteration 66, loss = 0.60146693\n",
      "Iteration 67, loss = 0.60095800\n",
      "Iteration 68, loss = 0.60045501\n",
      "Iteration 69, loss = 0.59995681\n",
      "Iteration 70, loss = 0.59946332\n",
      "Iteration 71, loss = 0.59897541\n",
      "Iteration 72, loss = 0.59849210\n",
      "Iteration 73, loss = 0.59801338\n",
      "Iteration 74, loss = 0.59753865\n",
      "Iteration 75, loss = 0.59706775\n",
      "Iteration 76, loss = 0.59660234\n",
      "Iteration 77, loss = 0.59614134\n",
      "Iteration 78, loss = 0.59568445\n",
      "Iteration 79, loss = 0.59523185\n",
      "Iteration 80, loss = 0.59478362\n",
      "Iteration 81, loss = 0.59433899\n",
      "Iteration 82, loss = 0.59389794\n",
      "Iteration 83, loss = 0.59346020\n",
      "Iteration 84, loss = 0.59302580\n",
      "Iteration 85, loss = 0.59259470\n",
      "Iteration 86, loss = 0.59216726\n",
      "Iteration 87, loss = 0.59174272\n",
      "Iteration 88, loss = 0.59132100\n",
      "Iteration 89, loss = 0.59090234\n",
      "Iteration 90, loss = 0.59048667\n",
      "Iteration 91, loss = 0.59007351\n",
      "Iteration 92, loss = 0.58966318\n",
      "Iteration 93, loss = 0.58925581\n",
      "Iteration 94, loss = 0.58885101\n",
      "Iteration 95, loss = 0.58844873\n",
      "Iteration 96, loss = 0.58804857\n",
      "Iteration 97, loss = 0.58765048\n",
      "Iteration 98, loss = 0.58725456\n",
      "Iteration 99, loss = 0.58686087\n",
      "Iteration 100, loss = 0.58646962\n",
      "Iteration 101, loss = 0.58608096\n",
      "Iteration 102, loss = 0.58569466\n",
      "Iteration 103, loss = 0.58531049\n",
      "Iteration 104, loss = 0.58492848\n",
      "Iteration 105, loss = 0.58454871\n",
      "Iteration 106, loss = 0.58417107\n",
      "Iteration 107, loss = 0.58379549\n",
      "Iteration 108, loss = 0.58342139\n",
      "Iteration 109, loss = 0.58304931\n",
      "Iteration 110, loss = 0.58267888\n",
      "Iteration 111, loss = 0.58231041\n",
      "Iteration 112, loss = 0.58194375\n",
      "Iteration 113, loss = 0.58157922\n",
      "Iteration 114, loss = 0.58121651\n",
      "Iteration 115, loss = 0.58085558\n",
      "Iteration 116, loss = 0.58049647\n",
      "Iteration 117, loss = 0.58013914\n",
      "Iteration 118, loss = 0.57978349\n",
      "Iteration 119, loss = 0.57942949\n",
      "Iteration 120, loss = 0.57907701\n",
      "Iteration 121, loss = 0.57872616\n",
      "Iteration 122, loss = 0.57837681\n",
      "Iteration 123, loss = 0.57802904\n",
      "Iteration 124, loss = 0.57768262\n",
      "Iteration 125, loss = 0.57733739\n",
      "Iteration 126, loss = 0.57699371\n",
      "Iteration 127, loss = 0.57665165\n",
      "Iteration 128, loss = 0.57631108\n",
      "Iteration 129, loss = 0.57597197\n",
      "Iteration 130, loss = 0.57563450\n",
      "Iteration 131, loss = 0.57529859\n",
      "Iteration 132, loss = 0.57496415\n",
      "Iteration 133, loss = 0.57463131\n",
      "Iteration 134, loss = 0.57429972\n",
      "Iteration 135, loss = 0.57396952\n",
      "Iteration 136, loss = 0.57364064\n",
      "Iteration 137, loss = 0.57331309\n",
      "Iteration 138, loss = 0.57298690\n",
      "Iteration 139, loss = 0.57266200\n",
      "Iteration 140, loss = 0.57233862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 141, loss = 0.57201711\n",
      "Iteration 142, loss = 0.57169727\n",
      "Iteration 143, loss = 0.57137875\n",
      "Iteration 144, loss = 0.57106154\n",
      "Iteration 145, loss = 0.57074553\n",
      "Iteration 146, loss = 0.57043072\n",
      "Iteration 147, loss = 0.57011672\n",
      "Iteration 148, loss = 0.56980383\n",
      "Iteration 149, loss = 0.56949227\n",
      "Iteration 150, loss = 0.56918205\n",
      "Iteration 151, loss = 0.56887290\n",
      "Iteration 152, loss = 0.56856483\n",
      "Iteration 153, loss = 0.56825797\n",
      "Iteration 154, loss = 0.56795245\n",
      "Iteration 155, loss = 0.56764811\n",
      "Iteration 156, loss = 0.56734487\n",
      "Iteration 157, loss = 0.56704270\n",
      "Iteration 158, loss = 0.56674156\n",
      "Iteration 159, loss = 0.56644148\n",
      "Iteration 160, loss = 0.56614244\n",
      "Iteration 161, loss = 0.56584441\n",
      "Iteration 162, loss = 0.56554759\n",
      "Iteration 163, loss = 0.56525180\n",
      "Iteration 164, loss = 0.56495699\n",
      "Iteration 165, loss = 0.56466315\n",
      "Iteration 166, loss = 0.56437046\n",
      "Iteration 167, loss = 0.56407876\n",
      "Iteration 168, loss = 0.56378804\n",
      "Iteration 169, loss = 0.56349866\n",
      "Iteration 170, loss = 0.56321040\n",
      "Iteration 171, loss = 0.56292298\n",
      "Iteration 172, loss = 0.56263642\n",
      "Iteration 173, loss = 0.56235091\n",
      "Iteration 174, loss = 0.56206635\n",
      "Iteration 175, loss = 0.56178265\n",
      "Iteration 176, loss = 0.56149968\n",
      "Iteration 177, loss = 0.56121761\n",
      "Iteration 178, loss = 0.56093640\n",
      "Iteration 179, loss = 0.56065602\n",
      "Iteration 180, loss = 0.56037647\n",
      "Iteration 181, loss = 0.56009784\n",
      "Iteration 182, loss = 0.55982015\n",
      "Iteration 183, loss = 0.55954321\n",
      "Iteration 184, loss = 0.55926704\n",
      "Iteration 185, loss = 0.55899185\n",
      "Iteration 186, loss = 0.55871738\n",
      "Iteration 187, loss = 0.55844377\n",
      "Iteration 188, loss = 0.55817078\n",
      "Iteration 189, loss = 0.55789882\n",
      "Iteration 190, loss = 0.55762751\n",
      "Iteration 191, loss = 0.55735682\n",
      "Iteration 192, loss = 0.55708686\n",
      "Iteration 193, loss = 0.55681765\n",
      "Iteration 194, loss = 0.55654917\n",
      "Iteration 195, loss = 0.55628147\n",
      "Iteration 196, loss = 0.55601453\n",
      "Iteration 197, loss = 0.55574832\n",
      "Iteration 198, loss = 0.55548283\n",
      "Iteration 199, loss = 0.55521805\n",
      "Iteration 200, loss = 0.55495402\n",
      "Iteration 201, loss = 0.55469074\n",
      "Iteration 202, loss = 0.55442817\n",
      "Iteration 203, loss = 0.55416592\n",
      "Iteration 204, loss = 0.55390436\n",
      "Iteration 205, loss = 0.55364359\n",
      "Iteration 206, loss = 0.55338351\n",
      "Iteration 207, loss = 0.55312413\n",
      "Iteration 208, loss = 0.55286543\n",
      "Iteration 209, loss = 0.55260743\n",
      "Iteration 210, loss = 0.55235019\n",
      "Iteration 211, loss = 0.55209365\n",
      "Iteration 212, loss = 0.55183787\n",
      "Iteration 213, loss = 0.55158286\n",
      "Iteration 214, loss = 0.55132863\n",
      "Iteration 215, loss = 0.55107501\n",
      "Iteration 216, loss = 0.55082212\n",
      "Iteration 217, loss = 0.55056998\n",
      "Iteration 218, loss = 0.55031850\n",
      "Iteration 219, loss = 0.55006797\n",
      "Iteration 220, loss = 0.54981824\n",
      "Iteration 221, loss = 0.54956912\n",
      "Iteration 222, loss = 0.54932049\n",
      "Iteration 223, loss = 0.54907250\n",
      "Iteration 224, loss = 0.54882514\n",
      "Iteration 225, loss = 0.54857839\n",
      "Iteration 226, loss = 0.54833228\n",
      "Iteration 227, loss = 0.54808673\n",
      "Iteration 228, loss = 0.54784164\n",
      "Iteration 229, loss = 0.54759711\n",
      "Iteration 230, loss = 0.54735329\n",
      "Iteration 231, loss = 0.54711004\n",
      "Iteration 232, loss = 0.54686745\n",
      "Iteration 233, loss = 0.54662547\n",
      "Iteration 234, loss = 0.54638406\n",
      "Iteration 235, loss = 0.54614323\n",
      "Iteration 236, loss = 0.54590303\n",
      "Iteration 237, loss = 0.54566343\n",
      "Iteration 238, loss = 0.54542439\n",
      "Iteration 239, loss = 0.54518598\n",
      "Iteration 240, loss = 0.54494817\n",
      "Iteration 241, loss = 0.54471091\n",
      "Iteration 242, loss = 0.54447405\n",
      "Iteration 243, loss = 0.54423756\n",
      "Iteration 244, loss = 0.54400160\n",
      "Iteration 245, loss = 0.54376622\n",
      "Iteration 246, loss = 0.54353132\n",
      "Iteration 247, loss = 0.54329686\n",
      "Iteration 248, loss = 0.54306291\n",
      "Iteration 249, loss = 0.54282946\n",
      "Iteration 250, loss = 0.54259645\n",
      "Iteration 251, loss = 0.54236395\n",
      "Iteration 252, loss = 0.54213194\n",
      "Iteration 253, loss = 0.54190044\n",
      "Iteration 254, loss = 0.54166943\n",
      "Iteration 255, loss = 0.54143890\n",
      "Iteration 256, loss = 0.54120887\n",
      "Iteration 257, loss = 0.54097934\n",
      "Iteration 258, loss = 0.54075031\n",
      "Iteration 259, loss = 0.54052178\n",
      "Iteration 260, loss = 0.54029387\n",
      "Iteration 261, loss = 0.54006668\n",
      "Iteration 262, loss = 0.53984013\n",
      "Iteration 263, loss = 0.53961410\n",
      "Iteration 264, loss = 0.53938866\n",
      "Iteration 265, loss = 0.53916376\n",
      "Iteration 266, loss = 0.53893938\n",
      "Iteration 267, loss = 0.53871549\n",
      "Iteration 268, loss = 0.53849211\n",
      "Iteration 269, loss = 0.53826922\n",
      "Iteration 270, loss = 0.53804682\n",
      "Iteration 271, loss = 0.53782479\n",
      "Iteration 272, loss = 0.53760319\n",
      "Iteration 273, loss = 0.53738216\n",
      "Iteration 274, loss = 0.53716169\n",
      "Iteration 275, loss = 0.53694174\n",
      "Iteration 276, loss = 0.53672235\n",
      "Iteration 277, loss = 0.53650333\n",
      "Iteration 278, loss = 0.53628473\n",
      "Iteration 279, loss = 0.53606661\n",
      "Iteration 280, loss = 0.53584895\n",
      "Iteration 281, loss = 0.53563192\n",
      "Iteration 282, loss = 0.53541540\n",
      "Iteration 283, loss = 0.53519934\n",
      "Iteration 284, loss = 0.53498370\n",
      "Iteration 285, loss = 0.53476851\n",
      "Iteration 286, loss = 0.53455366\n",
      "Iteration 287, loss = 0.53433918\n",
      "Iteration 288, loss = 0.53412515\n",
      "Iteration 289, loss = 0.53391164\n",
      "Iteration 290, loss = 0.53369858\n",
      "Iteration 291, loss = 0.53348610\n",
      "Iteration 292, loss = 0.53327417\n",
      "Iteration 293, loss = 0.53306268\n",
      "Iteration 294, loss = 0.53285149\n",
      "Iteration 295, loss = 0.53264069\n",
      "Iteration 296, loss = 0.53243029\n",
      "Iteration 297, loss = 0.53222031\n",
      "Iteration 298, loss = 0.53201078\n",
      "Iteration 299, loss = 0.53180178\n",
      "Iteration 300, loss = 0.53159321\n",
      "Iteration 301, loss = 0.53138507\n",
      "Iteration 302, loss = 0.53117733\n",
      "Iteration 303, loss = 0.53097001\n",
      "Iteration 304, loss = 0.53076309\n",
      "Iteration 305, loss = 0.53055657\n",
      "Iteration 306, loss = 0.53035048\n",
      "Iteration 307, loss = 0.53014487\n",
      "Iteration 308, loss = 0.52993967\n",
      "Iteration 309, loss = 0.52973488\n",
      "Iteration 310, loss = 0.52953049\n",
      "Iteration 311, loss = 0.52932636\n",
      "Iteration 312, loss = 0.52912256\n",
      "Iteration 313, loss = 0.52891890\n",
      "Iteration 314, loss = 0.52871573\n",
      "Iteration 315, loss = 0.52851294\n",
      "Iteration 316, loss = 0.52831051\n",
      "Iteration 317, loss = 0.52810846\n",
      "Iteration 318, loss = 0.52790677\n",
      "Iteration 319, loss = 0.52770545\n",
      "Iteration 320, loss = 0.52750450\n",
      "Iteration 321, loss = 0.52730393\n",
      "Iteration 322, loss = 0.52710374\n",
      "Iteration 323, loss = 0.52690392\n",
      "Iteration 324, loss = 0.52670447\n",
      "Iteration 325, loss = 0.52650535\n",
      "Iteration 326, loss = 0.52630651\n",
      "Iteration 327, loss = 0.52610804\n",
      "Iteration 328, loss = 0.52590993\n",
      "Iteration 329, loss = 0.52571221\n",
      "Iteration 330, loss = 0.52551500\n",
      "Iteration 331, loss = 0.52531816\n",
      "Iteration 332, loss = 0.52512169\n",
      "Iteration 333, loss = 0.52492558\n",
      "Iteration 334, loss = 0.52472994\n",
      "Iteration 335, loss = 0.52453476\n",
      "Iteration 336, loss = 0.52433991\n",
      "Iteration 337, loss = 0.52414531\n",
      "Iteration 338, loss = 0.52395108\n",
      "Iteration 339, loss = 0.52375722\n",
      "Iteration 340, loss = 0.52356371\n",
      "Iteration 341, loss = 0.52337061\n",
      "Iteration 342, loss = 0.52317804\n",
      "Iteration 343, loss = 0.52298584\n",
      "Iteration 344, loss = 0.52279401\n",
      "Iteration 345, loss = 0.52260263\n",
      "Iteration 346, loss = 0.52241165\n",
      "Iteration 347, loss = 0.52222105\n",
      "Iteration 348, loss = 0.52203082\n",
      "Iteration 349, loss = 0.52184094\n",
      "Iteration 350, loss = 0.52165149\n",
      "Iteration 351, loss = 0.52146243\n",
      "Iteration 352, loss = 0.52127372\n",
      "Iteration 353, loss = 0.52108538\n",
      "Iteration 354, loss = 0.52089739\n",
      "Iteration 355, loss = 0.52070975\n",
      "Iteration 356, loss = 0.52052251\n",
      "Iteration 357, loss = 0.52033565\n",
      "Iteration 358, loss = 0.52014915\n",
      "Iteration 359, loss = 0.51996300\n",
      "Iteration 360, loss = 0.51977720\n",
      "Iteration 361, loss = 0.51959172\n",
      "Iteration 362, loss = 0.51940658\n",
      "Iteration 363, loss = 0.51922178\n",
      "Iteration 364, loss = 0.51903730\n",
      "Iteration 365, loss = 0.51885319\n",
      "Iteration 366, loss = 0.51866944\n",
      "Iteration 367, loss = 0.51848601\n",
      "Iteration 368, loss = 0.51830303\n",
      "Iteration 369, loss = 0.51812035\n",
      "Iteration 370, loss = 0.51793790\n",
      "Iteration 371, loss = 0.51775576\n",
      "Iteration 372, loss = 0.51757395\n",
      "Iteration 373, loss = 0.51739246\n",
      "Iteration 374, loss = 0.51721131\n",
      "Iteration 375, loss = 0.51703047\n",
      "Iteration 376, loss = 0.51684992\n",
      "Iteration 377, loss = 0.51666951\n",
      "Iteration 378, loss = 0.51648943\n",
      "Iteration 379, loss = 0.51630965\n",
      "Iteration 380, loss = 0.51613019\n",
      "Iteration 381, loss = 0.51595103\n",
      "Iteration 382, loss = 0.51577217\n",
      "Iteration 383, loss = 0.51559349\n",
      "Iteration 384, loss = 0.51541502\n",
      "Iteration 385, loss = 0.51523683\n",
      "Iteration 386, loss = 0.51505893\n",
      "Iteration 387, loss = 0.51488132\n",
      "Iteration 388, loss = 0.51470399\n",
      "Iteration 389, loss = 0.51452695\n",
      "Iteration 390, loss = 0.51435019\n",
      "Iteration 391, loss = 0.51417359\n",
      "Iteration 392, loss = 0.51399721\n",
      "Iteration 393, loss = 0.51382110\n",
      "Iteration 394, loss = 0.51364525\n",
      "Iteration 395, loss = 0.51346969\n",
      "Iteration 396, loss = 0.51329429\n",
      "Iteration 397, loss = 0.51311909\n",
      "Iteration 398, loss = 0.51294402\n",
      "Iteration 399, loss = 0.51276921\n",
      "Iteration 400, loss = 0.51259465\n",
      "Iteration 401, loss = 0.51242035\n",
      "Iteration 402, loss = 0.51224632\n",
      "Iteration 403, loss = 0.51207255\n",
      "Iteration 404, loss = 0.51189905\n",
      "Iteration 405, loss = 0.51172582\n",
      "Iteration 406, loss = 0.51155286\n",
      "Iteration 407, loss = 0.51138017\n",
      "Iteration 408, loss = 0.51120782\n",
      "Iteration 409, loss = 0.51103578\n",
      "Iteration 410, loss = 0.51086403\n",
      "Iteration 411, loss = 0.51069260\n",
      "Iteration 412, loss = 0.51052145\n",
      "Iteration 413, loss = 0.51035059\n",
      "Iteration 414, loss = 0.51018000\n",
      "Iteration 415, loss = 0.51000970\n",
      "Iteration 416, loss = 0.50983968\n",
      "Iteration 417, loss = 0.50966994\n",
      "Iteration 418, loss = 0.50950031\n",
      "Iteration 419, loss = 0.50933086\n",
      "Iteration 420, loss = 0.50916172\n",
      "Iteration 421, loss = 0.50899287\n",
      "Iteration 422, loss = 0.50882429\n",
      "Iteration 423, loss = 0.50865610\n",
      "Iteration 424, loss = 0.50848831\n",
      "Iteration 425, loss = 0.50832080\n",
      "Iteration 426, loss = 0.50815357\n",
      "Iteration 427, loss = 0.50798682\n",
      "Iteration 428, loss = 0.50782038\n",
      "Iteration 429, loss = 0.50765441\n",
      "Iteration 430, loss = 0.50748874\n",
      "Iteration 431, loss = 0.50732336\n",
      "Iteration 432, loss = 0.50715830\n",
      "Iteration 433, loss = 0.50699353\n",
      "Iteration 434, loss = 0.50682904\n",
      "Iteration 435, loss = 0.50666484\n",
      "Iteration 436, loss = 0.50650091\n",
      "Iteration 437, loss = 0.50633726\n",
      "Iteration 438, loss = 0.50617389\n",
      "Iteration 439, loss = 0.50601079\n",
      "Iteration 440, loss = 0.50584802\n",
      "Iteration 441, loss = 0.50568562\n",
      "Iteration 442, loss = 0.50552350\n",
      "Iteration 443, loss = 0.50536165\n",
      "Iteration 444, loss = 0.50520008\n",
      "Iteration 445, loss = 0.50503877\n",
      "Iteration 446, loss = 0.50487773\n",
      "Iteration 447, loss = 0.50471701\n",
      "Iteration 448, loss = 0.50455652\n",
      "Iteration 449, loss = 0.50439618\n",
      "Iteration 450, loss = 0.50423608\n",
      "Iteration 451, loss = 0.50407620\n",
      "Iteration 452, loss = 0.50391659\n",
      "Iteration 453, loss = 0.50375729\n",
      "Iteration 454, loss = 0.50359831\n",
      "Iteration 455, loss = 0.50343958\n",
      "Iteration 456, loss = 0.50328111\n",
      "Iteration 457, loss = 0.50312296\n",
      "Iteration 458, loss = 0.50296515\n",
      "Iteration 459, loss = 0.50280767\n",
      "Iteration 460, loss = 0.50265046\n",
      "Iteration 461, loss = 0.50249351\n",
      "Iteration 462, loss = 0.50233682\n",
      "Iteration 463, loss = 0.50218038\n",
      "Iteration 464, loss = 0.50202420\n",
      "Iteration 465, loss = 0.50186826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 466, loss = 0.50171262\n",
      "Iteration 467, loss = 0.50155734\n",
      "Iteration 468, loss = 0.50140233\n",
      "Iteration 469, loss = 0.50124757\n",
      "Iteration 470, loss = 0.50109297\n",
      "Iteration 471, loss = 0.50093847\n",
      "Iteration 472, loss = 0.50078421\n",
      "Iteration 473, loss = 0.50063010\n",
      "Iteration 474, loss = 0.50047606\n",
      "Iteration 475, loss = 0.50032224\n",
      "Iteration 476, loss = 0.50016855\n",
      "Iteration 477, loss = 0.50001493\n",
      "Iteration 478, loss = 0.49986150\n",
      "Iteration 479, loss = 0.49970828\n",
      "Iteration 480, loss = 0.49955526\n",
      "Iteration 481, loss = 0.49940245\n",
      "Iteration 482, loss = 0.49924985\n",
      "Iteration 483, loss = 0.49909746\n",
      "Iteration 484, loss = 0.49894528\n",
      "Iteration 485, loss = 0.49879332\n",
      "Iteration 486, loss = 0.49864157\n",
      "Iteration 487, loss = 0.49849007\n",
      "Iteration 488, loss = 0.49833882\n",
      "Iteration 489, loss = 0.49818779\n",
      "Iteration 490, loss = 0.49803698\n",
      "Iteration 491, loss = 0.49788639\n",
      "Iteration 492, loss = 0.49773602\n",
      "Iteration 493, loss = 0.49758587\n",
      "Iteration 494, loss = 0.49743594\n",
      "Iteration 495, loss = 0.49728624\n",
      "Iteration 496, loss = 0.49713675\n",
      "Iteration 497, loss = 0.49698758\n",
      "Iteration 498, loss = 0.49683867\n",
      "Iteration 499, loss = 0.49669000\n",
      "Iteration 500, loss = 0.49654155\n",
      "Iteration 501, loss = 0.49639333\n",
      "Iteration 502, loss = 0.49624534\n",
      "Iteration 503, loss = 0.49609757\n",
      "Iteration 504, loss = 0.49595004\n",
      "Iteration 505, loss = 0.49580273\n",
      "Iteration 506, loss = 0.49565567\n",
      "Iteration 507, loss = 0.49550885\n",
      "Iteration 508, loss = 0.49536225\n",
      "Iteration 509, loss = 0.49521586\n",
      "Iteration 510, loss = 0.49506972\n",
      "Iteration 511, loss = 0.49492381\n",
      "Iteration 512, loss = 0.49477812\n",
      "Iteration 513, loss = 0.49463265\n",
      "Iteration 514, loss = 0.49448738\n",
      "Iteration 515, loss = 0.49434234\n",
      "Iteration 516, loss = 0.49419751\n",
      "Iteration 517, loss = 0.49405289\n",
      "Iteration 518, loss = 0.49390847\n",
      "Iteration 519, loss = 0.49376420\n",
      "Iteration 520, loss = 0.49362013\n",
      "Iteration 521, loss = 0.49347627\n",
      "Iteration 522, loss = 0.49333262\n",
      "Iteration 523, loss = 0.49318918\n",
      "Iteration 524, loss = 0.49304594\n",
      "Iteration 525, loss = 0.49290292\n",
      "Iteration 526, loss = 0.49276010\n",
      "Iteration 527, loss = 0.49261748\n",
      "Iteration 528, loss = 0.49247510\n",
      "Iteration 529, loss = 0.49233305\n",
      "Iteration 530, loss = 0.49219121\n",
      "Iteration 531, loss = 0.49204958\n",
      "Iteration 532, loss = 0.49190817\n",
      "Iteration 533, loss = 0.49176699\n",
      "Iteration 534, loss = 0.49162604\n",
      "Iteration 535, loss = 0.49148531\n",
      "Iteration 536, loss = 0.49134479\n",
      "Iteration 537, loss = 0.49120447\n",
      "Iteration 538, loss = 0.49106438\n",
      "Iteration 539, loss = 0.49092450\n",
      "Iteration 540, loss = 0.49078482\n",
      "Iteration 541, loss = 0.49064534\n",
      "Iteration 542, loss = 0.49050607\n",
      "Iteration 543, loss = 0.49036701\n",
      "Iteration 544, loss = 0.49022814\n",
      "Iteration 545, loss = 0.49008948\n",
      "Iteration 546, loss = 0.48995103\n",
      "Iteration 547, loss = 0.48981277\n",
      "Iteration 548, loss = 0.48967472\n",
      "Iteration 549, loss = 0.48953686\n",
      "Iteration 550, loss = 0.48939928\n",
      "Iteration 551, loss = 0.48926196\n",
      "Iteration 552, loss = 0.48912485\n",
      "Iteration 553, loss = 0.48898794\n",
      "Iteration 554, loss = 0.48885125\n",
      "Iteration 555, loss = 0.48871475\n",
      "Iteration 556, loss = 0.48857845\n",
      "Iteration 557, loss = 0.48844236\n",
      "Iteration 558, loss = 0.48830646\n",
      "Iteration 559, loss = 0.48817076\n",
      "Iteration 560, loss = 0.48803526\n",
      "Iteration 561, loss = 0.48789996\n",
      "Iteration 562, loss = 0.48776485\n",
      "Iteration 563, loss = 0.48762993\n",
      "Iteration 564, loss = 0.48749521\n",
      "Iteration 565, loss = 0.48736068\n",
      "Iteration 566, loss = 0.48722634\n",
      "Iteration 567, loss = 0.48709220\n",
      "Iteration 568, loss = 0.48695824\n",
      "Iteration 569, loss = 0.48682448\n",
      "Iteration 570, loss = 0.48669090\n",
      "Iteration 571, loss = 0.48655752\n",
      "Iteration 572, loss = 0.48642433\n",
      "Iteration 573, loss = 0.48629132\n",
      "Iteration 574, loss = 0.48615850\n",
      "Iteration 575, loss = 0.48602587\n",
      "Iteration 576, loss = 0.48589344\n",
      "Iteration 577, loss = 0.48576119\n",
      "Iteration 578, loss = 0.48562912\n",
      "Iteration 579, loss = 0.48549724\n",
      "Iteration 580, loss = 0.48536555\n",
      "Iteration 581, loss = 0.48523404\n",
      "Iteration 582, loss = 0.48510271\n",
      "Iteration 583, loss = 0.48497157\n",
      "Iteration 584, loss = 0.48484061\n",
      "Iteration 585, loss = 0.48470985\n",
      "Iteration 586, loss = 0.48457927\n",
      "Iteration 587, loss = 0.48444888\n",
      "Iteration 588, loss = 0.48431867\n",
      "Iteration 589, loss = 0.48418876\n",
      "Iteration 590, loss = 0.48405910\n",
      "Iteration 591, loss = 0.48392964\n",
      "Iteration 592, loss = 0.48380037\n",
      "Iteration 593, loss = 0.48367129\n",
      "Iteration 594, loss = 0.48354239\n",
      "Iteration 595, loss = 0.48341369\n",
      "Iteration 596, loss = 0.48328519\n",
      "Iteration 597, loss = 0.48315689\n",
      "Iteration 598, loss = 0.48302879\n",
      "Iteration 599, loss = 0.48290087\n",
      "Iteration 600, loss = 0.48277314\n",
      "Iteration 601, loss = 0.48264560\n",
      "Iteration 602, loss = 0.48251831\n",
      "Iteration 603, loss = 0.48239122\n",
      "Iteration 604, loss = 0.48226430\n",
      "Iteration 605, loss = 0.48213757\n",
      "Iteration 606, loss = 0.48201102\n",
      "Iteration 607, loss = 0.48188465\n",
      "Iteration 608, loss = 0.48175854\n",
      "Iteration 609, loss = 0.48163264\n",
      "Iteration 610, loss = 0.48150691\n",
      "Iteration 611, loss = 0.48138145\n",
      "Iteration 612, loss = 0.48125618\n",
      "Iteration 613, loss = 0.48113111\n",
      "Iteration 614, loss = 0.48100622\n",
      "Iteration 615, loss = 0.48088152\n",
      "Iteration 616, loss = 0.48075699\n",
      "Iteration 617, loss = 0.48063265\n",
      "Iteration 618, loss = 0.48050852\n",
      "Iteration 619, loss = 0.48038463\n",
      "Iteration 620, loss = 0.48026093\n",
      "Iteration 621, loss = 0.48013741\n",
      "Iteration 622, loss = 0.48001406\n",
      "Iteration 623, loss = 0.47989090\n",
      "Iteration 624, loss = 0.47976800\n",
      "Iteration 625, loss = 0.47964532\n",
      "Iteration 626, loss = 0.47952282\n",
      "Iteration 627, loss = 0.47940050\n",
      "Iteration 628, loss = 0.47927847\n",
      "Iteration 629, loss = 0.47915665\n",
      "Iteration 630, loss = 0.47903503\n",
      "Iteration 631, loss = 0.47891360\n",
      "Iteration 632, loss = 0.47879236\n",
      "Iteration 633, loss = 0.47867130\n",
      "Iteration 634, loss = 0.47855042\n",
      "Iteration 635, loss = 0.47842971\n",
      "Iteration 636, loss = 0.47830917\n",
      "Iteration 637, loss = 0.47818880\n",
      "Iteration 638, loss = 0.47806861\n",
      "Iteration 639, loss = 0.47794858\n",
      "Iteration 640, loss = 0.47782872\n",
      "Iteration 641, loss = 0.47770903\n",
      "Iteration 642, loss = 0.47758951\n",
      "Iteration 643, loss = 0.47747015\n",
      "Iteration 644, loss = 0.47735095\n",
      "Iteration 645, loss = 0.47723192\n",
      "Iteration 646, loss = 0.47711304\n",
      "Iteration 647, loss = 0.47699438\n",
      "Iteration 648, loss = 0.47687595\n",
      "Iteration 649, loss = 0.47675769\n",
      "Iteration 650, loss = 0.47663958\n",
      "Iteration 651, loss = 0.47652164\n",
      "Iteration 652, loss = 0.47640392\n",
      "Iteration 653, loss = 0.47628640\n",
      "Iteration 654, loss = 0.47616905\n",
      "Iteration 655, loss = 0.47605186\n",
      "Iteration 656, loss = 0.47593486\n",
      "Iteration 657, loss = 0.47581807\n",
      "Iteration 658, loss = 0.47570152\n",
      "Iteration 659, loss = 0.47558514\n",
      "Iteration 660, loss = 0.47546894\n",
      "Iteration 661, loss = 0.47535293\n",
      "Iteration 662, loss = 0.47523708\n",
      "Iteration 663, loss = 0.47512139\n",
      "Iteration 664, loss = 0.47500586\n",
      "Iteration 665, loss = 0.47489050\n",
      "Iteration 666, loss = 0.47477529\n",
      "Iteration 667, loss = 0.47466025\n",
      "Iteration 668, loss = 0.47454536\n",
      "Iteration 669, loss = 0.47443063\n",
      "Iteration 670, loss = 0.47431605\n",
      "Iteration 671, loss = 0.47420165\n",
      "Iteration 672, loss = 0.47408742\n",
      "Iteration 673, loss = 0.47397335\n",
      "Iteration 674, loss = 0.47385944\n",
      "Iteration 675, loss = 0.47374570\n",
      "Iteration 676, loss = 0.47363211\n",
      "Iteration 677, loss = 0.47351867\n",
      "Iteration 678, loss = 0.47340539\n",
      "Iteration 679, loss = 0.47329226\n",
      "Iteration 680, loss = 0.47317928\n",
      "Iteration 681, loss = 0.47306645\n",
      "Iteration 682, loss = 0.47295381\n",
      "Iteration 683, loss = 0.47284135\n",
      "Iteration 684, loss = 0.47272904\n",
      "Iteration 685, loss = 0.47261688\n",
      "Iteration 686, loss = 0.47250487\n",
      "Iteration 687, loss = 0.47239301\n",
      "Iteration 688, loss = 0.47228130\n",
      "Iteration 689, loss = 0.47216974\n",
      "Iteration 690, loss = 0.47205832\n",
      "Iteration 691, loss = 0.47194706\n",
      "Iteration 692, loss = 0.47183594\n",
      "Iteration 693, loss = 0.47172496\n",
      "Iteration 694, loss = 0.47161413\n",
      "Iteration 695, loss = 0.47150345\n",
      "Iteration 696, loss = 0.47139291\n",
      "Iteration 697, loss = 0.47128255\n",
      "Iteration 698, loss = 0.47117239\n",
      "Iteration 699, loss = 0.47106238\n",
      "Iteration 700, loss = 0.47095251\n",
      "Iteration 701, loss = 0.47084279\n",
      "Iteration 702, loss = 0.47073322\n",
      "Iteration 703, loss = 0.47062380\n",
      "Iteration 704, loss = 0.47051452\n",
      "Iteration 705, loss = 0.47040538\n",
      "Iteration 706, loss = 0.47029639\n",
      "Iteration 707, loss = 0.47018754\n",
      "Iteration 708, loss = 0.47007883\n",
      "Iteration 709, loss = 0.46997028\n",
      "Iteration 710, loss = 0.46986186\n",
      "Iteration 711, loss = 0.46975359\n",
      "Iteration 712, loss = 0.46964546\n",
      "Iteration 713, loss = 0.46953746\n",
      "Iteration 714, loss = 0.46942961\n",
      "Iteration 715, loss = 0.46932190\n",
      "Iteration 716, loss = 0.46921432\n",
      "Iteration 717, loss = 0.46910689\n",
      "Iteration 718, loss = 0.46899959\n",
      "Iteration 719, loss = 0.46889243\n",
      "Iteration 720, loss = 0.46878541\n",
      "Iteration 721, loss = 0.46867853\n",
      "Iteration 722, loss = 0.46857178\n",
      "Iteration 723, loss = 0.46846517\n",
      "Iteration 724, loss = 0.46835870\n",
      "Iteration 725, loss = 0.46825236\n",
      "Iteration 726, loss = 0.46814616\n",
      "Iteration 727, loss = 0.46804009\n",
      "Iteration 728, loss = 0.46793416\n",
      "Iteration 729, loss = 0.46782837\n",
      "Iteration 730, loss = 0.46772274\n",
      "Iteration 731, loss = 0.46761725\n",
      "Iteration 732, loss = 0.46751186\n",
      "Iteration 733, loss = 0.46740651\n",
      "Iteration 734, loss = 0.46730128\n",
      "Iteration 735, loss = 0.46719618\n",
      "Iteration 736, loss = 0.46709120\n",
      "Iteration 737, loss = 0.46698635\n",
      "Iteration 738, loss = 0.46688162\n",
      "Iteration 739, loss = 0.46677703\n",
      "Iteration 740, loss = 0.46667255\n",
      "Iteration 741, loss = 0.46656821\n",
      "Iteration 742, loss = 0.46646399\n",
      "Iteration 743, loss = 0.46635990\n",
      "Iteration 744, loss = 0.46625595\n",
      "Iteration 745, loss = 0.46615219\n",
      "Iteration 746, loss = 0.46604863\n",
      "Iteration 747, loss = 0.46594521\n",
      "Iteration 748, loss = 0.46584192\n",
      "Iteration 749, loss = 0.46573876\n",
      "Iteration 750, loss = 0.46563574\n",
      "Iteration 751, loss = 0.46553285\n",
      "Iteration 752, loss = 0.46543010\n",
      "Iteration 753, loss = 0.46532748\n",
      "Iteration 754, loss = 0.46522498\n",
      "Iteration 755, loss = 0.46512262\n",
      "Iteration 756, loss = 0.46502039\n",
      "Iteration 757, loss = 0.46491830\n",
      "Iteration 758, loss = 0.46481634\n",
      "Iteration 759, loss = 0.46471451\n",
      "Iteration 760, loss = 0.46461282\n",
      "Iteration 761, loss = 0.46451125\n",
      "Iteration 762, loss = 0.46440981\n",
      "Iteration 763, loss = 0.46430850\n",
      "Iteration 764, loss = 0.46420732\n",
      "Iteration 765, loss = 0.46410629\n",
      "Iteration 766, loss = 0.46400539\n",
      "Iteration 767, loss = 0.46390462\n",
      "Iteration 768, loss = 0.46380397\n",
      "Iteration 769, loss = 0.46370346\n",
      "Iteration 770, loss = 0.46360312\n",
      "Iteration 771, loss = 0.46350290\n",
      "Iteration 772, loss = 0.46340280\n",
      "Iteration 773, loss = 0.46330284\n",
      "Iteration 774, loss = 0.46320300\n",
      "Iteration 775, loss = 0.46310329\n",
      "Iteration 776, loss = 0.46300371\n",
      "Iteration 777, loss = 0.46290425\n",
      "Iteration 778, loss = 0.46280491\n",
      "Iteration 779, loss = 0.46270570\n",
      "Iteration 780, loss = 0.46260662\n",
      "Iteration 781, loss = 0.46250766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 782, loss = 0.46240882\n",
      "Iteration 783, loss = 0.46231010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.830000\n",
      "Training set loss: 0.462310\n",
      "training: inv-scaling with Nesterov's momentum\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.69564741\n",
      "Iteration 3, loss = 0.68962121\n",
      "Iteration 4, loss = 0.68410154\n",
      "Iteration 5, loss = 0.67914062\n",
      "Iteration 6, loss = 0.67470961\n",
      "Iteration 7, loss = 0.67074331\n",
      "Iteration 8, loss = 0.66718107\n",
      "Iteration 9, loss = 0.66394707\n",
      "Iteration 10, loss = 0.66103039\n",
      "Iteration 11, loss = 0.65833859\n",
      "Iteration 12, loss = 0.65583627\n",
      "Iteration 13, loss = 0.65349783\n",
      "Iteration 14, loss = 0.65132454\n",
      "Iteration 15, loss = 0.64929157\n",
      "Iteration 16, loss = 0.64737641\n",
      "Iteration 17, loss = 0.64556167\n",
      "Iteration 18, loss = 0.64383799\n",
      "Iteration 19, loss = 0.64219133\n",
      "Iteration 20, loss = 0.64060967\n",
      "Iteration 21, loss = 0.63909344\n",
      "Iteration 22, loss = 0.63763960\n",
      "Iteration 23, loss = 0.63624472\n",
      "Iteration 24, loss = 0.63490238\n",
      "Iteration 25, loss = 0.63360756\n",
      "Iteration 26, loss = 0.63235873\n",
      "Iteration 27, loss = 0.63115426\n",
      "Iteration 28, loss = 0.62999024\n",
      "Iteration 29, loss = 0.62886723\n",
      "Iteration 30, loss = 0.62777872\n",
      "Iteration 31, loss = 0.62672591\n",
      "Iteration 32, loss = 0.62570652\n",
      "Iteration 33, loss = 0.62471992\n",
      "Iteration 34, loss = 0.62376396\n",
      "Iteration 35, loss = 0.62283415\n",
      "Iteration 36, loss = 0.62192842\n",
      "Iteration 37, loss = 0.62104678\n",
      "Iteration 38, loss = 0.62018801\n",
      "Iteration 39, loss = 0.61935148\n",
      "Iteration 40, loss = 0.61853679\n",
      "Iteration 41, loss = 0.61774264\n",
      "Iteration 42, loss = 0.61696820\n",
      "Iteration 43, loss = 0.61621231\n",
      "Iteration 44, loss = 0.61547407\n",
      "Iteration 45, loss = 0.61475166\n",
      "Iteration 46, loss = 0.61404392\n",
      "Iteration 47, loss = 0.61335080\n",
      "Iteration 48, loss = 0.61267307\n",
      "Iteration 49, loss = 0.61200746\n",
      "Iteration 50, loss = 0.61135433\n",
      "Iteration 51, loss = 0.61071320\n",
      "Iteration 52, loss = 0.61008470\n",
      "Iteration 53, loss = 0.60946678\n",
      "Iteration 54, loss = 0.60885880\n",
      "Iteration 55, loss = 0.60826000\n",
      "Iteration 56, loss = 0.60766943\n",
      "Iteration 57, loss = 0.60708775\n",
      "Iteration 58, loss = 0.60651459\n",
      "Iteration 59, loss = 0.60594875\n",
      "Iteration 60, loss = 0.60539047\n",
      "Iteration 61, loss = 0.60483951\n",
      "Iteration 62, loss = 0.60429543\n",
      "Iteration 63, loss = 0.60375785\n",
      "Iteration 64, loss = 0.60322648\n",
      "Iteration 65, loss = 0.60270113\n",
      "Iteration 66, loss = 0.60218125\n",
      "Iteration 67, loss = 0.60166744\n",
      "Iteration 68, loss = 0.60115909\n",
      "Iteration 69, loss = 0.60065618\n",
      "Iteration 70, loss = 0.60015880\n",
      "Iteration 71, loss = 0.59966692\n",
      "Iteration 72, loss = 0.59917995\n",
      "Iteration 73, loss = 0.59869703\n",
      "Iteration 74, loss = 0.59821855\n",
      "Iteration 75, loss = 0.59774507\n",
      "Iteration 76, loss = 0.59727581\n",
      "Iteration 77, loss = 0.59681059\n",
      "Iteration 78, loss = 0.59634890\n",
      "Iteration 79, loss = 0.59589096\n",
      "Iteration 80, loss = 0.59543844\n",
      "Iteration 81, loss = 0.59498973\n",
      "Iteration 82, loss = 0.59454499\n",
      "Iteration 83, loss = 0.59410461\n",
      "Iteration 84, loss = 0.59366771\n",
      "Iteration 85, loss = 0.59323420\n",
      "Iteration 86, loss = 0.59280416\n",
      "Iteration 87, loss = 0.59237722\n",
      "Iteration 88, loss = 0.59195321\n",
      "Iteration 89, loss = 0.59153232\n",
      "Iteration 90, loss = 0.59111463\n",
      "Iteration 91, loss = 0.59069956\n",
      "Iteration 92, loss = 0.59028735\n",
      "Iteration 93, loss = 0.58987779\n",
      "Iteration 94, loss = 0.58947123\n",
      "Iteration 95, loss = 0.58906745\n",
      "Iteration 96, loss = 0.58866629\n",
      "Iteration 97, loss = 0.58826763\n",
      "Iteration 98, loss = 0.58787122\n",
      "Iteration 99, loss = 0.58747711\n",
      "Iteration 100, loss = 0.58708521\n",
      "Iteration 101, loss = 0.58669517\n",
      "Iteration 102, loss = 0.58630726\n",
      "Iteration 103, loss = 0.58592133\n",
      "Iteration 104, loss = 0.58553804\n",
      "Iteration 105, loss = 0.58515688\n",
      "Iteration 106, loss = 0.58477782\n",
      "Iteration 107, loss = 0.58440051\n",
      "Iteration 108, loss = 0.58402542\n",
      "Iteration 109, loss = 0.58365244\n",
      "Iteration 110, loss = 0.58328169\n",
      "Iteration 111, loss = 0.58291303\n",
      "Iteration 112, loss = 0.58254604\n",
      "Iteration 113, loss = 0.58218044\n",
      "Iteration 114, loss = 0.58181661\n",
      "Iteration 115, loss = 0.58145433\n",
      "Iteration 116, loss = 0.58109382\n",
      "Iteration 117, loss = 0.58073501\n",
      "Iteration 118, loss = 0.58037796\n",
      "Iteration 119, loss = 0.58002278\n",
      "Iteration 120, loss = 0.57966942\n",
      "Iteration 121, loss = 0.57931771\n",
      "Iteration 122, loss = 0.57896748\n",
      "Iteration 123, loss = 0.57861873\n",
      "Iteration 124, loss = 0.57827166\n",
      "Iteration 125, loss = 0.57792588\n",
      "Iteration 126, loss = 0.57758124\n",
      "Iteration 127, loss = 0.57723831\n",
      "Iteration 128, loss = 0.57689691\n",
      "Iteration 129, loss = 0.57655694\n",
      "Iteration 130, loss = 0.57621813\n",
      "Iteration 131, loss = 0.57588064\n",
      "Iteration 132, loss = 0.57554484\n",
      "Iteration 133, loss = 0.57521054\n",
      "Iteration 134, loss = 0.57487778\n",
      "Iteration 135, loss = 0.57454675\n",
      "Iteration 136, loss = 0.57421706\n",
      "Iteration 137, loss = 0.57388851\n",
      "Iteration 138, loss = 0.57356146\n",
      "Iteration 139, loss = 0.57323571\n",
      "Iteration 140, loss = 0.57291120\n",
      "Iteration 141, loss = 0.57258793\n",
      "Iteration 142, loss = 0.57226584\n",
      "Iteration 143, loss = 0.57194498\n",
      "Iteration 144, loss = 0.57162549\n",
      "Iteration 145, loss = 0.57130744\n",
      "Iteration 146, loss = 0.57099117\n",
      "Iteration 147, loss = 0.57067654\n",
      "Iteration 148, loss = 0.57036308\n",
      "Iteration 149, loss = 0.57005102\n",
      "Iteration 150, loss = 0.56973996\n",
      "Iteration 151, loss = 0.56942998\n",
      "Iteration 152, loss = 0.56912116\n",
      "Iteration 153, loss = 0.56881358\n",
      "Iteration 154, loss = 0.56850706\n",
      "Iteration 155, loss = 0.56820163\n",
      "Iteration 156, loss = 0.56789727\n",
      "Iteration 157, loss = 0.56759392\n",
      "Iteration 158, loss = 0.56729152\n",
      "Iteration 159, loss = 0.56699013\n",
      "Iteration 160, loss = 0.56668985\n",
      "Iteration 161, loss = 0.56639077\n",
      "Iteration 162, loss = 0.56609276\n",
      "Iteration 163, loss = 0.56579577\n",
      "Iteration 164, loss = 0.56549976\n",
      "Iteration 165, loss = 0.56520479\n",
      "Iteration 166, loss = 0.56491089\n",
      "Iteration 167, loss = 0.56461790\n",
      "Iteration 168, loss = 0.56432580\n",
      "Iteration 169, loss = 0.56403466\n",
      "Iteration 170, loss = 0.56374453\n",
      "Iteration 171, loss = 0.56345532\n",
      "Iteration 172, loss = 0.56316717\n",
      "Iteration 173, loss = 0.56288003\n",
      "Iteration 174, loss = 0.56259390\n",
      "Iteration 175, loss = 0.56230860\n",
      "Iteration 176, loss = 0.56202438\n",
      "Iteration 177, loss = 0.56174125\n",
      "Iteration 178, loss = 0.56145910\n",
      "Iteration 179, loss = 0.56117772\n",
      "Iteration 180, loss = 0.56089715\n",
      "Iteration 181, loss = 0.56061750\n",
      "Iteration 182, loss = 0.56033887\n",
      "Iteration 183, loss = 0.56006118\n",
      "Iteration 184, loss = 0.55978436\n",
      "Iteration 185, loss = 0.55950836\n",
      "Iteration 186, loss = 0.55923312\n",
      "Iteration 187, loss = 0.55895868\n",
      "Iteration 188, loss = 0.55868518\n",
      "Iteration 189, loss = 0.55841275\n",
      "Iteration 190, loss = 0.55814111\n",
      "Iteration 191, loss = 0.55787033\n",
      "Iteration 192, loss = 0.55760046\n",
      "Iteration 193, loss = 0.55733164\n",
      "Iteration 194, loss = 0.55706329\n",
      "Iteration 195, loss = 0.55679552\n",
      "Iteration 196, loss = 0.55652836\n",
      "Iteration 197, loss = 0.55626194\n",
      "Iteration 198, loss = 0.55599624\n",
      "Iteration 199, loss = 0.55573126\n",
      "Iteration 200, loss = 0.55546698\n",
      "Iteration 201, loss = 0.55520340\n",
      "Iteration 202, loss = 0.55494052\n",
      "Iteration 203, loss = 0.55467832\n",
      "Iteration 204, loss = 0.55441682\n",
      "Iteration 205, loss = 0.55415601\n",
      "Iteration 206, loss = 0.55389598\n",
      "Iteration 207, loss = 0.55363649\n",
      "Iteration 208, loss = 0.55337780\n",
      "Iteration 209, loss = 0.55311986\n",
      "Iteration 210, loss = 0.55286257\n",
      "Iteration 211, loss = 0.55260577\n",
      "Iteration 212, loss = 0.55234965\n",
      "Iteration 213, loss = 0.55209418\n",
      "Iteration 214, loss = 0.55183935\n",
      "Iteration 215, loss = 0.55158517\n",
      "Iteration 216, loss = 0.55133163\n",
      "Iteration 217, loss = 0.55107880\n",
      "Iteration 218, loss = 0.55082660\n",
      "Iteration 219, loss = 0.55057496\n",
      "Iteration 220, loss = 0.55032404\n",
      "Iteration 221, loss = 0.55007389\n",
      "Iteration 222, loss = 0.54982450\n",
      "Iteration 223, loss = 0.54957601\n",
      "Iteration 224, loss = 0.54932825\n",
      "Iteration 225, loss = 0.54908107\n",
      "Iteration 226, loss = 0.54883441\n",
      "Iteration 227, loss = 0.54858836\n",
      "Iteration 228, loss = 0.54834292\n",
      "Iteration 229, loss = 0.54809810\n",
      "Iteration 230, loss = 0.54785390\n",
      "Iteration 231, loss = 0.54761022\n",
      "Iteration 232, loss = 0.54736708\n",
      "Iteration 233, loss = 0.54712440\n",
      "Iteration 234, loss = 0.54688233\n",
      "Iteration 235, loss = 0.54664086\n",
      "Iteration 236, loss = 0.54639999\n",
      "Iteration 237, loss = 0.54615968\n",
      "Iteration 238, loss = 0.54591993\n",
      "Iteration 239, loss = 0.54568073\n",
      "Iteration 240, loss = 0.54544213\n",
      "Iteration 241, loss = 0.54520414\n",
      "Iteration 242, loss = 0.54496670\n",
      "Iteration 243, loss = 0.54472986\n",
      "Iteration 244, loss = 0.54449356\n",
      "Iteration 245, loss = 0.54425782\n",
      "Iteration 246, loss = 0.54402255\n",
      "Iteration 247, loss = 0.54378754\n",
      "Iteration 248, loss = 0.54355303\n",
      "Iteration 249, loss = 0.54331903\n",
      "Iteration 250, loss = 0.54308540\n",
      "Iteration 251, loss = 0.54285213\n",
      "Iteration 252, loss = 0.54261934\n",
      "Iteration 253, loss = 0.54238709\n",
      "Iteration 254, loss = 0.54215537\n",
      "Iteration 255, loss = 0.54192425\n",
      "Iteration 256, loss = 0.54169362\n",
      "Iteration 257, loss = 0.54146350\n",
      "Iteration 258, loss = 0.54123395\n",
      "Iteration 259, loss = 0.54100493\n",
      "Iteration 260, loss = 0.54077642\n",
      "Iteration 261, loss = 0.54054840\n",
      "Iteration 262, loss = 0.54032088\n",
      "Iteration 263, loss = 0.54009384\n",
      "Iteration 264, loss = 0.53986736\n",
      "Iteration 265, loss = 0.53964155\n",
      "Iteration 266, loss = 0.53941646\n",
      "Iteration 267, loss = 0.53919189\n",
      "Iteration 268, loss = 0.53896783\n",
      "Iteration 269, loss = 0.53874437\n",
      "Iteration 270, loss = 0.53852147\n",
      "Iteration 271, loss = 0.53829914\n",
      "Iteration 272, loss = 0.53807734\n",
      "Iteration 273, loss = 0.53785605\n",
      "Iteration 274, loss = 0.53763527\n",
      "Iteration 275, loss = 0.53741495\n",
      "Iteration 276, loss = 0.53719502\n",
      "Iteration 277, loss = 0.53697558\n",
      "Iteration 278, loss = 0.53675661\n",
      "Iteration 279, loss = 0.53653812\n",
      "Iteration 280, loss = 0.53632012\n",
      "Iteration 281, loss = 0.53610265\n",
      "Iteration 282, loss = 0.53588552\n",
      "Iteration 283, loss = 0.53566885\n",
      "Iteration 284, loss = 0.53545266\n",
      "Iteration 285, loss = 0.53523692\n",
      "Iteration 286, loss = 0.53502184\n",
      "Iteration 287, loss = 0.53480722\n",
      "Iteration 288, loss = 0.53459313\n",
      "Iteration 289, loss = 0.53437951\n",
      "Iteration 290, loss = 0.53416638\n",
      "Iteration 291, loss = 0.53395375\n",
      "Iteration 292, loss = 0.53374158\n",
      "Iteration 293, loss = 0.53352983\n",
      "Iteration 294, loss = 0.53331852\n",
      "Iteration 295, loss = 0.53310765\n",
      "Iteration 296, loss = 0.53289714\n",
      "Iteration 297, loss = 0.53268712\n",
      "Iteration 298, loss = 0.53247756\n",
      "Iteration 299, loss = 0.53226831\n",
      "Iteration 300, loss = 0.53205944\n",
      "Iteration 301, loss = 0.53185103\n",
      "Iteration 302, loss = 0.53164304\n",
      "Iteration 303, loss = 0.53143543\n",
      "Iteration 304, loss = 0.53122822\n",
      "Iteration 305, loss = 0.53102145\n",
      "Iteration 306, loss = 0.53081509\n",
      "Iteration 307, loss = 0.53060912\n",
      "Iteration 308, loss = 0.53040354\n",
      "Iteration 309, loss = 0.53019834\n",
      "Iteration 310, loss = 0.52999357\n",
      "Iteration 311, loss = 0.52978922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 312, loss = 0.52958537\n",
      "Iteration 313, loss = 0.52938192\n",
      "Iteration 314, loss = 0.52917887\n",
      "Iteration 315, loss = 0.52897641\n",
      "Iteration 316, loss = 0.52877422\n",
      "Iteration 317, loss = 0.52857230\n",
      "Iteration 318, loss = 0.52837056\n",
      "Iteration 319, loss = 0.52816917\n",
      "Iteration 320, loss = 0.52796814\n",
      "Iteration 321, loss = 0.52776747\n",
      "Iteration 322, loss = 0.52756716\n",
      "Iteration 323, loss = 0.52736720\n",
      "Iteration 324, loss = 0.52716762\n",
      "Iteration 325, loss = 0.52696839\n",
      "Iteration 326, loss = 0.52676953\n",
      "Iteration 327, loss = 0.52657103\n",
      "Iteration 328, loss = 0.52637288\n",
      "Iteration 329, loss = 0.52617515\n",
      "Iteration 330, loss = 0.52597773\n",
      "Iteration 331, loss = 0.52578056\n",
      "Iteration 332, loss = 0.52558375\n",
      "Iteration 333, loss = 0.52538731\n",
      "Iteration 334, loss = 0.52519122\n",
      "Iteration 335, loss = 0.52499548\n",
      "Iteration 336, loss = 0.52480009\n",
      "Iteration 337, loss = 0.52460505\n",
      "Iteration 338, loss = 0.52441036\n",
      "Iteration 339, loss = 0.52421611\n",
      "Iteration 340, loss = 0.52402239\n",
      "Iteration 341, loss = 0.52382908\n",
      "Iteration 342, loss = 0.52363596\n",
      "Iteration 343, loss = 0.52344320\n",
      "Iteration 344, loss = 0.52325078\n",
      "Iteration 345, loss = 0.52305873\n",
      "Iteration 346, loss = 0.52286704\n",
      "Iteration 347, loss = 0.52267587\n",
      "Iteration 348, loss = 0.52248506\n",
      "Iteration 349, loss = 0.52229468\n",
      "Iteration 350, loss = 0.52210468\n",
      "Iteration 351, loss = 0.52191504\n",
      "Iteration 352, loss = 0.52172576\n",
      "Iteration 353, loss = 0.52153684\n",
      "Iteration 354, loss = 0.52134827\n",
      "Iteration 355, loss = 0.52116008\n",
      "Iteration 356, loss = 0.52097233\n",
      "Iteration 357, loss = 0.52078491\n",
      "Iteration 358, loss = 0.52059783\n",
      "Iteration 359, loss = 0.52041110\n",
      "Iteration 360, loss = 0.52022477\n",
      "Iteration 361, loss = 0.52003887\n",
      "Iteration 362, loss = 0.51985333\n",
      "Iteration 363, loss = 0.51966812\n",
      "Iteration 364, loss = 0.51948325\n",
      "Iteration 365, loss = 0.51929874\n",
      "Iteration 366, loss = 0.51911457\n",
      "Iteration 367, loss = 0.51893074\n",
      "Iteration 368, loss = 0.51874723\n",
      "Iteration 369, loss = 0.51856406\n",
      "Iteration 370, loss = 0.51838121\n",
      "Iteration 371, loss = 0.51819869\n",
      "Iteration 372, loss = 0.51801649\n",
      "Iteration 373, loss = 0.51783468\n",
      "Iteration 374, loss = 0.51765325\n",
      "Iteration 375, loss = 0.51747198\n",
      "Iteration 376, loss = 0.51729103\n",
      "Iteration 377, loss = 0.51711044\n",
      "Iteration 378, loss = 0.51693017\n",
      "Iteration 379, loss = 0.51675021\n",
      "Iteration 380, loss = 0.51657056\n",
      "Iteration 381, loss = 0.51639122\n",
      "Iteration 382, loss = 0.51621207\n",
      "Iteration 383, loss = 0.51603316\n",
      "Iteration 384, loss = 0.51585455\n",
      "Iteration 385, loss = 0.51567627\n",
      "Iteration 386, loss = 0.51549828\n",
      "Iteration 387, loss = 0.51532058\n",
      "Iteration 388, loss = 0.51514316\n",
      "Iteration 389, loss = 0.51496586\n",
      "Iteration 390, loss = 0.51478885\n",
      "Iteration 391, loss = 0.51461214\n",
      "Iteration 392, loss = 0.51443570\n",
      "Iteration 393, loss = 0.51425955\n",
      "Iteration 394, loss = 0.51408368\n",
      "Iteration 395, loss = 0.51390811\n",
      "Iteration 396, loss = 0.51373279\n",
      "Iteration 397, loss = 0.51355758\n",
      "Iteration 398, loss = 0.51338263\n",
      "Iteration 399, loss = 0.51320796\n",
      "Iteration 400, loss = 0.51303355\n",
      "Iteration 401, loss = 0.51285942\n",
      "Iteration 402, loss = 0.51268552\n",
      "Iteration 403, loss = 0.51251168\n",
      "Iteration 404, loss = 0.51233805\n",
      "Iteration 405, loss = 0.51216468\n",
      "Iteration 406, loss = 0.51199156\n",
      "Iteration 407, loss = 0.51181869\n",
      "Iteration 408, loss = 0.51164609\n",
      "Iteration 409, loss = 0.51147375\n",
      "Iteration 410, loss = 0.51130167\n",
      "Iteration 411, loss = 0.51112990\n",
      "Iteration 412, loss = 0.51095842\n",
      "Iteration 413, loss = 0.51078719\n",
      "Iteration 414, loss = 0.51061627\n",
      "Iteration 415, loss = 0.51044567\n",
      "Iteration 416, loss = 0.51027536\n",
      "Iteration 417, loss = 0.51010532\n",
      "Iteration 418, loss = 0.50993556\n",
      "Iteration 419, loss = 0.50976607\n",
      "Iteration 420, loss = 0.50959689\n",
      "Iteration 421, loss = 0.50942799\n",
      "Iteration 422, loss = 0.50925939\n",
      "Iteration 423, loss = 0.50909109\n",
      "Iteration 424, loss = 0.50892290\n",
      "Iteration 425, loss = 0.50875494\n",
      "Iteration 426, loss = 0.50858725\n",
      "Iteration 427, loss = 0.50841989\n",
      "Iteration 428, loss = 0.50825280\n",
      "Iteration 429, loss = 0.50808607\n",
      "Iteration 430, loss = 0.50791969\n",
      "Iteration 431, loss = 0.50775359\n",
      "Iteration 432, loss = 0.50758777\n",
      "Iteration 433, loss = 0.50742239\n",
      "Iteration 434, loss = 0.50725732\n",
      "Iteration 435, loss = 0.50709272\n",
      "Iteration 436, loss = 0.50692841\n",
      "Iteration 437, loss = 0.50676440\n",
      "Iteration 438, loss = 0.50660068\n",
      "Iteration 439, loss = 0.50643724\n",
      "Iteration 440, loss = 0.50627408\n",
      "Iteration 441, loss = 0.50611120\n",
      "Iteration 442, loss = 0.50594859\n",
      "Iteration 443, loss = 0.50578626\n",
      "Iteration 444, loss = 0.50562419\n",
      "Iteration 445, loss = 0.50546240\n",
      "Iteration 446, loss = 0.50530092\n",
      "Iteration 447, loss = 0.50513981\n",
      "Iteration 448, loss = 0.50497897\n",
      "Iteration 449, loss = 0.50481840\n",
      "Iteration 450, loss = 0.50465808\n",
      "Iteration 451, loss = 0.50449801\n",
      "Iteration 452, loss = 0.50433820\n",
      "Iteration 453, loss = 0.50417870\n",
      "Iteration 454, loss = 0.50401944\n",
      "Iteration 455, loss = 0.50386033\n",
      "Iteration 456, loss = 0.50370150\n",
      "Iteration 457, loss = 0.50354294\n",
      "Iteration 458, loss = 0.50338463\n",
      "Iteration 459, loss = 0.50322657\n",
      "Iteration 460, loss = 0.50306879\n",
      "Iteration 461, loss = 0.50291133\n",
      "Iteration 462, loss = 0.50275413\n",
      "Iteration 463, loss = 0.50259720\n",
      "Iteration 464, loss = 0.50244058\n",
      "Iteration 465, loss = 0.50228425\n",
      "Iteration 466, loss = 0.50212824\n",
      "Iteration 467, loss = 0.50197249\n",
      "Iteration 468, loss = 0.50181703\n",
      "Iteration 469, loss = 0.50166193\n",
      "Iteration 470, loss = 0.50150709\n",
      "Iteration 471, loss = 0.50135252\n",
      "Iteration 472, loss = 0.50119822\n",
      "Iteration 473, loss = 0.50104418\n",
      "Iteration 474, loss = 0.50089040\n",
      "Iteration 475, loss = 0.50073686\n",
      "Iteration 476, loss = 0.50058354\n",
      "Iteration 477, loss = 0.50043028\n",
      "Iteration 478, loss = 0.50027725\n",
      "Iteration 479, loss = 0.50012443\n",
      "Iteration 480, loss = 0.49997163\n",
      "Iteration 481, loss = 0.49981904\n",
      "Iteration 482, loss = 0.49966666\n",
      "Iteration 483, loss = 0.49951428\n",
      "Iteration 484, loss = 0.49936208\n",
      "Iteration 485, loss = 0.49921007\n",
      "Iteration 486, loss = 0.49905826\n",
      "Iteration 487, loss = 0.49890665\n",
      "Iteration 488, loss = 0.49875523\n",
      "Iteration 489, loss = 0.49860402\n",
      "Iteration 490, loss = 0.49845301\n",
      "Iteration 491, loss = 0.49830221\n",
      "Iteration 492, loss = 0.49815162\n",
      "Iteration 493, loss = 0.49800124\n",
      "Iteration 494, loss = 0.49785111\n",
      "Iteration 495, loss = 0.49770122\n",
      "Iteration 496, loss = 0.49755155\n",
      "Iteration 497, loss = 0.49740209\n",
      "Iteration 498, loss = 0.49725285\n",
      "Iteration 499, loss = 0.49710383\n",
      "Iteration 500, loss = 0.49695502\n",
      "Iteration 501, loss = 0.49680643\n",
      "Iteration 502, loss = 0.49665806\n",
      "Iteration 503, loss = 0.49650992\n",
      "Iteration 504, loss = 0.49636211\n",
      "Iteration 505, loss = 0.49621452\n",
      "Iteration 506, loss = 0.49606716\n",
      "Iteration 507, loss = 0.49592003\n",
      "Iteration 508, loss = 0.49577312\n",
      "Iteration 509, loss = 0.49562645\n",
      "Iteration 510, loss = 0.49548002\n",
      "Iteration 511, loss = 0.49533382\n",
      "Iteration 512, loss = 0.49518783\n",
      "Iteration 513, loss = 0.49504207\n",
      "Iteration 514, loss = 0.49489652\n",
      "Iteration 515, loss = 0.49475122\n",
      "Iteration 516, loss = 0.49460614\n",
      "Iteration 517, loss = 0.49446129\n",
      "Iteration 518, loss = 0.49431665\n",
      "Iteration 519, loss = 0.49417223\n",
      "Iteration 520, loss = 0.49402803\n",
      "Iteration 521, loss = 0.49388405\n",
      "Iteration 522, loss = 0.49374027\n",
      "Iteration 523, loss = 0.49359671\n",
      "Iteration 524, loss = 0.49345336\n",
      "Iteration 525, loss = 0.49331015\n",
      "Iteration 526, loss = 0.49316711\n",
      "Iteration 527, loss = 0.49302428\n",
      "Iteration 528, loss = 0.49288163\n",
      "Iteration 529, loss = 0.49273919\n",
      "Iteration 530, loss = 0.49259695\n",
      "Iteration 531, loss = 0.49245491\n",
      "Iteration 532, loss = 0.49231311\n",
      "Iteration 533, loss = 0.49217151\n",
      "Iteration 534, loss = 0.49203011\n",
      "Iteration 535, loss = 0.49188897\n",
      "Iteration 536, loss = 0.49174811\n",
      "Iteration 537, loss = 0.49160747\n",
      "Iteration 538, loss = 0.49146705\n",
      "Iteration 539, loss = 0.49132683\n",
      "Iteration 540, loss = 0.49118683\n",
      "Iteration 541, loss = 0.49104703\n",
      "Iteration 542, loss = 0.49090743\n",
      "Iteration 543, loss = 0.49076805\n",
      "Iteration 544, loss = 0.49062886\n",
      "Iteration 545, loss = 0.49048988\n",
      "Iteration 546, loss = 0.49035110\n",
      "Iteration 547, loss = 0.49021256\n",
      "Iteration 548, loss = 0.49007422\n",
      "Iteration 549, loss = 0.48993609\n",
      "Iteration 550, loss = 0.48979816\n",
      "Iteration 551, loss = 0.48966044\n",
      "Iteration 552, loss = 0.48952293\n",
      "Iteration 553, loss = 0.48938561\n",
      "Iteration 554, loss = 0.48924850\n",
      "Iteration 555, loss = 0.48911158\n",
      "Iteration 556, loss = 0.48897486\n",
      "Iteration 557, loss = 0.48883840\n",
      "Iteration 558, loss = 0.48870219\n",
      "Iteration 559, loss = 0.48856620\n",
      "Iteration 560, loss = 0.48843040\n",
      "Iteration 561, loss = 0.48829481\n",
      "Iteration 562, loss = 0.48815942\n",
      "Iteration 563, loss = 0.48802422\n",
      "Iteration 564, loss = 0.48788923\n",
      "Iteration 565, loss = 0.48775443\n",
      "Iteration 566, loss = 0.48761982\n",
      "Iteration 567, loss = 0.48748541\n",
      "Iteration 568, loss = 0.48735119\n",
      "Iteration 569, loss = 0.48721716\n",
      "Iteration 570, loss = 0.48708333\n",
      "Iteration 571, loss = 0.48694969\n",
      "Iteration 572, loss = 0.48681624\n",
      "Iteration 573, loss = 0.48668298\n",
      "Iteration 574, loss = 0.48654990\n",
      "Iteration 575, loss = 0.48641702\n",
      "Iteration 576, loss = 0.48628432\n",
      "Iteration 577, loss = 0.48615181\n",
      "Iteration 578, loss = 0.48601948\n",
      "Iteration 579, loss = 0.48588734\n",
      "Iteration 580, loss = 0.48575539\n",
      "Iteration 581, loss = 0.48562362\n",
      "Iteration 582, loss = 0.48549205\n",
      "Iteration 583, loss = 0.48536066\n",
      "Iteration 584, loss = 0.48522945\n",
      "Iteration 585, loss = 0.48509843\n",
      "Iteration 586, loss = 0.48496758\n",
      "Iteration 587, loss = 0.48483692\n",
      "Iteration 588, loss = 0.48470645\n",
      "Iteration 589, loss = 0.48457615\n",
      "Iteration 590, loss = 0.48444604\n",
      "Iteration 591, loss = 0.48431612\n",
      "Iteration 592, loss = 0.48418638\n",
      "Iteration 593, loss = 0.48405682\n",
      "Iteration 594, loss = 0.48392745\n",
      "Iteration 595, loss = 0.48379825\n",
      "Iteration 596, loss = 0.48366928\n",
      "Iteration 597, loss = 0.48354063\n",
      "Iteration 598, loss = 0.48341216\n",
      "Iteration 599, loss = 0.48328389\n",
      "Iteration 600, loss = 0.48315581\n",
      "Iteration 601, loss = 0.48302791\n",
      "Iteration 602, loss = 0.48290020\n",
      "Iteration 603, loss = 0.48277268\n",
      "Iteration 604, loss = 0.48264537\n",
      "Iteration 605, loss = 0.48251827\n",
      "Iteration 606, loss = 0.48239134\n",
      "Iteration 607, loss = 0.48226461\n",
      "Iteration 608, loss = 0.48213805\n",
      "Iteration 609, loss = 0.48201168\n",
      "Iteration 610, loss = 0.48188561\n",
      "Iteration 611, loss = 0.48175977\n",
      "Iteration 612, loss = 0.48163412\n",
      "Iteration 613, loss = 0.48150865\n",
      "Iteration 614, loss = 0.48138336\n",
      "Iteration 615, loss = 0.48125826\n",
      "Iteration 616, loss = 0.48113334\n",
      "Iteration 617, loss = 0.48100859\n",
      "Iteration 618, loss = 0.48088405\n",
      "Iteration 619, loss = 0.48075976\n",
      "Iteration 620, loss = 0.48063564\n",
      "Iteration 621, loss = 0.48051171\n",
      "Iteration 622, loss = 0.48038795\n",
      "Iteration 623, loss = 0.48026438\n",
      "Iteration 624, loss = 0.48014097\n",
      "Iteration 625, loss = 0.48001777\n",
      "Iteration 626, loss = 0.47989486\n",
      "Iteration 627, loss = 0.47977216\n",
      "Iteration 628, loss = 0.47964965\n",
      "Iteration 629, loss = 0.47952731\n",
      "Iteration 630, loss = 0.47940516\n",
      "Iteration 631, loss = 0.47928318\n",
      "Iteration 632, loss = 0.47916138\n",
      "Iteration 633, loss = 0.47903979\n",
      "Iteration 634, loss = 0.47891838\n",
      "Iteration 635, loss = 0.47879717\n",
      "Iteration 636, loss = 0.47867624\n",
      "Iteration 637, loss = 0.47855550\n",
      "Iteration 638, loss = 0.47843493\n",
      "Iteration 639, loss = 0.47831454\n",
      "Iteration 640, loss = 0.47819433\n",
      "Iteration 641, loss = 0.47807429\n",
      "Iteration 642, loss = 0.47795442\n",
      "Iteration 643, loss = 0.47783471\n",
      "Iteration 644, loss = 0.47771519\n",
      "Iteration 645, loss = 0.47759585\n",
      "Iteration 646, loss = 0.47747667\n",
      "Iteration 647, loss = 0.47735766\n",
      "Iteration 648, loss = 0.47723881\n",
      "Iteration 649, loss = 0.47712013\n",
      "Iteration 650, loss = 0.47700161\n",
      "Iteration 651, loss = 0.47688325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 652, loss = 0.47676505\n",
      "Iteration 653, loss = 0.47664700\n",
      "Iteration 654, loss = 0.47652911\n",
      "Iteration 655, loss = 0.47641145\n",
      "Iteration 656, loss = 0.47629398\n",
      "Iteration 657, loss = 0.47617669\n",
      "Iteration 658, loss = 0.47605956\n",
      "Iteration 659, loss = 0.47594261\n",
      "Iteration 660, loss = 0.47582590\n",
      "Iteration 661, loss = 0.47570937\n",
      "Iteration 662, loss = 0.47559301\n",
      "Iteration 663, loss = 0.47547681\n",
      "Iteration 664, loss = 0.47536082\n",
      "Iteration 665, loss = 0.47524503\n",
      "Iteration 666, loss = 0.47512944\n",
      "Iteration 667, loss = 0.47501403\n",
      "Iteration 668, loss = 0.47489878\n",
      "Iteration 669, loss = 0.47478372\n",
      "Iteration 670, loss = 0.47466883\n",
      "Iteration 671, loss = 0.47455411\n",
      "Iteration 672, loss = 0.47443956\n",
      "Iteration 673, loss = 0.47432516\n",
      "Iteration 674, loss = 0.47421093\n",
      "Iteration 675, loss = 0.47409685\n",
      "Iteration 676, loss = 0.47398293\n",
      "Iteration 677, loss = 0.47386917\n",
      "Iteration 678, loss = 0.47375557\n",
      "Iteration 679, loss = 0.47364212\n",
      "Iteration 680, loss = 0.47352882\n",
      "Iteration 681, loss = 0.47341568\n",
      "Iteration 682, loss = 0.47330272\n",
      "Iteration 683, loss = 0.47318991\n",
      "Iteration 684, loss = 0.47307725\n",
      "Iteration 685, loss = 0.47296474\n",
      "Iteration 686, loss = 0.47285238\n",
      "Iteration 687, loss = 0.47274017\n",
      "Iteration 688, loss = 0.47262811\n",
      "Iteration 689, loss = 0.47251620\n",
      "Iteration 690, loss = 0.47240448\n",
      "Iteration 691, loss = 0.47229293\n",
      "Iteration 692, loss = 0.47218152\n",
      "Iteration 693, loss = 0.47207026\n",
      "Iteration 694, loss = 0.47195915\n",
      "Iteration 695, loss = 0.47184819\n",
      "Iteration 696, loss = 0.47173738\n",
      "Iteration 697, loss = 0.47162671\n",
      "Iteration 698, loss = 0.47151619\n",
      "Iteration 699, loss = 0.47140581\n",
      "Iteration 700, loss = 0.47129558\n",
      "Iteration 701, loss = 0.47118549\n",
      "Iteration 702, loss = 0.47107554\n",
      "Iteration 703, loss = 0.47096574\n",
      "Iteration 704, loss = 0.47085608\n",
      "Iteration 705, loss = 0.47074660\n",
      "Iteration 706, loss = 0.47063732\n",
      "Iteration 707, loss = 0.47052819\n",
      "Iteration 708, loss = 0.47041921\n",
      "Iteration 709, loss = 0.47031037\n",
      "Iteration 710, loss = 0.47020168\n",
      "Iteration 711, loss = 0.47009313\n",
      "Iteration 712, loss = 0.46998473\n",
      "Iteration 713, loss = 0.46987647\n",
      "Iteration 714, loss = 0.46976835\n",
      "Iteration 715, loss = 0.46966038\n",
      "Iteration 716, loss = 0.46955256\n",
      "Iteration 717, loss = 0.46944487\n",
      "Iteration 718, loss = 0.46933732\n",
      "Iteration 719, loss = 0.46922992\n",
      "Iteration 720, loss = 0.46912265\n",
      "Iteration 721, loss = 0.46901552\n",
      "Iteration 722, loss = 0.46890854\n",
      "Iteration 723, loss = 0.46880169\n",
      "Iteration 724, loss = 0.46869497\n",
      "Iteration 725, loss = 0.46858840\n",
      "Iteration 726, loss = 0.46848196\n",
      "Iteration 727, loss = 0.46837565\n",
      "Iteration 728, loss = 0.46826949\n",
      "Iteration 729, loss = 0.46816345\n",
      "Iteration 730, loss = 0.46805756\n",
      "Iteration 731, loss = 0.46795180\n",
      "Iteration 732, loss = 0.46784617\n",
      "Iteration 733, loss = 0.46774068\n",
      "Iteration 734, loss = 0.46763532\n",
      "Iteration 735, loss = 0.46753009\n",
      "Iteration 736, loss = 0.46742501\n",
      "Iteration 737, loss = 0.46732007\n",
      "Iteration 738, loss = 0.46721528\n",
      "Iteration 739, loss = 0.46711062\n",
      "Iteration 740, loss = 0.46700607\n",
      "Iteration 741, loss = 0.46690156\n",
      "Iteration 742, loss = 0.46679718\n",
      "Iteration 743, loss = 0.46669291\n",
      "Iteration 744, loss = 0.46658877\n",
      "Iteration 745, loss = 0.46648475\n",
      "Iteration 746, loss = 0.46638088\n",
      "Iteration 747, loss = 0.46627718\n",
      "Iteration 748, loss = 0.46617361\n",
      "Iteration 749, loss = 0.46607018\n",
      "Iteration 750, loss = 0.46596687\n",
      "Iteration 751, loss = 0.46586369\n",
      "Iteration 752, loss = 0.46576064\n",
      "Iteration 753, loss = 0.46565778\n",
      "Iteration 754, loss = 0.46555506\n",
      "Iteration 755, loss = 0.46545247\n",
      "Iteration 756, loss = 0.46535002\n",
      "Iteration 757, loss = 0.46524769\n",
      "Iteration 758, loss = 0.46514550\n",
      "Iteration 759, loss = 0.46504344\n",
      "Iteration 760, loss = 0.46494151\n",
      "Iteration 761, loss = 0.46483971\n",
      "Iteration 762, loss = 0.46473804\n",
      "Iteration 763, loss = 0.46463649\n",
      "Iteration 764, loss = 0.46453508\n",
      "Iteration 765, loss = 0.46443379\n",
      "Iteration 766, loss = 0.46433263\n",
      "Iteration 767, loss = 0.46423160\n",
      "Iteration 768, loss = 0.46413069\n",
      "Iteration 769, loss = 0.46402991\n",
      "Iteration 770, loss = 0.46392926\n",
      "Iteration 771, loss = 0.46382874\n",
      "Iteration 772, loss = 0.46372834\n",
      "Iteration 773, loss = 0.46362807\n",
      "Iteration 774, loss = 0.46352795\n",
      "Iteration 775, loss = 0.46342795\n",
      "Iteration 776, loss = 0.46332808\n",
      "Iteration 777, loss = 0.46322834\n",
      "Iteration 778, loss = 0.46312876\n",
      "Iteration 779, loss = 0.46302930\n",
      "Iteration 780, loss = 0.46292997\n",
      "Iteration 781, loss = 0.46283076\n",
      "Iteration 782, loss = 0.46273168\n",
      "Iteration 783, loss = 0.46263273\n",
      "Iteration 784, loss = 0.46253389\n",
      "Iteration 785, loss = 0.46243519\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.830000\n",
      "Training set loss: 0.462435\n",
      "training: adam\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.68106102\n",
      "Iteration 3, loss = 0.66182315\n",
      "Iteration 4, loss = 0.64448290\n",
      "Iteration 5, loss = 0.62787734\n",
      "Iteration 6, loss = 0.61134223\n",
      "Iteration 7, loss = 0.59451705\n",
      "Iteration 8, loss = 0.57753243\n",
      "Iteration 9, loss = 0.56077073\n",
      "Iteration 10, loss = 0.54456220\n",
      "Iteration 11, loss = 0.52877007\n",
      "Iteration 12, loss = 0.51324851\n",
      "Iteration 13, loss = 0.49790490\n",
      "Iteration 14, loss = 0.48289293\n",
      "Iteration 15, loss = 0.46840178\n",
      "Iteration 16, loss = 0.45456744\n",
      "Iteration 17, loss = 0.44149629\n",
      "Iteration 18, loss = 0.42917531\n",
      "Iteration 19, loss = 0.41766200\n",
      "Iteration 20, loss = 0.40713847\n",
      "Iteration 21, loss = 0.39766541\n",
      "Iteration 22, loss = 0.38921958\n",
      "Iteration 23, loss = 0.38174028\n",
      "Iteration 24, loss = 0.37518050\n",
      "Iteration 25, loss = 0.36950418\n",
      "Iteration 26, loss = 0.36466924\n",
      "Iteration 27, loss = 0.36059261\n",
      "Iteration 28, loss = 0.35718415\n",
      "Iteration 29, loss = 0.35435243\n",
      "Iteration 30, loss = 0.35201517\n",
      "Iteration 31, loss = 0.35009869\n",
      "Iteration 32, loss = 0.34848705\n",
      "Iteration 33, loss = 0.34711121\n",
      "Iteration 34, loss = 0.34594989\n",
      "Iteration 35, loss = 0.34494737\n",
      "Iteration 36, loss = 0.34404608\n",
      "Iteration 37, loss = 0.34320319\n",
      "Iteration 38, loss = 0.34241979\n",
      "Iteration 39, loss = 0.34168580\n",
      "Iteration 40, loss = 0.34096642\n",
      "Iteration 41, loss = 0.34027263\n",
      "Iteration 42, loss = 0.33961287\n",
      "Iteration 43, loss = 0.33898148\n",
      "Iteration 44, loss = 0.33838072\n",
      "Iteration 45, loss = 0.33781954\n",
      "Iteration 46, loss = 0.33728108\n",
      "Iteration 47, loss = 0.33680752\n",
      "Iteration 48, loss = 0.33637796\n",
      "Iteration 49, loss = 0.33598823\n",
      "Iteration 50, loss = 0.33565366\n",
      "Iteration 51, loss = 0.33538737\n",
      "Iteration 52, loss = 0.33513895\n",
      "Iteration 53, loss = 0.33494621\n",
      "Iteration 54, loss = 0.33479678\n",
      "Iteration 55, loss = 0.33467545\n",
      "Iteration 56, loss = 0.33457670\n",
      "Iteration 57, loss = 0.33446850\n",
      "Iteration 58, loss = 0.33440244\n",
      "Iteration 59, loss = 0.33432882\n",
      "Iteration 60, loss = 0.33424262\n",
      "Iteration 61, loss = 0.33413614\n",
      "Iteration 62, loss = 0.33403104\n",
      "Iteration 63, loss = 0.33389994\n",
      "Iteration 64, loss = 0.33375410\n",
      "Iteration 65, loss = 0.33358515\n",
      "Iteration 66, loss = 0.33339739\n",
      "Iteration 67, loss = 0.33318531\n",
      "Iteration 68, loss = 0.33296235\n",
      "Iteration 69, loss = 0.33271561\n",
      "Iteration 70, loss = 0.33246511\n",
      "Iteration 71, loss = 0.33220658\n",
      "Iteration 72, loss = 0.33193758\n",
      "Iteration 73, loss = 0.33163835\n",
      "Iteration 74, loss = 0.33136625\n",
      "Iteration 75, loss = 0.33107465\n",
      "Iteration 76, loss = 0.33076836\n",
      "Iteration 77, loss = 0.33046313\n",
      "Iteration 78, loss = 0.33015092\n",
      "Iteration 79, loss = 0.32983391\n",
      "Iteration 80, loss = 0.32953166\n",
      "Iteration 81, loss = 0.32920634\n",
      "Iteration 82, loss = 0.32886338\n",
      "Iteration 83, loss = 0.32853861\n",
      "Iteration 84, loss = 0.32818711\n",
      "Iteration 85, loss = 0.32782762\n",
      "Iteration 86, loss = 0.32747796\n",
      "Iteration 87, loss = 0.32711024\n",
      "Iteration 88, loss = 0.32670376\n",
      "Iteration 89, loss = 0.32633913\n",
      "Iteration 90, loss = 0.32594329\n",
      "Iteration 91, loss = 0.32551346\n",
      "Iteration 92, loss = 0.32508099\n",
      "Iteration 93, loss = 0.32464489\n",
      "Iteration 94, loss = 0.32421735\n",
      "Iteration 95, loss = 0.32375698\n",
      "Iteration 96, loss = 0.32331006\n",
      "Iteration 97, loss = 0.32285890\n",
      "Iteration 98, loss = 0.32239123\n",
      "Iteration 99, loss = 0.32191958\n",
      "Iteration 100, loss = 0.32143744\n",
      "Iteration 101, loss = 0.32092783\n",
      "Iteration 102, loss = 0.32044167\n",
      "Iteration 103, loss = 0.31992657\n",
      "Iteration 104, loss = 0.31940652\n",
      "Iteration 105, loss = 0.31888343\n",
      "Iteration 106, loss = 0.31837019\n",
      "Iteration 107, loss = 0.31782398\n",
      "Iteration 108, loss = 0.31730417\n",
      "Iteration 109, loss = 0.31676623\n",
      "Iteration 110, loss = 0.31621686\n",
      "Iteration 111, loss = 0.31568013\n",
      "Iteration 112, loss = 0.31510787\n",
      "Iteration 113, loss = 0.31452934\n",
      "Iteration 114, loss = 0.31398626\n",
      "Iteration 115, loss = 0.31340610\n",
      "Iteration 116, loss = 0.31280496\n",
      "Iteration 117, loss = 0.31226205\n",
      "Iteration 118, loss = 0.31167575\n",
      "Iteration 119, loss = 0.31107725\n",
      "Iteration 120, loss = 0.31047811\n",
      "Iteration 121, loss = 0.30992243\n",
      "Iteration 122, loss = 0.30932603\n",
      "Iteration 123, loss = 0.30868689\n",
      "Iteration 124, loss = 0.30812768\n",
      "Iteration 125, loss = 0.30754240\n",
      "Iteration 126, loss = 0.30689195\n",
      "Iteration 127, loss = 0.30629549\n",
      "Iteration 128, loss = 0.30572876\n",
      "Iteration 129, loss = 0.30508611\n",
      "Iteration 130, loss = 0.30451731\n",
      "Iteration 131, loss = 0.30393184\n",
      "Iteration 132, loss = 0.30331231\n",
      "Iteration 133, loss = 0.30273305\n",
      "Iteration 134, loss = 0.30214459\n",
      "Iteration 135, loss = 0.30154584\n",
      "Iteration 136, loss = 0.30096520\n",
      "Iteration 137, loss = 0.30034795\n",
      "Iteration 138, loss = 0.29971214\n",
      "Iteration 139, loss = 0.29912353\n",
      "Iteration 140, loss = 0.29854480\n",
      "Iteration 141, loss = 0.29792588\n",
      "Iteration 142, loss = 0.29734779\n",
      "Iteration 143, loss = 0.29672453\n",
      "Iteration 144, loss = 0.29616375\n",
      "Iteration 145, loss = 0.29554531\n",
      "Iteration 146, loss = 0.29498765\n",
      "Iteration 147, loss = 0.29438345\n",
      "Iteration 148, loss = 0.29381684\n",
      "Iteration 149, loss = 0.29324904\n",
      "Iteration 150, loss = 0.29262852\n",
      "Iteration 151, loss = 0.29204227\n",
      "Iteration 152, loss = 0.29150410\n",
      "Iteration 153, loss = 0.29093672\n",
      "Iteration 154, loss = 0.29032202\n",
      "Iteration 155, loss = 0.28967352\n",
      "Iteration 156, loss = 0.28914841\n",
      "Iteration 157, loss = 0.28862377\n",
      "Iteration 158, loss = 0.28802386\n",
      "Iteration 159, loss = 0.28738939\n",
      "Iteration 160, loss = 0.28680421\n",
      "Iteration 161, loss = 0.28625078\n",
      "Iteration 162, loss = 0.28566011\n",
      "Iteration 163, loss = 0.28516522\n",
      "Iteration 164, loss = 0.28459941\n",
      "Iteration 165, loss = 0.28397617\n",
      "Iteration 166, loss = 0.28334586\n",
      "Iteration 167, loss = 0.28281704\n",
      "Iteration 168, loss = 0.28229347\n",
      "Iteration 169, loss = 0.28169962\n",
      "Iteration 170, loss = 0.28111462\n",
      "Iteration 171, loss = 0.28053396\n",
      "Iteration 172, loss = 0.27988434\n",
      "Iteration 173, loss = 0.27933167\n",
      "Iteration 174, loss = 0.27879728\n",
      "Iteration 175, loss = 0.27820285\n",
      "Iteration 176, loss = 0.27758827\n",
      "Iteration 177, loss = 0.27701117\n",
      "Iteration 178, loss = 0.27645986\n",
      "Iteration 179, loss = 0.27590075\n",
      "Iteration 180, loss = 0.27530162\n",
      "Iteration 181, loss = 0.27468103\n",
      "Iteration 182, loss = 0.27408370\n",
      "Iteration 183, loss = 0.27360496\n",
      "Iteration 184, loss = 0.27297710\n",
      "Iteration 185, loss = 0.27246666\n",
      "Iteration 186, loss = 0.27182307\n",
      "Iteration 187, loss = 0.27127458\n",
      "Iteration 188, loss = 0.27074470\n",
      "Iteration 189, loss = 0.27010247\n",
      "Iteration 190, loss = 0.26951256\n",
      "Iteration 191, loss = 0.26892960\n",
      "Iteration 192, loss = 0.26832284\n",
      "Iteration 193, loss = 0.26775259\n",
      "Iteration 194, loss = 0.26711712\n",
      "Iteration 195, loss = 0.26656496\n",
      "Iteration 196, loss = 0.26594466\n",
      "Iteration 197, loss = 0.26542074\n",
      "Iteration 198, loss = 0.26482027\n",
      "Iteration 199, loss = 0.26415016\n",
      "Iteration 200, loss = 0.26352350\n",
      "Iteration 201, loss = 0.26292460\n",
      "Iteration 202, loss = 0.26231991\n",
      "Iteration 203, loss = 0.26172159\n",
      "Iteration 204, loss = 0.26110581\n",
      "Iteration 205, loss = 0.26054166\n",
      "Iteration 206, loss = 0.25990907\n",
      "Iteration 207, loss = 0.25928567\n",
      "Iteration 208, loss = 0.25869000\n",
      "Iteration 209, loss = 0.25807948\n",
      "Iteration 210, loss = 0.25746593\n",
      "Iteration 211, loss = 0.25689963\n",
      "Iteration 212, loss = 0.25633573\n",
      "Iteration 213, loss = 0.25568074\n",
      "Iteration 214, loss = 0.25500922\n",
      "Iteration 215, loss = 0.25446965\n",
      "Iteration 216, loss = 0.25382962\n",
      "Iteration 217, loss = 0.25323343\n",
      "Iteration 218, loss = 0.25260743\n",
      "Iteration 219, loss = 0.25198806\n",
      "Iteration 220, loss = 0.25136822\n",
      "Iteration 221, loss = 0.25080349\n",
      "Iteration 222, loss = 0.25022441\n",
      "Iteration 223, loss = 0.24956934\n",
      "Iteration 224, loss = 0.24891779\n",
      "Iteration 225, loss = 0.24831614\n",
      "Iteration 226, loss = 0.24773137\n",
      "Iteration 227, loss = 0.24710182\n",
      "Iteration 228, loss = 0.24650446\n",
      "Iteration 229, loss = 0.24595941\n",
      "Iteration 230, loss = 0.24539008\n",
      "Iteration 231, loss = 0.24469075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 232, loss = 0.24403913\n",
      "Iteration 233, loss = 0.24354180\n",
      "Iteration 234, loss = 0.24298885\n",
      "Iteration 235, loss = 0.24235997\n",
      "Iteration 236, loss = 0.24167101\n",
      "Iteration 237, loss = 0.24103968\n",
      "Iteration 238, loss = 0.24048915\n",
      "Iteration 239, loss = 0.23985274\n",
      "Iteration 240, loss = 0.23919244\n",
      "Iteration 241, loss = 0.23862566\n",
      "Iteration 242, loss = 0.23811825\n",
      "Iteration 243, loss = 0.23749388\n",
      "Iteration 244, loss = 0.23692177\n",
      "Iteration 245, loss = 0.23630636\n",
      "Iteration 246, loss = 0.23565192\n",
      "Iteration 247, loss = 0.23510214\n",
      "Iteration 248, loss = 0.23457556\n",
      "Iteration 249, loss = 0.23393911\n",
      "Iteration 250, loss = 0.23329366\n",
      "Iteration 251, loss = 0.23275499\n",
      "Iteration 252, loss = 0.23221235\n",
      "Iteration 253, loss = 0.23159961\n",
      "Iteration 254, loss = 0.23093758\n",
      "Iteration 255, loss = 0.23033375\n",
      "Iteration 256, loss = 0.22983410\n",
      "Iteration 257, loss = 0.22925146\n",
      "Iteration 258, loss = 0.22864393\n",
      "Iteration 259, loss = 0.22814676\n",
      "Iteration 260, loss = 0.22756800\n",
      "Iteration 261, loss = 0.22695855\n",
      "Iteration 262, loss = 0.22647510\n",
      "Iteration 263, loss = 0.22591574\n",
      "Iteration 264, loss = 0.22534767\n",
      "Iteration 265, loss = 0.22472516\n",
      "Iteration 266, loss = 0.22422919\n",
      "Iteration 267, loss = 0.22361821\n",
      "Iteration 268, loss = 0.22315183\n",
      "Iteration 269, loss = 0.22249612\n",
      "Iteration 270, loss = 0.22203278\n",
      "Iteration 271, loss = 0.22144425\n",
      "Iteration 272, loss = 0.22095783\n",
      "Iteration 273, loss = 0.22042056\n",
      "Iteration 274, loss = 0.21985320\n",
      "Iteration 275, loss = 0.21941662\n",
      "Iteration 276, loss = 0.21888025\n",
      "Iteration 277, loss = 0.21840756\n",
      "Iteration 278, loss = 0.21781169\n",
      "Iteration 279, loss = 0.21731284\n",
      "Iteration 280, loss = 0.21678988\n",
      "Iteration 281, loss = 0.21632406\n",
      "Iteration 282, loss = 0.21576703\n",
      "Iteration 283, loss = 0.21532653\n",
      "Iteration 284, loss = 0.21475927\n",
      "Iteration 285, loss = 0.21431990\n",
      "Iteration 286, loss = 0.21379597\n",
      "Iteration 287, loss = 0.21329747\n",
      "Iteration 288, loss = 0.21280547\n",
      "Iteration 289, loss = 0.21232848\n",
      "Iteration 290, loss = 0.21185813\n",
      "Iteration 291, loss = 0.21129245\n",
      "Iteration 292, loss = 0.21088415\n",
      "Iteration 293, loss = 0.21042937\n",
      "Iteration 294, loss = 0.20988951\n",
      "Iteration 295, loss = 0.20946492\n",
      "Iteration 296, loss = 0.20902786\n",
      "Iteration 297, loss = 0.20851935\n",
      "Iteration 298, loss = 0.20800161\n",
      "Iteration 299, loss = 0.20756074\n",
      "Iteration 300, loss = 0.20704131\n",
      "Iteration 301, loss = 0.20660872\n",
      "Iteration 302, loss = 0.20614241\n",
      "Iteration 303, loss = 0.20571052\n",
      "Iteration 304, loss = 0.20526414\n",
      "Iteration 305, loss = 0.20480375\n",
      "Iteration 306, loss = 0.20435712\n",
      "Iteration 307, loss = 0.20393085\n",
      "Iteration 308, loss = 0.20343335\n",
      "Iteration 309, loss = 0.20298950\n",
      "Iteration 310, loss = 0.20258710\n",
      "Iteration 311, loss = 0.20212209\n",
      "Iteration 312, loss = 0.20173781\n",
      "Iteration 313, loss = 0.20130726\n",
      "Iteration 314, loss = 0.20086012\n",
      "Iteration 315, loss = 0.20047758\n",
      "Iteration 316, loss = 0.20008396\n",
      "Iteration 317, loss = 0.19963582\n",
      "Iteration 318, loss = 0.19913018\n",
      "Iteration 319, loss = 0.19873501\n",
      "Iteration 320, loss = 0.19830750\n",
      "Iteration 321, loss = 0.19789338\n",
      "Iteration 322, loss = 0.19751747\n",
      "Iteration 323, loss = 0.19708615\n",
      "Iteration 324, loss = 0.19665571\n",
      "Iteration 325, loss = 0.19628658\n",
      "Iteration 326, loss = 0.19585815\n",
      "Iteration 327, loss = 0.19550631\n",
      "Iteration 328, loss = 0.19506295\n",
      "Iteration 329, loss = 0.19475687\n",
      "Iteration 330, loss = 0.19431619\n",
      "Iteration 331, loss = 0.19391536\n",
      "Iteration 332, loss = 0.19347793\n",
      "Iteration 333, loss = 0.19315229\n",
      "Iteration 334, loss = 0.19269341\n",
      "Iteration 335, loss = 0.19242925\n",
      "Iteration 336, loss = 0.19201804\n",
      "Iteration 337, loss = 0.19164852\n",
      "Iteration 338, loss = 0.19125227\n",
      "Iteration 339, loss = 0.19095825\n",
      "Iteration 340, loss = 0.19047884\n",
      "Iteration 341, loss = 0.19017629\n",
      "Iteration 342, loss = 0.18981108\n",
      "Iteration 343, loss = 0.18944326\n",
      "Iteration 344, loss = 0.18898744\n",
      "Iteration 345, loss = 0.18874145\n",
      "Iteration 346, loss = 0.18838836\n",
      "Iteration 347, loss = 0.18801830\n",
      "Iteration 348, loss = 0.18753749\n",
      "Iteration 349, loss = 0.18729181\n",
      "Iteration 350, loss = 0.18698950\n",
      "Iteration 351, loss = 0.18663479\n",
      "Iteration 352, loss = 0.18621270\n",
      "Iteration 353, loss = 0.18578538\n",
      "Iteration 354, loss = 0.18556343\n",
      "Iteration 355, loss = 0.18529295\n",
      "Iteration 356, loss = 0.18489918\n",
      "Iteration 357, loss = 0.18443110\n",
      "Iteration 358, loss = 0.18413095\n",
      "Iteration 359, loss = 0.18386715\n",
      "Iteration 360, loss = 0.18354556\n",
      "Iteration 361, loss = 0.18318440\n",
      "Iteration 362, loss = 0.18275519\n",
      "Iteration 363, loss = 0.18237190\n",
      "Iteration 364, loss = 0.18216784\n",
      "Iteration 365, loss = 0.18189757\n",
      "Iteration 366, loss = 0.18148136\n",
      "Iteration 367, loss = 0.18100461\n",
      "Iteration 368, loss = 0.18075581\n",
      "Iteration 369, loss = 0.18055929\n",
      "Iteration 370, loss = 0.18019775\n",
      "Iteration 371, loss = 0.17982249\n",
      "Iteration 372, loss = 0.17936421\n",
      "Iteration 373, loss = 0.17887804\n",
      "Iteration 374, loss = 0.17862517\n",
      "Iteration 375, loss = 0.17843540\n",
      "Iteration 376, loss = 0.17793605\n",
      "Iteration 377, loss = 0.17741067\n",
      "Iteration 378, loss = 0.17704248\n",
      "Iteration 379, loss = 0.17685422\n",
      "Iteration 380, loss = 0.17659805\n",
      "Iteration 381, loss = 0.17620594\n",
      "Iteration 382, loss = 0.17573591\n",
      "Iteration 383, loss = 0.17527606\n",
      "Iteration 384, loss = 0.17517391\n",
      "Iteration 385, loss = 0.17485572\n",
      "Iteration 386, loss = 0.17463782\n",
      "Iteration 387, loss = 0.17418125\n",
      "Iteration 388, loss = 0.17380924\n",
      "Iteration 389, loss = 0.17337054\n",
      "Iteration 390, loss = 0.17321824\n",
      "Iteration 391, loss = 0.17286651\n",
      "Iteration 392, loss = 0.17252751\n",
      "Iteration 393, loss = 0.17216757\n",
      "Iteration 394, loss = 0.17195767\n",
      "Iteration 395, loss = 0.17163878\n",
      "Iteration 396, loss = 0.17129493\n",
      "Iteration 397, loss = 0.17098998\n",
      "Iteration 398, loss = 0.17073361\n",
      "Iteration 399, loss = 0.17043130\n",
      "Iteration 400, loss = 0.17008667\n",
      "Iteration 401, loss = 0.16973343\n",
      "Iteration 402, loss = 0.16949500\n",
      "Iteration 403, loss = 0.16920844\n",
      "Iteration 404, loss = 0.16898042\n",
      "Iteration 405, loss = 0.16862155\n",
      "Iteration 406, loss = 0.16838155\n",
      "Iteration 407, loss = 0.16804876\n",
      "Iteration 408, loss = 0.16777566\n",
      "Iteration 409, loss = 0.16743939\n",
      "Iteration 410, loss = 0.16720655\n",
      "Iteration 411, loss = 0.16694653\n",
      "Iteration 412, loss = 0.16661108\n",
      "Iteration 413, loss = 0.16640956\n",
      "Iteration 414, loss = 0.16614806\n",
      "Iteration 415, loss = 0.16578960\n",
      "Iteration 416, loss = 0.16564695\n",
      "Iteration 417, loss = 0.16544599\n",
      "Iteration 418, loss = 0.16517118\n",
      "Iteration 419, loss = 0.16483925\n",
      "Iteration 420, loss = 0.16451022\n",
      "Iteration 421, loss = 0.16428900\n",
      "Iteration 422, loss = 0.16405611\n",
      "Iteration 423, loss = 0.16372970\n",
      "Iteration 424, loss = 0.16345832\n",
      "Iteration 425, loss = 0.16322024\n",
      "Iteration 426, loss = 0.16300070\n",
      "Iteration 427, loss = 0.16272105\n",
      "Iteration 428, loss = 0.16240834\n",
      "Iteration 429, loss = 0.16220725\n",
      "Iteration 430, loss = 0.16193952\n",
      "Iteration 431, loss = 0.16169062\n",
      "Iteration 432, loss = 0.16146562\n",
      "Iteration 433, loss = 0.16118126\n",
      "Iteration 434, loss = 0.16092873\n",
      "Iteration 435, loss = 0.16074655\n",
      "Iteration 436, loss = 0.16046399\n",
      "Iteration 437, loss = 0.16021778\n",
      "Iteration 438, loss = 0.16002693\n",
      "Iteration 439, loss = 0.15980431\n",
      "Iteration 440, loss = 0.15949560\n",
      "Iteration 441, loss = 0.15922404\n",
      "Iteration 442, loss = 0.15905308\n",
      "Iteration 443, loss = 0.15885318\n",
      "Iteration 444, loss = 0.15853057\n",
      "Iteration 445, loss = 0.15835002\n",
      "Iteration 446, loss = 0.15816555\n",
      "Iteration 447, loss = 0.15795466\n",
      "Iteration 448, loss = 0.15772906\n",
      "Iteration 449, loss = 0.15746784\n",
      "Iteration 450, loss = 0.15718992\n",
      "Iteration 451, loss = 0.15697733\n",
      "Iteration 452, loss = 0.15678109\n",
      "Iteration 453, loss = 0.15649487\n",
      "Iteration 454, loss = 0.15630659\n",
      "Iteration 455, loss = 0.15606208\n",
      "Iteration 456, loss = 0.15589123\n",
      "Iteration 457, loss = 0.15563704\n",
      "Iteration 458, loss = 0.15544610\n",
      "Iteration 459, loss = 0.15520165\n",
      "Iteration 460, loss = 0.15501864\n",
      "Iteration 461, loss = 0.15480615\n",
      "Iteration 462, loss = 0.15463870\n",
      "Iteration 463, loss = 0.15437804\n",
      "Iteration 464, loss = 0.15413475\n",
      "Iteration 465, loss = 0.15392861\n",
      "Iteration 466, loss = 0.15368514\n",
      "Iteration 467, loss = 0.15344917\n",
      "Iteration 468, loss = 0.15329280\n",
      "Iteration 469, loss = 0.15308263\n",
      "Iteration 470, loss = 0.15286316\n",
      "Iteration 471, loss = 0.15261030\n",
      "Iteration 472, loss = 0.15240550\n",
      "Iteration 473, loss = 0.15220125\n",
      "Iteration 474, loss = 0.15206655\n",
      "Iteration 475, loss = 0.15179313\n",
      "Iteration 476, loss = 0.15164946\n",
      "Iteration 477, loss = 0.15146889\n",
      "Iteration 478, loss = 0.15128068\n",
      "Iteration 479, loss = 0.15107409\n",
      "Iteration 480, loss = 0.15080641\n",
      "Iteration 481, loss = 0.15057833\n",
      "Iteration 482, loss = 0.15046747\n",
      "Iteration 483, loss = 0.15034186\n",
      "Iteration 484, loss = 0.14987475\n",
      "Iteration 485, loss = 0.14969051\n",
      "Iteration 486, loss = 0.14945768\n",
      "Iteration 487, loss = 0.14921273\n",
      "Iteration 488, loss = 0.14894993\n",
      "Iteration 489, loss = 0.14875375\n",
      "Iteration 490, loss = 0.14857701\n",
      "Iteration 491, loss = 0.14835426\n",
      "Iteration 492, loss = 0.14817097\n",
      "Iteration 493, loss = 0.14781022\n",
      "Iteration 494, loss = 0.14774648\n",
      "Iteration 495, loss = 0.14752287\n",
      "Iteration 496, loss = 0.14745243\n",
      "Iteration 497, loss = 0.14721029\n",
      "Iteration 498, loss = 0.14696238\n",
      "Iteration 499, loss = 0.14684159\n",
      "Iteration 500, loss = 0.14658427\n",
      "Iteration 501, loss = 0.14629733\n",
      "Iteration 502, loss = 0.14608167\n",
      "Iteration 503, loss = 0.14605318\n",
      "Iteration 504, loss = 0.14574008\n",
      "Iteration 505, loss = 0.14559893\n",
      "Iteration 506, loss = 0.14534732\n",
      "Iteration 507, loss = 0.14521013\n",
      "Iteration 508, loss = 0.14503312\n",
      "Iteration 509, loss = 0.14493232\n",
      "Iteration 510, loss = 0.14472805\n",
      "Iteration 511, loss = 0.14453071\n",
      "Iteration 512, loss = 0.14427711\n",
      "Iteration 513, loss = 0.14411240\n",
      "Iteration 514, loss = 0.14404950\n",
      "Iteration 515, loss = 0.14381556\n",
      "Iteration 516, loss = 0.14368398\n",
      "Iteration 517, loss = 0.14345241\n",
      "Iteration 518, loss = 0.14337528\n",
      "Iteration 519, loss = 0.14319328\n",
      "Iteration 520, loss = 0.14299595\n",
      "Iteration 521, loss = 0.14279886\n",
      "Iteration 522, loss = 0.14261235\n",
      "Iteration 523, loss = 0.14243649\n",
      "Iteration 524, loss = 0.14227725\n",
      "Iteration 525, loss = 0.14215623\n",
      "Iteration 526, loss = 0.14199039\n",
      "Iteration 527, loss = 0.14179533\n",
      "Iteration 528, loss = 0.14163158\n",
      "Iteration 529, loss = 0.14149969\n",
      "Iteration 530, loss = 0.14134182\n",
      "Iteration 531, loss = 0.14115171\n",
      "Iteration 532, loss = 0.14098537\n",
      "Iteration 533, loss = 0.14083705\n",
      "Iteration 534, loss = 0.14070830\n",
      "Iteration 535, loss = 0.14054385\n",
      "Iteration 536, loss = 0.14041558\n",
      "Iteration 537, loss = 0.14022036\n",
      "Iteration 538, loss = 0.14009220\n",
      "Iteration 539, loss = 0.13994386\n",
      "Iteration 540, loss = 0.13980388\n",
      "Iteration 541, loss = 0.13965685\n",
      "Iteration 542, loss = 0.13951456\n",
      "Iteration 543, loss = 0.13934297\n",
      "Iteration 544, loss = 0.13921804\n",
      "Iteration 545, loss = 0.13907623\n",
      "Iteration 546, loss = 0.13892121\n",
      "Iteration 547, loss = 0.13880575\n",
      "Iteration 548, loss = 0.13864266\n",
      "Iteration 549, loss = 0.13851372\n",
      "Iteration 550, loss = 0.13837205\n",
      "Iteration 551, loss = 0.13823036\n",
      "Iteration 552, loss = 0.13807789\n",
      "Iteration 553, loss = 0.13796197\n",
      "Iteration 554, loss = 0.13781136\n",
      "Iteration 555, loss = 0.13768362\n",
      "Iteration 556, loss = 0.13753916\n",
      "Iteration 557, loss = 0.13747854\n",
      "Iteration 558, loss = 0.13723609\n",
      "Iteration 559, loss = 0.13715544\n",
      "Iteration 560, loss = 0.13702464\n",
      "Iteration 561, loss = 0.13687598\n",
      "Iteration 562, loss = 0.13674686\n",
      "Iteration 563, loss = 0.13663060\n",
      "Iteration 564, loss = 0.13653667\n",
      "Iteration 565, loss = 0.13634601\n",
      "Iteration 566, loss = 0.13620435\n",
      "Iteration 567, loss = 0.13611210\n",
      "Iteration 568, loss = 0.13594636\n",
      "Iteration 569, loss = 0.13581789\n",
      "Iteration 570, loss = 0.13567826\n",
      "Iteration 571, loss = 0.13559942\n",
      "Iteration 572, loss = 0.13547062\n",
      "Iteration 573, loss = 0.13534366\n",
      "Iteration 574, loss = 0.13521570\n",
      "Iteration 575, loss = 0.13512774\n",
      "Iteration 576, loss = 0.13496415\n",
      "Iteration 577, loss = 0.13484074\n",
      "Iteration 578, loss = 0.13475219\n",
      "Iteration 579, loss = 0.13468679\n",
      "Iteration 580, loss = 0.13451424\n",
      "Iteration 581, loss = 0.13439538\n",
      "Iteration 582, loss = 0.13428328\n",
      "Iteration 583, loss = 0.13420988\n",
      "Iteration 584, loss = 0.13405169\n",
      "Iteration 585, loss = 0.13392217\n",
      "Iteration 586, loss = 0.13380904\n",
      "Iteration 587, loss = 0.13369023\n",
      "Iteration 588, loss = 0.13351930\n",
      "Iteration 589, loss = 0.13350696\n",
      "Iteration 590, loss = 0.13329100\n",
      "Iteration 591, loss = 0.13325096\n",
      "Iteration 592, loss = 0.13309286\n",
      "Iteration 593, loss = 0.13297175\n",
      "Iteration 594, loss = 0.13286948\n",
      "Iteration 595, loss = 0.13275903\n",
      "Iteration 596, loss = 0.13262375\n",
      "Iteration 597, loss = 0.13249879\n",
      "Iteration 598, loss = 0.13237862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 599, loss = 0.13223304\n",
      "Iteration 600, loss = 0.13211814\n",
      "Iteration 601, loss = 0.13209311\n",
      "Iteration 602, loss = 0.13191744\n",
      "Iteration 603, loss = 0.13179801\n",
      "Iteration 604, loss = 0.13171524\n",
      "Iteration 605, loss = 0.13163799\n",
      "Iteration 606, loss = 0.13149064\n",
      "Iteration 607, loss = 0.13140115\n",
      "Iteration 608, loss = 0.13129096\n",
      "Iteration 609, loss = 0.13117188\n",
      "Iteration 610, loss = 0.13102326\n",
      "Iteration 611, loss = 0.13089736\n",
      "Iteration 612, loss = 0.13079430\n",
      "Iteration 613, loss = 0.13068195\n",
      "Iteration 614, loss = 0.13061653\n",
      "Iteration 615, loss = 0.13048183\n",
      "Iteration 616, loss = 0.13039929\n",
      "Iteration 617, loss = 0.13029216\n",
      "Iteration 618, loss = 0.13015652\n",
      "Iteration 619, loss = 0.13008484\n",
      "Iteration 620, loss = 0.12996051\n",
      "Iteration 621, loss = 0.12986886\n",
      "Iteration 622, loss = 0.12973580\n",
      "Iteration 623, loss = 0.12963704\n",
      "Iteration 624, loss = 0.12953267\n",
      "Iteration 625, loss = 0.12945026\n",
      "Iteration 626, loss = 0.12934476\n",
      "Iteration 627, loss = 0.12922505\n",
      "Iteration 628, loss = 0.12913031\n",
      "Iteration 629, loss = 0.12902588\n",
      "Iteration 630, loss = 0.12890833\n",
      "Iteration 631, loss = 0.12888610\n",
      "Iteration 632, loss = 0.12876292\n",
      "Iteration 633, loss = 0.12863314\n",
      "Iteration 634, loss = 0.12855772\n",
      "Iteration 635, loss = 0.12844742\n",
      "Iteration 636, loss = 0.12836017\n",
      "Iteration 637, loss = 0.12825739\n",
      "Iteration 638, loss = 0.12817089\n",
      "Iteration 639, loss = 0.12806318\n",
      "Iteration 640, loss = 0.12797694\n",
      "Iteration 641, loss = 0.12787727\n",
      "Iteration 642, loss = 0.12778866\n",
      "Iteration 643, loss = 0.12767160\n",
      "Iteration 644, loss = 0.12757738\n",
      "Iteration 645, loss = 0.12755067\n",
      "Iteration 646, loss = 0.12743823\n",
      "Iteration 647, loss = 0.12730192\n",
      "Iteration 648, loss = 0.12721686\n",
      "Iteration 649, loss = 0.12719592\n",
      "Iteration 650, loss = 0.12705949\n",
      "Iteration 651, loss = 0.12693913\n",
      "Iteration 652, loss = 0.12686718\n",
      "Iteration 653, loss = 0.12679512\n",
      "Iteration 654, loss = 0.12670758\n",
      "Iteration 655, loss = 0.12660548\n",
      "Iteration 656, loss = 0.12650563\n",
      "Iteration 657, loss = 0.12638984\n",
      "Iteration 658, loss = 0.12631842\n",
      "Iteration 659, loss = 0.12624489\n",
      "Iteration 660, loss = 0.12615820\n",
      "Iteration 661, loss = 0.12605848\n",
      "Iteration 662, loss = 0.12595396\n",
      "Iteration 663, loss = 0.12589373\n",
      "Iteration 664, loss = 0.12579932\n",
      "Iteration 665, loss = 0.12567362\n",
      "Iteration 666, loss = 0.12559931\n",
      "Iteration 667, loss = 0.12552312\n",
      "Iteration 668, loss = 0.12545677\n",
      "Iteration 669, loss = 0.12537727\n",
      "Iteration 670, loss = 0.12531980\n",
      "Iteration 671, loss = 0.12522854\n",
      "Iteration 672, loss = 0.12515724\n",
      "Iteration 673, loss = 0.12505407\n",
      "Iteration 674, loss = 0.12496346\n",
      "Iteration 675, loss = 0.12486309\n",
      "Iteration 676, loss = 0.12477064\n",
      "Iteration 677, loss = 0.12470752\n",
      "Iteration 678, loss = 0.12462289\n",
      "Iteration 679, loss = 0.12455171\n",
      "Iteration 680, loss = 0.12446016\n",
      "Iteration 681, loss = 0.12436988\n",
      "Iteration 682, loss = 0.12425750\n",
      "Iteration 683, loss = 0.12416651\n",
      "Iteration 684, loss = 0.12408908\n",
      "Iteration 685, loss = 0.12397797\n",
      "Iteration 686, loss = 0.12376115\n",
      "Iteration 687, loss = 0.12346985\n",
      "Iteration 688, loss = 0.12323276\n",
      "Iteration 689, loss = 0.12322658\n",
      "Iteration 690, loss = 0.12318258\n",
      "Iteration 691, loss = 0.12311981\n",
      "Iteration 692, loss = 0.12309122\n",
      "Iteration 693, loss = 0.12296758\n",
      "Iteration 694, loss = 0.12284295\n",
      "Iteration 695, loss = 0.12275112\n",
      "Iteration 696, loss = 0.12260202\n",
      "Iteration 697, loss = 0.12250075\n",
      "Iteration 698, loss = 0.12235148\n",
      "Iteration 699, loss = 0.12232559\n",
      "Iteration 700, loss = 0.12221489\n",
      "Iteration 701, loss = 0.12208988\n",
      "Iteration 702, loss = 0.12201814\n",
      "Iteration 703, loss = 0.12192308\n",
      "Iteration 704, loss = 0.12179126\n",
      "Iteration 705, loss = 0.12174725\n",
      "Iteration 706, loss = 0.12167696\n",
      "Iteration 707, loss = 0.12152088\n",
      "Iteration 708, loss = 0.12144729\n",
      "Iteration 709, loss = 0.12140156\n",
      "Iteration 710, loss = 0.12129901\n",
      "Iteration 711, loss = 0.12121942\n",
      "Iteration 712, loss = 0.12109363\n",
      "Iteration 713, loss = 0.12095368\n",
      "Iteration 714, loss = 0.12086329\n",
      "Iteration 715, loss = 0.12081152\n",
      "Iteration 716, loss = 0.12063401\n",
      "Iteration 717, loss = 0.12059248\n",
      "Iteration 718, loss = 0.12052708\n",
      "Iteration 719, loss = 0.12043151\n",
      "Iteration 720, loss = 0.12032949\n",
      "Iteration 721, loss = 0.12018166\n",
      "Iteration 722, loss = 0.12008496\n",
      "Iteration 723, loss = 0.11997808\n",
      "Iteration 724, loss = 0.11993023\n",
      "Iteration 725, loss = 0.11984632\n",
      "Iteration 726, loss = 0.11970113\n",
      "Iteration 727, loss = 0.11962261\n",
      "Iteration 728, loss = 0.11955316\n",
      "Iteration 729, loss = 0.11944253\n",
      "Iteration 730, loss = 0.11931197\n",
      "Iteration 731, loss = 0.11923332\n",
      "Iteration 732, loss = 0.11912276\n",
      "Iteration 733, loss = 0.11901473\n",
      "Iteration 734, loss = 0.11898247\n",
      "Iteration 735, loss = 0.11885631\n",
      "Iteration 736, loss = 0.11877836\n",
      "Iteration 737, loss = 0.11872570\n",
      "Iteration 738, loss = 0.11858952\n",
      "Iteration 739, loss = 0.11856420\n",
      "Iteration 740, loss = 0.11847932\n",
      "Iteration 741, loss = 0.11837724\n",
      "Iteration 742, loss = 0.11819384\n",
      "Iteration 743, loss = 0.11818709\n",
      "Iteration 744, loss = 0.11814888\n",
      "Iteration 745, loss = 0.11804949\n",
      "Iteration 746, loss = 0.11789868\n",
      "Iteration 747, loss = 0.11772076\n",
      "Iteration 748, loss = 0.11766584\n",
      "Iteration 749, loss = 0.11754544\n",
      "Iteration 750, loss = 0.11752242\n",
      "Iteration 751, loss = 0.11744978\n",
      "Iteration 752, loss = 0.11733344\n",
      "Iteration 753, loss = 0.11715445\n",
      "Iteration 754, loss = 0.11718578\n",
      "Iteration 755, loss = 0.11713024\n",
      "Iteration 756, loss = 0.11702305\n",
      "Iteration 757, loss = 0.11686767\n",
      "Iteration 758, loss = 0.11675707\n",
      "Iteration 759, loss = 0.11673511\n",
      "Iteration 760, loss = 0.11662493\n",
      "Iteration 761, loss = 0.11650457\n",
      "Iteration 762, loss = 0.11642211\n",
      "Iteration 763, loss = 0.11635986\n",
      "Iteration 764, loss = 0.11624933\n",
      "Iteration 765, loss = 0.11618465\n",
      "Iteration 766, loss = 0.11614727\n",
      "Iteration 767, loss = 0.11599748\n",
      "Iteration 768, loss = 0.11585033\n",
      "Iteration 769, loss = 0.11584295\n",
      "Iteration 770, loss = 0.11580739\n",
      "Iteration 771, loss = 0.11567214\n",
      "Iteration 772, loss = 0.11553335\n",
      "Iteration 773, loss = 0.11548526\n",
      "Iteration 774, loss = 0.11536353\n",
      "Iteration 775, loss = 0.11520807\n",
      "Iteration 776, loss = 0.11519707\n",
      "Iteration 777, loss = 0.11517273\n",
      "Iteration 778, loss = 0.11494642\n",
      "Iteration 779, loss = 0.11491800\n",
      "Iteration 780, loss = 0.11489770\n",
      "Iteration 781, loss = 0.11483297\n",
      "Iteration 782, loss = 0.11466653\n",
      "Iteration 783, loss = 0.11458737\n",
      "Iteration 784, loss = 0.11446837\n",
      "Iteration 785, loss = 0.11446884\n",
      "Iteration 786, loss = 0.11434244\n",
      "Iteration 787, loss = 0.11428254\n",
      "Iteration 788, loss = 0.11418693\n",
      "Iteration 789, loss = 0.11407841\n",
      "Iteration 790, loss = 0.11403154\n",
      "Iteration 791, loss = 0.11394899\n",
      "Iteration 792, loss = 0.11385446\n",
      "Iteration 793, loss = 0.11375016\n",
      "Iteration 794, loss = 0.11370457\n",
      "Iteration 795, loss = 0.11358063\n",
      "Iteration 796, loss = 0.11353479\n",
      "Iteration 797, loss = 0.11345905\n",
      "Iteration 798, loss = 0.11334180\n",
      "Iteration 799, loss = 0.11321279\n",
      "Iteration 800, loss = 0.11312961\n",
      "Iteration 801, loss = 0.11310019\n",
      "Iteration 802, loss = 0.11305044\n",
      "Iteration 803, loss = 0.11293374\n",
      "Iteration 804, loss = 0.11288794\n",
      "Iteration 805, loss = 0.11279672\n",
      "Iteration 806, loss = 0.11263910\n",
      "Iteration 807, loss = 0.11262653\n",
      "Iteration 808, loss = 0.11246152\n",
      "Iteration 809, loss = 0.11248621\n",
      "Iteration 810, loss = 0.11233775\n",
      "Iteration 811, loss = 0.11233866\n",
      "Iteration 812, loss = 0.11214903\n",
      "Iteration 813, loss = 0.11219764\n",
      "Iteration 814, loss = 0.11211943\n",
      "Iteration 815, loss = 0.11203273\n",
      "Iteration 816, loss = 0.11189837\n",
      "Iteration 817, loss = 0.11191106\n",
      "Iteration 818, loss = 0.11184804\n",
      "Iteration 819, loss = 0.11168508\n",
      "Iteration 820, loss = 0.11166774\n",
      "Iteration 821, loss = 0.11160812\n",
      "Iteration 822, loss = 0.11154115\n",
      "Iteration 823, loss = 0.11140143\n",
      "Iteration 824, loss = 0.11126801\n",
      "Iteration 825, loss = 0.11120685\n",
      "Iteration 826, loss = 0.11110760\n",
      "Iteration 827, loss = 0.11102857\n",
      "Iteration 828, loss = 0.11097856\n",
      "Iteration 829, loss = 0.11096304\n",
      "Iteration 830, loss = 0.11078818\n",
      "Iteration 831, loss = 0.11080223\n",
      "Iteration 832, loss = 0.11068683\n",
      "Iteration 833, loss = 0.11073132\n",
      "Iteration 834, loss = 0.11064236\n",
      "Iteration 835, loss = 0.11051527\n",
      "Iteration 836, loss = 0.11038160\n",
      "Iteration 837, loss = 0.11053116\n",
      "Iteration 838, loss = 0.11046412\n",
      "Iteration 839, loss = 0.11024105\n",
      "Iteration 840, loss = 0.11023105\n",
      "Iteration 841, loss = 0.11019739\n",
      "Iteration 842, loss = 0.11017671\n",
      "Iteration 843, loss = 0.10997735\n",
      "Iteration 844, loss = 0.10985593\n",
      "Iteration 845, loss = 0.10986477\n",
      "Iteration 846, loss = 0.10985216\n",
      "Iteration 847, loss = 0.10965972\n",
      "Iteration 848, loss = 0.10957652\n",
      "Iteration 849, loss = 0.10958404\n",
      "Iteration 850, loss = 0.10952843\n",
      "Iteration 851, loss = 0.10940841\n",
      "Iteration 852, loss = 0.10923116\n",
      "Iteration 853, loss = 0.10938066\n",
      "Iteration 854, loss = 0.10933552\n",
      "Iteration 855, loss = 0.10917231\n",
      "Iteration 856, loss = 0.10901646\n",
      "Iteration 857, loss = 0.10907534\n",
      "Iteration 858, loss = 0.10893936\n",
      "Iteration 859, loss = 0.10883297\n",
      "Iteration 860, loss = 0.10874496\n",
      "Iteration 861, loss = 0.10866679\n",
      "Iteration 862, loss = 0.10861021\n",
      "Iteration 863, loss = 0.10857752\n",
      "Iteration 864, loss = 0.10847167\n",
      "Iteration 865, loss = 0.10841410\n",
      "Iteration 866, loss = 0.10836826\n",
      "Iteration 867, loss = 0.10826264\n",
      "Iteration 868, loss = 0.10823124\n",
      "Iteration 869, loss = 0.10818385\n",
      "Iteration 870, loss = 0.10807635\n",
      "Iteration 871, loss = 0.10801207\n",
      "Iteration 872, loss = 0.10791342\n",
      "Iteration 873, loss = 0.10790171\n",
      "Iteration 874, loss = 0.10782756\n",
      "Iteration 875, loss = 0.10771817\n",
      "Iteration 876, loss = 0.10772856\n",
      "Iteration 877, loss = 0.10763885\n",
      "Iteration 878, loss = 0.10757323\n",
      "Iteration 879, loss = 0.10754462\n",
      "Iteration 880, loss = 0.10745236\n",
      "Iteration 881, loss = 0.10735632\n",
      "Iteration 882, loss = 0.10731222\n",
      "Iteration 883, loss = 0.10724952\n",
      "Iteration 884, loss = 0.10718988\n",
      "Iteration 885, loss = 0.10713791\n",
      "Iteration 886, loss = 0.10705661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.950000\n",
      "Training set loss: 0.107057\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# different learning rate schedules and momentum parameters\n",
    "params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0.9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0.9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0.9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0.9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'adam', 'learning_rate_init': 0.01}]\n",
    "\n",
    "labels = [\"constant learning-rate\", \"constant with momentum\",\n",
    "          \"constant with Nesterov's momentum\",\n",
    "          \"inv-scaling learning-rate\", \"inv-scaling with momentum\",\n",
    "          \"inv-scaling with Nesterov's momentum\", \"adam\"]\n",
    "\n",
    "plot_args = [{'c': 'red', 'linestyle': '-'},\n",
    "             {'c': 'green', 'linestyle': '-'},\n",
    "             {'c': 'blue', 'linestyle': '-'},\n",
    "             {'c': 'red', 'linestyle': '--'},\n",
    "             {'c': 'green', 'linestyle': '--'},\n",
    "             {'c': 'blue', 'linestyle': '--'},\n",
    "             {'c': 'black', 'linestyle': '-'}]\n",
    "\n",
    "\n",
    "def plot_on_dataset(X, y, ax, name):\n",
    "    # for each dataset, plot learning for each learning strategy\n",
    "    print(\"\\nlearning on dataset %s\" % name)\n",
    "    ax.set_title(name)\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    mlps = []\n",
    "    if name == \"digits\":\n",
    "        # digits is larger but converges fairly quickly\n",
    "        max_iter = 201068\n",
    "    else:\n",
    "        max_iter = 196810\n",
    "\n",
    "    for label, param in zip(labels, params):\n",
    "        print(\"training: %s\" % label)\n",
    "        mlp = MLPClassifier(verbose=True, random_state=0,\n",
    "                            max_iter=max_iter, **param)\n",
    "        mlp.fit(X, y)\n",
    "        mlps.append(mlp)\n",
    "        print(\"Training set score: %f\" % mlp.score(X, y))\n",
    "        print(\"Training set loss: %f\" % mlp.loss_)\n",
    "    for mlp, label, args in zip(mlps, labels, plot_args):\n",
    "            ax.plot(mlp.loss_curve_, label=label, **args)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "# load / generate some toy datasets\n",
    "digits = datasets.load_digits()\n",
    "data_sets = [(rose_data, rose_target),\n",
    "             (digits.data, digits.target),\n",
    "             datasets.make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "             datasets.make_moons(noise=0.3, random_state=0)]\n",
    "\n",
    "for ax, data, name in zip(axes.ravel(), data_sets, ['rose', 'digits',\n",
    "                                                    'circles', 'moons']):\n",
    "    plot_on_dataset(*data, ax=ax, name=name)\n",
    "\n",
    "fig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AndreVermeulen\\Documents\\My Book\\apress\\Industrial Machine Learning\\book\\GitHub\\Upload\\industrial-machine-learning\\Results\\Chapter 04\n"
     ]
    }
   ],
   "source": [
    "imagepath = os.path.join(*[os.path.dirname(os.path.dirname(os.getcwd())),'Results','Chapter 04'])\n",
    "print(imagepath)\n",
    "if not os.path.exists(imagepath):\n",
    "    os.makedirs(imagepath)\n",
    "graphName = 'Chapter-004-Example-025-01-01.jpg'\n",
    "imagename = os.path.join(*[os.path.dirname(os.path.dirname(os.getcwd())),'Results','Chapter 04',graphName])\n",
    "fig.savefig(imagename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! 2019-10-19 17:40:07.840458\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print('Done!',str(now))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
