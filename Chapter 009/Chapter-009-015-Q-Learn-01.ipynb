{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apress - Industrialized Machine Learning Examples\n",
    "\n",
    "Andreas Francois Vermeulen\n",
    "2019\n",
    "\n",
    "### This is an example add-on to a book and needs to be accepted as part of that copyright."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8X8WoA090Yf"
   },
   "source": [
    "## Chapter-009-015-Q-Learn-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6i-5O0kK90Yg"
   },
   "source": [
    "### Install keras-rl library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3508,
     "status": "ok",
     "timestamp": 1547296907105,
     "user": {
      "displayName": "Andre Vermeulen",
      "photoUrl": "",
      "userId": "07958753266952227006"
     },
     "user_tz": 0
    },
    "id": "M1EtbsJL90Yi",
    "outputId": "1b39ce79-c31e-4326-e385-695a0f62ea2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-rl\n",
      "  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
      "Collecting keras>=2.0.7 (from keras-rl)\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from keras>=2.0.7->keras-rl) (1.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from keras>=2.0.7->keras-rl) (1.14.6)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\andrevermeulen\\appdata\\roaming\\python\\python36\\site-packages (from keras>=2.0.7->keras-rl) (1.2.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\andrevermeulen\\appdata\\roaming\\python\\python36\\site-packages (from keras>=2.0.7->keras-rl) (5.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from keras>=2.0.7->keras-rl) (1.0.9)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from keras>=2.0.7->keras-rl) (1.12.0)\n",
      "Building wheels for collected packages: keras-rl\n",
      "  Building wheel for keras-rl (setup.py): started\n",
      "  Building wheel for keras-rl (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\AndreVermeulen\\AppData\\Local\\pip\\Cache\\wheels\\7d\\4d\\84\\9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
      "Successfully built keras-rl\n",
      "Installing collected packages: keras, keras-rl\n",
      "Successfully installed keras-2.2.4 keras-rl-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6975,
     "status": "ok",
     "timestamp": 1547296910585,
     "user": {
      "displayName": "Andre Vermeulen",
      "photoUrl": "",
      "userId": "07958753266952227006"
     },
     "user_tz": 0
    },
    "id": "L8BGzFvwCCHf",
    "outputId": "fcea7570-6c0f-48ed-ab81-7470bc00c0d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyglet\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
      "Requirement already satisfied: future in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from pyglet) (0.17.1)\n",
      "Installing collected packages: pyglet\n",
      "Successfully installed pyglet-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPbJABoQ90Yk"
   },
   "source": [
    "### Install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11310,
     "status": "ok",
     "timestamp": 1547296914931,
     "user": {
      "displayName": "Andre Vermeulen",
      "photoUrl": "",
      "userId": "07958753266952227006"
     },
     "user_tz": 0
    },
    "id": "czKZen-V90Yl",
    "outputId": "28d7c907-818f-4e06-8845-ff0463c29c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from h5py) (1.14.6)\n",
      "Requirement already satisfied: six in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from h5py) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYnz7vPx90Yr"
   },
   "source": [
    " ### Install dependencies for CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15357,
     "status": "ok",
     "timestamp": 1547296918990,
     "user": {
      "displayName": "Andre Vermeulen",
      "photoUrl": "",
      "userId": "07958753266952227006"
     },
     "user_tz": 0
    },
    "id": "zePbmymd90Ys",
    "outputId": "e2473c06-09f3-4224-f9ae-ec051950d3e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/57/e2fc4123ff2b4e3d61ae9b3d08c6878aecf2d5ec69b585ed53bc2400607f/gym-0.12.1.tar.gz (1.5MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\andrevermeulen\\appdata\\roaming\\python\\python36\\site-packages (from gym) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from gym) (1.14.6)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from gym) (2.18.4)\n",
      "Requirement already satisfied: six in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from requests>=2.0->gym) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from requests>=2.0->gym) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from requests>=2.0->gym) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: future in c:\\users\\andrevermeulen\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages (from pyglet>=1.2.0->gym) (0.17.1)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\AndreVermeulen\\AppData\\Local\\pip\\Cache\\wheels\\57\\b0\\13\\4153e1acab826fbe612c95b1336a63a3fa6416902a8d74a1b7\n",
      "Successfully built gym\n",
      "Installing collected packages: gym\n",
      "Successfully installed gym-0.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CgxW89vg90Yv"
   },
   "source": [
    "# You are ready to perform the Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wc-MuFJJ90Yv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7f0_cd6l90Y1"
   },
   "source": [
    "You need to set several variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZeuOxo6z90Y1"
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3_fLaOF90Y4"
   },
   "source": [
    "Get the environment and extract the number of actions available in the Cartpole problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15326,
     "status": "ok",
     "timestamp": 1547296918993,
     "user": {
      "displayName": "Andre Vermeulen",
      "photoUrl": "",
      "userId": "07958753266952227006"
     },
     "user_tz": 0
    },
    "id": "JQMSKzqK90Y5",
    "outputId": "5c306122-2861-482b-e4cb-2c58500c6b70"
   },
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(20)\n",
    "env.seed(20)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiatn9a790Y7"
   },
   "source": [
    "Create a single hidden layer neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gYzRl7yc90Y8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AndreVermeulen\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15764,
     "status": "ok",
     "timestamp": 1547296919446,
     "user": {
      "displayName": "Andre Vermeulen",
      "photoUrl": "",
      "userId": "07958753266952227006"
     },
     "user_tz": 0
    },
    "id": "LbohKsit90Y_",
    "outputId": "32aaa82e-0569-4a5d-cabb-b3ba24c73cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWYNd_Rs90ZC"
   },
   "source": [
    "Next you configure and compile our agent. Suggest you use the policy as Epsilon Greedy and you set the memory as Sequential Memory because you must to store the result of actions you Cart performed and the rewards it gets for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zEjBDdgH90ZD"
   },
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "memory = SequentialMemory(limit=50000, \n",
    "                          window_length=1\n",
    "                         )\n",
    "\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=nb_actions, \n",
    "               memory=memory, \n",
    "               nb_steps_warmup=1000, \n",
    "               target_model_update=1e-2, \n",
    "               policy=policy,\n",
    "               enable_dueling_network=False,\n",
    "               dueling_type='avg'\n",
    "              )\n",
    "\n",
    "dqn.compile(Adam(lr=1e-3), \n",
    "            metrics=['mae']\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1J_Pjlt90ZG"
   },
   "source": [
    "Time to perform the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4134
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30500,
     "status": "ok",
     "timestamp": 1547296934191,
     "user": {
      "displayName": "Andre Vermeulen",
      "photoUrl": "",
      "userId": "07958753266952227006"
     },
     "user_tz": 0
    },
    "id": "r0osd6ba90ZH",
    "outputId": "ede6af2a-4a6d-4cd9-d27b-1897b46296ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AndreVermeulen\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AndreVermeulen\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "   27/5000: episode: 1, duration: 6.879s, episode steps: 27, steps per second: 4, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.259 [0.000, 1.000], mean observation: 0.024 [-2.483, 3.517], loss: 0.488774, mean_absolute_error: 0.559568, mean_q: 0.032028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AndreVermeulen\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   39/5000: episode: 2, duration: 0.392s, episode steps: 12, steps per second: 31, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.109 [-1.985, 3.012], loss: 0.425160, mean_absolute_error: 0.618904, mean_q: 0.251136\n",
      "   47/5000: episode: 3, duration: 0.266s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.585, 2.572], loss: 0.368349, mean_absolute_error: 0.598510, mean_q: 0.366937\n",
      "   57/5000: episode: 4, duration: 0.316s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-1.943, 2.996], loss: 0.318007, mean_absolute_error: 0.579923, mean_q: 0.488692\n",
      "   67/5000: episode: 5, duration: 0.316s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.914, 3.078], loss: 0.272978, mean_absolute_error: 0.560278, mean_q: 0.626414\n",
      "   76/5000: episode: 6, duration: 0.281s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.184 [-1.740, 2.910], loss: 0.264149, mean_absolute_error: 0.534188, mean_q: 0.676515\n",
      "   86/5000: episode: 7, duration: 0.301s, episode steps: 10, steps per second: 33, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.941, 3.038], loss: 0.249932, mean_absolute_error: 0.525814, mean_q: 0.785196\n",
      "   94/5000: episode: 8, duration: 0.231s, episode steps: 8, steps per second: 35, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.606, 2.572], loss: 0.256869, mean_absolute_error: 0.517822, mean_q: 0.841272\n",
      "  105/5000: episode: 9, duration: 0.333s, episode steps: 11, steps per second: 33, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.159 [-1.718, 2.894], loss: 0.247150, mean_absolute_error: 0.498363, mean_q: 0.938483\n",
      "  115/5000: episode: 10, duration: 0.315s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.960, 3.039], loss: 0.253202, mean_absolute_error: 0.496497, mean_q: 1.047590\n",
      "  124/5000: episode: 11, duration: 0.283s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.740, 2.884], loss: 0.261198, mean_absolute_error: 0.482500, mean_q: 1.048863\n",
      "  133/5000: episode: 12, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.755, 2.839], loss: 0.243711, mean_absolute_error: 0.456126, mean_q: 1.157018\n",
      "  142/5000: episode: 13, duration: 0.283s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.765, 2.806], loss: 0.261198, mean_absolute_error: 0.458673, mean_q: 1.221911\n",
      "  152/5000: episode: 14, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.129 [-1.370, 2.231], loss: 0.262365, mean_absolute_error: 0.437090, mean_q: 1.293737\n",
      "  164/5000: episode: 15, duration: 0.349s, episode steps: 12, steps per second: 34, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.119 [-1.945, 3.076], loss: 0.304685, mean_absolute_error: 0.451316, mean_q: 1.393372\n",
      "  174/5000: episode: 16, duration: 0.337s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.142 [-1.145, 1.950], loss: 0.337590, mean_absolute_error: 0.433174, mean_q: 1.468558\n",
      "  183/5000: episode: 17, duration: 0.279s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-1.731, 2.740], loss: 0.328974, mean_absolute_error: 0.479078, mean_q: 1.416424\n",
      "  193/5000: episode: 18, duration: 0.316s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [-2.000, 3.043], loss: 0.336840, mean_absolute_error: 0.511232, mean_q: 1.518268\n",
      "  202/5000: episode: 19, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.142 [-1.403, 2.303], loss: 0.316156, mean_absolute_error: 0.546396, mean_q: 1.591282\n",
      "  212/5000: episode: 20, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.134 [-1.517, 2.522], loss: 0.346161, mean_absolute_error: 0.597613, mean_q: 1.724200\n",
      "  224/5000: episode: 21, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.111 [-1.971, 3.067], loss: 0.332698, mean_absolute_error: 0.647966, mean_q: 1.736444\n",
      "  233/5000: episode: 22, duration: 0.282s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.785, 2.803], loss: 0.362434, mean_absolute_error: 0.712294, mean_q: 1.833513\n",
      "  242/5000: episode: 23, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.760, 2.902], loss: 0.323599, mean_absolute_error: 0.719477, mean_q: 1.888633\n",
      "  251/5000: episode: 24, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.136 [-1.357, 2.309], loss: 0.388639, mean_absolute_error: 0.789575, mean_q: 1.981556\n",
      "  261/5000: episode: 25, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.947, 3.122], loss: 0.363816, mean_absolute_error: 0.813488, mean_q: 1.973190\n",
      "  270/5000: episode: 26, duration: 0.316s, episode steps: 9, steps per second: 29, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.774, 2.815], loss: 0.390195, mean_absolute_error: 0.873135, mean_q: 2.107662\n",
      "  280/5000: episode: 27, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [-1.982, 3.049], loss: 0.344068, mean_absolute_error: 0.886439, mean_q: 2.117549\n",
      "  289/5000: episode: 28, duration: 0.283s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.729, 2.808], loss: 0.390713, mean_absolute_error: 0.931866, mean_q: 2.219899\n",
      "  298/5000: episode: 29, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.746, 2.878], loss: 0.414427, mean_absolute_error: 0.960500, mean_q: 2.298027\n",
      "  307/5000: episode: 30, duration: 0.282s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.784, 2.847], loss: 0.338951, mean_absolute_error: 0.962158, mean_q: 2.327415\n",
      "  317/5000: episode: 31, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.967, 3.106], loss: 0.480701, mean_absolute_error: 1.126984, mean_q: 2.486200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  326/5000: episode: 32, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.810, 2.834], loss: 0.315337, mean_absolute_error: 1.057048, mean_q: 2.423726\n",
      "  336/5000: episode: 33, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.124 [-1.603, 2.504], loss: 0.459477, mean_absolute_error: 1.161498, mean_q: 2.567931\n",
      "  346/5000: episode: 34, duration: 0.316s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.131 [-1.783, 2.718], loss: 0.443892, mean_absolute_error: 1.210896, mean_q: 2.607481\n",
      "  354/5000: episode: 35, duration: 0.268s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.172 [-1.543, 2.567], loss: 0.483632, mean_absolute_error: 1.257646, mean_q: 2.686663\n",
      "  365/5000: episode: 36, duration: 0.365s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.147 [-1.719, 2.816], loss: 0.369100, mean_absolute_error: 1.163454, mean_q: 2.707107\n",
      "  374/5000: episode: 37, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.810, 2.879], loss: 0.419519, mean_absolute_error: 1.243905, mean_q: 2.838531\n",
      "  387/5000: episode: 38, duration: 0.416s, episode steps: 13, steps per second: 31, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.104 [-1.524, 2.375], loss: 0.467283, mean_absolute_error: 1.288352, mean_q: 2.775866\n",
      "  398/5000: episode: 39, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.141 [-1.729, 2.825], loss: 0.412207, mean_absolute_error: 1.309419, mean_q: 2.802896\n",
      "  408/5000: episode: 40, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.166 [-1.916, 3.099], loss: 0.370308, mean_absolute_error: 1.290957, mean_q: 2.934108\n",
      "  420/5000: episode: 41, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.122 [-1.540, 2.497], loss: 0.451774, mean_absolute_error: 1.339302, mean_q: 2.988039\n",
      "  429/5000: episode: 42, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.775, 2.851], loss: 0.404135, mean_absolute_error: 1.358462, mean_q: 3.002572\n",
      "  439/5000: episode: 43, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.127 [-1.565, 2.506], loss: 0.342295, mean_absolute_error: 1.327748, mean_q: 3.054984\n",
      "  449/5000: episode: 44, duration: 0.283s, episode steps: 10, steps per second: 35, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-1.973, 3.075], loss: 0.354178, mean_absolute_error: 1.363353, mean_q: 3.177703\n",
      "  461/5000: episode: 45, duration: 0.398s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.127 [-1.908, 2.983], loss: 0.377906, mean_absolute_error: 1.399169, mean_q: 3.282138\n",
      "  470/5000: episode: 46, duration: 0.283s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [-1.782, 2.777], loss: 0.336522, mean_absolute_error: 1.407219, mean_q: 3.349811\n",
      "  480/5000: episode: 47, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.117 [-1.946, 2.961], loss: 0.402688, mean_absolute_error: 1.488652, mean_q: 3.371430\n",
      "  491/5000: episode: 48, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.124 [-1.779, 2.846], loss: 0.485210, mean_absolute_error: 1.548294, mean_q: 3.396802\n",
      "  504/5000: episode: 49, duration: 0.433s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.106 [-1.566, 2.513], loss: 0.324618, mean_absolute_error: 1.536581, mean_q: 3.364424\n",
      "  516/5000: episode: 50, duration: 0.384s, episode steps: 12, steps per second: 31, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.111 [-1.909, 2.978], loss: 0.284911, mean_absolute_error: 1.523429, mean_q: 3.566558\n",
      "  527/5000: episode: 51, duration: 0.365s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.158 [-1.531, 2.556], loss: 0.339903, mean_absolute_error: 1.617115, mean_q: 3.560281\n",
      "  537/5000: episode: 52, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.156 [-1.539, 2.425], loss: 0.277680, mean_absolute_error: 1.627319, mean_q: 3.553014\n",
      "  545/5000: episode: 53, duration: 0.266s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.136 [-1.361, 2.206], loss: 0.336139, mean_absolute_error: 1.692994, mean_q: 3.592931\n",
      "  558/5000: episode: 54, duration: 0.416s, episode steps: 13, steps per second: 31, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.087 [-1.381, 2.152], loss: 0.273533, mean_absolute_error: 1.679410, mean_q: 3.670758\n",
      "  568/5000: episode: 55, duration: 0.316s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.147 [-1.360, 2.216], loss: 0.337697, mean_absolute_error: 1.749617, mean_q: 3.742568\n",
      "  576/5000: episode: 56, duration: 0.266s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.160 [-1.327, 2.200], loss: 0.330739, mean_absolute_error: 1.772104, mean_q: 3.719711\n",
      "  587/5000: episode: 57, duration: 0.365s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.125 [-1.357, 2.126], loss: 0.301769, mean_absolute_error: 1.763191, mean_q: 3.743032\n",
      "  597/5000: episode: 58, duration: 0.283s, episode steps: 10, steps per second: 35, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.135 [-1.542, 2.423], loss: 0.301642, mean_absolute_error: 1.774662, mean_q: 3.872483\n",
      "  607/5000: episode: 59, duration: 0.316s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.115 [-1.575, 2.346], loss: 0.286524, mean_absolute_error: 1.782906, mean_q: 3.962113\n",
      "  617/5000: episode: 60, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.125 [-1.583, 2.510], loss: 0.250448, mean_absolute_error: 1.746012, mean_q: 4.011471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  628/5000: episode: 61, duration: 0.365s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.130 [-1.563, 2.453], loss: 0.281247, mean_absolute_error: 1.779749, mean_q: 3.984269\n",
      "  637/5000: episode: 62, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.146 [-1.605, 2.494], loss: 0.350577, mean_absolute_error: 1.821586, mean_q: 4.126432\n",
      "  647/5000: episode: 63, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.141 [-1.530, 2.549], loss: 0.334946, mean_absolute_error: 1.839072, mean_q: 4.064948\n",
      "  659/5000: episode: 64, duration: 0.383s, episode steps: 12, steps per second: 31, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.116 [-1.733, 2.677], loss: 0.294383, mean_absolute_error: 1.854028, mean_q: 4.172778\n",
      "  669/5000: episode: 65, duration: 0.300s, episode steps: 10, steps per second: 33, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.147 [-1.598, 2.538], loss: 0.305637, mean_absolute_error: 1.873667, mean_q: 4.312636\n",
      "  679/5000: episode: 66, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.127 [-1.604, 2.408], loss: 0.282777, mean_absolute_error: 1.931094, mean_q: 4.285975\n",
      "  689/5000: episode: 67, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.111 [-1.585, 2.337], loss: 0.240331, mean_absolute_error: 1.928973, mean_q: 4.407722\n",
      "  697/5000: episode: 68, duration: 0.234s, episode steps: 8, steps per second: 34, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.556, 2.533], loss: 0.328238, mean_absolute_error: 1.981759, mean_q: 4.391328\n",
      "  706/5000: episode: 69, duration: 0.298s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.171 [-1.544, 2.525], loss: 0.341693, mean_absolute_error: 2.037296, mean_q: 4.422721\n",
      "  716/5000: episode: 70, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [-1.952, 3.016], loss: 0.303693, mean_absolute_error: 2.000312, mean_q: 4.544377\n",
      "  725/5000: episode: 71, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.147 [-1.597, 2.488], loss: 0.334499, mean_absolute_error: 2.043078, mean_q: 4.548231\n",
      "  735/5000: episode: 72, duration: 0.316s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.137 [-1.369, 2.227], loss: 0.277875, mean_absolute_error: 2.046824, mean_q: 4.629027\n",
      "  748/5000: episode: 73, duration: 0.416s, episode steps: 13, steps per second: 31, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.071 [-1.605, 2.367], loss: 0.300528, mean_absolute_error: 2.095731, mean_q: 4.711730\n",
      "  759/5000: episode: 74, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.110 [-1.804, 2.587], loss: 0.300598, mean_absolute_error: 2.107491, mean_q: 4.679991\n",
      "  768/5000: episode: 75, duration: 0.266s, episode steps: 9, steps per second: 34, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.136 [-1.584, 2.449], loss: 0.306264, mean_absolute_error: 2.126068, mean_q: 4.634685\n",
      "  777/5000: episode: 76, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.141 [-1.579, 2.464], loss: 0.237227, mean_absolute_error: 2.136322, mean_q: 4.824139\n",
      "  786/5000: episode: 77, duration: 0.301s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.168 [-1.545, 2.517], loss: 0.265333, mean_absolute_error: 2.178063, mean_q: 4.915071\n",
      "  798/5000: episode: 78, duration: 0.397s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.092 [-1.609, 2.373], loss: 0.418937, mean_absolute_error: 2.262381, mean_q: 4.701038\n",
      "  807/5000: episode: 79, duration: 0.282s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.572, 2.462], loss: 0.312458, mean_absolute_error: 2.241973, mean_q: 4.772359\n",
      "  817/5000: episode: 80, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.142 [-1.534, 2.533], loss: 0.268732, mean_absolute_error: 2.245632, mean_q: 5.008961\n",
      "  829/5000: episode: 81, duration: 0.400s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.122 [-1.744, 2.734], loss: 0.259985, mean_absolute_error: 2.251575, mean_q: 5.085669\n",
      "  839/5000: episode: 82, duration: 0.316s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.141 [-1.723, 2.655], loss: 0.291566, mean_absolute_error: 2.213988, mean_q: 4.863065\n",
      "  847/5000: episode: 83, duration: 0.266s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.560, 2.547], loss: 0.246910, mean_absolute_error: 2.227443, mean_q: 4.940377\n",
      "  859/5000: episode: 84, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.088 [-1.755, 2.650], loss: 0.218946, mean_absolute_error: 2.206791, mean_q: 5.005589\n",
      "  869/5000: episode: 85, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.133 [-1.780, 2.746], loss: 0.264151, mean_absolute_error: 2.275164, mean_q: 5.167806\n",
      "  878/5000: episode: 86, duration: 0.298s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.775, 2.823], loss: 0.306319, mean_absolute_error: 2.283650, mean_q: 5.034586\n",
      "  887/5000: episode: 87, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.800, 2.831], loss: 0.272903, mean_absolute_error: 2.333664, mean_q: 5.187873\n",
      "  896/5000: episode: 88, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.770, 2.837], loss: 0.226584, mean_absolute_error: 2.351148, mean_q: 5.344970\n",
      "  906/5000: episode: 89, duration: 0.316s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.115 [-1.713, 2.620], loss: 0.173920, mean_absolute_error: 2.362146, mean_q: 5.386808\n",
      "  915/5000: episode: 90, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.169 [-1.534, 2.508], loss: 0.179995, mean_absolute_error: 2.334933, mean_q: 5.215996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  926/5000: episode: 91, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.131 [-1.605, 2.504], loss: 0.221861, mean_absolute_error: 2.394267, mean_q: 5.227825\n",
      "  940/5000: episode: 92, duration: 0.466s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.076 [-1.767, 2.639], loss: 0.234456, mean_absolute_error: 2.383928, mean_q: 5.185652\n",
      "  950/5000: episode: 93, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.153 [-1.743, 2.734], loss: 0.209340, mean_absolute_error: 2.435891, mean_q: 5.386909\n",
      "  960/5000: episode: 94, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.144 [-1.780, 2.741], loss: 0.188456, mean_absolute_error: 2.491442, mean_q: 5.547806\n",
      "  971/5000: episode: 95, duration: 0.349s, episode steps: 11, steps per second: 32, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.120 [-1.571, 2.396], loss: 0.189644, mean_absolute_error: 2.481573, mean_q: 5.412788\n",
      "  980/5000: episode: 96, duration: 0.284s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.177 [-1.534, 2.545], loss: 0.189842, mean_absolute_error: 2.408925, mean_q: 5.210032\n",
      "  989/5000: episode: 97, duration: 0.264s, episode steps: 9, steps per second: 34, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.783, 2.790], loss: 0.165696, mean_absolute_error: 2.468892, mean_q: 5.419670\n",
      "  998/5000: episode: 98, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.133 [-1.542, 2.458], loss: 0.218963, mean_absolute_error: 2.499982, mean_q: 5.420950\n",
      " 1008/5000: episode: 99, duration: 1.434s, episode steps: 10, steps per second: 7, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.112 [-1.591, 2.485], loss: 0.153986, mean_absolute_error: 2.453990, mean_q: 5.395000\n",
      " 1019/5000: episode: 100, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.123 [-1.721, 2.800], loss: 0.186852, mean_absolute_error: 2.469938, mean_q: 5.393770\n",
      " 1027/5000: episode: 101, duration: 0.266s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.530, 2.590], loss: 0.246293, mean_absolute_error: 2.553360, mean_q: 5.524856\n",
      " 1036/5000: episode: 102, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.140 [-1.611, 2.509], loss: 0.189310, mean_absolute_error: 2.545203, mean_q: 5.530814\n",
      " 1045/5000: episode: 103, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.760, 2.816], loss: 0.141199, mean_absolute_error: 2.540973, mean_q: 5.550422\n",
      " 1055/5000: episode: 104, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.138 [-1.728, 2.720], loss: 0.167757, mean_absolute_error: 2.602097, mean_q: 5.606769\n",
      " 1066/5000: episode: 105, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.117 [-1.771, 2.763], loss: 0.134916, mean_absolute_error: 2.573780, mean_q: 5.535724\n",
      " 1075/5000: episode: 106, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.714, 2.771], loss: 0.143647, mean_absolute_error: 2.645073, mean_q: 5.694438\n",
      " 1084/5000: episode: 107, duration: 0.284s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.159 [-1.608, 2.546], loss: 0.132325, mean_absolute_error: 2.633126, mean_q: 5.629706\n",
      " 1092/5000: episode: 108, duration: 0.248s, episode steps: 8, steps per second: 32, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.156 [-1.342, 2.263], loss: 0.130499, mean_absolute_error: 2.614967, mean_q: 5.524388\n",
      " 1102/5000: episode: 109, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.130 [-1.355, 2.233], loss: 0.105110, mean_absolute_error: 2.676186, mean_q: 5.672437\n",
      " 1111/5000: episode: 110, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.182 [-1.531, 2.557], loss: 0.089713, mean_absolute_error: 2.663137, mean_q: 5.668180\n",
      " 1119/5000: episode: 111, duration: 0.266s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.536, 2.522], loss: 0.131603, mean_absolute_error: 2.617994, mean_q: 5.527958\n",
      " 1128/5000: episode: 112, duration: 0.283s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.141 [-1.593, 2.505], loss: 0.117370, mean_absolute_error: 2.564280, mean_q: 5.421706\n",
      " 1137/5000: episode: 113, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.771, 2.836], loss: 0.131789, mean_absolute_error: 2.744420, mean_q: 5.825056\n",
      " 1147/5000: episode: 114, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.541, 2.597], loss: 0.141049, mean_absolute_error: 2.714312, mean_q: 5.702760\n",
      " 1156/5000: episode: 115, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.593, 2.495], loss: 0.107788, mean_absolute_error: 2.772195, mean_q: 5.819170\n",
      " 1165/5000: episode: 116, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.148 [-1.578, 2.493], loss: 0.137732, mean_absolute_error: 2.824341, mean_q: 5.834097\n",
      " 1175/5000: episode: 117, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.107 [-1.400, 2.171], loss: 0.094062, mean_absolute_error: 2.786428, mean_q: 5.734238\n",
      " 1185/5000: episode: 118, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.112 [-1.604, 2.383], loss: 0.081744, mean_absolute_error: 2.843937, mean_q: 5.820480\n",
      " 1195/5000: episode: 119, duration: 0.299s, episode steps: 10, steps per second: 33, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.129 [-1.350, 2.189], loss: 0.104243, mean_absolute_error: 2.848871, mean_q: 5.760877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1206/5000: episode: 120, duration: 0.353s, episode steps: 11, steps per second: 31, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.125 [-1.343, 2.076], loss: 0.086425, mean_absolute_error: 2.800220, mean_q: 5.657528\n",
      " 1215/5000: episode: 121, duration: 0.296s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.131 [-1.348, 2.119], loss: 0.091371, mean_absolute_error: 2.925205, mean_q: 5.917897\n",
      " 1226/5000: episode: 122, duration: 0.334s, episode steps: 11, steps per second: 33, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.143 [-0.946, 1.816], loss: 0.078870, mean_absolute_error: 2.886313, mean_q: 5.823234\n",
      " 1239/5000: episode: 123, duration: 0.432s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.099 [-1.383, 2.055], loss: 0.105392, mean_absolute_error: 2.840794, mean_q: 5.622478\n",
      " 1253/5000: episode: 124, duration: 0.465s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.062 [-1.196, 1.709], loss: 0.079480, mean_absolute_error: 2.980455, mean_q: 5.908644\n",
      " 1266/5000: episode: 125, duration: 0.416s, episode steps: 13, steps per second: 31, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.097 [-1.187, 1.857], loss: 0.112914, mean_absolute_error: 2.960140, mean_q: 5.730504\n",
      " 1279/5000: episode: 126, duration: 0.416s, episode steps: 13, steps per second: 31, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.111 [-0.754, 1.256], loss: 0.101062, mean_absolute_error: 3.008953, mean_q: 5.797168\n",
      " 1289/5000: episode: 127, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-3.004, 1.932], loss: 0.132203, mean_absolute_error: 3.030958, mean_q: 5.776895\n",
      " 1299/5000: episode: 128, duration: 0.317s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.120 [-2.596, 1.604], loss: 0.427947, mean_absolute_error: 3.168927, mean_q: 6.076849\n",
      " 1307/5000: episode: 129, duration: 0.265s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-2.576, 1.518], loss: 0.101482, mean_absolute_error: 2.948376, mean_q: 5.692698\n",
      " 1317/5000: episode: 130, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.122 [-2.537, 1.533], loss: 0.142079, mean_absolute_error: 3.104376, mean_q: 5.992174\n",
      " 1328/5000: episode: 131, duration: 0.367s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.147 [-2.882, 1.729], loss: 0.353579, mean_absolute_error: 3.147271, mean_q: 6.015495\n",
      " 1338/5000: episode: 132, duration: 0.316s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-3.090, 1.943], loss: 0.934822, mean_absolute_error: 3.255977, mean_q: 6.149192\n",
      " 1348/5000: episode: 133, duration: 0.335s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.139 [-2.558, 1.591], loss: 0.162096, mean_absolute_error: 3.328633, mean_q: 6.396234\n",
      " 1357/5000: episode: 134, duration: 0.269s, episode steps: 9, steps per second: 33, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.776, 1.716], loss: 1.080034, mean_absolute_error: 3.490022, mean_q: 6.577833\n",
      " 1366/5000: episode: 135, duration: 0.308s, episode steps: 9, steps per second: 29, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-2.754, 1.736], loss: 0.138946, mean_absolute_error: 3.254776, mean_q: 6.266291\n",
      " 1375/5000: episode: 136, duration: 0.282s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.149 [-1.906, 1.151], loss: 0.734845, mean_absolute_error: 3.531404, mean_q: 6.684824\n",
      " 1388/5000: episode: 137, duration: 0.433s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.092 [-1.400, 0.776], loss: 0.758690, mean_absolute_error: 3.395889, mean_q: 6.421634\n",
      " 1399/5000: episode: 138, duration: 0.349s, episode steps: 11, steps per second: 32, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.141 [-2.820, 1.726], loss: 1.046587, mean_absolute_error: 3.495542, mean_q: 6.552734\n",
      " 1408/5000: episode: 139, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.818, 1.800], loss: 0.199863, mean_absolute_error: 3.252190, mean_q: 6.198734\n",
      " 1417/5000: episode: 140, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.168 [-2.833, 1.723], loss: 1.112513, mean_absolute_error: 3.473633, mean_q: 6.501526\n",
      " 1426/5000: episode: 141, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.165 [-2.881, 1.782], loss: 0.847480, mean_absolute_error: 3.472377, mean_q: 6.537423\n",
      " 1437/5000: episode: 142, duration: 0.365s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.122 [-1.784, 1.183], loss: 1.527532, mean_absolute_error: 3.638276, mean_q: 6.698230\n",
      " 1464/5000: episode: 143, duration: 0.884s, episode steps: 27, steps per second: 31, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.053 [-0.891, 0.448], loss: 0.686717, mean_absolute_error: 3.493426, mean_q: 6.523605\n",
      " 1484/5000: episode: 144, duration: 0.649s, episode steps: 20, steps per second: 31, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-0.797, 0.372], loss: 1.139285, mean_absolute_error: 3.639090, mean_q: 6.809660\n",
      " 1510/5000: episode: 145, duration: 0.834s, episode steps: 26, steps per second: 31, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.075 [-1.087, 0.559], loss: 0.622846, mean_absolute_error: 3.546354, mean_q: 6.688383\n",
      " 1522/5000: episode: 146, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.092 [-1.325, 0.819], loss: 1.398746, mean_absolute_error: 3.808490, mean_q: 7.094597\n",
      " 1538/5000: episode: 147, duration: 0.533s, episode steps: 16, steps per second: 30, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-0.946, 0.415], loss: 1.297666, mean_absolute_error: 3.805726, mean_q: 7.103794\n",
      " 1554/5000: episode: 148, duration: 0.534s, episode steps: 16, steps per second: 30, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-0.823, 0.441], loss: 1.177981, mean_absolute_error: 3.826615, mean_q: 7.151224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1578/5000: episode: 149, duration: 0.799s, episode steps: 24, steps per second: 30, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.035 [-1.204, 0.637], loss: 0.934159, mean_absolute_error: 3.897507, mean_q: 7.342466\n",
      " 1592/5000: episode: 150, duration: 0.466s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.056 [-1.592, 1.027], loss: 0.907176, mean_absolute_error: 3.992537, mean_q: 7.586948\n",
      " 1602/5000: episode: 151, duration: 0.316s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.126 [-1.632, 0.947], loss: 1.038503, mean_absolute_error: 3.974708, mean_q: 7.551618\n",
      " 1611/5000: episode: 152, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-2.861, 1.719], loss: 0.676283, mean_absolute_error: 3.895174, mean_q: 7.435695\n",
      " 1620/5000: episode: 153, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.140 [-2.284, 1.398], loss: 2.392820, mean_absolute_error: 4.180858, mean_q: 7.687681\n",
      " 1630/5000: episode: 154, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-3.036, 1.999], loss: 1.462898, mean_absolute_error: 4.094791, mean_q: 7.662118\n",
      " 1641/5000: episode: 155, duration: 0.349s, episode steps: 11, steps per second: 32, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.132 [-2.813, 1.771], loss: 1.009145, mean_absolute_error: 4.281491, mean_q: 8.050413\n",
      " 1653/5000: episode: 156, duration: 0.400s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.106 [-2.548, 1.599], loss: 1.635631, mean_absolute_error: 4.384463, mean_q: 8.205703\n",
      " 1662/5000: episode: 157, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.169 [-2.890, 1.731], loss: 2.155893, mean_absolute_error: 4.297804, mean_q: 7.956074\n",
      " 1674/5000: episode: 158, duration: 0.383s, episode steps: 12, steps per second: 31, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.111 [-2.220, 1.399], loss: 1.483707, mean_absolute_error: 4.306428, mean_q: 8.004289\n",
      " 1683/5000: episode: 159, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.141 [-2.478, 1.554], loss: 2.229901, mean_absolute_error: 4.409370, mean_q: 8.079327\n",
      " 1691/5000: episode: 160, duration: 0.265s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.138 [-2.219, 1.417], loss: 1.837057, mean_absolute_error: 4.565343, mean_q: 8.364630\n",
      " 1700/5000: episode: 161, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.165 [-2.264, 1.329], loss: 1.766141, mean_absolute_error: 4.338509, mean_q: 7.930308\n",
      " 1715/5000: episode: 162, duration: 0.500s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.081 [-1.715, 1.027], loss: 1.868730, mean_absolute_error: 4.482786, mean_q: 8.180789\n",
      " 1728/5000: episode: 163, duration: 0.432s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.094 [-1.512, 1.010], loss: 1.113696, mean_absolute_error: 4.372967, mean_q: 8.196995\n",
      " 1741/5000: episode: 164, duration: 0.433s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.108 [-1.485, 0.773], loss: 2.038326, mean_absolute_error: 4.587880, mean_q: 8.499694\n",
      " 1753/5000: episode: 165, duration: 0.398s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.101 [-1.620, 0.948], loss: 1.445231, mean_absolute_error: 4.474195, mean_q: 8.320100\n",
      " 1767/5000: episode: 166, duration: 0.466s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.092 [-1.966, 1.221], loss: 1.143524, mean_absolute_error: 4.333721, mean_q: 8.134027\n",
      " 1778/5000: episode: 167, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.111 [-1.999, 1.169], loss: 1.575856, mean_absolute_error: 4.393340, mean_q: 8.157597\n",
      " 1788/5000: episode: 168, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.117 [-1.679, 1.015], loss: 1.383866, mean_absolute_error: 4.354191, mean_q: 8.139591\n",
      " 1797/5000: episode: 169, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.129 [-1.728, 0.992], loss: 1.792161, mean_absolute_error: 4.449513, mean_q: 8.203452\n",
      " 1810/5000: episode: 170, duration: 0.400s, episode steps: 13, steps per second: 32, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.115 [-1.718, 0.994], loss: 1.737067, mean_absolute_error: 4.463138, mean_q: 8.191215\n",
      " 1825/5000: episode: 171, duration: 0.465s, episode steps: 15, steps per second: 32, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.089 [-1.654, 0.961], loss: 1.727677, mean_absolute_error: 4.411493, mean_q: 8.117517\n",
      " 1834/5000: episode: 172, duration: 0.284s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.137 [-2.274, 1.418], loss: 3.202683, mean_absolute_error: 4.692734, mean_q: 8.456663\n",
      " 1844/5000: episode: 173, duration: 0.319s, episode steps: 10, steps per second: 31, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.131 [-2.500, 1.585], loss: 2.261182, mean_absolute_error: 4.630635, mean_q: 8.384647\n",
      " 1858/5000: episode: 174, duration: 0.463s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.089 [-2.091, 1.344], loss: 1.657160, mean_absolute_error: 4.582872, mean_q: 8.374322\n",
      " 1868/5000: episode: 175, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.120 [-2.206, 1.410], loss: 2.165185, mean_absolute_error: 4.660518, mean_q: 8.410015\n",
      " 1877/5000: episode: 176, duration: 0.301s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.145 [-2.253, 1.342], loss: 0.662353, mean_absolute_error: 4.365117, mean_q: 8.161876\n",
      " 1885/5000: episode: 177, duration: 0.266s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.125 [-2.182, 1.382], loss: 2.292407, mean_absolute_error: 4.617244, mean_q: 8.427958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1894/5000: episode: 178, duration: 0.282s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.144 [-2.253, 1.369], loss: 1.988425, mean_absolute_error: 4.427250, mean_q: 8.125143\n",
      " 1907/5000: episode: 179, duration: 0.416s, episode steps: 13, steps per second: 31, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.090 [-2.728, 1.804], loss: 1.714531, mean_absolute_error: 4.638865, mean_q: 8.543249\n",
      " 1917/5000: episode: 180, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.132 [-2.602, 1.617], loss: 1.580971, mean_absolute_error: 4.708157, mean_q: 8.626022\n",
      " 1926/5000: episode: 181, duration: 0.298s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.121 [-1.874, 1.150], loss: 1.818895, mean_absolute_error: 4.573816, mean_q: 8.353855\n",
      " 1939/5000: episode: 182, duration: 0.432s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.076 [-1.543, 1.002], loss: 1.902071, mean_absolute_error: 4.718022, mean_q: 8.624557\n",
      " 1955/5000: episode: 183, duration: 0.532s, episode steps: 16, steps per second: 30, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.063 [-1.446, 0.827], loss: 0.955459, mean_absolute_error: 4.726705, mean_q: 8.872162\n",
      " 1968/5000: episode: 184, duration: 0.433s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.111 [-1.941, 1.191], loss: 2.129240, mean_absolute_error: 4.717170, mean_q: 8.662220\n",
      " 1981/5000: episode: 185, duration: 0.433s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.079 [-2.116, 1.384], loss: 1.165054, mean_absolute_error: 4.737461, mean_q: 8.936129\n",
      " 1991/5000: episode: 186, duration: 0.315s, episode steps: 10, steps per second: 32, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.111 [-1.969, 1.203], loss: 1.416145, mean_absolute_error: 4.755599, mean_q: 8.910024\n",
      " 2005/5000: episode: 187, duration: 0.416s, episode steps: 14, steps per second: 34, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.088 [-1.658, 0.972], loss: 0.928365, mean_absolute_error: 4.675191, mean_q: 8.811303\n",
      " 2019/5000: episode: 188, duration: 0.450s, episode steps: 14, steps per second: 31, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.091 [-1.741, 1.201], loss: 1.509687, mean_absolute_error: 4.720678, mean_q: 8.831271\n",
      " 2034/5000: episode: 189, duration: 0.499s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.105 [-1.315, 0.743], loss: 2.412943, mean_absolute_error: 4.729182, mean_q: 8.589341\n",
      " 2050/5000: episode: 190, duration: 0.533s, episode steps: 16, steps per second: 30, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.099 [-1.102, 0.438], loss: 1.444509, mean_absolute_error: 4.669090, mean_q: 8.597153\n",
      " 2078/5000: episode: 191, duration: 0.901s, episode steps: 28, steps per second: 31, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.033 [-0.867, 0.433], loss: 1.254463, mean_absolute_error: 4.759018, mean_q: 8.902196\n",
      " 2095/5000: episode: 192, duration: 0.549s, episode steps: 17, steps per second: 31, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.093 [-1.029, 0.373], loss: 2.194160, mean_absolute_error: 4.843036, mean_q: 8.819132\n",
      " 2112/5000: episode: 193, duration: 0.566s, episode steps: 17, steps per second: 30, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.073 [-1.128, 0.584], loss: 1.566350, mean_absolute_error: 4.732231, mean_q: 8.709888\n",
      " 2133/5000: episode: 194, duration: 0.649s, episode steps: 21, steps per second: 32, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.090 [-1.005, 0.541], loss: 1.306152, mean_absolute_error: 4.790381, mean_q: 8.956412\n",
      " 2153/5000: episode: 195, duration: 0.649s, episode steps: 20, steps per second: 31, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.066 [-1.008, 0.590], loss: 1.195114, mean_absolute_error: 4.796137, mean_q: 8.990114\n",
      " 2173/5000: episode: 196, duration: 0.666s, episode steps: 20, steps per second: 30, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.054 [-1.073, 0.631], loss: 1.378427, mean_absolute_error: 4.849928, mean_q: 9.062593\n",
      " 2189/5000: episode: 197, duration: 0.533s, episode steps: 16, steps per second: 30, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.092 [-1.194, 0.811], loss: 1.237232, mean_absolute_error: 4.840028, mean_q: 9.068827\n",
      " 2205/5000: episode: 198, duration: 0.500s, episode steps: 16, steps per second: 32, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.118 [-1.302, 0.558], loss: 1.742726, mean_absolute_error: 4.880469, mean_q: 9.037115\n",
      " 2239/5000: episode: 199, duration: 1.116s, episode steps: 34, steps per second: 30, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.046 [-0.808, 0.606], loss: 1.694521, mean_absolute_error: 4.944570, mean_q: 9.152669\n",
      " 2328/5000: episode: 200, duration: 2.918s, episode steps: 89, steps per second: 31, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.205 [-0.358, 1.671], loss: 1.378245, mean_absolute_error: 4.957324, mean_q: 9.253411\n",
      " 2345/5000: episode: 201, duration: 0.549s, episode steps: 17, steps per second: 31, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.119 [-1.152, 0.602], loss: 1.475702, mean_absolute_error: 5.267777, mean_q: 9.951674\n",
      " 2362/5000: episode: 202, duration: 0.566s, episode steps: 17, steps per second: 30, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.105 [-0.892, 0.416], loss: 1.212656, mean_absolute_error: 5.080312, mean_q: 9.596205\n",
      " 2386/5000: episode: 203, duration: 0.783s, episode steps: 24, steps per second: 31, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-0.755, 0.354], loss: 1.617210, mean_absolute_error: 5.104127, mean_q: 9.527904\n",
      " 2423/5000: episode: 204, duration: 1.234s, episode steps: 37, steps per second: 30, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.117 [-0.690, 0.191], loss: 1.110414, mean_absolute_error: 5.281094, mean_q: 9.975618\n",
      " 2497/5000: episode: 205, duration: 2.418s, episode steps: 74, steps per second: 31, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.084 [-1.544, 1.340], loss: 1.261044, mean_absolute_error: 5.253278, mean_q: 9.909914\n",
      " 2551/5000: episode: 206, duration: 1.733s, episode steps: 54, steps per second: 31, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.123 [-0.383, 0.926], loss: 1.530904, mean_absolute_error: 5.420112, mean_q: 10.178898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2686/5000: episode: 207, duration: 4.335s, episode steps: 135, steps per second: 31, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.137 [-0.759, 0.594], loss: 1.271002, mean_absolute_error: 5.550431, mean_q: 10.531247\n",
      " 2788/5000: episode: 208, duration: 3.384s, episode steps: 102, steps per second: 30, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.278 [-1.536, 0.859], loss: 1.552405, mean_absolute_error: 5.714554, mean_q: 10.818386\n",
      " 2824/5000: episode: 209, duration: 1.183s, episode steps: 36, steps per second: 30, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.112 [-0.674, 0.190], loss: 1.674960, mean_absolute_error: 5.868788, mean_q: 11.099743\n",
      " 2854/5000: episode: 210, duration: 0.999s, episode steps: 30, steps per second: 30, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-1.033, 0.351], loss: 1.593707, mean_absolute_error: 5.961418, mean_q: 11.338479\n",
      " 2885/5000: episode: 211, duration: 1.017s, episode steps: 31, steps per second: 30, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.108 [-0.631, 0.406], loss: 1.223287, mean_absolute_error: 6.022363, mean_q: 11.505571\n",
      " 2916/5000: episode: 212, duration: 1.017s, episode steps: 31, steps per second: 30, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.119 [-1.060, 0.248], loss: 1.833142, mean_absolute_error: 6.080263, mean_q: 11.530210\n",
      " 3001/5000: episode: 213, duration: 2.750s, episode steps: 85, steps per second: 31, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.038 [-0.617, 0.373], loss: 1.557183, mean_absolute_error: 6.092715, mean_q: 11.571988\n",
      " 3045/5000: episode: 214, duration: 1.400s, episode steps: 44, steps per second: 31, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.024, 0.378], loss: 2.429090, mean_absolute_error: 6.250246, mean_q: 11.692609\n",
      " 3084/5000: episode: 215, duration: 1.300s, episode steps: 39, steps per second: 30, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.081 [-0.699, 0.427], loss: 2.143888, mean_absolute_error: 6.324884, mean_q: 11.898494\n",
      " 3127/5000: episode: 216, duration: 1.367s, episode steps: 43, steps per second: 31, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.112 [-0.761, 0.564], loss: 1.355661, mean_absolute_error: 6.337866, mean_q: 12.110701\n",
      " 3228/5000: episode: 217, duration: 3.301s, episode steps: 101, steps per second: 31, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.141 [-0.309, 0.785], loss: 1.759751, mean_absolute_error: 6.537331, mean_q: 12.448636\n",
      " 3277/5000: episode: 218, duration: 1.632s, episode steps: 49, steps per second: 30, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.148 [-0.748, 0.318], loss: 1.606499, mean_absolute_error: 6.699921, mean_q: 12.866691\n",
      " 3323/5000: episode: 219, duration: 1.468s, episode steps: 46, steps per second: 31, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.146 [-0.703, 0.413], loss: 2.111612, mean_absolute_error: 6.792576, mean_q: 12.921493\n",
      " 3386/5000: episode: 220, duration: 2.083s, episode steps: 63, steps per second: 30, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.051 [-1.039, 0.399], loss: 2.063142, mean_absolute_error: 6.871744, mean_q: 13.109642\n",
      " 3439/5000: episode: 221, duration: 1.733s, episode steps: 53, steps per second: 31, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.103 [-0.927, 0.631], loss: 1.646843, mean_absolute_error: 6.946269, mean_q: 13.336655\n",
      " 3477/5000: episode: 222, duration: 1.217s, episode steps: 38, steps per second: 31, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.144 [-0.700, 0.231], loss: 2.018339, mean_absolute_error: 7.109046, mean_q: 13.634936\n",
      " 3556/5000: episode: 223, duration: 2.601s, episode steps: 79, steps per second: 30, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.112 [-0.948, 0.348], loss: 2.025342, mean_absolute_error: 7.186490, mean_q: 13.800739\n",
      " 3660/5000: episode: 224, duration: 3.452s, episode steps: 104, steps per second: 30, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.066 [-0.938, 0.545], loss: 1.990261, mean_absolute_error: 7.337687, mean_q: 14.117855\n",
      " 3707/5000: episode: 225, duration: 1.566s, episode steps: 47, steps per second: 30, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.145 [-0.891, 0.215], loss: 2.181052, mean_absolute_error: 7.478990, mean_q: 14.361399\n",
      " 3807/5000: episode: 226, duration: 3.386s, episode steps: 100, steps per second: 30, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.069 [-0.725, 0.379], loss: 1.796827, mean_absolute_error: 7.577691, mean_q: 14.642399\n",
      " 3898/5000: episode: 227, duration: 3.101s, episode steps: 91, steps per second: 29, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.158 [-0.276, 0.896], loss: 2.027407, mean_absolute_error: 7.860151, mean_q: 15.212181\n",
      " 3951/5000: episode: 228, duration: 1.783s, episode steps: 53, steps per second: 30, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.149 [-0.882, 0.234], loss: 2.035780, mean_absolute_error: 7.888814, mean_q: 15.314336\n",
      " 4015/5000: episode: 229, duration: 2.100s, episode steps: 64, steps per second: 30, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.122 [-0.737, 0.380], loss: 2.564125, mean_absolute_error: 8.060459, mean_q: 15.551910\n",
      " 4054/5000: episode: 230, duration: 1.299s, episode steps: 39, steps per second: 30, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.131 [-0.692, 0.234], loss: 2.697598, mean_absolute_error: 8.182070, mean_q: 15.738955\n",
      " 4114/5000: episode: 231, duration: 1.984s, episode steps: 60, steps per second: 30, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.114 [-0.758, 0.332], loss: 2.140859, mean_absolute_error: 8.238832, mean_q: 15.966074\n",
      " 4172/5000: episode: 232, duration: 1.934s, episode steps: 58, steps per second: 30, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.127 [-0.752, 0.270], loss: 2.339540, mean_absolute_error: 8.417425, mean_q: 16.314898\n",
      " 4272/5000: episode: 233, duration: 3.334s, episode steps: 100, steps per second: 30, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.147 [-0.554, 0.706], loss: 2.059442, mean_absolute_error: 8.537967, mean_q: 16.656458\n",
      " 4357/5000: episode: 234, duration: 2.818s, episode steps: 85, steps per second: 30, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.102 [-0.722, 0.290], loss: 2.197664, mean_absolute_error: 8.762758, mean_q: 17.084335\n",
      " 4399/5000: episode: 235, duration: 1.333s, episode steps: 42, steps per second: 32, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.152 [-0.742, 0.242], loss: 3.316548, mean_absolute_error: 8.849257, mean_q: 17.112902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4475/5000: episode: 236, duration: 2.502s, episode steps: 76, steps per second: 30, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.102 [-0.856, 0.245], loss: 2.155948, mean_absolute_error: 8.938640, mean_q: 17.464560\n",
      " 4551/5000: episode: 237, duration: 2.500s, episode steps: 76, steps per second: 30, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.117 [-0.766, 0.207], loss: 2.764911, mean_absolute_error: 9.155194, mean_q: 17.809622\n",
      " 4618/5000: episode: 238, duration: 2.184s, episode steps: 67, steps per second: 31, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.113 [-0.855, 0.244], loss: 2.674924, mean_absolute_error: 9.253394, mean_q: 18.076935\n",
      " 4759/5000: episode: 239, duration: 4.652s, episode steps: 141, steps per second: 30, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.145 [-0.459, 0.876], loss: 3.038724, mean_absolute_error: 9.511333, mean_q: 18.548922\n",
      " 4807/5000: episode: 240, duration: 1.583s, episode steps: 48, steps per second: 30, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.127 [-0.934, 0.192], loss: 2.787666, mean_absolute_error: 9.672446, mean_q: 18.886686\n",
      " 4852/5000: episode: 241, duration: 1.450s, episode steps: 45, steps per second: 31, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.118 [-1.028, 0.152], loss: 2.835341, mean_absolute_error: 9.788714, mean_q: 19.103331\n",
      " 4902/5000: episode: 242, duration: 1.600s, episode steps: 50, steps per second: 31, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.159 [-0.729, 0.239], loss: 3.229759, mean_absolute_error: 9.787331, mean_q: 19.025177\n",
      " 4966/5000: episode: 243, duration: 2.101s, episode steps: 64, steps per second: 30, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.139 [-0.761, 0.322], loss: 2.656743, mean_absolute_error: 9.878555, mean_q: 19.373676\n",
      "done, took 171.022 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)\n",
    "except:\n",
    "  dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1547297393963,
     "user": {
      "displayName": "Andre Vermeulen",
      "photoUrl": "",
      "userId": "07958753266952227006"
     },
     "user_tz": 0
    },
    "id": "xBVgeLRODNgG",
    "outputId": "a99a7286-5c10-4bc5-a653-f0b008be8f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 68.000, steps: 68\n",
      "Episode 2: reward: 52.000, steps: 52\n",
      "Episode 3: reward: 49.000, steps: 49\n",
      "Episode 4: reward: 54.000, steps: 54\n",
      "Episode 5: reward: 128.000, steps: 128\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  dqn.test(env, nb_episodes=5, visualize=True, verbose=2)\n",
    "except:\n",
    "  dqn.test(env, nb_episodes=5, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! 2019-04-24 22:01:56.467509\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print('Done!',str(now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGaJ1fOZ90ZK"
   },
   "source": [
    "Your can now test the reinforcement learning model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Chapter-04-013-Q-Learn-01.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
