{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apress - Industrialized Machine Learning Examples\n",
    "\n",
    "Andreas Francois Vermeulen\n",
    "2019\n",
    "\n",
    "### This is an example add-on to a book and needs to be accepted as part of that copyright."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 004 Example 026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AndreVermeulen\\Documents\\My Book\\apress\\Industrialized Machine Learning\\book\\IML\\Data\\Roses01.csv\n"
     ]
    }
   ],
   "source": [
    "fileName = '../../Data/Roses01.csv'\n",
    "fileFullName = os.path.abspath(fileName)\n",
    "print(fileFullName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 6)\n",
      "Index(['F01', 'F02', 'F03', 'F04', 'T', 'T2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "rosedf= pd.read_csv(fileFullName, header=0)\n",
    "print(rosedf.shape)\n",
    "print(rosedf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rose = np.array(rosedf)\n",
    "rose_data = np.array(rosedf[['F01', 'F02', 'F03', 'F04']].copy(deep=True))\n",
    "rose_target = np.array(rosedf['T'].copy(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning on dataset rose\n",
      "training: constant learning-rate\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.03169308\n",
      "Iteration 3, loss = 1.01164057\n",
      "Iteration 4, loss = 0.99231479\n",
      "Iteration 5, loss = 0.97288808\n",
      "Iteration 6, loss = 0.95295324\n",
      "Iteration 7, loss = 0.93316384\n",
      "Iteration 8, loss = 0.91358201\n",
      "Iteration 9, loss = 0.89448215\n",
      "Iteration 10, loss = 0.87559231\n",
      "Iteration 11, loss = 0.85657310\n",
      "Iteration 12, loss = 0.83798337\n",
      "Iteration 13, loss = 0.82008910\n",
      "Iteration 14, loss = 0.80268342\n",
      "Iteration 15, loss = 0.78564117\n",
      "Iteration 16, loss = 0.76902675\n",
      "Iteration 17, loss = 0.75293209\n",
      "Iteration 18, loss = 0.73739946\n",
      "Iteration 19, loss = 0.72239835\n",
      "Iteration 20, loss = 0.70786694\n",
      "Iteration 21, loss = 0.69380993\n",
      "Iteration 22, loss = 0.68022047\n",
      "Iteration 23, loss = 0.66709115\n",
      "Iteration 24, loss = 0.65442376\n",
      "Iteration 25, loss = 0.64220671\n",
      "Iteration 26, loss = 0.63044138\n",
      "Iteration 27, loss = 0.61912946\n",
      "Iteration 28, loss = 0.60825704\n",
      "Iteration 29, loss = 0.59781314\n",
      "Iteration 30, loss = 0.58778543\n",
      "Iteration 31, loss = 0.57816109\n",
      "Iteration 32, loss = 0.56892304\n",
      "Iteration 33, loss = 0.56005060\n",
      "Iteration 34, loss = 0.55153417\n",
      "Iteration 35, loss = 0.54335818\n",
      "Iteration 36, loss = 0.53550030\n",
      "Iteration 37, loss = 0.52794485\n",
      "Iteration 38, loss = 0.52067373\n",
      "Iteration 39, loss = 0.51367420\n",
      "Iteration 40, loss = 0.50693436\n",
      "Iteration 41, loss = 0.50043424\n",
      "Iteration 42, loss = 0.49416394\n",
      "Iteration 43, loss = 0.48811593\n",
      "Iteration 44, loss = 0.48228033\n",
      "Iteration 45, loss = 0.47663935\n",
      "Iteration 46, loss = 0.47118602\n",
      "Iteration 47, loss = 0.46590887\n",
      "Iteration 48, loss = 0.46079178\n",
      "Iteration 49, loss = 0.45582632\n",
      "Iteration 50, loss = 0.45100839\n",
      "Iteration 51, loss = 0.44632533\n",
      "Iteration 52, loss = 0.44176553\n",
      "Iteration 53, loss = 0.43732948\n",
      "Iteration 54, loss = 0.43300869\n",
      "Iteration 55, loss = 0.42879605\n",
      "Iteration 56, loss = 0.42468855\n",
      "Iteration 57, loss = 0.42067872\n",
      "Iteration 58, loss = 0.41676544\n",
      "Iteration 59, loss = 0.41294003\n",
      "Iteration 60, loss = 0.40919735\n",
      "Iteration 61, loss = 0.40553291\n",
      "Iteration 62, loss = 0.40193690\n",
      "Iteration 63, loss = 0.39840948\n",
      "Iteration 64, loss = 0.39494640\n",
      "Iteration 65, loss = 0.39154670\n",
      "Iteration 66, loss = 0.38820422\n",
      "Iteration 67, loss = 0.38492235\n",
      "Iteration 68, loss = 0.38169940\n",
      "Iteration 69, loss = 0.37852644\n",
      "Iteration 70, loss = 0.37541338\n",
      "Iteration 71, loss = 0.37236000\n",
      "Iteration 72, loss = 0.36935330\n",
      "Iteration 73, loss = 0.36639677\n",
      "Iteration 74, loss = 0.36348861\n",
      "Iteration 75, loss = 0.36063099\n",
      "Iteration 76, loss = 0.35782548\n",
      "Iteration 77, loss = 0.35506386\n",
      "Iteration 78, loss = 0.35234599\n",
      "Iteration 79, loss = 0.34966686\n",
      "Iteration 80, loss = 0.34701876\n",
      "Iteration 81, loss = 0.34440217\n",
      "Iteration 82, loss = 0.34182147\n",
      "Iteration 83, loss = 0.33927292\n",
      "Iteration 84, loss = 0.33675985\n",
      "Iteration 85, loss = 0.33427908\n",
      "Iteration 86, loss = 0.33183036\n",
      "Iteration 87, loss = 0.32941355\n",
      "Iteration 88, loss = 0.32702612\n",
      "Iteration 89, loss = 0.32466712\n",
      "Iteration 90, loss = 0.32233727\n",
      "Iteration 91, loss = 0.32004644\n",
      "Iteration 92, loss = 0.31778689\n",
      "Iteration 93, loss = 0.31555430\n",
      "Iteration 94, loss = 0.31334682\n",
      "Iteration 95, loss = 0.31116713\n",
      "Iteration 96, loss = 0.30901466\n",
      "Iteration 97, loss = 0.30688718\n",
      "Iteration 98, loss = 0.30478615\n",
      "Iteration 99, loss = 0.30270876\n",
      "Iteration 100, loss = 0.30065512\n",
      "Iteration 101, loss = 0.29862615\n",
      "Iteration 102, loss = 0.29662085\n",
      "Iteration 103, loss = 0.29463729\n",
      "Iteration 104, loss = 0.29267615\n",
      "Iteration 105, loss = 0.29073681\n",
      "Iteration 106, loss = 0.28881782\n",
      "Iteration 107, loss = 0.28691959\n",
      "Iteration 108, loss = 0.28504171\n",
      "Iteration 109, loss = 0.28318359\n",
      "Iteration 110, loss = 0.28134521\n",
      "Iteration 111, loss = 0.27952565\n",
      "Iteration 112, loss = 0.27772486\n",
      "Iteration 113, loss = 0.27594333\n",
      "Iteration 114, loss = 0.27418044\n",
      "Iteration 115, loss = 0.27243571\n",
      "Iteration 116, loss = 0.27070894\n",
      "Iteration 117, loss = 0.26899963\n",
      "Iteration 118, loss = 0.26730721\n",
      "Iteration 119, loss = 0.26563152\n",
      "Iteration 120, loss = 0.26397287\n",
      "Iteration 121, loss = 0.26233165\n",
      "Iteration 122, loss = 0.26070679\n",
      "Iteration 123, loss = 0.25909844\n",
      "Iteration 124, loss = 0.25750720\n",
      "Iteration 125, loss = 0.25593262\n",
      "Iteration 126, loss = 0.25437376\n",
      "Iteration 127, loss = 0.25283116\n",
      "Iteration 128, loss = 0.25130382\n",
      "Iteration 129, loss = 0.24979189\n",
      "Iteration 130, loss = 0.24829522\n",
      "Iteration 131, loss = 0.24681264\n",
      "Iteration 132, loss = 0.24534501\n",
      "Iteration 133, loss = 0.24389222\n",
      "Iteration 134, loss = 0.24245458\n",
      "Iteration 135, loss = 0.24103085\n",
      "Iteration 136, loss = 0.23962153\n",
      "Iteration 137, loss = 0.23822621\n",
      "Iteration 138, loss = 0.23684468\n",
      "Iteration 139, loss = 0.23547759\n",
      "Iteration 140, loss = 0.23412383\n",
      "Iteration 141, loss = 0.23278479\n",
      "Iteration 142, loss = 0.23145944\n",
      "Iteration 143, loss = 0.23014747\n",
      "Iteration 144, loss = 0.22884837\n",
      "Iteration 145, loss = 0.22756220\n",
      "Iteration 146, loss = 0.22628855\n",
      "Iteration 147, loss = 0.22502770\n",
      "Iteration 148, loss = 0.22377981\n",
      "Iteration 149, loss = 0.22254416\n",
      "Iteration 150, loss = 0.22132004\n",
      "Iteration 151, loss = 0.22010673\n",
      "Iteration 152, loss = 0.21890507\n",
      "Iteration 153, loss = 0.21771519\n",
      "Iteration 154, loss = 0.21653690\n",
      "Iteration 155, loss = 0.21537012\n",
      "Iteration 156, loss = 0.21421476\n",
      "Iteration 157, loss = 0.21307105\n",
      "Iteration 158, loss = 0.21193984\n",
      "Iteration 159, loss = 0.21081915\n",
      "Iteration 160, loss = 0.20970886\n",
      "Iteration 161, loss = 0.20860904\n",
      "Iteration 162, loss = 0.20751961\n",
      "Iteration 163, loss = 0.20644075\n",
      "Iteration 164, loss = 0.20537209\n",
      "Iteration 165, loss = 0.20431408\n",
      "Iteration 166, loss = 0.20326605\n",
      "Iteration 167, loss = 0.20222812\n",
      "Iteration 168, loss = 0.20120030\n",
      "Iteration 169, loss = 0.20018263\n",
      "Iteration 170, loss = 0.19917452\n",
      "Iteration 171, loss = 0.19817623\n",
      "Iteration 172, loss = 0.19718724\n",
      "Iteration 173, loss = 0.19620765\n",
      "Iteration 174, loss = 0.19523738\n",
      "Iteration 175, loss = 0.19427651\n",
      "Iteration 176, loss = 0.19332449\n",
      "Iteration 177, loss = 0.19238192\n",
      "Iteration 178, loss = 0.19144840\n",
      "Iteration 179, loss = 0.19052367\n",
      "Iteration 180, loss = 0.18960755\n",
      "Iteration 181, loss = 0.18870024\n",
      "Iteration 182, loss = 0.18780150\n",
      "Iteration 183, loss = 0.18691114\n",
      "Iteration 184, loss = 0.18602931\n",
      "Iteration 185, loss = 0.18515594\n",
      "Iteration 186, loss = 0.18429078\n",
      "Iteration 187, loss = 0.18343353\n",
      "Iteration 188, loss = 0.18258421\n",
      "Iteration 189, loss = 0.18174264\n",
      "Iteration 190, loss = 0.18090917\n",
      "Iteration 191, loss = 0.18008418\n",
      "Iteration 192, loss = 0.17926661\n",
      "Iteration 193, loss = 0.17845653\n",
      "Iteration 194, loss = 0.17765401\n",
      "Iteration 195, loss = 0.17685863\n",
      "Iteration 196, loss = 0.17607052\n",
      "Iteration 197, loss = 0.17528947\n",
      "Iteration 198, loss = 0.17451541\n",
      "Iteration 199, loss = 0.17374844\n",
      "Iteration 200, loss = 0.17298824\n",
      "Iteration 201, loss = 0.17223491\n",
      "Iteration 202, loss = 0.17148885\n",
      "Iteration 203, loss = 0.17074951\n",
      "Iteration 204, loss = 0.17001679\n",
      "Iteration 205, loss = 0.16929062\n",
      "Iteration 206, loss = 0.16857089\n",
      "Iteration 207, loss = 0.16785770\n",
      "Iteration 208, loss = 0.16715072\n",
      "Iteration 209, loss = 0.16644997\n",
      "Iteration 210, loss = 0.16575550\n",
      "Iteration 211, loss = 0.16506718\n",
      "Iteration 212, loss = 0.16438500\n",
      "Iteration 213, loss = 0.16370887\n",
      "Iteration 214, loss = 0.16303863\n",
      "Iteration 215, loss = 0.16237404\n",
      "Iteration 216, loss = 0.16171541\n",
      "Iteration 217, loss = 0.16106233\n",
      "Iteration 218, loss = 0.16041481\n",
      "Iteration 219, loss = 0.15977297\n",
      "Iteration 220, loss = 0.15913650\n",
      "Iteration 221, loss = 0.15850554\n",
      "Iteration 222, loss = 0.15787988\n",
      "Iteration 223, loss = 0.15725958\n",
      "Iteration 224, loss = 0.15664454\n",
      "Iteration 225, loss = 0.15603467\n",
      "Iteration 226, loss = 0.15542995\n",
      "Iteration 227, loss = 0.15483036\n",
      "Iteration 228, loss = 0.15423584\n",
      "Iteration 229, loss = 0.15364645\n",
      "Iteration 230, loss = 0.15306198\n",
      "Iteration 231, loss = 0.15248234\n",
      "Iteration 232, loss = 0.15190759\n",
      "Iteration 233, loss = 0.15133751\n",
      "Iteration 234, loss = 0.15077220\n",
      "Iteration 235, loss = 0.15021171\n",
      "Iteration 236, loss = 0.14965564\n",
      "Iteration 237, loss = 0.14910417\n",
      "Iteration 238, loss = 0.14855729\n",
      "Iteration 239, loss = 0.14801477\n",
      "Iteration 240, loss = 0.14747683\n",
      "Iteration 241, loss = 0.14694319\n",
      "Iteration 242, loss = 0.14641376\n",
      "Iteration 243, loss = 0.14588883\n",
      "Iteration 244, loss = 0.14536803\n",
      "Iteration 245, loss = 0.14485143\n",
      "Iteration 246, loss = 0.14433898\n",
      "Iteration 247, loss = 0.14383059\n",
      "Iteration 248, loss = 0.14332646\n",
      "Iteration 249, loss = 0.14282623\n",
      "Iteration 250, loss = 0.14233001\n",
      "Iteration 251, loss = 0.14183790\n",
      "Iteration 252, loss = 0.14134951\n",
      "Iteration 253, loss = 0.14086510\n",
      "Iteration 254, loss = 0.14038441\n",
      "Iteration 255, loss = 0.13990752\n",
      "Iteration 256, loss = 0.13943435\n",
      "Iteration 257, loss = 0.13896486\n",
      "Iteration 258, loss = 0.13849916\n",
      "Iteration 259, loss = 0.13803694\n",
      "Iteration 260, loss = 0.13757849\n",
      "Iteration 261, loss = 0.13712343\n",
      "Iteration 262, loss = 0.13667197\n",
      "Iteration 263, loss = 0.13622402\n",
      "Iteration 264, loss = 0.13577954\n",
      "Iteration 265, loss = 0.13533848\n",
      "Iteration 266, loss = 0.13490078\n",
      "Iteration 267, loss = 0.13446642\n",
      "Iteration 268, loss = 0.13403529\n",
      "Iteration 269, loss = 0.13360758\n",
      "Iteration 270, loss = 0.13318293\n",
      "Iteration 271, loss = 0.13276170\n",
      "Iteration 272, loss = 0.13234348\n",
      "Iteration 273, loss = 0.13192843\n",
      "Iteration 274, loss = 0.13151645\n",
      "Iteration 275, loss = 0.13110767\n",
      "Iteration 276, loss = 0.13070188\n",
      "Iteration 277, loss = 0.13029918\n",
      "Iteration 278, loss = 0.12989945\n",
      "Iteration 279, loss = 0.12950270\n",
      "Iteration 280, loss = 0.12910890\n",
      "Iteration 281, loss = 0.12871801\n",
      "Iteration 282, loss = 0.12833001\n",
      "Iteration 283, loss = 0.12794485\n",
      "Iteration 284, loss = 0.12756253\n",
      "Iteration 285, loss = 0.12718295\n",
      "Iteration 286, loss = 0.12680623\n",
      "Iteration 287, loss = 0.12643213\n",
      "Iteration 288, loss = 0.12606086\n",
      "Iteration 289, loss = 0.12569215\n",
      "Iteration 290, loss = 0.12532622\n",
      "Iteration 291, loss = 0.12496282\n",
      "Iteration 292, loss = 0.12460209\n",
      "Iteration 293, loss = 0.12424391\n",
      "Iteration 294, loss = 0.12388833\n",
      "Iteration 295, loss = 0.12353526\n",
      "Iteration 296, loss = 0.12318470\n",
      "Iteration 297, loss = 0.12283664\n",
      "Iteration 298, loss = 0.12249101\n",
      "Iteration 299, loss = 0.12214788\n",
      "Iteration 300, loss = 0.12180710\n",
      "Iteration 301, loss = 0.12146879\n",
      "Iteration 302, loss = 0.12113277\n",
      "Iteration 303, loss = 0.12079919\n",
      "Iteration 304, loss = 0.12046788\n",
      "Iteration 305, loss = 0.12013897\n",
      "Iteration 306, loss = 0.11981224\n",
      "Iteration 307, loss = 0.11948792\n",
      "Iteration 308, loss = 0.11916569\n",
      "Iteration 309, loss = 0.11884586\n",
      "Iteration 310, loss = 0.11852805\n",
      "Iteration 311, loss = 0.11821262\n",
      "Iteration 312, loss = 0.11789918\n",
      "Iteration 313, loss = 0.11758802\n",
      "Iteration 314, loss = 0.11727888\n",
      "Iteration 315, loss = 0.11697192\n",
      "Iteration 316, loss = 0.11666701\n",
      "Iteration 317, loss = 0.11636416\n",
      "Iteration 318, loss = 0.11606343\n",
      "Iteration 319, loss = 0.11576465\n",
      "Iteration 320, loss = 0.11546802\n",
      "Iteration 321, loss = 0.11517323\n",
      "Iteration 322, loss = 0.11488060\n",
      "Iteration 323, loss = 0.11458976\n",
      "Iteration 324, loss = 0.11430099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 325, loss = 0.11401408\n",
      "Iteration 326, loss = 0.11372911\n",
      "Iteration 327, loss = 0.11344604\n",
      "Iteration 328, loss = 0.11316481\n",
      "Iteration 329, loss = 0.11288549\n",
      "Iteration 330, loss = 0.11260781\n",
      "Iteration 331, loss = 0.11233190\n",
      "Iteration 332, loss = 0.11205771\n",
      "Iteration 333, loss = 0.11178540\n",
      "Iteration 334, loss = 0.11151480\n",
      "Iteration 335, loss = 0.11124601\n",
      "Iteration 336, loss = 0.11097896\n",
      "Iteration 337, loss = 0.11071358\n",
      "Iteration 338, loss = 0.11045004\n",
      "Iteration 339, loss = 0.11018811\n",
      "Iteration 340, loss = 0.10992802\n",
      "Iteration 341, loss = 0.10966954\n",
      "Iteration 342, loss = 0.10941275\n",
      "Iteration 343, loss = 0.10915762\n",
      "Iteration 344, loss = 0.10890409\n",
      "Iteration 345, loss = 0.10865224\n",
      "Iteration 346, loss = 0.10840187\n",
      "Iteration 347, loss = 0.10815322\n",
      "Iteration 348, loss = 0.10790607\n",
      "Iteration 349, loss = 0.10766049\n",
      "Iteration 350, loss = 0.10741651\n",
      "Iteration 351, loss = 0.10717397\n",
      "Iteration 352, loss = 0.10693305\n",
      "Iteration 353, loss = 0.10669355\n",
      "Iteration 354, loss = 0.10645561\n",
      "Iteration 355, loss = 0.10621915\n",
      "Iteration 356, loss = 0.10598406\n",
      "Iteration 357, loss = 0.10575012\n",
      "Iteration 358, loss = 0.10551754\n",
      "Iteration 359, loss = 0.10528643\n",
      "Iteration 360, loss = 0.10505676\n",
      "Iteration 361, loss = 0.10482846\n",
      "Iteration 362, loss = 0.10460167\n",
      "Iteration 363, loss = 0.10437615\n",
      "Iteration 364, loss = 0.10415207\n",
      "Iteration 365, loss = 0.10392933\n",
      "Iteration 366, loss = 0.10370788\n",
      "Iteration 367, loss = 0.10348783\n",
      "Iteration 368, loss = 0.10326900\n",
      "Iteration 369, loss = 0.10305155\n",
      "Iteration 370, loss = 0.10283541\n",
      "Iteration 371, loss = 0.10262056\n",
      "Iteration 372, loss = 0.10240716\n",
      "Iteration 373, loss = 0.10219507\n",
      "Iteration 374, loss = 0.10198445\n",
      "Iteration 375, loss = 0.10177520\n",
      "Iteration 376, loss = 0.10156712\n",
      "Iteration 377, loss = 0.10136041\n",
      "Iteration 378, loss = 0.10115484\n",
      "Iteration 379, loss = 0.10095049\n",
      "Iteration 380, loss = 0.10074741\n",
      "Iteration 381, loss = 0.10054545\n",
      "Iteration 382, loss = 0.10034474\n",
      "Iteration 383, loss = 0.10014518\n",
      "Iteration 384, loss = 0.09994681\n",
      "Iteration 385, loss = 0.09974966\n",
      "Iteration 386, loss = 0.09955340\n",
      "Iteration 387, loss = 0.09935831\n",
      "Iteration 388, loss = 0.09916440\n",
      "Iteration 389, loss = 0.09897161\n",
      "Iteration 390, loss = 0.09877997\n",
      "Iteration 391, loss = 0.09858940\n",
      "Iteration 392, loss = 0.09839991\n",
      "Iteration 393, loss = 0.09821157\n",
      "Iteration 394, loss = 0.09802425\n",
      "Iteration 395, loss = 0.09783801\n",
      "Iteration 396, loss = 0.09765286\n",
      "Iteration 397, loss = 0.09746874\n",
      "Iteration 398, loss = 0.09728570\n",
      "Iteration 399, loss = 0.09710365\n",
      "Iteration 400, loss = 0.09692234\n",
      "Iteration 401, loss = 0.09674213\n",
      "Iteration 402, loss = 0.09656286\n",
      "Iteration 403, loss = 0.09638460\n",
      "Iteration 404, loss = 0.09620743\n",
      "Iteration 405, loss = 0.09603117\n",
      "Iteration 406, loss = 0.09585597\n",
      "Iteration 407, loss = 0.09568180\n",
      "Iteration 408, loss = 0.09550859\n",
      "Iteration 409, loss = 0.09533665\n",
      "Iteration 410, loss = 0.09516565\n",
      "Iteration 411, loss = 0.09499554\n",
      "Iteration 412, loss = 0.09482644\n",
      "Iteration 413, loss = 0.09465825\n",
      "Iteration 414, loss = 0.09449095\n",
      "Iteration 415, loss = 0.09432462\n",
      "Iteration 416, loss = 0.09415920\n",
      "Iteration 417, loss = 0.09399463\n",
      "Iteration 418, loss = 0.09383102\n",
      "Iteration 419, loss = 0.09366832\n",
      "Iteration 420, loss = 0.09350659\n",
      "Iteration 421, loss = 0.09334580\n",
      "Iteration 422, loss = 0.09318582\n",
      "Iteration 423, loss = 0.09302673\n",
      "Iteration 424, loss = 0.09286853\n",
      "Iteration 425, loss = 0.09271112\n",
      "Iteration 426, loss = 0.09255460\n",
      "Iteration 427, loss = 0.09239895\n",
      "Iteration 428, loss = 0.09224407\n",
      "Iteration 429, loss = 0.09209008\n",
      "Iteration 430, loss = 0.09193701\n",
      "Iteration 431, loss = 0.09178470\n",
      "Iteration 432, loss = 0.09163325\n",
      "Iteration 433, loss = 0.09148260\n",
      "Iteration 434, loss = 0.09133271\n",
      "Iteration 435, loss = 0.09118364\n",
      "Iteration 436, loss = 0.09103536\n",
      "Iteration 437, loss = 0.09088783\n",
      "Iteration 438, loss = 0.09074109\n",
      "Iteration 439, loss = 0.09059513\n",
      "Iteration 440, loss = 0.09044991\n",
      "Iteration 441, loss = 0.09030545\n",
      "Iteration 442, loss = 0.09016176\n",
      "Iteration 443, loss = 0.09001874\n",
      "Iteration 444, loss = 0.08987592\n",
      "Iteration 445, loss = 0.08973391\n",
      "Iteration 446, loss = 0.08959253\n",
      "Iteration 447, loss = 0.08945188\n",
      "Iteration 448, loss = 0.08931196\n",
      "Iteration 449, loss = 0.08917272\n",
      "Iteration 450, loss = 0.08903442\n",
      "Iteration 451, loss = 0.08889675\n",
      "Iteration 452, loss = 0.08875973\n",
      "Iteration 453, loss = 0.08862341\n",
      "Iteration 454, loss = 0.08848777\n",
      "Iteration 455, loss = 0.08835275\n",
      "Iteration 456, loss = 0.08821831\n",
      "Iteration 457, loss = 0.08808405\n",
      "Iteration 458, loss = 0.08794952\n",
      "Iteration 459, loss = 0.08781667\n",
      "Iteration 460, loss = 0.08768459\n",
      "Iteration 461, loss = 0.08755381\n",
      "Iteration 462, loss = 0.08742369\n",
      "Iteration 463, loss = 0.08729428\n",
      "Iteration 464, loss = 0.08716550\n",
      "Iteration 465, loss = 0.08703735\n",
      "Iteration 466, loss = 0.08690986\n",
      "Iteration 467, loss = 0.08678293\n",
      "Iteration 468, loss = 0.08665645\n",
      "Iteration 469, loss = 0.08653075\n",
      "Iteration 470, loss = 0.08640568\n",
      "Iteration 471, loss = 0.08628131\n",
      "Iteration 472, loss = 0.08615766\n",
      "Iteration 473, loss = 0.08603458\n",
      "Iteration 474, loss = 0.08591213\n",
      "Iteration 475, loss = 0.08579028\n",
      "Iteration 476, loss = 0.08566903\n",
      "Iteration 477, loss = 0.08554834\n",
      "Iteration 478, loss = 0.08542824\n",
      "Iteration 479, loss = 0.08530875\n",
      "Iteration 480, loss = 0.08518975\n",
      "Iteration 481, loss = 0.08507138\n",
      "Iteration 482, loss = 0.08495367\n",
      "Iteration 483, loss = 0.08483651\n",
      "Iteration 484, loss = 0.08471988\n",
      "Iteration 485, loss = 0.08460389\n",
      "Iteration 486, loss = 0.08448838\n",
      "Iteration 487, loss = 0.08437343\n",
      "Iteration 488, loss = 0.08425906\n",
      "Iteration 489, loss = 0.08414521\n",
      "Iteration 490, loss = 0.08403187\n",
      "Iteration 491, loss = 0.08391911\n",
      "Iteration 492, loss = 0.08380690\n",
      "Iteration 493, loss = 0.08369518\n",
      "Iteration 494, loss = 0.08358400\n",
      "Iteration 495, loss = 0.08347337\n",
      "Iteration 496, loss = 0.08336322\n",
      "Iteration 497, loss = 0.08325362\n",
      "Iteration 498, loss = 0.08314461\n",
      "Iteration 499, loss = 0.08303609\n",
      "Iteration 500, loss = 0.08292807\n",
      "Iteration 501, loss = 0.08282055\n",
      "Iteration 502, loss = 0.08271353\n",
      "Iteration 503, loss = 0.08260699\n",
      "Iteration 504, loss = 0.08250093\n",
      "Iteration 505, loss = 0.08239544\n",
      "Iteration 506, loss = 0.08229033\n",
      "Iteration 507, loss = 0.08218575\n",
      "Iteration 508, loss = 0.08208162\n",
      "Iteration 509, loss = 0.08197802\n",
      "Iteration 510, loss = 0.08187483\n",
      "Iteration 511, loss = 0.08177213\n",
      "Iteration 512, loss = 0.08166992\n",
      "Iteration 513, loss = 0.08156813\n",
      "Iteration 514, loss = 0.08146680\n",
      "Iteration 515, loss = 0.08136594\n",
      "Iteration 516, loss = 0.08126555\n",
      "Iteration 517, loss = 0.08116556\n",
      "Iteration 518, loss = 0.08106606\n",
      "Iteration 519, loss = 0.08096701\n",
      "Iteration 520, loss = 0.08086837\n",
      "Iteration 521, loss = 0.08077019\n",
      "Iteration 522, loss = 0.08067244\n",
      "Iteration 523, loss = 0.08057517\n",
      "Iteration 524, loss = 0.08047826\n",
      "Iteration 525, loss = 0.08038182\n",
      "Iteration 526, loss = 0.08028581\n",
      "Iteration 527, loss = 0.08019020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.980000\n",
      "Training set loss: 0.080190\n",
      "training: constant with momentum\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.03169308\n",
      "Iteration 3, loss = 0.99435081\n",
      "Iteration 4, loss = 0.94494013\n",
      "Iteration 5, loss = 0.88462739\n",
      "Iteration 6, loss = 0.81528498\n",
      "Iteration 7, loss = 0.74424065\n",
      "Iteration 8, loss = 0.67212121\n",
      "Iteration 9, loss = 0.60257981\n",
      "Iteration 10, loss = 0.53971027\n",
      "Iteration 11, loss = 0.48582472\n",
      "Iteration 12, loss = 0.44093531\n",
      "Iteration 13, loss = 0.40358107\n",
      "Iteration 14, loss = 0.37206419\n",
      "Iteration 15, loss = 0.34491884\n",
      "Iteration 16, loss = 0.32091627\n",
      "Iteration 17, loss = 0.29924247\n",
      "Iteration 18, loss = 0.27936705\n",
      "Iteration 19, loss = 0.26102326\n",
      "Iteration 20, loss = 0.24403246\n",
      "Iteration 21, loss = 0.22821738\n",
      "Iteration 22, loss = 0.21355478\n",
      "Iteration 23, loss = 0.19999300\n",
      "Iteration 24, loss = 0.18752084\n",
      "Iteration 25, loss = 0.17615042\n",
      "Iteration 26, loss = 0.16583613\n",
      "Iteration 27, loss = 0.15649638\n",
      "Iteration 28, loss = 0.14808470\n",
      "Iteration 29, loss = 0.14050426\n",
      "Iteration 30, loss = 0.13367100\n",
      "Iteration 31, loss = 0.12751686\n",
      "Iteration 32, loss = 0.12195171\n",
      "Iteration 33, loss = 0.11690583\n",
      "Iteration 34, loss = 0.11233870\n",
      "Iteration 35, loss = 0.10820115\n",
      "Iteration 36, loss = 0.10443034\n",
      "Iteration 37, loss = 0.10098472\n",
      "Iteration 38, loss = 0.09783288\n",
      "Iteration 39, loss = 0.09494294\n",
      "Iteration 40, loss = 0.09228787\n",
      "Iteration 41, loss = 0.08984609\n",
      "Iteration 42, loss = 0.08759074\n",
      "Iteration 43, loss = 0.08550706\n",
      "Iteration 44, loss = 0.08358159\n",
      "Iteration 45, loss = 0.08179669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.08013567\n",
      "Iteration 47, loss = 0.07859092\n",
      "Iteration 48, loss = 0.07715418\n",
      "Iteration 49, loss = 0.07581545\n",
      "Iteration 50, loss = 0.07456405\n",
      "Iteration 51, loss = 0.07339779\n",
      "Iteration 52, loss = 0.07231014\n",
      "Iteration 53, loss = 0.07130028\n",
      "Iteration 54, loss = 0.07035466\n",
      "Iteration 55, loss = 0.06946935\n",
      "Iteration 56, loss = 0.06863895\n",
      "Iteration 57, loss = 0.06786085\n",
      "Iteration 58, loss = 0.06713102\n",
      "Iteration 59, loss = 0.06644423\n",
      "Iteration 60, loss = 0.06579700\n",
      "Iteration 61, loss = 0.06518699\n",
      "Iteration 62, loss = 0.06461110\n",
      "Iteration 63, loss = 0.06406631\n",
      "Iteration 64, loss = 0.06355071\n",
      "Iteration 65, loss = 0.06306216\n",
      "Iteration 66, loss = 0.06259826\n",
      "Iteration 67, loss = 0.06215701\n",
      "Iteration 68, loss = 0.06173693\n",
      "Iteration 69, loss = 0.06133645\n",
      "Iteration 70, loss = 0.06095394\n",
      "Iteration 71, loss = 0.06058803\n",
      "Iteration 72, loss = 0.06023775\n",
      "Iteration 73, loss = 0.05990224\n",
      "Iteration 74, loss = 0.05958032\n",
      "Iteration 75, loss = 0.05927106\n",
      "Iteration 76, loss = 0.05897355\n",
      "Iteration 77, loss = 0.05868691\n",
      "Iteration 78, loss = 0.05841045\n",
      "Iteration 79, loss = 0.05814396\n",
      "Iteration 80, loss = 0.05788597\n",
      "Iteration 81, loss = 0.05763605\n",
      "Iteration 82, loss = 0.05739448\n",
      "Iteration 83, loss = 0.05716140\n",
      "Iteration 84, loss = 0.05693526\n",
      "Iteration 85, loss = 0.05671564\n",
      "Iteration 86, loss = 0.05650210\n",
      "Iteration 87, loss = 0.05629428\n",
      "Iteration 88, loss = 0.05609186\n",
      "Iteration 89, loss = 0.05589457\n",
      "Iteration 90, loss = 0.05570214\n",
      "Iteration 91, loss = 0.05551433\n",
      "Iteration 92, loss = 0.05533088\n",
      "Iteration 93, loss = 0.05515161\n",
      "Iteration 94, loss = 0.05497630\n",
      "Iteration 95, loss = 0.05480476\n",
      "Iteration 96, loss = 0.05463684\n",
      "Iteration 97, loss = 0.05447237\n",
      "Iteration 98, loss = 0.05431121\n",
      "Iteration 99, loss = 0.05415322\n",
      "Iteration 100, loss = 0.05399826\n",
      "Iteration 101, loss = 0.05384622\n",
      "Iteration 102, loss = 0.05369698\n",
      "Iteration 103, loss = 0.05355045\n",
      "Iteration 104, loss = 0.05340651\n",
      "Iteration 105, loss = 0.05326508\n",
      "Iteration 106, loss = 0.05312607\n",
      "Iteration 107, loss = 0.05298939\n",
      "Iteration 108, loss = 0.05285498\n",
      "Iteration 109, loss = 0.05272275\n",
      "Iteration 110, loss = 0.05259265\n",
      "Iteration 111, loss = 0.05246460\n",
      "Iteration 112, loss = 0.05233854\n",
      "Iteration 113, loss = 0.05221442\n",
      "Iteration 114, loss = 0.05209218\n",
      "Iteration 115, loss = 0.05197177\n",
      "Iteration 116, loss = 0.05185314\n",
      "Iteration 117, loss = 0.05173624\n",
      "Iteration 118, loss = 0.05162103\n",
      "Iteration 119, loss = 0.05150747\n",
      "Iteration 120, loss = 0.05139551\n",
      "Iteration 121, loss = 0.05128514\n",
      "Iteration 122, loss = 0.05117629\n",
      "Iteration 123, loss = 0.05106893\n",
      "Iteration 124, loss = 0.05096303\n",
      "Iteration 125, loss = 0.05085856\n",
      "Iteration 126, loss = 0.05075548\n",
      "Iteration 127, loss = 0.05065377\n",
      "Iteration 128, loss = 0.05055340\n",
      "Iteration 129, loss = 0.05045434\n",
      "Iteration 130, loss = 0.05035656\n",
      "Iteration 131, loss = 0.05026004\n",
      "Iteration 132, loss = 0.05016476\n",
      "Iteration 133, loss = 0.05007069\n",
      "Iteration 134, loss = 0.04997781\n",
      "Iteration 135, loss = 0.04988609\n",
      "Iteration 136, loss = 0.04979551\n",
      "Iteration 137, loss = 0.04970606\n",
      "Iteration 138, loss = 0.04961770\n",
      "Iteration 139, loss = 0.04953043\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.980000\n",
      "Training set loss: 0.049530\n",
      "training: constant with Nesterov's momentum\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.01343901\n",
      "Iteration 3, loss = 0.96292777\n",
      "Iteration 4, loss = 0.90034172\n",
      "Iteration 5, loss = 0.82770885\n",
      "Iteration 6, loss = 0.75319809\n",
      "Iteration 7, loss = 0.67880650\n",
      "Iteration 8, loss = 0.60866879\n",
      "Iteration 9, loss = 0.54558441\n",
      "Iteration 10, loss = 0.49132855\n",
      "Iteration 11, loss = 0.44611795\n",
      "Iteration 12, loss = 0.40852952\n",
      "Iteration 13, loss = 0.37685448\n",
      "Iteration 14, loss = 0.34946349\n",
      "Iteration 15, loss = 0.32520662\n",
      "Iteration 16, loss = 0.30324227\n",
      "Iteration 17, loss = 0.28312156\n",
      "Iteration 18, loss = 0.26458018\n",
      "Iteration 19, loss = 0.24745431\n",
      "Iteration 20, loss = 0.23155474\n",
      "Iteration 21, loss = 0.21681679\n",
      "Iteration 22, loss = 0.20323218\n",
      "Iteration 23, loss = 0.19078329\n",
      "Iteration 24, loss = 0.17944913\n",
      "Iteration 25, loss = 0.16917614\n",
      "Iteration 26, loss = 0.15988408\n",
      "Iteration 27, loss = 0.15149708\n",
      "Iteration 28, loss = 0.14391904\n",
      "Iteration 29, loss = 0.13705833\n",
      "Iteration 30, loss = 0.13085219\n",
      "Iteration 31, loss = 0.12524471\n",
      "Iteration 32, loss = 0.12015843\n",
      "Iteration 33, loss = 0.11553435\n",
      "Iteration 34, loss = 0.11132201\n",
      "Iteration 35, loss = 0.10747696\n",
      "Iteration 36, loss = 0.10396295\n",
      "Iteration 37, loss = 0.10074254\n",
      "Iteration 38, loss = 0.09778705\n",
      "Iteration 39, loss = 0.09506460\n",
      "Iteration 40, loss = 0.09255349\n",
      "Iteration 41, loss = 0.09023828\n",
      "Iteration 42, loss = 0.08809592\n",
      "Iteration 43, loss = 0.08611202\n",
      "Iteration 44, loss = 0.08426877\n",
      "Iteration 45, loss = 0.08255908\n",
      "Iteration 46, loss = 0.08096838\n",
      "Iteration 47, loss = 0.07949226\n",
      "Iteration 48, loss = 0.07811632\n",
      "Iteration 49, loss = 0.07683216\n",
      "Iteration 50, loss = 0.07563191\n",
      "Iteration 51, loss = 0.07450868\n",
      "Iteration 52, loss = 0.07345610\n",
      "Iteration 53, loss = 0.07246868\n",
      "Iteration 54, loss = 0.07154140\n",
      "Iteration 55, loss = 0.07066966\n",
      "Iteration 56, loss = 0.06984983\n",
      "Iteration 57, loss = 0.06907750\n",
      "Iteration 58, loss = 0.06834920\n",
      "Iteration 59, loss = 0.06766142\n",
      "Iteration 60, loss = 0.06701121\n",
      "Iteration 61, loss = 0.06639615\n",
      "Iteration 62, loss = 0.06581333\n",
      "Iteration 63, loss = 0.06526177\n",
      "Iteration 64, loss = 0.06473750\n",
      "Iteration 65, loss = 0.06423812\n",
      "Iteration 66, loss = 0.06376187\n",
      "Iteration 67, loss = 0.06330760\n",
      "Iteration 68, loss = 0.06287350\n",
      "Iteration 69, loss = 0.06245982\n",
      "Iteration 70, loss = 0.06206390\n",
      "Iteration 71, loss = 0.06168426\n",
      "Iteration 72, loss = 0.06131982\n",
      "Iteration 73, loss = 0.06096966\n",
      "Iteration 74, loss = 0.06063303\n",
      "Iteration 75, loss = 0.06030887\n",
      "Iteration 76, loss = 0.05999631\n",
      "Iteration 77, loss = 0.05969435\n",
      "Iteration 78, loss = 0.05940262\n",
      "Iteration 79, loss = 0.05912038\n",
      "Iteration 80, loss = 0.05884711\n",
      "Iteration 81, loss = 0.05858237\n",
      "Iteration 82, loss = 0.05832559\n",
      "Iteration 83, loss = 0.05807635\n",
      "Iteration 84, loss = 0.05783413\n",
      "Iteration 85, loss = 0.05759856\n",
      "Iteration 86, loss = 0.05736946\n",
      "Iteration 87, loss = 0.05714620\n",
      "Iteration 88, loss = 0.05692874\n",
      "Iteration 89, loss = 0.05671668\n",
      "Iteration 90, loss = 0.05650975\n",
      "Iteration 91, loss = 0.05630770\n",
      "Iteration 92, loss = 0.05611037\n",
      "Iteration 93, loss = 0.05591741\n",
      "Iteration 94, loss = 0.05572869\n",
      "Iteration 95, loss = 0.05554404\n",
      "Iteration 96, loss = 0.05536329\n",
      "Iteration 97, loss = 0.05518622\n",
      "Iteration 98, loss = 0.05501271\n",
      "Iteration 99, loss = 0.05484262\n",
      "Iteration 100, loss = 0.05467582\n",
      "Iteration 101, loss = 0.05451225\n",
      "Iteration 102, loss = 0.05435166\n",
      "Iteration 103, loss = 0.05419406\n",
      "Iteration 104, loss = 0.05403929\n",
      "Iteration 105, loss = 0.05388725\n",
      "Iteration 106, loss = 0.05373787\n",
      "Iteration 107, loss = 0.05359105\n",
      "Iteration 108, loss = 0.05344670\n",
      "Iteration 109, loss = 0.05330476\n",
      "Iteration 110, loss = 0.05316515\n",
      "Iteration 111, loss = 0.05302784\n",
      "Iteration 112, loss = 0.05289271\n",
      "Iteration 113, loss = 0.05275972\n",
      "Iteration 114, loss = 0.05262882\n",
      "Iteration 115, loss = 0.05249992\n",
      "Iteration 116, loss = 0.05237299\n",
      "Iteration 117, loss = 0.05224797\n",
      "Iteration 118, loss = 0.05212481\n",
      "Iteration 119, loss = 0.05200346\n",
      "Iteration 120, loss = 0.05188389\n",
      "Iteration 121, loss = 0.05176605\n",
      "Iteration 122, loss = 0.05164989\n",
      "Iteration 123, loss = 0.05153539\n",
      "Iteration 124, loss = 0.05142249\n",
      "Iteration 125, loss = 0.05131117\n",
      "Iteration 126, loss = 0.05120139\n",
      "Iteration 127, loss = 0.05109311\n",
      "Iteration 128, loss = 0.05098630\n",
      "Iteration 129, loss = 0.05088094\n",
      "Iteration 130, loss = 0.05077698\n",
      "Iteration 131, loss = 0.05067442\n",
      "Iteration 132, loss = 0.05057321\n",
      "Iteration 133, loss = 0.05047333\n",
      "Iteration 134, loss = 0.05037475\n",
      "Iteration 135, loss = 0.05027744\n",
      "Iteration 136, loss = 0.05018138\n",
      "Iteration 137, loss = 0.05008654\n",
      "Iteration 138, loss = 0.04999292\n",
      "Iteration 139, loss = 0.04990057\n",
      "Iteration 140, loss = 0.04980922\n",
      "Iteration 141, loss = 0.04971908\n",
      "Iteration 142, loss = 0.04963004\n",
      "Iteration 143, loss = 0.04954217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.980000\n",
      "Training set loss: 0.049542\n",
      "training: inv-scaling learning-rate\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.03169308\n",
      "Iteration 3, loss = 1.03003298\n",
      "Iteration 4, loss = 1.02886267\n",
      "Iteration 5, loss = 1.02790848\n",
      "Iteration 6, loss = 1.02708422\n",
      "Iteration 7, loss = 1.02634996\n",
      "Iteration 8, loss = 1.02568121\n",
      "Iteration 9, loss = 1.02506363\n",
      "Iteration 10, loss = 1.02448595\n",
      "Iteration 11, loss = 1.02394271\n",
      "Iteration 12, loss = 1.02342786\n",
      "Iteration 13, loss = 1.02293781\n",
      "Iteration 14, loss = 1.02246867\n",
      "Iteration 15, loss = 1.02201841\n",
      "Iteration 16, loss = 1.02158494\n",
      "Iteration 17, loss = 1.02116645\n",
      "Iteration 18, loss = 1.02076150\n",
      "Iteration 19, loss = 1.02036955\n",
      "Iteration 20, loss = 1.01998922\n",
      "Iteration 21, loss = 1.01961960\n",
      "Iteration 22, loss = 1.01925962\n",
      "Iteration 23, loss = 1.01890874\n",
      "Iteration 24, loss = 1.01856613\n",
      "Iteration 25, loss = 1.01823081\n",
      "Iteration 26, loss = 1.01790284\n",
      "Iteration 27, loss = 1.01758143\n",
      "Iteration 28, loss = 1.01726596\n",
      "Iteration 29, loss = 1.01695630\n",
      "Iteration 30, loss = 1.01665182\n",
      "Iteration 31, loss = 1.01635298\n",
      "Iteration 32, loss = 1.01605903\n",
      "Iteration 33, loss = 1.01576941\n",
      "Iteration 34, loss = 1.01548430\n",
      "Iteration 35, loss = 1.01520374\n",
      "Iteration 36, loss = 1.01492745\n",
      "Iteration 37, loss = 1.01465519\n",
      "Iteration 38, loss = 1.01438632\n",
      "Iteration 39, loss = 1.01412128\n",
      "Iteration 40, loss = 1.01385985\n",
      "Iteration 41, loss = 1.01360180\n",
      "Iteration 42, loss = 1.01334713\n",
      "Iteration 43, loss = 1.01309567\n",
      "Iteration 44, loss = 1.01284721\n",
      "Iteration 45, loss = 1.01260126\n",
      "Iteration 46, loss = 1.01235834\n",
      "Iteration 47, loss = 1.01211850\n",
      "Iteration 48, loss = 1.01188142\n",
      "Iteration 49, loss = 1.01164705\n",
      "Iteration 50, loss = 1.01141530\n",
      "Iteration 51, loss = 1.01118612\n",
      "Iteration 52, loss = 1.01095932\n",
      "Iteration 53, loss = 1.01073480\n",
      "Iteration 54, loss = 1.01051258\n",
      "Iteration 55, loss = 1.01029259\n",
      "Iteration 56, loss = 1.01007472\n",
      "Iteration 57, loss = 1.00985851\n",
      "Iteration 58, loss = 1.00964454\n",
      "Iteration 59, loss = 1.00943252\n",
      "Iteration 60, loss = 1.00922238\n",
      "Iteration 61, loss = 1.00901423\n",
      "Iteration 62, loss = 1.00880789\n",
      "Iteration 63, loss = 1.00860281\n",
      "Iteration 64, loss = 1.00839938\n",
      "Iteration 65, loss = 1.00819765\n",
      "Iteration 66, loss = 1.00799747\n",
      "Iteration 67, loss = 1.00779887\n",
      "Iteration 68, loss = 1.00760175\n",
      "Iteration 69, loss = 1.00740509\n",
      "Iteration 70, loss = 1.00720901\n",
      "Iteration 71, loss = 1.00701448\n",
      "Iteration 72, loss = 1.00682149\n",
      "Iteration 73, loss = 1.00662972\n",
      "Iteration 74, loss = 1.00643893\n",
      "Iteration 75, loss = 1.00624956\n",
      "Iteration 76, loss = 1.00606154\n",
      "Iteration 77, loss = 1.00587498\n",
      "Iteration 78, loss = 1.00568980\n",
      "Iteration 79, loss = 1.00550588\n",
      "Iteration 80, loss = 1.00532321\n",
      "Iteration 81, loss = 1.00514174\n",
      "Iteration 82, loss = 1.00496141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 83, loss = 1.00478223\n",
      "Iteration 84, loss = 1.00460420\n",
      "Iteration 85, loss = 1.00442728\n",
      "Iteration 86, loss = 1.00425149\n",
      "Iteration 87, loss = 1.00407688\n",
      "Iteration 88, loss = 1.00390337\n",
      "Iteration 89, loss = 1.00373100\n",
      "Iteration 90, loss = 1.00355971\n",
      "Iteration 91, loss = 1.00338954\n",
      "Iteration 92, loss = 1.00322055\n",
      "Iteration 93, loss = 1.00305260\n",
      "Iteration 94, loss = 1.00288560\n",
      "Iteration 95, loss = 1.00271955\n",
      "Iteration 96, loss = 1.00255442\n",
      "Iteration 97, loss = 1.00239021\n",
      "Iteration 98, loss = 1.00222688\n",
      "Iteration 99, loss = 1.00206438\n",
      "Iteration 100, loss = 1.00190268\n",
      "Iteration 101, loss = 1.00174183\n",
      "Iteration 102, loss = 1.00158182\n",
      "Iteration 103, loss = 1.00142270\n",
      "Iteration 104, loss = 1.00126445\n",
      "Iteration 105, loss = 1.00110715\n",
      "Iteration 106, loss = 1.00095064\n",
      "Iteration 107, loss = 1.00079500\n",
      "Iteration 108, loss = 1.00064012\n",
      "Iteration 109, loss = 1.00048599\n",
      "Iteration 110, loss = 1.00033261\n",
      "Iteration 111, loss = 1.00017998\n",
      "Iteration 112, loss = 1.00002808\n",
      "Iteration 113, loss = 0.99987690\n",
      "Iteration 114, loss = 0.99972626\n",
      "Iteration 115, loss = 0.99957604\n",
      "Iteration 116, loss = 0.99942649\n",
      "Iteration 117, loss = 0.99927761\n",
      "Iteration 118, loss = 0.99912934\n",
      "Iteration 119, loss = 0.99898173\n",
      "Iteration 120, loss = 0.99883469\n",
      "Iteration 121, loss = 0.99868801\n",
      "Iteration 122, loss = 0.99854196\n",
      "Iteration 123, loss = 0.99839654\n",
      "Iteration 124, loss = 0.99825174\n",
      "Iteration 125, loss = 0.99810755\n",
      "Iteration 126, loss = 0.99796398\n",
      "Iteration 127, loss = 0.99782103\n",
      "Iteration 128, loss = 0.99767866\n",
      "Iteration 129, loss = 0.99753688\n",
      "Iteration 130, loss = 0.99739568\n",
      "Iteration 131, loss = 0.99725504\n",
      "Iteration 132, loss = 0.99711496\n",
      "Iteration 133, loss = 0.99697537\n",
      "Iteration 134, loss = 0.99683636\n",
      "Iteration 135, loss = 0.99669789\n",
      "Iteration 136, loss = 0.99655996\n",
      "Iteration 137, loss = 0.99642255\n",
      "Iteration 138, loss = 0.99628557\n",
      "Iteration 139, loss = 0.99614880\n",
      "Iteration 140, loss = 0.99601216\n",
      "Iteration 141, loss = 0.99587566\n",
      "Iteration 142, loss = 0.99573949\n",
      "Iteration 143, loss = 0.99560355\n",
      "Iteration 144, loss = 0.99546803\n",
      "Iteration 145, loss = 0.99533299\n",
      "Iteration 146, loss = 0.99519845\n",
      "Iteration 147, loss = 0.99506418\n",
      "Iteration 148, loss = 0.99493015\n",
      "Iteration 149, loss = 0.99479654\n",
      "Iteration 150, loss = 0.99466332\n",
      "Iteration 151, loss = 0.99453048\n",
      "Iteration 152, loss = 0.99439810\n",
      "Iteration 153, loss = 0.99426618\n",
      "Iteration 154, loss = 0.99413470\n",
      "Iteration 155, loss = 0.99400366\n",
      "Iteration 156, loss = 0.99387307\n",
      "Iteration 157, loss = 0.99374286\n",
      "Iteration 158, loss = 0.99361269\n",
      "Iteration 159, loss = 0.99348276\n",
      "Iteration 160, loss = 0.99335303\n",
      "Iteration 161, loss = 0.99322372\n",
      "Iteration 162, loss = 0.99309476\n",
      "Iteration 163, loss = 0.99296626\n",
      "Iteration 164, loss = 0.99283804\n",
      "Iteration 165, loss = 0.99270992\n",
      "Iteration 166, loss = 0.99258221\n",
      "Iteration 167, loss = 0.99245489\n",
      "Iteration 168, loss = 0.99232798\n",
      "Iteration 169, loss = 0.99220147\n",
      "Iteration 170, loss = 0.99207534\n",
      "Iteration 171, loss = 0.99194961\n",
      "Iteration 172, loss = 0.99182428\n",
      "Iteration 173, loss = 0.99169933\n",
      "Iteration 174, loss = 0.99157476\n",
      "Iteration 175, loss = 0.99145059\n",
      "Iteration 176, loss = 0.99132678\n",
      "Iteration 177, loss = 0.99120334\n",
      "Iteration 178, loss = 0.99108027\n",
      "Iteration 179, loss = 0.99095754\n",
      "Iteration 180, loss = 0.99083477\n",
      "Iteration 181, loss = 0.99071236\n",
      "Iteration 182, loss = 0.99059031\n",
      "Iteration 183, loss = 0.99046860\n",
      "Iteration 184, loss = 0.99034727\n",
      "Iteration 185, loss = 0.99022642\n",
      "Iteration 186, loss = 0.99010596\n",
      "Iteration 187, loss = 0.98998581\n",
      "Iteration 188, loss = 0.98986596\n",
      "Iteration 189, loss = 0.98974638\n",
      "Iteration 190, loss = 0.98962714\n",
      "Iteration 191, loss = 0.98950822\n",
      "Iteration 192, loss = 0.98938950\n",
      "Iteration 193, loss = 0.98927081\n",
      "Iteration 194, loss = 0.98915243\n",
      "Iteration 195, loss = 0.98903438\n",
      "Iteration 196, loss = 0.98891663\n",
      "Iteration 197, loss = 0.98879880\n",
      "Iteration 198, loss = 0.98868127\n",
      "Iteration 199, loss = 0.98856406\n",
      "Iteration 200, loss = 0.98844715\n",
      "Iteration 201, loss = 0.98833055\n",
      "Iteration 202, loss = 0.98821426\n",
      "Iteration 203, loss = 0.98809826\n",
      "Iteration 204, loss = 0.98798257\n",
      "Iteration 205, loss = 0.98786717\n",
      "Iteration 206, loss = 0.98775208\n",
      "Iteration 207, loss = 0.98763729\n",
      "Iteration 208, loss = 0.98752280\n",
      "Iteration 209, loss = 0.98740859\n",
      "Iteration 210, loss = 0.98729466\n",
      "Iteration 211, loss = 0.98718101\n",
      "Iteration 212, loss = 0.98706759\n",
      "Iteration 213, loss = 0.98695444\n",
      "Iteration 214, loss = 0.98684158\n",
      "Iteration 215, loss = 0.98672903\n",
      "Iteration 216, loss = 0.98661683\n",
      "Iteration 217, loss = 0.98650497\n",
      "Iteration 218, loss = 0.98639342\n",
      "Iteration 219, loss = 0.98628214\n",
      "Iteration 220, loss = 0.98617113\n",
      "Iteration 221, loss = 0.98606038\n",
      "Iteration 222, loss = 0.98594989\n",
      "Iteration 223, loss = 0.98583967\n",
      "Iteration 224, loss = 0.98572973\n",
      "Iteration 225, loss = 0.98562006\n",
      "Iteration 226, loss = 0.98551065\n",
      "Iteration 227, loss = 0.98540149\n",
      "Iteration 228, loss = 0.98529259\n",
      "Iteration 229, loss = 0.98518393\n",
      "Iteration 230, loss = 0.98507553\n",
      "Iteration 231, loss = 0.98496734\n",
      "Iteration 232, loss = 0.98485898\n",
      "Iteration 233, loss = 0.98475082\n",
      "Iteration 234, loss = 0.98464264\n",
      "Iteration 235, loss = 0.98453447\n",
      "Iteration 236, loss = 0.98442656\n",
      "Iteration 237, loss = 0.98431888\n",
      "Iteration 238, loss = 0.98421144\n",
      "Iteration 239, loss = 0.98410424\n",
      "Iteration 240, loss = 0.98399727\n",
      "Iteration 241, loss = 0.98389057\n",
      "Iteration 242, loss = 0.98378414\n",
      "Iteration 243, loss = 0.98367795\n",
      "Iteration 244, loss = 0.98357200\n",
      "Iteration 245, loss = 0.98346627\n",
      "Iteration 246, loss = 0.98336073\n",
      "Iteration 247, loss = 0.98325537\n",
      "Iteration 248, loss = 0.98315025\n",
      "Iteration 249, loss = 0.98304536\n",
      "Iteration 250, loss = 0.98294070\n",
      "Iteration 251, loss = 0.98283628\n",
      "Iteration 252, loss = 0.98273211\n",
      "Iteration 253, loss = 0.98262816\n",
      "Iteration 254, loss = 0.98252441\n",
      "Iteration 255, loss = 0.98242089\n",
      "Iteration 256, loss = 0.98231758\n",
      "Iteration 257, loss = 0.98221446\n",
      "Iteration 258, loss = 0.98211119\n",
      "Iteration 259, loss = 0.98200812\n",
      "Iteration 260, loss = 0.98190528\n",
      "Iteration 261, loss = 0.98180270\n",
      "Iteration 262, loss = 0.98170032\n",
      "Iteration 263, loss = 0.98159814\n",
      "Iteration 264, loss = 0.98149613\n",
      "Iteration 265, loss = 0.98139388\n",
      "Iteration 266, loss = 0.98129184\n",
      "Iteration 267, loss = 0.98118999\n",
      "Iteration 268, loss = 0.98108834\n",
      "Iteration 269, loss = 0.98098690\n",
      "Iteration 270, loss = 0.98088565\n",
      "Iteration 271, loss = 0.98078462\n",
      "Iteration 272, loss = 0.98068378\n",
      "Iteration 273, loss = 0.98058314\n",
      "Iteration 274, loss = 0.98048269\n",
      "Iteration 275, loss = 0.98038243\n",
      "Iteration 276, loss = 0.98028236\n",
      "Iteration 277, loss = 0.98018246\n",
      "Iteration 278, loss = 0.98008269\n",
      "Iteration 279, loss = 0.97998311\n",
      "Iteration 280, loss = 0.97988372\n",
      "Iteration 281, loss = 0.97978452\n",
      "Iteration 282, loss = 0.97968542\n",
      "Iteration 283, loss = 0.97958651\n",
      "Iteration 284, loss = 0.97948778\n",
      "Iteration 285, loss = 0.97938923\n",
      "Iteration 286, loss = 0.97929086\n",
      "Iteration 287, loss = 0.97919267\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.360000\n",
      "Training set loss: 0.979193\n",
      "training: inv-scaling with momentum\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.01343901\n",
      "Iteration 3, loss = 0.99510827\n",
      "Iteration 4, loss = 0.97865956\n",
      "Iteration 5, loss = 0.96370225\n",
      "Iteration 6, loss = 0.94996885\n",
      "Iteration 7, loss = 0.93732218\n",
      "Iteration 8, loss = 0.92563050\n",
      "Iteration 9, loss = 0.91476074\n",
      "Iteration 10, loss = 0.90459802\n",
      "Iteration 11, loss = 0.89504324\n",
      "Iteration 12, loss = 0.88601052\n",
      "Iteration 13, loss = 0.87744420\n",
      "Iteration 14, loss = 0.86930660\n",
      "Iteration 15, loss = 0.86156642\n",
      "Iteration 16, loss = 0.85421123\n",
      "Iteration 17, loss = 0.84720967\n",
      "Iteration 18, loss = 0.84055041\n",
      "Iteration 19, loss = 0.83422639\n",
      "Iteration 20, loss = 0.82820965\n",
      "Iteration 21, loss = 0.82249651\n",
      "Iteration 22, loss = 0.81706306\n",
      "Iteration 23, loss = 0.81188604\n",
      "Iteration 24, loss = 0.80694723\n",
      "Iteration 25, loss = 0.80223587\n",
      "Iteration 26, loss = 0.79773572\n",
      "Iteration 27, loss = 0.79344051\n",
      "Iteration 28, loss = 0.78933308\n",
      "Iteration 29, loss = 0.78539575\n",
      "Iteration 30, loss = 0.78162415\n",
      "Iteration 31, loss = 0.77799554\n",
      "Iteration 32, loss = 0.77450067\n",
      "Iteration 33, loss = 0.77113012\n",
      "Iteration 34, loss = 0.76787916\n",
      "Iteration 35, loss = 0.76473666\n",
      "Iteration 36, loss = 0.76169678\n",
      "Iteration 37, loss = 0.75875505\n",
      "Iteration 38, loss = 0.75590633\n",
      "Iteration 39, loss = 0.75314718\n",
      "Iteration 40, loss = 0.75046858\n",
      "Iteration 41, loss = 0.74786475\n",
      "Iteration 42, loss = 0.74533237\n",
      "Iteration 43, loss = 0.74286718\n",
      "Iteration 44, loss = 0.74046536\n",
      "Iteration 45, loss = 0.73812320\n",
      "Iteration 46, loss = 0.73583702\n",
      "Iteration 47, loss = 0.73360538\n",
      "Iteration 48, loss = 0.73142640\n",
      "Iteration 49, loss = 0.72929621\n",
      "Iteration 50, loss = 0.72721188\n",
      "Iteration 51, loss = 0.72517144\n",
      "Iteration 52, loss = 0.72317296\n",
      "Iteration 53, loss = 0.72121523\n",
      "Iteration 54, loss = 0.71929656\n",
      "Iteration 55, loss = 0.71741375\n",
      "Iteration 56, loss = 0.71556513\n",
      "Iteration 57, loss = 0.71374903\n",
      "Iteration 58, loss = 0.71196423\n",
      "Iteration 59, loss = 0.71020955\n",
      "Iteration 60, loss = 0.70848361\n",
      "Iteration 61, loss = 0.70678608\n",
      "Iteration 62, loss = 0.70511540\n",
      "Iteration 63, loss = 0.70347048\n",
      "Iteration 64, loss = 0.70185115\n",
      "Iteration 65, loss = 0.70025579\n",
      "Iteration 66, loss = 0.69868361\n",
      "Iteration 67, loss = 0.69713368\n",
      "Iteration 68, loss = 0.69560429\n",
      "Iteration 69, loss = 0.69409606\n",
      "Iteration 70, loss = 0.69260764\n",
      "Iteration 71, loss = 0.69113825\n",
      "Iteration 72, loss = 0.68968763\n",
      "Iteration 73, loss = 0.68825536\n",
      "Iteration 74, loss = 0.68684079\n",
      "Iteration 75, loss = 0.68544370\n",
      "Iteration 76, loss = 0.68406321\n",
      "Iteration 77, loss = 0.68269925\n",
      "Iteration 78, loss = 0.68135137\n",
      "Iteration 79, loss = 0.68001884\n",
      "Iteration 80, loss = 0.67870155\n",
      "Iteration 81, loss = 0.67739912\n",
      "Iteration 82, loss = 0.67611139\n",
      "Iteration 83, loss = 0.67483747\n",
      "Iteration 84, loss = 0.67357750\n",
      "Iteration 85, loss = 0.67233137\n",
      "Iteration 86, loss = 0.67109819\n",
      "Iteration 87, loss = 0.66987861\n",
      "Iteration 88, loss = 0.66867230\n",
      "Iteration 89, loss = 0.66747885\n",
      "Iteration 90, loss = 0.66629782\n",
      "Iteration 91, loss = 0.66512886\n",
      "Iteration 92, loss = 0.66397185\n",
      "Iteration 93, loss = 0.66282660\n",
      "Iteration 94, loss = 0.66169248\n",
      "Iteration 95, loss = 0.66056890\n",
      "Iteration 96, loss = 0.65945640\n",
      "Iteration 97, loss = 0.65835442\n",
      "Iteration 98, loss = 0.65726370\n",
      "Iteration 99, loss = 0.65618325\n",
      "Iteration 100, loss = 0.65511264\n",
      "Iteration 101, loss = 0.65405186\n",
      "Iteration 102, loss = 0.65300058\n",
      "Iteration 103, loss = 0.65195851\n",
      "Iteration 104, loss = 0.65092571\n",
      "Iteration 105, loss = 0.64990186\n",
      "Iteration 106, loss = 0.64888711\n",
      "Iteration 107, loss = 0.64788133\n",
      "Iteration 108, loss = 0.64688390\n",
      "Iteration 109, loss = 0.64589468\n",
      "Iteration 110, loss = 0.64491358\n",
      "Iteration 111, loss = 0.64394064\n",
      "Iteration 112, loss = 0.64297590\n",
      "Iteration 113, loss = 0.64201896\n",
      "Iteration 114, loss = 0.64106998\n",
      "Iteration 115, loss = 0.64012902\n",
      "Iteration 116, loss = 0.63919597\n",
      "Iteration 117, loss = 0.63827066\n",
      "Iteration 118, loss = 0.63735274\n",
      "Iteration 119, loss = 0.63644208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 120, loss = 0.63553853\n",
      "Iteration 121, loss = 0.63464187\n",
      "Iteration 122, loss = 0.63375213\n",
      "Iteration 123, loss = 0.63286909\n",
      "Iteration 124, loss = 0.63199286\n",
      "Iteration 125, loss = 0.63112319\n",
      "Iteration 126, loss = 0.63025992\n",
      "Iteration 127, loss = 0.62940291\n",
      "Iteration 128, loss = 0.62855206\n",
      "Iteration 129, loss = 0.62770727\n",
      "Iteration 130, loss = 0.62686853\n",
      "Iteration 131, loss = 0.62603582\n",
      "Iteration 132, loss = 0.62520897\n",
      "Iteration 133, loss = 0.62438785\n",
      "Iteration 134, loss = 0.62357242\n",
      "Iteration 135, loss = 0.62276264\n",
      "Iteration 136, loss = 0.62195843\n",
      "Iteration 137, loss = 0.62115962\n",
      "Iteration 138, loss = 0.62036620\n",
      "Iteration 139, loss = 0.61957823\n",
      "Iteration 140, loss = 0.61879550\n",
      "Iteration 141, loss = 0.61801806\n",
      "Iteration 142, loss = 0.61724582\n",
      "Iteration 143, loss = 0.61647861\n",
      "Iteration 144, loss = 0.61571644\n",
      "Iteration 145, loss = 0.61495929\n",
      "Iteration 146, loss = 0.61420703\n",
      "Iteration 147, loss = 0.61345960\n",
      "Iteration 148, loss = 0.61271691\n",
      "Iteration 149, loss = 0.61197889\n",
      "Iteration 150, loss = 0.61124553\n",
      "Iteration 151, loss = 0.61051678\n",
      "Iteration 152, loss = 0.60979254\n",
      "Iteration 153, loss = 0.60907284\n",
      "Iteration 154, loss = 0.60835763\n",
      "Iteration 155, loss = 0.60764683\n",
      "Iteration 156, loss = 0.60694039\n",
      "Iteration 157, loss = 0.60623825\n",
      "Iteration 158, loss = 0.60554034\n",
      "Iteration 159, loss = 0.60484664\n",
      "Iteration 160, loss = 0.60415709\n",
      "Iteration 161, loss = 0.60347163\n",
      "Iteration 162, loss = 0.60279024\n",
      "Iteration 163, loss = 0.60211290\n",
      "Iteration 164, loss = 0.60143957\n",
      "Iteration 165, loss = 0.60077018\n",
      "Iteration 166, loss = 0.60010473\n",
      "Iteration 167, loss = 0.59944317\n",
      "Iteration 168, loss = 0.59878540\n",
      "Iteration 169, loss = 0.59813139\n",
      "Iteration 170, loss = 0.59748114\n",
      "Iteration 171, loss = 0.59683459\n",
      "Iteration 172, loss = 0.59619169\n",
      "Iteration 173, loss = 0.59555242\n",
      "Iteration 174, loss = 0.59491672\n",
      "Iteration 175, loss = 0.59428454\n",
      "Iteration 176, loss = 0.59365584\n",
      "Iteration 177, loss = 0.59303060\n",
      "Iteration 178, loss = 0.59240879\n",
      "Iteration 179, loss = 0.59179039\n",
      "Iteration 180, loss = 0.59117541\n",
      "Iteration 181, loss = 0.59056375\n",
      "Iteration 182, loss = 0.58995537\n",
      "Iteration 183, loss = 0.58935024\n",
      "Iteration 184, loss = 0.58874834\n",
      "Iteration 185, loss = 0.58814966\n",
      "Iteration 186, loss = 0.58755416\n",
      "Iteration 187, loss = 0.58696179\n",
      "Iteration 188, loss = 0.58637257\n",
      "Iteration 189, loss = 0.58578647\n",
      "Iteration 190, loss = 0.58520342\n",
      "Iteration 191, loss = 0.58462341\n",
      "Iteration 192, loss = 0.58404642\n",
      "Iteration 193, loss = 0.58347247\n",
      "Iteration 194, loss = 0.58290148\n",
      "Iteration 195, loss = 0.58233349\n",
      "Iteration 196, loss = 0.58176848\n",
      "Iteration 197, loss = 0.58120637\n",
      "Iteration 198, loss = 0.58064713\n",
      "Iteration 199, loss = 0.58009074\n",
      "Iteration 200, loss = 0.57953715\n",
      "Iteration 201, loss = 0.57898632\n",
      "Iteration 202, loss = 0.57843825\n",
      "Iteration 203, loss = 0.57789289\n",
      "Iteration 204, loss = 0.57735022\n",
      "Iteration 205, loss = 0.57681021\n",
      "Iteration 206, loss = 0.57627288\n",
      "Iteration 207, loss = 0.57573818\n",
      "Iteration 208, loss = 0.57520607\n",
      "Iteration 209, loss = 0.57467651\n",
      "Iteration 210, loss = 0.57414950\n",
      "Iteration 211, loss = 0.57362500\n",
      "Iteration 212, loss = 0.57310302\n",
      "Iteration 213, loss = 0.57258353\n",
      "Iteration 214, loss = 0.57206645\n",
      "Iteration 215, loss = 0.57155178\n",
      "Iteration 216, loss = 0.57103955\n",
      "Iteration 217, loss = 0.57052972\n",
      "Iteration 218, loss = 0.57002224\n",
      "Iteration 219, loss = 0.56951709\n",
      "Iteration 220, loss = 0.56901423\n",
      "Iteration 221, loss = 0.56851367\n",
      "Iteration 222, loss = 0.56801541\n",
      "Iteration 223, loss = 0.56751942\n",
      "Iteration 224, loss = 0.56702569\n",
      "Iteration 225, loss = 0.56653422\n",
      "Iteration 226, loss = 0.56604496\n",
      "Iteration 227, loss = 0.56555791\n",
      "Iteration 228, loss = 0.56507305\n",
      "Iteration 229, loss = 0.56459037\n",
      "Iteration 230, loss = 0.56410983\n",
      "Iteration 231, loss = 0.56363143\n",
      "Iteration 232, loss = 0.56315514\n",
      "Iteration 233, loss = 0.56268098\n",
      "Iteration 234, loss = 0.56220890\n",
      "Iteration 235, loss = 0.56173892\n",
      "Iteration 236, loss = 0.56127098\n",
      "Iteration 237, loss = 0.56080507\n",
      "Iteration 238, loss = 0.56034117\n",
      "Iteration 239, loss = 0.55987928\n",
      "Iteration 240, loss = 0.55941942\n",
      "Iteration 241, loss = 0.55896153\n",
      "Iteration 242, loss = 0.55850556\n",
      "Iteration 243, loss = 0.55805152\n",
      "Iteration 244, loss = 0.55759941\n",
      "Iteration 245, loss = 0.55714921\n",
      "Iteration 246, loss = 0.55670092\n",
      "Iteration 247, loss = 0.55625447\n",
      "Iteration 248, loss = 0.55580989\n",
      "Iteration 249, loss = 0.55536715\n",
      "Iteration 250, loss = 0.55492624\n",
      "Iteration 251, loss = 0.55448715\n",
      "Iteration 252, loss = 0.55404984\n",
      "Iteration 253, loss = 0.55361433\n",
      "Iteration 254, loss = 0.55318065\n",
      "Iteration 255, loss = 0.55274872\n",
      "Iteration 256, loss = 0.55231854\n",
      "Iteration 257, loss = 0.55189017\n",
      "Iteration 258, loss = 0.55146354\n",
      "Iteration 259, loss = 0.55103864\n",
      "Iteration 260, loss = 0.55061546\n",
      "Iteration 261, loss = 0.55019398\n",
      "Iteration 262, loss = 0.54977421\n",
      "Iteration 263, loss = 0.54935613\n",
      "Iteration 264, loss = 0.54893970\n",
      "Iteration 265, loss = 0.54852489\n",
      "Iteration 266, loss = 0.54811169\n",
      "Iteration 267, loss = 0.54770011\n",
      "Iteration 268, loss = 0.54729015\n",
      "Iteration 269, loss = 0.54688178\n",
      "Iteration 270, loss = 0.54647499\n",
      "Iteration 271, loss = 0.54606979\n",
      "Iteration 272, loss = 0.54566614\n",
      "Iteration 273, loss = 0.54526407\n",
      "Iteration 274, loss = 0.54486355\n",
      "Iteration 275, loss = 0.54446459\n",
      "Iteration 276, loss = 0.54406716\n",
      "Iteration 277, loss = 0.54367125\n",
      "Iteration 278, loss = 0.54327686\n",
      "Iteration 279, loss = 0.54288397\n",
      "Iteration 280, loss = 0.54249257\n",
      "Iteration 281, loss = 0.54210263\n",
      "Iteration 282, loss = 0.54171414\n",
      "Iteration 283, loss = 0.54132710\n",
      "Iteration 284, loss = 0.54094146\n",
      "Iteration 285, loss = 0.54055726\n",
      "Iteration 286, loss = 0.54017447\n",
      "Iteration 287, loss = 0.53979312\n",
      "Iteration 288, loss = 0.53941323\n",
      "Iteration 289, loss = 0.53903475\n",
      "Iteration 290, loss = 0.53865766\n",
      "Iteration 291, loss = 0.53828197\n",
      "Iteration 292, loss = 0.53790770\n",
      "Iteration 293, loss = 0.53753479\n",
      "Iteration 294, loss = 0.53716326\n",
      "Iteration 295, loss = 0.53679309\n",
      "Iteration 296, loss = 0.53642427\n",
      "Iteration 297, loss = 0.53605678\n",
      "Iteration 298, loss = 0.53569064\n",
      "Iteration 299, loss = 0.53532582\n",
      "Iteration 300, loss = 0.53496233\n",
      "Iteration 301, loss = 0.53460013\n",
      "Iteration 302, loss = 0.53423922\n",
      "Iteration 303, loss = 0.53387957\n",
      "Iteration 304, loss = 0.53352118\n",
      "Iteration 305, loss = 0.53316404\n",
      "Iteration 306, loss = 0.53280814\n",
      "Iteration 307, loss = 0.53245345\n",
      "Iteration 308, loss = 0.53209999\n",
      "Iteration 309, loss = 0.53174775\n",
      "Iteration 310, loss = 0.53139672\n",
      "Iteration 311, loss = 0.53104690\n",
      "Iteration 312, loss = 0.53069833\n",
      "Iteration 313, loss = 0.53035095\n",
      "Iteration 314, loss = 0.53000477\n",
      "Iteration 315, loss = 0.52965978\n",
      "Iteration 316, loss = 0.52931592\n",
      "Iteration 317, loss = 0.52897321\n",
      "Iteration 318, loss = 0.52863166\n",
      "Iteration 319, loss = 0.52829126\n",
      "Iteration 320, loss = 0.52795203\n",
      "Iteration 321, loss = 0.52761397\n",
      "Iteration 322, loss = 0.52727700\n",
      "Iteration 323, loss = 0.52694116\n",
      "Iteration 324, loss = 0.52660644\n",
      "Iteration 325, loss = 0.52627285\n",
      "Iteration 326, loss = 0.52594036\n",
      "Iteration 327, loss = 0.52560897\n",
      "Iteration 328, loss = 0.52527868\n",
      "Iteration 329, loss = 0.52494950\n",
      "Iteration 330, loss = 0.52462140\n",
      "Iteration 331, loss = 0.52429437\n",
      "Iteration 332, loss = 0.52396841\n",
      "Iteration 333, loss = 0.52364351\n",
      "Iteration 334, loss = 0.52331967\n",
      "Iteration 335, loss = 0.52299689\n",
      "Iteration 336, loss = 0.52267516\n",
      "Iteration 337, loss = 0.52235446\n",
      "Iteration 338, loss = 0.52203481\n",
      "Iteration 339, loss = 0.52171617\n",
      "Iteration 340, loss = 0.52139855\n",
      "Iteration 341, loss = 0.52108188\n",
      "Iteration 342, loss = 0.52076621\n",
      "Iteration 343, loss = 0.52045153\n",
      "Iteration 344, loss = 0.52013787\n",
      "Iteration 345, loss = 0.51982520\n",
      "Iteration 346, loss = 0.51951356\n",
      "Iteration 347, loss = 0.51920290\n",
      "Iteration 348, loss = 0.51889321\n",
      "Iteration 349, loss = 0.51858451\n",
      "Iteration 350, loss = 0.51827678\n",
      "Iteration 351, loss = 0.51797001\n",
      "Iteration 352, loss = 0.51766420\n",
      "Iteration 353, loss = 0.51735926\n",
      "Iteration 354, loss = 0.51705526\n",
      "Iteration 355, loss = 0.51675218\n",
      "Iteration 356, loss = 0.51645003\n",
      "Iteration 357, loss = 0.51614880\n",
      "Iteration 358, loss = 0.51584849\n",
      "Iteration 359, loss = 0.51554911\n",
      "Iteration 360, loss = 0.51525075\n",
      "Iteration 361, loss = 0.51495331\n",
      "Iteration 362, loss = 0.51465678\n",
      "Iteration 363, loss = 0.51436116\n",
      "Iteration 364, loss = 0.51406643\n",
      "Iteration 365, loss = 0.51377258\n",
      "Iteration 366, loss = 0.51347962\n",
      "Iteration 367, loss = 0.51318754\n",
      "Iteration 368, loss = 0.51289633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 369, loss = 0.51260603\n",
      "Iteration 370, loss = 0.51231669\n",
      "Iteration 371, loss = 0.51202822\n",
      "Iteration 372, loss = 0.51174058\n",
      "Iteration 373, loss = 0.51145378\n",
      "Iteration 374, loss = 0.51116784\n",
      "Iteration 375, loss = 0.51088272\n",
      "Iteration 376, loss = 0.51059847\n",
      "Iteration 377, loss = 0.51031509\n",
      "Iteration 378, loss = 0.51003252\n",
      "Iteration 379, loss = 0.50975071\n",
      "Iteration 380, loss = 0.50946971\n",
      "Iteration 381, loss = 0.50918953\n",
      "Iteration 382, loss = 0.50891016\n",
      "Iteration 383, loss = 0.50863158\n",
      "Iteration 384, loss = 0.50835380\n",
      "Iteration 385, loss = 0.50807682\n",
      "Iteration 386, loss = 0.50780062\n",
      "Iteration 387, loss = 0.50752522\n",
      "Iteration 388, loss = 0.50725062\n",
      "Iteration 389, loss = 0.50697682\n",
      "Iteration 390, loss = 0.50670388\n",
      "Iteration 391, loss = 0.50643171\n",
      "Iteration 392, loss = 0.50616034\n",
      "Iteration 393, loss = 0.50588976\n",
      "Iteration 394, loss = 0.50561994\n",
      "Iteration 395, loss = 0.50535088\n",
      "Iteration 396, loss = 0.50508257\n",
      "Iteration 397, loss = 0.50481501\n",
      "Iteration 398, loss = 0.50454820\n",
      "Iteration 399, loss = 0.50428212\n",
      "Iteration 400, loss = 0.50401678\n",
      "Iteration 401, loss = 0.50375217\n",
      "Iteration 402, loss = 0.50348832\n",
      "Iteration 403, loss = 0.50322519\n",
      "Iteration 404, loss = 0.50296284\n",
      "Iteration 405, loss = 0.50270129\n",
      "Iteration 406, loss = 0.50244044\n",
      "Iteration 407, loss = 0.50218028\n",
      "Iteration 408, loss = 0.50192083\n",
      "Iteration 409, loss = 0.50166211\n",
      "Iteration 410, loss = 0.50140414\n",
      "Iteration 411, loss = 0.50114686\n",
      "Iteration 412, loss = 0.50089029\n",
      "Iteration 413, loss = 0.50063441\n",
      "Iteration 414, loss = 0.50037920\n",
      "Iteration 415, loss = 0.50012466\n",
      "Iteration 416, loss = 0.49987081\n",
      "Iteration 417, loss = 0.49961764\n",
      "Iteration 418, loss = 0.49936514\n",
      "Iteration 419, loss = 0.49911333\n",
      "Iteration 420, loss = 0.49886218\n",
      "Iteration 421, loss = 0.49861171\n",
      "Iteration 422, loss = 0.49836194\n",
      "Iteration 423, loss = 0.49811283\n",
      "Iteration 424, loss = 0.49786442\n",
      "Iteration 425, loss = 0.49761674\n",
      "Iteration 426, loss = 0.49736973\n",
      "Iteration 427, loss = 0.49712338\n",
      "Iteration 428, loss = 0.49687768\n",
      "Iteration 429, loss = 0.49663263\n",
      "Iteration 430, loss = 0.49638822\n",
      "Iteration 431, loss = 0.49614446\n",
      "Iteration 432, loss = 0.49590142\n",
      "Iteration 433, loss = 0.49565900\n",
      "Iteration 434, loss = 0.49541721\n",
      "Iteration 435, loss = 0.49517605\n",
      "Iteration 436, loss = 0.49493553\n",
      "Iteration 437, loss = 0.49469562\n",
      "Iteration 438, loss = 0.49445633\n",
      "Iteration 439, loss = 0.49421766\n",
      "Iteration 440, loss = 0.49397960\n",
      "Iteration 441, loss = 0.49374215\n",
      "Iteration 442, loss = 0.49350532\n",
      "Iteration 443, loss = 0.49326916\n",
      "Iteration 444, loss = 0.49303361\n",
      "Iteration 445, loss = 0.49279868\n",
      "Iteration 446, loss = 0.49256433\n",
      "Iteration 447, loss = 0.49233058\n",
      "Iteration 448, loss = 0.49209742\n",
      "Iteration 449, loss = 0.49186483\n",
      "Iteration 450, loss = 0.49163282\n",
      "Iteration 451, loss = 0.49140138\n",
      "Iteration 452, loss = 0.49117050\n",
      "Iteration 453, loss = 0.49094020\n",
      "Iteration 454, loss = 0.49071047\n",
      "Iteration 455, loss = 0.49048129\n",
      "Iteration 456, loss = 0.49025268\n",
      "Iteration 457, loss = 0.49002463\n",
      "Iteration 458, loss = 0.48979713\n",
      "Iteration 459, loss = 0.48957016\n",
      "Iteration 460, loss = 0.48934374\n",
      "Iteration 461, loss = 0.48911787\n",
      "Iteration 462, loss = 0.48889255\n",
      "Iteration 463, loss = 0.48866777\n",
      "Iteration 464, loss = 0.48844354\n",
      "Iteration 465, loss = 0.48821984\n",
      "Iteration 466, loss = 0.48799665\n",
      "Iteration 467, loss = 0.48777396\n",
      "Iteration 468, loss = 0.48755179\n",
      "Iteration 469, loss = 0.48733015\n",
      "Iteration 470, loss = 0.48710902\n",
      "Iteration 471, loss = 0.48688841\n",
      "Iteration 472, loss = 0.48666832\n",
      "Iteration 473, loss = 0.48644875\n",
      "Iteration 474, loss = 0.48622969\n",
      "Iteration 475, loss = 0.48601114\n",
      "Iteration 476, loss = 0.48579310\n",
      "Iteration 477, loss = 0.48557557\n",
      "Iteration 478, loss = 0.48535855\n",
      "Iteration 479, loss = 0.48514202\n",
      "Iteration 480, loss = 0.48492601\n",
      "Iteration 481, loss = 0.48471049\n",
      "Iteration 482, loss = 0.48449548\n",
      "Iteration 483, loss = 0.48428096\n",
      "Iteration 484, loss = 0.48406695\n",
      "Iteration 485, loss = 0.48385347\n",
      "Iteration 486, loss = 0.48364052\n",
      "Iteration 487, loss = 0.48342806\n",
      "Iteration 488, loss = 0.48321609\n",
      "Iteration 489, loss = 0.48300463\n",
      "Iteration 490, loss = 0.48279367\n",
      "Iteration 491, loss = 0.48258320\n",
      "Iteration 492, loss = 0.48237320\n",
      "Iteration 493, loss = 0.48216369\n",
      "Iteration 494, loss = 0.48195467\n",
      "Iteration 495, loss = 0.48174613\n",
      "Iteration 496, loss = 0.48153812\n",
      "Iteration 497, loss = 0.48133058\n",
      "Iteration 498, loss = 0.48112352\n",
      "Iteration 499, loss = 0.48091692\n",
      "Iteration 500, loss = 0.48071078\n",
      "Iteration 501, loss = 0.48050510\n",
      "Iteration 502, loss = 0.48029988\n",
      "Iteration 503, loss = 0.48009511\n",
      "Iteration 504, loss = 0.47989079\n",
      "Iteration 505, loss = 0.47968693\n",
      "Iteration 506, loss = 0.47948351\n",
      "Iteration 507, loss = 0.47928051\n",
      "Iteration 508, loss = 0.47907794\n",
      "Iteration 509, loss = 0.47887582\n",
      "Iteration 510, loss = 0.47867414\n",
      "Iteration 511, loss = 0.47847285\n",
      "Iteration 512, loss = 0.47827200\n",
      "Iteration 513, loss = 0.47807157\n",
      "Iteration 514, loss = 0.47787158\n",
      "Iteration 515, loss = 0.47767201\n",
      "Iteration 516, loss = 0.47747287\n",
      "Iteration 517, loss = 0.47727415\n",
      "Iteration 518, loss = 0.47707586\n",
      "Iteration 519, loss = 0.47687798\n",
      "Iteration 520, loss = 0.47668056\n",
      "Iteration 521, loss = 0.47648357\n",
      "Iteration 522, loss = 0.47628699\n",
      "Iteration 523, loss = 0.47609079\n",
      "Iteration 524, loss = 0.47589497\n",
      "Iteration 525, loss = 0.47569957\n",
      "Iteration 526, loss = 0.47550458\n",
      "Iteration 527, loss = 0.47531001\n",
      "Iteration 528, loss = 0.47511584\n",
      "Iteration 529, loss = 0.47492209\n",
      "Iteration 530, loss = 0.47472876\n",
      "Iteration 531, loss = 0.47453584\n",
      "Iteration 532, loss = 0.47434331\n",
      "Iteration 533, loss = 0.47415119\n",
      "Iteration 534, loss = 0.47395948\n",
      "Iteration 535, loss = 0.47376815\n",
      "Iteration 536, loss = 0.47357723\n",
      "Iteration 537, loss = 0.47338670\n",
      "Iteration 538, loss = 0.47319656\n",
      "Iteration 539, loss = 0.47300681\n",
      "Iteration 540, loss = 0.47281744\n",
      "Iteration 541, loss = 0.47262841\n",
      "Iteration 542, loss = 0.47243974\n",
      "Iteration 543, loss = 0.47225140\n",
      "Iteration 544, loss = 0.47206341\n",
      "Iteration 545, loss = 0.47187580\n",
      "Iteration 546, loss = 0.47168856\n",
      "Iteration 547, loss = 0.47150170\n",
      "Iteration 548, loss = 0.47131521\n",
      "Iteration 549, loss = 0.47112910\n",
      "Iteration 550, loss = 0.47094335\n",
      "Iteration 551, loss = 0.47075794\n",
      "Iteration 552, loss = 0.47057290\n",
      "Iteration 553, loss = 0.47038825\n",
      "Iteration 554, loss = 0.47020392\n",
      "Iteration 555, loss = 0.47001995\n",
      "Iteration 556, loss = 0.46983635\n",
      "Iteration 557, loss = 0.46965311\n",
      "Iteration 558, loss = 0.46947023\n",
      "Iteration 559, loss = 0.46928771\n",
      "Iteration 560, loss = 0.46910556\n",
      "Iteration 561, loss = 0.46892376\n",
      "Iteration 562, loss = 0.46874231\n",
      "Iteration 563, loss = 0.46856122\n",
      "Iteration 564, loss = 0.46838049\n",
      "Iteration 565, loss = 0.46820011\n",
      "Iteration 566, loss = 0.46802006\n",
      "Iteration 567, loss = 0.46784035\n",
      "Iteration 568, loss = 0.46766098\n",
      "Iteration 569, loss = 0.46748196\n",
      "Iteration 570, loss = 0.46730325\n",
      "Iteration 571, loss = 0.46712487\n",
      "Iteration 572, loss = 0.46694681\n",
      "Iteration 573, loss = 0.46676910\n",
      "Iteration 574, loss = 0.46659172\n",
      "Iteration 575, loss = 0.46641467\n",
      "Iteration 576, loss = 0.46623797\n",
      "Iteration 577, loss = 0.46606159\n",
      "Iteration 578, loss = 0.46588555\n",
      "Iteration 579, loss = 0.46570985\n",
      "Iteration 580, loss = 0.46553448\n",
      "Iteration 581, loss = 0.46535945\n",
      "Iteration 582, loss = 0.46518474\n",
      "Iteration 583, loss = 0.46501039\n",
      "Iteration 584, loss = 0.46483635\n",
      "Iteration 585, loss = 0.46466262\n",
      "Iteration 586, loss = 0.46448922\n",
      "Iteration 587, loss = 0.46431614\n",
      "Iteration 588, loss = 0.46414338\n",
      "Iteration 589, loss = 0.46397095\n",
      "Iteration 590, loss = 0.46379883\n",
      "Iteration 591, loss = 0.46362706\n",
      "Iteration 592, loss = 0.46345561\n",
      "Iteration 593, loss = 0.46328448\n",
      "Iteration 594, loss = 0.46311366\n",
      "Iteration 595, loss = 0.46294319\n",
      "Iteration 596, loss = 0.46277311\n",
      "Iteration 597, loss = 0.46260336\n",
      "Iteration 598, loss = 0.46243389\n",
      "Iteration 599, loss = 0.46226474\n",
      "Iteration 600, loss = 0.46209590\n",
      "Iteration 601, loss = 0.46192737\n",
      "Iteration 602, loss = 0.46175917\n",
      "Iteration 603, loss = 0.46159127\n",
      "Iteration 604, loss = 0.46142369\n",
      "Iteration 605, loss = 0.46125641\n",
      "Iteration 606, loss = 0.46108944\n",
      "Iteration 607, loss = 0.46092278\n",
      "Iteration 608, loss = 0.46075642\n",
      "Iteration 609, loss = 0.46059036\n",
      "Iteration 610, loss = 0.46042459\n",
      "Iteration 611, loss = 0.46025913\n",
      "Iteration 612, loss = 0.46009396\n",
      "Iteration 613, loss = 0.45992909\n",
      "Iteration 614, loss = 0.45976452\n",
      "Iteration 615, loss = 0.45960026\n",
      "Iteration 616, loss = 0.45943629\n",
      "Iteration 617, loss = 0.45927261\n",
      "Iteration 618, loss = 0.45910923\n",
      "Iteration 619, loss = 0.45894615\n",
      "Iteration 620, loss = 0.45878335\n",
      "Iteration 621, loss = 0.45862084\n",
      "Iteration 622, loss = 0.45845862\n",
      "Iteration 623, loss = 0.45829669\n",
      "Iteration 624, loss = 0.45813504\n",
      "Iteration 625, loss = 0.45797368\n",
      "Iteration 626, loss = 0.45781261\n",
      "Iteration 627, loss = 0.45765183\n",
      "Iteration 628, loss = 0.45749130\n",
      "Iteration 629, loss = 0.45733100\n",
      "Iteration 630, loss = 0.45717098\n",
      "Iteration 631, loss = 0.45701122\n",
      "Iteration 632, loss = 0.45685174\n",
      "Iteration 633, loss = 0.45669254\n",
      "Iteration 634, loss = 0.45653361\n",
      "Iteration 635, loss = 0.45637495\n",
      "Iteration 636, loss = 0.45621656\n",
      "Iteration 637, loss = 0.45605844\n",
      "Iteration 638, loss = 0.45590059\n",
      "Iteration 639, loss = 0.45574302\n",
      "Iteration 640, loss = 0.45558571\n",
      "Iteration 641, loss = 0.45542866\n",
      "Iteration 642, loss = 0.45527187\n",
      "Iteration 643, loss = 0.45511534\n",
      "Iteration 644, loss = 0.45495907\n",
      "Iteration 645, loss = 0.45480307\n",
      "Iteration 646, loss = 0.45464732\n",
      "Iteration 647, loss = 0.45449184\n",
      "Iteration 648, loss = 0.45433662\n",
      "Iteration 649, loss = 0.45418166\n",
      "Iteration 650, loss = 0.45402696\n",
      "Iteration 651, loss = 0.45387251\n",
      "Iteration 652, loss = 0.45371833\n",
      "Iteration 653, loss = 0.45356440\n",
      "Iteration 654, loss = 0.45341073\n",
      "Iteration 655, loss = 0.45325731\n",
      "Iteration 656, loss = 0.45310415\n",
      "Iteration 657, loss = 0.45295125\n",
      "Iteration 658, loss = 0.45279855\n",
      "Iteration 659, loss = 0.45264611\n",
      "Iteration 660, loss = 0.45249393\n",
      "Iteration 661, loss = 0.45234197\n",
      "Iteration 662, loss = 0.45219025\n",
      "Iteration 663, loss = 0.45203878\n",
      "Iteration 664, loss = 0.45188755\n",
      "Iteration 665, loss = 0.45173657\n",
      "Iteration 666, loss = 0.45158584\n",
      "Iteration 667, loss = 0.45143535\n",
      "Iteration 668, loss = 0.45128511\n",
      "Iteration 669, loss = 0.45113508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 670, loss = 0.45098528\n",
      "Iteration 671, loss = 0.45083572\n",
      "Iteration 672, loss = 0.45068640\n",
      "Iteration 673, loss = 0.45053732\n",
      "Iteration 674, loss = 0.45038847\n",
      "Iteration 675, loss = 0.45023986\n",
      "Iteration 676, loss = 0.45009150\n",
      "Iteration 677, loss = 0.44994342\n",
      "Iteration 678, loss = 0.44979564\n",
      "Iteration 679, loss = 0.44964810\n",
      "Iteration 680, loss = 0.44950082\n",
      "Iteration 681, loss = 0.44935377\n",
      "Iteration 682, loss = 0.44920691\n",
      "Iteration 683, loss = 0.44906029\n",
      "Iteration 684, loss = 0.44891390\n",
      "Iteration 685, loss = 0.44876775\n",
      "Iteration 686, loss = 0.44862182\n",
      "Iteration 687, loss = 0.44847613\n",
      "Iteration 688, loss = 0.44833066\n",
      "Iteration 689, loss = 0.44818537\n",
      "Iteration 690, loss = 0.44804031\n",
      "Iteration 691, loss = 0.44789547\n",
      "Iteration 692, loss = 0.44775087\n",
      "Iteration 693, loss = 0.44760648\n",
      "Iteration 694, loss = 0.44746237\n",
      "Iteration 695, loss = 0.44731851\n",
      "Iteration 696, loss = 0.44717488\n",
      "Iteration 697, loss = 0.44703148\n",
      "Iteration 698, loss = 0.44688829\n",
      "Iteration 699, loss = 0.44674533\n",
      "Iteration 700, loss = 0.44660259\n",
      "Iteration 701, loss = 0.44646003\n",
      "Iteration 702, loss = 0.44631770\n",
      "Iteration 703, loss = 0.44617558\n",
      "Iteration 704, loss = 0.44603368\n",
      "Iteration 705, loss = 0.44589200\n",
      "Iteration 706, loss = 0.44575054\n",
      "Iteration 707, loss = 0.44560929\n",
      "Iteration 708, loss = 0.44546826\n",
      "Iteration 709, loss = 0.44532744\n",
      "Iteration 710, loss = 0.44518688\n",
      "Iteration 711, loss = 0.44504659\n",
      "Iteration 712, loss = 0.44490656\n",
      "Iteration 713, loss = 0.44476674\n",
      "Iteration 714, loss = 0.44462713\n",
      "Iteration 715, loss = 0.44448775\n",
      "Iteration 716, loss = 0.44434858\n",
      "Iteration 717, loss = 0.44420962\n",
      "Iteration 718, loss = 0.44407088\n",
      "Iteration 719, loss = 0.44393235\n",
      "Iteration 720, loss = 0.44379404\n",
      "Iteration 721, loss = 0.44365595\n",
      "Iteration 722, loss = 0.44351808\n",
      "Iteration 723, loss = 0.44338041\n",
      "Iteration 724, loss = 0.44324303\n",
      "Iteration 725, loss = 0.44310588\n",
      "Iteration 726, loss = 0.44296900\n",
      "Iteration 727, loss = 0.44283234\n",
      "Iteration 728, loss = 0.44269589\n",
      "Iteration 729, loss = 0.44255965\n",
      "Iteration 730, loss = 0.44242362\n",
      "Iteration 731, loss = 0.44228779\n",
      "Iteration 732, loss = 0.44215218\n",
      "Iteration 733, loss = 0.44201678\n",
      "Iteration 734, loss = 0.44188158\n",
      "Iteration 735, loss = 0.44174658\n",
      "Iteration 736, loss = 0.44161179\n",
      "Iteration 737, loss = 0.44147719\n",
      "Iteration 738, loss = 0.44134281\n",
      "Iteration 739, loss = 0.44120862\n",
      "Iteration 740, loss = 0.44107463\n",
      "Iteration 741, loss = 0.44094084\n",
      "Iteration 742, loss = 0.44080724\n",
      "Iteration 743, loss = 0.44067383\n",
      "Iteration 744, loss = 0.44054063\n",
      "Iteration 745, loss = 0.44040769\n",
      "Iteration 746, loss = 0.44027496\n",
      "Iteration 747, loss = 0.44014242\n",
      "Iteration 748, loss = 0.44001008\n",
      "Iteration 749, loss = 0.43987794\n",
      "Iteration 750, loss = 0.43974599\n",
      "Iteration 751, loss = 0.43961424\n",
      "Iteration 752, loss = 0.43948268\n",
      "Iteration 753, loss = 0.43935128\n",
      "Iteration 754, loss = 0.43922006\n",
      "Iteration 755, loss = 0.43908902\n",
      "Iteration 756, loss = 0.43895810\n",
      "Iteration 757, loss = 0.43882741\n",
      "Iteration 758, loss = 0.43869691\n",
      "Iteration 759, loss = 0.43856661\n",
      "Iteration 760, loss = 0.43843650\n",
      "Iteration 761, loss = 0.43830658\n",
      "Iteration 762, loss = 0.43817686\n",
      "Iteration 763, loss = 0.43804732\n",
      "Iteration 764, loss = 0.43791798\n",
      "Iteration 765, loss = 0.43778881\n",
      "Iteration 766, loss = 0.43765980\n",
      "Iteration 767, loss = 0.43753096\n",
      "Iteration 768, loss = 0.43740232\n",
      "Iteration 769, loss = 0.43727388\n",
      "Iteration 770, loss = 0.43714563\n",
      "Iteration 771, loss = 0.43701756\n",
      "Iteration 772, loss = 0.43688967\n",
      "Iteration 773, loss = 0.43676196\n",
      "Iteration 774, loss = 0.43663443\n",
      "Iteration 775, loss = 0.43650706\n",
      "Iteration 776, loss = 0.43637985\n",
      "Iteration 777, loss = 0.43625282\n",
      "Iteration 778, loss = 0.43612597\n",
      "Iteration 779, loss = 0.43599929\n",
      "Iteration 780, loss = 0.43587279\n",
      "Iteration 781, loss = 0.43574646\n",
      "Iteration 782, loss = 0.43562030\n",
      "Iteration 783, loss = 0.43549432\n",
      "Iteration 784, loss = 0.43536851\n",
      "Iteration 785, loss = 0.43524287\n",
      "Iteration 786, loss = 0.43511740\n",
      "Iteration 787, loss = 0.43499212\n",
      "Iteration 788, loss = 0.43486701\n",
      "Iteration 789, loss = 0.43474207\n",
      "Iteration 790, loss = 0.43461730\n",
      "Iteration 791, loss = 0.43449270\n",
      "Iteration 792, loss = 0.43436828\n",
      "Iteration 793, loss = 0.43424401\n",
      "Iteration 794, loss = 0.43411992\n",
      "Iteration 795, loss = 0.43399599\n",
      "Iteration 796, loss = 0.43387223\n",
      "Iteration 797, loss = 0.43374865\n",
      "Iteration 798, loss = 0.43362526\n",
      "Iteration 799, loss = 0.43350212\n",
      "Iteration 800, loss = 0.43337920\n",
      "Iteration 801, loss = 0.43325645\n",
      "Iteration 802, loss = 0.43313388\n",
      "Iteration 803, loss = 0.43301148\n",
      "Iteration 804, loss = 0.43288926\n",
      "Iteration 805, loss = 0.43276720\n",
      "Iteration 806, loss = 0.43264532\n",
      "Iteration 807, loss = 0.43252361\n",
      "Iteration 808, loss = 0.43240207\n",
      "Iteration 809, loss = 0.43228069\n",
      "Iteration 810, loss = 0.43215949\n",
      "Iteration 811, loss = 0.43203846\n",
      "Iteration 812, loss = 0.43191759\n",
      "Iteration 813, loss = 0.43179688\n",
      "Iteration 814, loss = 0.43167634\n",
      "Iteration 815, loss = 0.43155596\n",
      "Iteration 816, loss = 0.43143574\n",
      "Iteration 817, loss = 0.43131568\n",
      "Iteration 818, loss = 0.43119578\n",
      "Iteration 819, loss = 0.43107604\n",
      "Iteration 820, loss = 0.43095646\n",
      "Iteration 821, loss = 0.43083704\n",
      "Iteration 822, loss = 0.43071778\n",
      "Iteration 823, loss = 0.43059867\n",
      "Iteration 824, loss = 0.43047973\n",
      "Iteration 825, loss = 0.43036093\n",
      "Iteration 826, loss = 0.43024230\n",
      "Iteration 827, loss = 0.43012383\n",
      "Iteration 828, loss = 0.43000552\n",
      "Iteration 829, loss = 0.42988743\n",
      "Iteration 830, loss = 0.42976952\n",
      "Iteration 831, loss = 0.42965176\n",
      "Iteration 832, loss = 0.42953415\n",
      "Iteration 833, loss = 0.42941672\n",
      "Iteration 834, loss = 0.42929944\n",
      "Iteration 835, loss = 0.42918232\n",
      "Iteration 836, loss = 0.42906535\n",
      "Iteration 837, loss = 0.42894854\n",
      "Iteration 838, loss = 0.42883188\n",
      "Iteration 839, loss = 0.42871538\n",
      "Iteration 840, loss = 0.42859903\n",
      "Iteration 841, loss = 0.42848283\n",
      "Iteration 842, loss = 0.42836679\n",
      "Iteration 843, loss = 0.42825090\n",
      "Iteration 844, loss = 0.42813515\n",
      "Iteration 845, loss = 0.42801955\n",
      "Iteration 846, loss = 0.42790407\n",
      "Iteration 847, loss = 0.42778875\n",
      "Iteration 848, loss = 0.42767356\n",
      "Iteration 849, loss = 0.42755852\n",
      "Iteration 850, loss = 0.42744363\n",
      "Iteration 851, loss = 0.42732888\n",
      "Iteration 852, loss = 0.42721426\n",
      "Iteration 853, loss = 0.42709977\n",
      "Iteration 854, loss = 0.42698541\n",
      "Iteration 855, loss = 0.42687120\n",
      "Iteration 856, loss = 0.42675711\n",
      "Iteration 857, loss = 0.42664314\n",
      "Iteration 858, loss = 0.42652930\n",
      "Iteration 859, loss = 0.42641561\n",
      "Iteration 860, loss = 0.42630208\n",
      "Iteration 861, loss = 0.42618874\n",
      "Iteration 862, loss = 0.42607557\n",
      "Iteration 863, loss = 0.42596255\n",
      "Iteration 864, loss = 0.42584971\n",
      "Iteration 865, loss = 0.42573702\n",
      "Iteration 866, loss = 0.42562448\n",
      "Iteration 867, loss = 0.42551207\n",
      "Iteration 868, loss = 0.42539981\n",
      "Iteration 869, loss = 0.42528769\n",
      "Iteration 870, loss = 0.42517571\n",
      "Iteration 871, loss = 0.42506388\n",
      "Iteration 872, loss = 0.42495218\n",
      "Iteration 873, loss = 0.42484063\n",
      "Iteration 874, loss = 0.42472920\n",
      "Iteration 875, loss = 0.42461789\n",
      "Iteration 876, loss = 0.42450672\n",
      "Iteration 877, loss = 0.42439568\n",
      "Iteration 878, loss = 0.42428479\n",
      "Iteration 879, loss = 0.42417403\n",
      "Iteration 880, loss = 0.42406340\n",
      "Iteration 881, loss = 0.42395291\n",
      "Iteration 882, loss = 0.42384255\n",
      "Iteration 883, loss = 0.42373233\n",
      "Iteration 884, loss = 0.42362225\n",
      "Iteration 885, loss = 0.42351235\n",
      "Iteration 886, loss = 0.42340263\n",
      "Iteration 887, loss = 0.42329306\n",
      "Iteration 888, loss = 0.42318362\n",
      "Iteration 889, loss = 0.42307430\n",
      "Iteration 890, loss = 0.42296511\n",
      "Iteration 891, loss = 0.42285605\n",
      "Iteration 892, loss = 0.42274712\n",
      "Iteration 893, loss = 0.42263833\n",
      "Iteration 894, loss = 0.42252966\n",
      "Iteration 895, loss = 0.42242110\n",
      "Iteration 896, loss = 0.42231267\n",
      "Iteration 897, loss = 0.42220437\n",
      "Iteration 898, loss = 0.42209622\n",
      "Iteration 899, loss = 0.42198823\n",
      "Iteration 900, loss = 0.42188037\n",
      "Iteration 901, loss = 0.42177261\n",
      "Iteration 902, loss = 0.42166497\n",
      "Iteration 903, loss = 0.42155746\n",
      "Iteration 904, loss = 0.42145006\n",
      "Iteration 905, loss = 0.42134277\n",
      "Iteration 906, loss = 0.42123561\n",
      "Iteration 907, loss = 0.42112857\n",
      "Iteration 908, loss = 0.42102167\n",
      "Iteration 909, loss = 0.42091489\n",
      "Iteration 910, loss = 0.42080824\n",
      "Iteration 911, loss = 0.42070172\n",
      "Iteration 912, loss = 0.42059532\n",
      "Iteration 913, loss = 0.42048904\n",
      "Iteration 914, loss = 0.42038289\n",
      "Iteration 915, loss = 0.42027685\n",
      "Iteration 916, loss = 0.42017095\n",
      "Iteration 917, loss = 0.42006516\n",
      "Iteration 918, loss = 0.41995950\n",
      "Iteration 919, loss = 0.41985396\n",
      "Iteration 920, loss = 0.41974854\n",
      "Iteration 921, loss = 0.41964325\n",
      "Iteration 922, loss = 0.41953807\n",
      "Iteration 923, loss = 0.41943302\n",
      "Iteration 924, loss = 0.41932808\n",
      "Iteration 925, loss = 0.41922326\n",
      "Iteration 926, loss = 0.41911856\n",
      "Iteration 927, loss = 0.41901399\n",
      "Iteration 928, loss = 0.41890953\n",
      "Iteration 929, loss = 0.41880520\n",
      "Iteration 930, loss = 0.41870098\n",
      "Iteration 931, loss = 0.41859688\n",
      "Iteration 932, loss = 0.41849290\n",
      "Iteration 933, loss = 0.41838904\n",
      "Iteration 934, loss = 0.41828527\n",
      "Iteration 935, loss = 0.41818161\n",
      "Iteration 936, loss = 0.41807806\n",
      "Iteration 937, loss = 0.41797463\n",
      "Iteration 938, loss = 0.41787132\n",
      "Iteration 939, loss = 0.41776812\n",
      "Iteration 940, loss = 0.41766505\n",
      "Iteration 941, loss = 0.41756210\n",
      "Iteration 942, loss = 0.41745926\n",
      "Iteration 943, loss = 0.41735654\n",
      "Iteration 944, loss = 0.41725394\n",
      "Iteration 945, loss = 0.41715146\n",
      "Iteration 946, loss = 0.41704906\n",
      "Iteration 947, loss = 0.41694680\n",
      "Iteration 948, loss = 0.41684465\n",
      "Iteration 949, loss = 0.41674261\n",
      "Iteration 950, loss = 0.41664069\n",
      "Iteration 951, loss = 0.41653889\n",
      "Iteration 952, loss = 0.41643720\n",
      "Iteration 953, loss = 0.41633562\n",
      "Iteration 954, loss = 0.41623416\n",
      "Iteration 955, loss = 0.41613281\n",
      "Iteration 956, loss = 0.41603159\n",
      "Iteration 957, loss = 0.41593047\n",
      "Iteration 958, loss = 0.41582947\n",
      "Iteration 959, loss = 0.41572857\n",
      "Iteration 960, loss = 0.41562779\n",
      "Iteration 961, loss = 0.41552712\n",
      "Iteration 962, loss = 0.41542656\n",
      "Iteration 963, loss = 0.41532608\n",
      "Iteration 964, loss = 0.41522571\n",
      "Iteration 965, loss = 0.41512545\n",
      "Iteration 966, loss = 0.41502530\n",
      "Iteration 967, loss = 0.41492525\n",
      "Iteration 968, loss = 0.41482532\n",
      "Iteration 969, loss = 0.41472549\n",
      "Iteration 970, loss = 0.41462576\n",
      "Iteration 971, loss = 0.41452613\n",
      "Iteration 972, loss = 0.41442661\n",
      "Iteration 973, loss = 0.41432723\n",
      "Iteration 974, loss = 0.41422797\n",
      "Iteration 975, loss = 0.41412881\n",
      "Iteration 976, loss = 0.41402976\n",
      "Iteration 977, loss = 0.41393083\n",
      "Iteration 978, loss = 0.41383200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.913333\n",
      "Training set loss: 0.413832\n",
      "training: inv-scaling with Nesterov's momentum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.03169308\n",
      "Iteration 3, loss = 1.01185616\n",
      "Iteration 4, loss = 0.99403369\n",
      "Iteration 5, loss = 0.97785415\n",
      "Iteration 6, loss = 0.96313564\n",
      "Iteration 7, loss = 0.94963823\n",
      "Iteration 8, loss = 0.93724336\n",
      "Iteration 9, loss = 0.92579975\n",
      "Iteration 10, loss = 0.91517366\n",
      "Iteration 11, loss = 0.90523907\n",
      "Iteration 12, loss = 0.89589542\n",
      "Iteration 13, loss = 0.88706629\n",
      "Iteration 14, loss = 0.87868364\n",
      "Iteration 15, loss = 0.87071417\n",
      "Iteration 16, loss = 0.86311459\n",
      "Iteration 17, loss = 0.85587378\n",
      "Iteration 18, loss = 0.84896385\n",
      "Iteration 19, loss = 0.84237125\n",
      "Iteration 20, loss = 0.83607521\n",
      "Iteration 21, loss = 0.83007069\n",
      "Iteration 22, loss = 0.82434776\n",
      "Iteration 23, loss = 0.81889504\n",
      "Iteration 24, loss = 0.81369494\n",
      "Iteration 25, loss = 0.80871692\n",
      "Iteration 26, loss = 0.80396038\n",
      "Iteration 27, loss = 0.79941647\n",
      "Iteration 28, loss = 0.79506989\n",
      "Iteration 29, loss = 0.79090793\n",
      "Iteration 30, loss = 0.78692033\n",
      "Iteration 31, loss = 0.78309772\n",
      "Iteration 32, loss = 0.77942120\n",
      "Iteration 33, loss = 0.77588193\n",
      "Iteration 34, loss = 0.77247059\n",
      "Iteration 35, loss = 0.76917763\n",
      "Iteration 36, loss = 0.76599683\n",
      "Iteration 37, loss = 0.76292171\n",
      "Iteration 38, loss = 0.75994365\n",
      "Iteration 39, loss = 0.75706093\n",
      "Iteration 40, loss = 0.75427218\n",
      "Iteration 41, loss = 0.75156770\n",
      "Iteration 42, loss = 0.74894044\n",
      "Iteration 43, loss = 0.74638687\n",
      "Iteration 44, loss = 0.74390210\n",
      "Iteration 45, loss = 0.74148247\n",
      "Iteration 46, loss = 0.73912451\n",
      "Iteration 47, loss = 0.73682519\n",
      "Iteration 48, loss = 0.73458182\n",
      "Iteration 49, loss = 0.73239096\n",
      "Iteration 50, loss = 0.73024851\n",
      "Iteration 51, loss = 0.72815348\n",
      "Iteration 52, loss = 0.72610339\n",
      "Iteration 53, loss = 0.72409629\n",
      "Iteration 54, loss = 0.72212934\n",
      "Iteration 55, loss = 0.72020103\n",
      "Iteration 56, loss = 0.71830917\n",
      "Iteration 57, loss = 0.71645224\n",
      "Iteration 58, loss = 0.71462908\n",
      "Iteration 59, loss = 0.71283814\n",
      "Iteration 60, loss = 0.71107809\n",
      "Iteration 61, loss = 0.70934781\n",
      "Iteration 62, loss = 0.70764593\n",
      "Iteration 63, loss = 0.70597108\n",
      "Iteration 64, loss = 0.70432199\n",
      "Iteration 65, loss = 0.70269803\n",
      "Iteration 66, loss = 0.70109814\n",
      "Iteration 67, loss = 0.69952135\n",
      "Iteration 68, loss = 0.69796735\n",
      "Iteration 69, loss = 0.69643491\n",
      "Iteration 70, loss = 0.69492301\n",
      "Iteration 71, loss = 0.69343169\n",
      "Iteration 72, loss = 0.69195924\n",
      "Iteration 73, loss = 0.69050561\n",
      "Iteration 74, loss = 0.68907045\n",
      "Iteration 75, loss = 0.68765326\n",
      "Iteration 76, loss = 0.68625312\n",
      "Iteration 77, loss = 0.68486954\n",
      "Iteration 78, loss = 0.68350195\n",
      "Iteration 79, loss = 0.68215041\n",
      "Iteration 80, loss = 0.68081420\n",
      "Iteration 81, loss = 0.67949281\n",
      "Iteration 82, loss = 0.67818623\n",
      "Iteration 83, loss = 0.67689393\n",
      "Iteration 84, loss = 0.67561557\n",
      "Iteration 85, loss = 0.67435112\n",
      "Iteration 86, loss = 0.67309993\n",
      "Iteration 87, loss = 0.67186229\n",
      "Iteration 88, loss = 0.67063751\n",
      "Iteration 89, loss = 0.66942599\n",
      "Iteration 90, loss = 0.66822689\n",
      "Iteration 91, loss = 0.66703989\n",
      "Iteration 92, loss = 0.66586541\n",
      "Iteration 93, loss = 0.66470313\n",
      "Iteration 94, loss = 0.66355234\n",
      "Iteration 95, loss = 0.66241314\n",
      "Iteration 96, loss = 0.66128503\n",
      "Iteration 97, loss = 0.66016750\n",
      "Iteration 98, loss = 0.65906015\n",
      "Iteration 99, loss = 0.65796370\n",
      "Iteration 100, loss = 0.65687732\n",
      "Iteration 101, loss = 0.65580099\n",
      "Iteration 102, loss = 0.65473452\n",
      "Iteration 103, loss = 0.65367757\n",
      "Iteration 104, loss = 0.65263080\n",
      "Iteration 105, loss = 0.65159386\n",
      "Iteration 106, loss = 0.65056615\n",
      "Iteration 107, loss = 0.64954740\n",
      "Iteration 108, loss = 0.64853737\n",
      "Iteration 109, loss = 0.64753581\n",
      "Iteration 110, loss = 0.64654280\n",
      "Iteration 111, loss = 0.64555807\n",
      "Iteration 112, loss = 0.64458175\n",
      "Iteration 113, loss = 0.64361379\n",
      "Iteration 114, loss = 0.64265373\n",
      "Iteration 115, loss = 0.64170161\n",
      "Iteration 116, loss = 0.64075715\n",
      "Iteration 117, loss = 0.63982022\n",
      "Iteration 118, loss = 0.63889097\n",
      "Iteration 119, loss = 0.63796897\n",
      "Iteration 120, loss = 0.63705423\n",
      "Iteration 121, loss = 0.63614685\n",
      "Iteration 122, loss = 0.63524663\n",
      "Iteration 123, loss = 0.63435336\n",
      "Iteration 124, loss = 0.63346709\n",
      "Iteration 125, loss = 0.63258759\n",
      "Iteration 126, loss = 0.63171494\n",
      "Iteration 127, loss = 0.63084876\n",
      "Iteration 128, loss = 0.62998899\n",
      "Iteration 129, loss = 0.62913566\n",
      "Iteration 130, loss = 0.62828858\n",
      "Iteration 131, loss = 0.62744752\n",
      "Iteration 132, loss = 0.62661256\n",
      "Iteration 133, loss = 0.62578354\n",
      "Iteration 134, loss = 0.62496041\n",
      "Iteration 135, loss = 0.62414296\n",
      "Iteration 136, loss = 0.62333126\n",
      "Iteration 137, loss = 0.62252527\n",
      "Iteration 138, loss = 0.62172470\n",
      "Iteration 139, loss = 0.62092958\n",
      "Iteration 140, loss = 0.62013987\n",
      "Iteration 141, loss = 0.61935546\n",
      "Iteration 142, loss = 0.61857636\n",
      "Iteration 143, loss = 0.61780249\n",
      "Iteration 144, loss = 0.61703370\n",
      "Iteration 145, loss = 0.61627001\n",
      "Iteration 146, loss = 0.61551134\n",
      "Iteration 147, loss = 0.61475762\n",
      "Iteration 148, loss = 0.61400874\n",
      "Iteration 149, loss = 0.61326460\n",
      "Iteration 150, loss = 0.61252516\n",
      "Iteration 151, loss = 0.61179035\n",
      "Iteration 152, loss = 0.61106014\n",
      "Iteration 153, loss = 0.61033450\n",
      "Iteration 154, loss = 0.60961337\n",
      "Iteration 155, loss = 0.60889669\n",
      "Iteration 156, loss = 0.60818442\n",
      "Iteration 157, loss = 0.60747655\n",
      "Iteration 158, loss = 0.60677299\n",
      "Iteration 159, loss = 0.60607373\n",
      "Iteration 160, loss = 0.60537873\n",
      "Iteration 161, loss = 0.60468793\n",
      "Iteration 162, loss = 0.60400131\n",
      "Iteration 163, loss = 0.60331880\n",
      "Iteration 164, loss = 0.60264040\n",
      "Iteration 165, loss = 0.60196601\n",
      "Iteration 166, loss = 0.60129554\n",
      "Iteration 167, loss = 0.60062894\n",
      "Iteration 168, loss = 0.59996619\n",
      "Iteration 169, loss = 0.59930724\n",
      "Iteration 170, loss = 0.59865210\n",
      "Iteration 171, loss = 0.59800071\n",
      "Iteration 172, loss = 0.59735303\n",
      "Iteration 173, loss = 0.59670902\n",
      "Iteration 174, loss = 0.59606864\n",
      "Iteration 175, loss = 0.59543183\n",
      "Iteration 176, loss = 0.59479858\n",
      "Iteration 177, loss = 0.59416883\n",
      "Iteration 178, loss = 0.59354259\n",
      "Iteration 179, loss = 0.59291982\n",
      "Iteration 180, loss = 0.59230050\n",
      "Iteration 181, loss = 0.59168455\n",
      "Iteration 182, loss = 0.59107196\n",
      "Iteration 183, loss = 0.59046268\n",
      "Iteration 184, loss = 0.58985666\n",
      "Iteration 185, loss = 0.58925387\n",
      "Iteration 186, loss = 0.58865428\n",
      "Iteration 187, loss = 0.58805786\n",
      "Iteration 188, loss = 0.58746458\n",
      "Iteration 189, loss = 0.58687440\n",
      "Iteration 190, loss = 0.58628730\n",
      "Iteration 191, loss = 0.58570331\n",
      "Iteration 192, loss = 0.58512236\n",
      "Iteration 193, loss = 0.58454443\n",
      "Iteration 194, loss = 0.58396953\n",
      "Iteration 195, loss = 0.58339758\n",
      "Iteration 196, loss = 0.58282858\n",
      "Iteration 197, loss = 0.58226260\n",
      "Iteration 198, loss = 0.58169951\n",
      "Iteration 199, loss = 0.58113930\n",
      "Iteration 200, loss = 0.58058194\n",
      "Iteration 201, loss = 0.58002742\n",
      "Iteration 202, loss = 0.57947569\n",
      "Iteration 203, loss = 0.57892679\n",
      "Iteration 204, loss = 0.57838063\n",
      "Iteration 205, loss = 0.57783722\n",
      "Iteration 206, loss = 0.57729648\n",
      "Iteration 207, loss = 0.57675837\n",
      "Iteration 208, loss = 0.57622287\n",
      "Iteration 209, loss = 0.57569001\n",
      "Iteration 210, loss = 0.57515974\n",
      "Iteration 211, loss = 0.57463203\n",
      "Iteration 212, loss = 0.57410684\n",
      "Iteration 213, loss = 0.57358416\n",
      "Iteration 214, loss = 0.57306394\n",
      "Iteration 215, loss = 0.57254618\n",
      "Iteration 216, loss = 0.57203088\n",
      "Iteration 217, loss = 0.57151800\n",
      "Iteration 218, loss = 0.57100753\n",
      "Iteration 219, loss = 0.57049944\n",
      "Iteration 220, loss = 0.56999370\n",
      "Iteration 221, loss = 0.56949030\n",
      "Iteration 222, loss = 0.56898923\n",
      "Iteration 223, loss = 0.56849048\n",
      "Iteration 224, loss = 0.56799399\n",
      "Iteration 225, loss = 0.56749975\n",
      "Iteration 226, loss = 0.56700775\n",
      "Iteration 227, loss = 0.56651798\n",
      "Iteration 228, loss = 0.56603042\n",
      "Iteration 229, loss = 0.56554503\n",
      "Iteration 230, loss = 0.56506181\n",
      "Iteration 231, loss = 0.56458079\n",
      "Iteration 232, loss = 0.56410192\n",
      "Iteration 233, loss = 0.56362519\n",
      "Iteration 234, loss = 0.56315056\n",
      "Iteration 235, loss = 0.56267801\n",
      "Iteration 236, loss = 0.56220753\n",
      "Iteration 237, loss = 0.56173911\n",
      "Iteration 238, loss = 0.56127274\n",
      "Iteration 239, loss = 0.56080841\n",
      "Iteration 240, loss = 0.56034608\n",
      "Iteration 241, loss = 0.55988573\n",
      "Iteration 242, loss = 0.55942735\n",
      "Iteration 243, loss = 0.55897093\n",
      "Iteration 244, loss = 0.55851645\n",
      "Iteration 245, loss = 0.55806391\n",
      "Iteration 246, loss = 0.55761327\n",
      "Iteration 247, loss = 0.55716455\n",
      "Iteration 248, loss = 0.55671773\n",
      "Iteration 249, loss = 0.55627282\n",
      "Iteration 250, loss = 0.55582981\n",
      "Iteration 251, loss = 0.55538867\n",
      "Iteration 252, loss = 0.55494941\n",
      "Iteration 253, loss = 0.55451198\n",
      "Iteration 254, loss = 0.55407635\n",
      "Iteration 255, loss = 0.55364252\n",
      "Iteration 256, loss = 0.55321048\n",
      "Iteration 257, loss = 0.55278022\n",
      "Iteration 258, loss = 0.55235173\n",
      "Iteration 259, loss = 0.55192498\n",
      "Iteration 260, loss = 0.55149998\n",
      "Iteration 261, loss = 0.55107670\n",
      "Iteration 262, loss = 0.55065511\n",
      "Iteration 263, loss = 0.55023520\n",
      "Iteration 264, loss = 0.54981691\n",
      "Iteration 265, loss = 0.54940029\n",
      "Iteration 266, loss = 0.54898535\n",
      "Iteration 267, loss = 0.54857207\n",
      "Iteration 268, loss = 0.54816042\n",
      "Iteration 269, loss = 0.54775038\n",
      "Iteration 270, loss = 0.54734195\n",
      "Iteration 271, loss = 0.54693513\n",
      "Iteration 272, loss = 0.54652990\n",
      "Iteration 273, loss = 0.54612624\n",
      "Iteration 274, loss = 0.54572414\n",
      "Iteration 275, loss = 0.54532359\n",
      "Iteration 276, loss = 0.54492459\n",
      "Iteration 277, loss = 0.54452714\n",
      "Iteration 278, loss = 0.54413122\n",
      "Iteration 279, loss = 0.54373682\n",
      "Iteration 280, loss = 0.54334392\n",
      "Iteration 281, loss = 0.54295250\n",
      "Iteration 282, loss = 0.54256256\n",
      "Iteration 283, loss = 0.54217409\n",
      "Iteration 284, loss = 0.54178708\n",
      "Iteration 285, loss = 0.54140153\n",
      "Iteration 286, loss = 0.54101741\n",
      "Iteration 287, loss = 0.54063469\n",
      "Iteration 288, loss = 0.54025340\n",
      "Iteration 289, loss = 0.53987351\n",
      "Iteration 290, loss = 0.53949502\n",
      "Iteration 291, loss = 0.53911794\n",
      "Iteration 292, loss = 0.53874227\n",
      "Iteration 293, loss = 0.53836801\n",
      "Iteration 294, loss = 0.53799511\n",
      "Iteration 295, loss = 0.53762357\n",
      "Iteration 296, loss = 0.53725340\n",
      "Iteration 297, loss = 0.53688456\n",
      "Iteration 298, loss = 0.53651707\n",
      "Iteration 299, loss = 0.53615090\n",
      "Iteration 300, loss = 0.53578604\n",
      "Iteration 301, loss = 0.53542250\n",
      "Iteration 302, loss = 0.53506027\n",
      "Iteration 303, loss = 0.53469932\n",
      "Iteration 304, loss = 0.53433964\n",
      "Iteration 305, loss = 0.53398125\n",
      "Iteration 306, loss = 0.53362411\n",
      "Iteration 307, loss = 0.53326825\n",
      "Iteration 308, loss = 0.53291364\n",
      "Iteration 309, loss = 0.53256028\n",
      "Iteration 310, loss = 0.53220816\n",
      "Iteration 311, loss = 0.53185723\n",
      "Iteration 312, loss = 0.53150749\n",
      "Iteration 313, loss = 0.53115896\n",
      "Iteration 314, loss = 0.53081164\n",
      "Iteration 315, loss = 0.53046553\n",
      "Iteration 316, loss = 0.53012067\n",
      "Iteration 317, loss = 0.52977700\n",
      "Iteration 318, loss = 0.52943451\n",
      "Iteration 319, loss = 0.52909319\n",
      "Iteration 320, loss = 0.52875303\n",
      "Iteration 321, loss = 0.52841402\n",
      "Iteration 322, loss = 0.52807615\n",
      "Iteration 323, loss = 0.52773941\n",
      "Iteration 324, loss = 0.52740381\n",
      "Iteration 325, loss = 0.52706932\n",
      "Iteration 326, loss = 0.52673595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 327, loss = 0.52640369\n",
      "Iteration 328, loss = 0.52607253\n",
      "Iteration 329, loss = 0.52574247\n",
      "Iteration 330, loss = 0.52541350\n",
      "Iteration 331, loss = 0.52508562\n",
      "Iteration 332, loss = 0.52475881\n",
      "Iteration 333, loss = 0.52443306\n",
      "Iteration 334, loss = 0.52410839\n",
      "Iteration 335, loss = 0.52378479\n",
      "Iteration 336, loss = 0.52346224\n",
      "Iteration 337, loss = 0.52314075\n",
      "Iteration 338, loss = 0.52282031\n",
      "Iteration 339, loss = 0.52250089\n",
      "Iteration 340, loss = 0.52218253\n",
      "Iteration 341, loss = 0.52186521\n",
      "Iteration 342, loss = 0.52154890\n",
      "Iteration 343, loss = 0.52123360\n",
      "Iteration 344, loss = 0.52091930\n",
      "Iteration 345, loss = 0.52060600\n",
      "Iteration 346, loss = 0.52029370\n",
      "Iteration 347, loss = 0.51998237\n",
      "Iteration 348, loss = 0.51967204\n",
      "Iteration 349, loss = 0.51936270\n",
      "Iteration 350, loss = 0.51905434\n",
      "Iteration 351, loss = 0.51874696\n",
      "Iteration 352, loss = 0.51844053\n",
      "Iteration 353, loss = 0.51813504\n",
      "Iteration 354, loss = 0.51783049\n",
      "Iteration 355, loss = 0.51752691\n",
      "Iteration 356, loss = 0.51722427\n",
      "Iteration 357, loss = 0.51692256\n",
      "Iteration 358, loss = 0.51662177\n",
      "Iteration 359, loss = 0.51632191\n",
      "Iteration 360, loss = 0.51602294\n",
      "Iteration 361, loss = 0.51572487\n",
      "Iteration 362, loss = 0.51542769\n",
      "Iteration 363, loss = 0.51513141\n",
      "Iteration 364, loss = 0.51483601\n",
      "Iteration 365, loss = 0.51454151\n",
      "Iteration 366, loss = 0.51424785\n",
      "Iteration 367, loss = 0.51395507\n",
      "Iteration 368, loss = 0.51366317\n",
      "Iteration 369, loss = 0.51337213\n",
      "Iteration 370, loss = 0.51308197\n",
      "Iteration 371, loss = 0.51279267\n",
      "Iteration 372, loss = 0.51250423\n",
      "Iteration 373, loss = 0.51221664\n",
      "Iteration 374, loss = 0.51192990\n",
      "Iteration 375, loss = 0.51164401\n",
      "Iteration 376, loss = 0.51135896\n",
      "Iteration 377, loss = 0.51107473\n",
      "Iteration 378, loss = 0.51079134\n",
      "Iteration 379, loss = 0.51050877\n",
      "Iteration 380, loss = 0.51022702\n",
      "Iteration 381, loss = 0.50994611\n",
      "Iteration 382, loss = 0.50966604\n",
      "Iteration 383, loss = 0.50938677\n",
      "Iteration 384, loss = 0.50910831\n",
      "Iteration 385, loss = 0.50883064\n",
      "Iteration 386, loss = 0.50855377\n",
      "Iteration 387, loss = 0.50827769\n",
      "Iteration 388, loss = 0.50800240\n",
      "Iteration 389, loss = 0.50772789\n",
      "Iteration 390, loss = 0.50745416\n",
      "Iteration 391, loss = 0.50718125\n",
      "Iteration 392, loss = 0.50690913\n",
      "Iteration 393, loss = 0.50663778\n",
      "Iteration 394, loss = 0.50636719\n",
      "Iteration 395, loss = 0.50609738\n",
      "Iteration 396, loss = 0.50582831\n",
      "Iteration 397, loss = 0.50556000\n",
      "Iteration 398, loss = 0.50529244\n",
      "Iteration 399, loss = 0.50502562\n",
      "Iteration 400, loss = 0.50475956\n",
      "Iteration 401, loss = 0.50449420\n",
      "Iteration 402, loss = 0.50422957\n",
      "Iteration 403, loss = 0.50396567\n",
      "Iteration 404, loss = 0.50370249\n",
      "Iteration 405, loss = 0.50344003\n",
      "Iteration 406, loss = 0.50317829\n",
      "Iteration 407, loss = 0.50291727\n",
      "Iteration 408, loss = 0.50265696\n",
      "Iteration 409, loss = 0.50239736\n",
      "Iteration 410, loss = 0.50213844\n",
      "Iteration 411, loss = 0.50188026\n",
      "Iteration 412, loss = 0.50162279\n",
      "Iteration 413, loss = 0.50136603\n",
      "Iteration 414, loss = 0.50110996\n",
      "Iteration 415, loss = 0.50085459\n",
      "Iteration 416, loss = 0.50059990\n",
      "Iteration 417, loss = 0.50034590\n",
      "Iteration 418, loss = 0.50009258\n",
      "Iteration 419, loss = 0.49983995\n",
      "Iteration 420, loss = 0.49958799\n",
      "Iteration 421, loss = 0.49933674\n",
      "Iteration 422, loss = 0.49908616\n",
      "Iteration 423, loss = 0.49883628\n",
      "Iteration 424, loss = 0.49858706\n",
      "Iteration 425, loss = 0.49833851\n",
      "Iteration 426, loss = 0.49809061\n",
      "Iteration 427, loss = 0.49784337\n",
      "Iteration 428, loss = 0.49759677\n",
      "Iteration 429, loss = 0.49735079\n",
      "Iteration 430, loss = 0.49710545\n",
      "Iteration 431, loss = 0.49686074\n",
      "Iteration 432, loss = 0.49661666\n",
      "Iteration 433, loss = 0.49637321\n",
      "Iteration 434, loss = 0.49613038\n",
      "Iteration 435, loss = 0.49588815\n",
      "Iteration 436, loss = 0.49564653\n",
      "Iteration 437, loss = 0.49540552\n",
      "Iteration 438, loss = 0.49516512\n",
      "Iteration 439, loss = 0.49492533\n",
      "Iteration 440, loss = 0.49468615\n",
      "Iteration 441, loss = 0.49444756\n",
      "Iteration 442, loss = 0.49420957\n",
      "Iteration 443, loss = 0.49397219\n",
      "Iteration 444, loss = 0.49373542\n",
      "Iteration 445, loss = 0.49349923\n",
      "Iteration 446, loss = 0.49326360\n",
      "Iteration 447, loss = 0.49302856\n",
      "Iteration 448, loss = 0.49279410\n",
      "Iteration 449, loss = 0.49256023\n",
      "Iteration 450, loss = 0.49232695\n",
      "Iteration 451, loss = 0.49209424\n",
      "Iteration 452, loss = 0.49186210\n",
      "Iteration 453, loss = 0.49163054\n",
      "Iteration 454, loss = 0.49139954\n",
      "Iteration 455, loss = 0.49116912\n",
      "Iteration 456, loss = 0.49093926\n",
      "Iteration 457, loss = 0.49070996\n",
      "Iteration 458, loss = 0.49048123\n",
      "Iteration 459, loss = 0.49025306\n",
      "Iteration 460, loss = 0.49002544\n",
      "Iteration 461, loss = 0.48979837\n",
      "Iteration 462, loss = 0.48957185\n",
      "Iteration 463, loss = 0.48934587\n",
      "Iteration 464, loss = 0.48912043\n",
      "Iteration 465, loss = 0.48889554\n",
      "Iteration 466, loss = 0.48867119\n",
      "Iteration 467, loss = 0.48844737\n",
      "Iteration 468, loss = 0.48822409\n",
      "Iteration 469, loss = 0.48800134\n",
      "Iteration 470, loss = 0.48777912\n",
      "Iteration 471, loss = 0.48755742\n",
      "Iteration 472, loss = 0.48733623\n",
      "Iteration 473, loss = 0.48711556\n",
      "Iteration 474, loss = 0.48689541\n",
      "Iteration 475, loss = 0.48667577\n",
      "Iteration 476, loss = 0.48645664\n",
      "Iteration 477, loss = 0.48623803\n",
      "Iteration 478, loss = 0.48601992\n",
      "Iteration 479, loss = 0.48580232\n",
      "Iteration 480, loss = 0.48558523\n",
      "Iteration 481, loss = 0.48536867\n",
      "Iteration 482, loss = 0.48515263\n",
      "Iteration 483, loss = 0.48493708\n",
      "Iteration 484, loss = 0.48472203\n",
      "Iteration 485, loss = 0.48450744\n",
      "Iteration 486, loss = 0.48429332\n",
      "Iteration 487, loss = 0.48407969\n",
      "Iteration 488, loss = 0.48386658\n",
      "Iteration 489, loss = 0.48365396\n",
      "Iteration 490, loss = 0.48344182\n",
      "Iteration 491, loss = 0.48323016\n",
      "Iteration 492, loss = 0.48301894\n",
      "Iteration 493, loss = 0.48280820\n",
      "Iteration 494, loss = 0.48259794\n",
      "Iteration 495, loss = 0.48238814\n",
      "Iteration 496, loss = 0.48217881\n",
      "Iteration 497, loss = 0.48196999\n",
      "Iteration 498, loss = 0.48176164\n",
      "Iteration 499, loss = 0.48155376\n",
      "Iteration 500, loss = 0.48134634\n",
      "Iteration 501, loss = 0.48113939\n",
      "Iteration 502, loss = 0.48093289\n",
      "Iteration 503, loss = 0.48072686\n",
      "Iteration 504, loss = 0.48052128\n",
      "Iteration 505, loss = 0.48031615\n",
      "Iteration 506, loss = 0.48011144\n",
      "Iteration 507, loss = 0.47990719\n",
      "Iteration 508, loss = 0.47970338\n",
      "Iteration 509, loss = 0.47950002\n",
      "Iteration 510, loss = 0.47929711\n",
      "Iteration 511, loss = 0.47909465\n",
      "Iteration 512, loss = 0.47889264\n",
      "Iteration 513, loss = 0.47869107\n",
      "Iteration 514, loss = 0.47848994\n",
      "Iteration 515, loss = 0.47828919\n",
      "Iteration 516, loss = 0.47808887\n",
      "Iteration 517, loss = 0.47788898\n",
      "Iteration 518, loss = 0.47768951\n",
      "Iteration 519, loss = 0.47749047\n",
      "Iteration 520, loss = 0.47729185\n",
      "Iteration 521, loss = 0.47709361\n",
      "Iteration 522, loss = 0.47689577\n",
      "Iteration 523, loss = 0.47669832\n",
      "Iteration 524, loss = 0.47650127\n",
      "Iteration 525, loss = 0.47630459\n",
      "Iteration 526, loss = 0.47610830\n",
      "Iteration 527, loss = 0.47591240\n",
      "Iteration 528, loss = 0.47571691\n",
      "Iteration 529, loss = 0.47552179\n",
      "Iteration 530, loss = 0.47532704\n",
      "Iteration 531, loss = 0.47513269\n",
      "Iteration 532, loss = 0.47493873\n",
      "Iteration 533, loss = 0.47474516\n",
      "Iteration 534, loss = 0.47455199\n",
      "Iteration 535, loss = 0.47435918\n",
      "Iteration 536, loss = 0.47416676\n",
      "Iteration 537, loss = 0.47397483\n",
      "Iteration 538, loss = 0.47378329\n",
      "Iteration 539, loss = 0.47359216\n",
      "Iteration 540, loss = 0.47340143\n",
      "Iteration 541, loss = 0.47321110\n",
      "Iteration 542, loss = 0.47302115\n",
      "Iteration 543, loss = 0.47283160\n",
      "Iteration 544, loss = 0.47264242\n",
      "Iteration 545, loss = 0.47245361\n",
      "Iteration 546, loss = 0.47226519\n",
      "Iteration 547, loss = 0.47207716\n",
      "Iteration 548, loss = 0.47188957\n",
      "Iteration 549, loss = 0.47170241\n",
      "Iteration 550, loss = 0.47151564\n",
      "Iteration 551, loss = 0.47132925\n",
      "Iteration 552, loss = 0.47114325\n",
      "Iteration 553, loss = 0.47095764\n",
      "Iteration 554, loss = 0.47077241\n",
      "Iteration 555, loss = 0.47058756\n",
      "Iteration 556, loss = 0.47040309\n",
      "Iteration 557, loss = 0.47021904\n",
      "Iteration 558, loss = 0.47003535\n",
      "Iteration 559, loss = 0.46985200\n",
      "Iteration 560, loss = 0.46966901\n",
      "Iteration 561, loss = 0.46948639\n",
      "Iteration 562, loss = 0.46930409\n",
      "Iteration 563, loss = 0.46912215\n",
      "Iteration 564, loss = 0.46894056\n",
      "Iteration 565, loss = 0.46875934\n",
      "Iteration 566, loss = 0.46857846\n",
      "Iteration 567, loss = 0.46839794\n",
      "Iteration 568, loss = 0.46821778\n",
      "Iteration 569, loss = 0.46803797\n",
      "Iteration 570, loss = 0.46785851\n",
      "Iteration 571, loss = 0.46767938\n",
      "Iteration 572, loss = 0.46750057\n",
      "Iteration 573, loss = 0.46732218\n",
      "Iteration 574, loss = 0.46714415\n",
      "Iteration 575, loss = 0.46696647\n",
      "Iteration 576, loss = 0.46678915\n",
      "Iteration 577, loss = 0.46661214\n",
      "Iteration 578, loss = 0.46643546\n",
      "Iteration 579, loss = 0.46625912\n",
      "Iteration 580, loss = 0.46608312\n",
      "Iteration 581, loss = 0.46590748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 582, loss = 0.46573219\n",
      "Iteration 583, loss = 0.46555724\n",
      "Iteration 584, loss = 0.46538265\n",
      "Iteration 585, loss = 0.46520841\n",
      "Iteration 586, loss = 0.46503450\n",
      "Iteration 587, loss = 0.46486092\n",
      "Iteration 588, loss = 0.46468769\n",
      "Iteration 589, loss = 0.46451478\n",
      "Iteration 590, loss = 0.46434217\n",
      "Iteration 591, loss = 0.46416996\n",
      "Iteration 592, loss = 0.46399808\n",
      "Iteration 593, loss = 0.46382653\n",
      "Iteration 594, loss = 0.46365527\n",
      "Iteration 595, loss = 0.46348432\n",
      "Iteration 596, loss = 0.46331370\n",
      "Iteration 597, loss = 0.46314340\n",
      "Iteration 598, loss = 0.46297342\n",
      "Iteration 599, loss = 0.46280376\n",
      "Iteration 600, loss = 0.46263442\n",
      "Iteration 601, loss = 0.46246540\n",
      "Iteration 602, loss = 0.46229669\n",
      "Iteration 603, loss = 0.46212829\n",
      "Iteration 604, loss = 0.46196017\n",
      "Iteration 605, loss = 0.46179235\n",
      "Iteration 606, loss = 0.46162484\n",
      "Iteration 607, loss = 0.46145765\n",
      "Iteration 608, loss = 0.46129077\n",
      "Iteration 609, loss = 0.46112419\n",
      "Iteration 610, loss = 0.46095790\n",
      "Iteration 611, loss = 0.46079187\n",
      "Iteration 612, loss = 0.46062614\n",
      "Iteration 613, loss = 0.46046070\n",
      "Iteration 614, loss = 0.46029555\n",
      "Iteration 615, loss = 0.46013075\n",
      "Iteration 616, loss = 0.45996627\n",
      "Iteration 617, loss = 0.45980208\n",
      "Iteration 618, loss = 0.45963820\n",
      "Iteration 619, loss = 0.45947462\n",
      "Iteration 620, loss = 0.45931133\n",
      "Iteration 621, loss = 0.45914833\n",
      "Iteration 622, loss = 0.45898563\n",
      "Iteration 623, loss = 0.45882329\n",
      "Iteration 624, loss = 0.45866125\n",
      "Iteration 625, loss = 0.45849950\n",
      "Iteration 626, loss = 0.45833804\n",
      "Iteration 627, loss = 0.45817687\n",
      "Iteration 628, loss = 0.45801599\n",
      "Iteration 629, loss = 0.45785541\n",
      "Iteration 630, loss = 0.45769512\n",
      "Iteration 631, loss = 0.45753511\n",
      "Iteration 632, loss = 0.45737538\n",
      "Iteration 633, loss = 0.45721592\n",
      "Iteration 634, loss = 0.45705672\n",
      "Iteration 635, loss = 0.45689783\n",
      "Iteration 636, loss = 0.45673926\n",
      "Iteration 637, loss = 0.45658096\n",
      "Iteration 638, loss = 0.45642295\n",
      "Iteration 639, loss = 0.45626519\n",
      "Iteration 640, loss = 0.45610768\n",
      "Iteration 641, loss = 0.45595044\n",
      "Iteration 642, loss = 0.45579348\n",
      "Iteration 643, loss = 0.45563678\n",
      "Iteration 644, loss = 0.45548037\n",
      "Iteration 645, loss = 0.45532423\n",
      "Iteration 646, loss = 0.45516836\n",
      "Iteration 647, loss = 0.45501276\n",
      "Iteration 648, loss = 0.45485741\n",
      "Iteration 649, loss = 0.45470232\n",
      "Iteration 650, loss = 0.45454748\n",
      "Iteration 651, loss = 0.45439291\n",
      "Iteration 652, loss = 0.45423859\n",
      "Iteration 653, loss = 0.45408454\n",
      "Iteration 654, loss = 0.45393074\n",
      "Iteration 655, loss = 0.45377720\n",
      "Iteration 656, loss = 0.45362392\n",
      "Iteration 657, loss = 0.45347090\n",
      "Iteration 658, loss = 0.45331814\n",
      "Iteration 659, loss = 0.45316564\n",
      "Iteration 660, loss = 0.45301338\n",
      "Iteration 661, loss = 0.45286137\n",
      "Iteration 662, loss = 0.45270961\n",
      "Iteration 663, loss = 0.45255810\n",
      "Iteration 664, loss = 0.45240681\n",
      "Iteration 665, loss = 0.45225576\n",
      "Iteration 666, loss = 0.45210496\n",
      "Iteration 667, loss = 0.45195440\n",
      "Iteration 668, loss = 0.45180409\n",
      "Iteration 669, loss = 0.45165404\n",
      "Iteration 670, loss = 0.45150425\n",
      "Iteration 671, loss = 0.45135475\n",
      "Iteration 672, loss = 0.45120550\n",
      "Iteration 673, loss = 0.45105649\n",
      "Iteration 674, loss = 0.45090773\n",
      "Iteration 675, loss = 0.45075921\n",
      "Iteration 676, loss = 0.45061094\n",
      "Iteration 677, loss = 0.45046291\n",
      "Iteration 678, loss = 0.45031511\n",
      "Iteration 679, loss = 0.45016752\n",
      "Iteration 680, loss = 0.45002016\n",
      "Iteration 681, loss = 0.44987304\n",
      "Iteration 682, loss = 0.44972617\n",
      "Iteration 683, loss = 0.44957958\n",
      "Iteration 684, loss = 0.44943323\n",
      "Iteration 685, loss = 0.44928715\n",
      "Iteration 686, loss = 0.44914133\n",
      "Iteration 687, loss = 0.44899575\n",
      "Iteration 688, loss = 0.44885041\n",
      "Iteration 689, loss = 0.44870530\n",
      "Iteration 690, loss = 0.44856044\n",
      "Iteration 691, loss = 0.44841580\n",
      "Iteration 692, loss = 0.44827139\n",
      "Iteration 693, loss = 0.44812721\n",
      "Iteration 694, loss = 0.44798327\n",
      "Iteration 695, loss = 0.44783955\n",
      "Iteration 696, loss = 0.44769606\n",
      "Iteration 697, loss = 0.44755280\n",
      "Iteration 698, loss = 0.44740978\n",
      "Iteration 699, loss = 0.44726699\n",
      "Iteration 700, loss = 0.44712443\n",
      "Iteration 701, loss = 0.44698210\n",
      "Iteration 702, loss = 0.44683999\n",
      "Iteration 703, loss = 0.44669811\n",
      "Iteration 704, loss = 0.44655650\n",
      "Iteration 705, loss = 0.44641513\n",
      "Iteration 706, loss = 0.44627399\n",
      "Iteration 707, loss = 0.44613308\n",
      "Iteration 708, loss = 0.44599239\n",
      "Iteration 709, loss = 0.44585193\n",
      "Iteration 710, loss = 0.44571168\n",
      "Iteration 711, loss = 0.44557166\n",
      "Iteration 712, loss = 0.44543185\n",
      "Iteration 713, loss = 0.44529226\n",
      "Iteration 714, loss = 0.44515289\n",
      "Iteration 715, loss = 0.44501374\n",
      "Iteration 716, loss = 0.44487478\n",
      "Iteration 717, loss = 0.44473603\n",
      "Iteration 718, loss = 0.44459748\n",
      "Iteration 719, loss = 0.44445915\n",
      "Iteration 720, loss = 0.44432102\n",
      "Iteration 721, loss = 0.44418311\n",
      "Iteration 722, loss = 0.44404540\n",
      "Iteration 723, loss = 0.44390790\n",
      "Iteration 724, loss = 0.44377061\n",
      "Iteration 725, loss = 0.44363353\n",
      "Iteration 726, loss = 0.44349666\n",
      "Iteration 727, loss = 0.44335999\n",
      "Iteration 728, loss = 0.44322353\n",
      "Iteration 729, loss = 0.44308727\n",
      "Iteration 730, loss = 0.44295121\n",
      "Iteration 731, loss = 0.44281533\n",
      "Iteration 732, loss = 0.44267966\n",
      "Iteration 733, loss = 0.44254419\n",
      "Iteration 734, loss = 0.44240893\n",
      "Iteration 735, loss = 0.44227386\n",
      "Iteration 736, loss = 0.44213898\n",
      "Iteration 737, loss = 0.44200431\n",
      "Iteration 738, loss = 0.44186984\n",
      "Iteration 739, loss = 0.44173557\n",
      "Iteration 740, loss = 0.44160149\n",
      "Iteration 741, loss = 0.44146763\n",
      "Iteration 742, loss = 0.44133396\n",
      "Iteration 743, loss = 0.44120050\n",
      "Iteration 744, loss = 0.44106723\n",
      "Iteration 745, loss = 0.44093420\n",
      "Iteration 746, loss = 0.44080137\n",
      "Iteration 747, loss = 0.44066874\n",
      "Iteration 748, loss = 0.44053630\n",
      "Iteration 749, loss = 0.44040406\n",
      "Iteration 750, loss = 0.44027201\n",
      "Iteration 751, loss = 0.44014016\n",
      "Iteration 752, loss = 0.44000850\n",
      "Iteration 753, loss = 0.43987701\n",
      "Iteration 754, loss = 0.43974569\n",
      "Iteration 755, loss = 0.43961454\n",
      "Iteration 756, loss = 0.43948357\n",
      "Iteration 757, loss = 0.43935279\n",
      "Iteration 758, loss = 0.43922220\n",
      "Iteration 759, loss = 0.43909180\n",
      "Iteration 760, loss = 0.43896161\n",
      "Iteration 761, loss = 0.43883159\n",
      "Iteration 762, loss = 0.43870177\n",
      "Iteration 763, loss = 0.43857214\n",
      "Iteration 764, loss = 0.43844271\n",
      "Iteration 765, loss = 0.43831344\n",
      "Iteration 766, loss = 0.43818436\n",
      "Iteration 767, loss = 0.43805545\n",
      "Iteration 768, loss = 0.43792673\n",
      "Iteration 769, loss = 0.43779820\n",
      "Iteration 770, loss = 0.43766984\n",
      "Iteration 771, loss = 0.43754166\n",
      "Iteration 772, loss = 0.43741368\n",
      "Iteration 773, loss = 0.43728587\n",
      "Iteration 774, loss = 0.43715824\n",
      "Iteration 775, loss = 0.43703079\n",
      "Iteration 776, loss = 0.43690352\n",
      "Iteration 777, loss = 0.43677642\n",
      "Iteration 778, loss = 0.43664951\n",
      "Iteration 779, loss = 0.43652277\n",
      "Iteration 780, loss = 0.43639621\n",
      "Iteration 781, loss = 0.43626983\n",
      "Iteration 782, loss = 0.43614363\n",
      "Iteration 783, loss = 0.43601760\n",
      "Iteration 784, loss = 0.43589173\n",
      "Iteration 785, loss = 0.43576601\n",
      "Iteration 786, loss = 0.43564046\n",
      "Iteration 787, loss = 0.43551507\n",
      "Iteration 788, loss = 0.43538986\n",
      "Iteration 789, loss = 0.43526482\n",
      "Iteration 790, loss = 0.43513995\n",
      "Iteration 791, loss = 0.43501525\n",
      "Iteration 792, loss = 0.43489073\n",
      "Iteration 793, loss = 0.43476638\n",
      "Iteration 794, loss = 0.43464220\n",
      "Iteration 795, loss = 0.43451819\n",
      "Iteration 796, loss = 0.43439435\n",
      "Iteration 797, loss = 0.43427068\n",
      "Iteration 798, loss = 0.43414718\n",
      "Iteration 799, loss = 0.43402387\n",
      "Iteration 800, loss = 0.43390072\n",
      "Iteration 801, loss = 0.43377773\n",
      "Iteration 802, loss = 0.43365488\n",
      "Iteration 803, loss = 0.43353221\n",
      "Iteration 804, loss = 0.43340969\n",
      "Iteration 805, loss = 0.43328734\n",
      "Iteration 806, loss = 0.43316516\n",
      "Iteration 807, loss = 0.43304314\n",
      "Iteration 808, loss = 0.43292128\n",
      "Iteration 809, loss = 0.43279958\n",
      "Iteration 810, loss = 0.43267805\n",
      "Iteration 811, loss = 0.43255669\n",
      "Iteration 812, loss = 0.43243550\n",
      "Iteration 813, loss = 0.43231450\n",
      "Iteration 814, loss = 0.43219370\n",
      "Iteration 815, loss = 0.43207306\n",
      "Iteration 816, loss = 0.43195258\n",
      "Iteration 817, loss = 0.43183227\n",
      "Iteration 818, loss = 0.43171212\n",
      "Iteration 819, loss = 0.43159214\n",
      "Iteration 820, loss = 0.43147232\n",
      "Iteration 821, loss = 0.43135267\n",
      "Iteration 822, loss = 0.43123317\n",
      "Iteration 823, loss = 0.43111384\n",
      "Iteration 824, loss = 0.43099467\n",
      "Iteration 825, loss = 0.43087564\n",
      "Iteration 826, loss = 0.43075675\n",
      "Iteration 827, loss = 0.43063802\n",
      "Iteration 828, loss = 0.43051945\n",
      "Iteration 829, loss = 0.43040103\n",
      "Iteration 830, loss = 0.43028279\n",
      "Iteration 831, loss = 0.43016470\n",
      "Iteration 832, loss = 0.43004674\n",
      "Iteration 833, loss = 0.42992893\n",
      "Iteration 834, loss = 0.42981128\n",
      "Iteration 835, loss = 0.42969376\n",
      "Iteration 836, loss = 0.42957637\n",
      "Iteration 837, loss = 0.42945914\n",
      "Iteration 838, loss = 0.42934211\n",
      "Iteration 839, loss = 0.42922526\n",
      "Iteration 840, loss = 0.42910857\n",
      "Iteration 841, loss = 0.42899203\n",
      "Iteration 842, loss = 0.42887564\n",
      "Iteration 843, loss = 0.42875941\n",
      "Iteration 844, loss = 0.42864333\n",
      "Iteration 845, loss = 0.42852740\n",
      "Iteration 846, loss = 0.42841162\n",
      "Iteration 847, loss = 0.42829599\n",
      "Iteration 848, loss = 0.42818050\n",
      "Iteration 849, loss = 0.42806517\n",
      "Iteration 850, loss = 0.42794998\n",
      "Iteration 851, loss = 0.42783494\n",
      "Iteration 852, loss = 0.42772004\n",
      "Iteration 853, loss = 0.42760525\n",
      "Iteration 854, loss = 0.42749060\n",
      "Iteration 855, loss = 0.42737610\n",
      "Iteration 856, loss = 0.42726173\n",
      "Iteration 857, loss = 0.42714751\n",
      "Iteration 858, loss = 0.42703343\n",
      "Iteration 859, loss = 0.42691949\n",
      "Iteration 860, loss = 0.42680569\n",
      "Iteration 861, loss = 0.42669204\n",
      "Iteration 862, loss = 0.42657852\n",
      "Iteration 863, loss = 0.42646514\n",
      "Iteration 864, loss = 0.42635195\n",
      "Iteration 865, loss = 0.42623891\n",
      "Iteration 866, loss = 0.42612600\n",
      "Iteration 867, loss = 0.42601324\n",
      "Iteration 868, loss = 0.42590062\n",
      "Iteration 869, loss = 0.42578814\n",
      "Iteration 870, loss = 0.42567580\n",
      "Iteration 871, loss = 0.42556360\n",
      "Iteration 872, loss = 0.42545154\n",
      "Iteration 873, loss = 0.42533962\n",
      "Iteration 874, loss = 0.42522782\n",
      "Iteration 875, loss = 0.42511615\n",
      "Iteration 876, loss = 0.42500462\n",
      "Iteration 877, loss = 0.42489322\n",
      "Iteration 878, loss = 0.42478196\n",
      "Iteration 879, loss = 0.42467083\n",
      "Iteration 880, loss = 0.42455985\n",
      "Iteration 881, loss = 0.42444898\n",
      "Iteration 882, loss = 0.42433823\n",
      "Iteration 883, loss = 0.42422760\n",
      "Iteration 884, loss = 0.42411710\n",
      "Iteration 885, loss = 0.42400672\n",
      "Iteration 886, loss = 0.42389647\n",
      "Iteration 887, loss = 0.42378636\n",
      "Iteration 888, loss = 0.42367637\n",
      "Iteration 889, loss = 0.42356651\n",
      "Iteration 890, loss = 0.42345679\n",
      "Iteration 891, loss = 0.42334720\n",
      "Iteration 892, loss = 0.42323774\n",
      "Iteration 893, loss = 0.42312841\n",
      "Iteration 894, loss = 0.42301921\n",
      "Iteration 895, loss = 0.42291014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 896, loss = 0.42280120\n",
      "Iteration 897, loss = 0.42269237\n",
      "Iteration 898, loss = 0.42258366\n",
      "Iteration 899, loss = 0.42247506\n",
      "Iteration 900, loss = 0.42236657\n",
      "Iteration 901, loss = 0.42225819\n",
      "Iteration 902, loss = 0.42214994\n",
      "Iteration 903, loss = 0.42204181\n",
      "Iteration 904, loss = 0.42193381\n",
      "Iteration 905, loss = 0.42182595\n",
      "Iteration 906, loss = 0.42171821\n",
      "Iteration 907, loss = 0.42161060\n",
      "Iteration 908, loss = 0.42150315\n",
      "Iteration 909, loss = 0.42139585\n",
      "Iteration 910, loss = 0.42128867\n",
      "Iteration 911, loss = 0.42118163\n",
      "Iteration 912, loss = 0.42107471\n",
      "Iteration 913, loss = 0.42096792\n",
      "Iteration 914, loss = 0.42086124\n",
      "Iteration 915, loss = 0.42075465\n",
      "Iteration 916, loss = 0.42064818\n",
      "Iteration 917, loss = 0.42054182\n",
      "Iteration 918, loss = 0.42043559\n",
      "Iteration 919, loss = 0.42032947\n",
      "Iteration 920, loss = 0.42022348\n",
      "Iteration 921, loss = 0.42011760\n",
      "Iteration 922, loss = 0.42001182\n",
      "Iteration 923, loss = 0.41990614\n",
      "Iteration 924, loss = 0.41980057\n",
      "Iteration 925, loss = 0.41969512\n",
      "Iteration 926, loss = 0.41958976\n",
      "Iteration 927, loss = 0.41948450\n",
      "Iteration 928, loss = 0.41937936\n",
      "Iteration 929, loss = 0.41927433\n",
      "Iteration 930, loss = 0.41916942\n",
      "Iteration 931, loss = 0.41906462\n",
      "Iteration 932, loss = 0.41895994\n",
      "Iteration 933, loss = 0.41885537\n",
      "Iteration 934, loss = 0.41875092\n",
      "Iteration 935, loss = 0.41864658\n",
      "Iteration 936, loss = 0.41854236\n",
      "Iteration 937, loss = 0.41843825\n",
      "Iteration 938, loss = 0.41833426\n",
      "Iteration 939, loss = 0.41823039\n",
      "Iteration 940, loss = 0.41812664\n",
      "Iteration 941, loss = 0.41802301\n",
      "Iteration 942, loss = 0.41791950\n",
      "Iteration 943, loss = 0.41781610\n",
      "Iteration 944, loss = 0.41771281\n",
      "Iteration 945, loss = 0.41760965\n",
      "Iteration 946, loss = 0.41750659\n",
      "Iteration 947, loss = 0.41740370\n",
      "Iteration 948, loss = 0.41730094\n",
      "Iteration 949, loss = 0.41719831\n",
      "Iteration 950, loss = 0.41709579\n",
      "Iteration 951, loss = 0.41699340\n",
      "Iteration 952, loss = 0.41689112\n",
      "Iteration 953, loss = 0.41678896\n",
      "Iteration 954, loss = 0.41668692\n",
      "Iteration 955, loss = 0.41658502\n",
      "Iteration 956, loss = 0.41648323\n",
      "Iteration 957, loss = 0.41638156\n",
      "Iteration 958, loss = 0.41628000\n",
      "Iteration 959, loss = 0.41617855\n",
      "Iteration 960, loss = 0.41607722\n",
      "Iteration 961, loss = 0.41597606\n",
      "Iteration 962, loss = 0.41587502\n",
      "Iteration 963, loss = 0.41577409\n",
      "Iteration 964, loss = 0.41567330\n",
      "Iteration 965, loss = 0.41557262\n",
      "Iteration 966, loss = 0.41547206\n",
      "Iteration 967, loss = 0.41537161\n",
      "Iteration 968, loss = 0.41527128\n",
      "Iteration 969, loss = 0.41517106\n",
      "Iteration 970, loss = 0.41507095\n",
      "Iteration 971, loss = 0.41497093\n",
      "Iteration 972, loss = 0.41487103\n",
      "Iteration 973, loss = 0.41477123\n",
      "Iteration 974, loss = 0.41467154\n",
      "Iteration 975, loss = 0.41457196\n",
      "Iteration 976, loss = 0.41447246\n",
      "Iteration 977, loss = 0.41437306\n",
      "Iteration 978, loss = 0.41427377\n",
      "Iteration 979, loss = 0.41417458\n",
      "Iteration 980, loss = 0.41407548\n",
      "Iteration 981, loss = 0.41397649\n",
      "Iteration 982, loss = 0.41387757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.913333\n",
      "Training set loss: 0.413878\n",
      "training: adam\n",
      "Iteration 1, loss = 1.05300692\n",
      "Iteration 2, loss = 1.00270013\n",
      "Iteration 3, loss = 0.95682085\n",
      "Iteration 4, loss = 0.91151167\n",
      "Iteration 5, loss = 0.86576511\n",
      "Iteration 6, loss = 0.82112995\n",
      "Iteration 7, loss = 0.77847520\n",
      "Iteration 8, loss = 0.73645678\n",
      "Iteration 9, loss = 0.69496229\n",
      "Iteration 10, loss = 0.65467840\n",
      "Iteration 11, loss = 0.61636567\n",
      "Iteration 12, loss = 0.58054656\n",
      "Iteration 13, loss = 0.54745038\n",
      "Iteration 14, loss = 0.51718493\n",
      "Iteration 15, loss = 0.48968444\n",
      "Iteration 16, loss = 0.46476917\n",
      "Iteration 17, loss = 0.44221236\n",
      "Iteration 18, loss = 0.42179290\n",
      "Iteration 19, loss = 0.40314052\n",
      "Iteration 20, loss = 0.38593677\n",
      "Iteration 21, loss = 0.36999165\n",
      "Iteration 22, loss = 0.35521206\n",
      "Iteration 23, loss = 0.34140816\n",
      "Iteration 24, loss = 0.32837464\n",
      "Iteration 25, loss = 0.31593766\n",
      "Iteration 26, loss = 0.30402141\n",
      "Iteration 27, loss = 0.29258774\n",
      "Iteration 28, loss = 0.28159377\n",
      "Iteration 29, loss = 0.27101125\n",
      "Iteration 30, loss = 0.26082378\n",
      "Iteration 31, loss = 0.25101834\n",
      "Iteration 32, loss = 0.24159608\n",
      "Iteration 33, loss = 0.23254591\n",
      "Iteration 34, loss = 0.22386227\n",
      "Iteration 35, loss = 0.21554094\n",
      "Iteration 36, loss = 0.20757572\n",
      "Iteration 37, loss = 0.19994423\n",
      "Iteration 38, loss = 0.19264524\n",
      "Iteration 39, loss = 0.18567642\n",
      "Iteration 40, loss = 0.17902749\n",
      "Iteration 41, loss = 0.17268590\n",
      "Iteration 42, loss = 0.16663760\n",
      "Iteration 43, loss = 0.16087787\n",
      "Iteration 44, loss = 0.15539803\n",
      "Iteration 45, loss = 0.15018384\n",
      "Iteration 46, loss = 0.14522187\n",
      "Iteration 47, loss = 0.14050016\n",
      "Iteration 48, loss = 0.13600805\n",
      "Iteration 49, loss = 0.13174114\n",
      "Iteration 50, loss = 0.12768924\n",
      "Iteration 51, loss = 0.12384273\n",
      "Iteration 52, loss = 0.12019833\n",
      "Iteration 53, loss = 0.11674967\n",
      "Iteration 54, loss = 0.11348743\n",
      "Iteration 55, loss = 0.11040013\n",
      "Iteration 56, loss = 0.10748306\n",
      "Iteration 57, loss = 0.10472537\n",
      "Iteration 58, loss = 0.10211987\n",
      "Iteration 59, loss = 0.09965920\n",
      "Iteration 60, loss = 0.09733799\n",
      "Iteration 61, loss = 0.09514610\n",
      "Iteration 62, loss = 0.09307671\n",
      "Iteration 63, loss = 0.09112328\n",
      "Iteration 64, loss = 0.08927881\n",
      "Iteration 65, loss = 0.08753591\n",
      "Iteration 66, loss = 0.08588897\n",
      "Iteration 67, loss = 0.08433234\n",
      "Iteration 68, loss = 0.08285993\n",
      "Iteration 69, loss = 0.08146676\n",
      "Iteration 70, loss = 0.08014810\n",
      "Iteration 71, loss = 0.07889799\n",
      "Iteration 72, loss = 0.07771260\n",
      "Iteration 73, loss = 0.07658724\n",
      "Iteration 74, loss = 0.07551787\n",
      "Iteration 75, loss = 0.07450468\n",
      "Iteration 76, loss = 0.07353900\n",
      "Iteration 77, loss = 0.07261923\n",
      "Iteration 78, loss = 0.07174308\n",
      "Iteration 79, loss = 0.07090642\n",
      "Iteration 80, loss = 0.07010690\n",
      "Iteration 81, loss = 0.06934225\n",
      "Iteration 82, loss = 0.06861006\n",
      "Iteration 83, loss = 0.06790735\n",
      "Iteration 84, loss = 0.06723319\n",
      "Iteration 85, loss = 0.06658483\n",
      "Iteration 86, loss = 0.06596145\n",
      "Iteration 87, loss = 0.06536122\n",
      "Iteration 88, loss = 0.06478243\n",
      "Iteration 89, loss = 0.06422377\n",
      "Iteration 90, loss = 0.06368453\n",
      "Iteration 91, loss = 0.06316397\n",
      "Iteration 92, loss = 0.06266180\n",
      "Iteration 93, loss = 0.06217640\n",
      "Iteration 94, loss = 0.06170580\n",
      "Iteration 95, loss = 0.06125061\n",
      "Iteration 96, loss = 0.06080839\n",
      "Iteration 97, loss = 0.06037899\n",
      "Iteration 98, loss = 0.05996241\n",
      "Iteration 99, loss = 0.05955715\n",
      "Iteration 100, loss = 0.05916341\n",
      "Iteration 101, loss = 0.05878127\n",
      "Iteration 102, loss = 0.05841105\n",
      "Iteration 103, loss = 0.05805106\n",
      "Iteration 104, loss = 0.05770012\n",
      "Iteration 105, loss = 0.05736001\n",
      "Iteration 106, loss = 0.05702833\n",
      "Iteration 107, loss = 0.05670329\n",
      "Iteration 108, loss = 0.05638863\n",
      "Iteration 109, loss = 0.05608246\n",
      "Iteration 110, loss = 0.05578276\n",
      "Iteration 111, loss = 0.05549293\n",
      "Iteration 112, loss = 0.05520856\n",
      "Iteration 113, loss = 0.05493104\n",
      "Iteration 114, loss = 0.05466012\n",
      "Iteration 115, loss = 0.05439574\n",
      "Iteration 116, loss = 0.05413915\n",
      "Iteration 117, loss = 0.05388929\n",
      "Iteration 118, loss = 0.05364456\n",
      "Iteration 119, loss = 0.05340641\n",
      "Iteration 120, loss = 0.05317236\n",
      "Iteration 121, loss = 0.05294306\n",
      "Iteration 122, loss = 0.05271940\n",
      "Iteration 123, loss = 0.05250179\n",
      "Iteration 124, loss = 0.05228851\n",
      "Iteration 125, loss = 0.05207827\n",
      "Iteration 126, loss = 0.05187400\n",
      "Iteration 127, loss = 0.05167415\n",
      "Iteration 128, loss = 0.05147776\n",
      "Iteration 129, loss = 0.05128567\n",
      "Iteration 130, loss = 0.05109655\n",
      "Iteration 131, loss = 0.05091129\n",
      "Iteration 132, loss = 0.05073016\n",
      "Iteration 133, loss = 0.05055451\n",
      "Iteration 134, loss = 0.05038130\n",
      "Iteration 135, loss = 0.05020957\n",
      "Iteration 136, loss = 0.05004171\n",
      "Iteration 137, loss = 0.04987855\n",
      "Iteration 138, loss = 0.04971873\n",
      "Iteration 139, loss = 0.04956154\n",
      "Iteration 140, loss = 0.04940630\n",
      "Iteration 141, loss = 0.04925439\n",
      "Iteration 142, loss = 0.04910593\n",
      "Iteration 143, loss = 0.04896006\n",
      "Iteration 144, loss = 0.04881680\n",
      "Iteration 145, loss = 0.04867647\n",
      "Iteration 146, loss = 0.04853885\n",
      "Iteration 147, loss = 0.04840322\n",
      "Iteration 148, loss = 0.04827077\n",
      "Iteration 149, loss = 0.04814034\n",
      "Iteration 150, loss = 0.04801149\n",
      "Iteration 151, loss = 0.04788647\n",
      "Iteration 152, loss = 0.04776367\n",
      "Iteration 153, loss = 0.04764251\n",
      "Iteration 154, loss = 0.04752286\n",
      "Iteration 155, loss = 0.04740562\n",
      "Iteration 156, loss = 0.04729114\n",
      "Iteration 157, loss = 0.04717767\n",
      "Iteration 158, loss = 0.04706629\n",
      "Iteration 159, loss = 0.04695781\n",
      "Iteration 160, loss = 0.04685136\n",
      "Iteration 161, loss = 0.04674644\n",
      "Iteration 162, loss = 0.04664283\n",
      "Iteration 163, loss = 0.04654072\n",
      "Iteration 164, loss = 0.04644181\n",
      "Iteration 165, loss = 0.04634300\n",
      "Iteration 166, loss = 0.04624763\n",
      "Iteration 167, loss = 0.04615298\n",
      "Iteration 168, loss = 0.04605910\n",
      "Iteration 169, loss = 0.04596757\n",
      "Iteration 170, loss = 0.04587744\n",
      "Iteration 171, loss = 0.04578831\n",
      "Iteration 172, loss = 0.04570108\n",
      "Iteration 173, loss = 0.04561463\n",
      "Iteration 174, loss = 0.04553083\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.980000\n",
      "Training set loss: 0.045531\n",
      "\n",
      "learning on dataset digits\n",
      "training: constant learning-rate\n",
      "Iteration 1, loss = 2.16498699\n",
      "Iteration 2, loss = 1.74530399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.35033016\n",
      "Iteration 4, loss = 1.02209366\n",
      "Iteration 5, loss = 0.78633710\n",
      "Iteration 6, loss = 0.63162398\n",
      "Iteration 7, loss = 0.52768449\n",
      "Iteration 8, loss = 0.44966734\n",
      "Iteration 9, loss = 0.39599675\n",
      "Iteration 10, loss = 0.35910073\n",
      "Iteration 11, loss = 0.32342909\n",
      "Iteration 12, loss = 0.29851906\n",
      "Iteration 13, loss = 0.27719303\n",
      "Iteration 14, loss = 0.25945514\n",
      "Iteration 15, loss = 0.24380177\n",
      "Iteration 16, loss = 0.22995671\n",
      "Iteration 17, loss = 0.21926040\n",
      "Iteration 18, loss = 0.21013782\n",
      "Iteration 19, loss = 0.19933330\n",
      "Iteration 20, loss = 0.19267995\n",
      "Iteration 21, loss = 0.18506944\n",
      "Iteration 22, loss = 0.17803897\n",
      "Iteration 23, loss = 0.17285625\n",
      "Iteration 24, loss = 0.16716209\n",
      "Iteration 25, loss = 0.16166121\n",
      "Iteration 26, loss = 0.15874452\n",
      "Iteration 27, loss = 0.15420410\n",
      "Iteration 28, loss = 0.14980178\n",
      "Iteration 29, loss = 0.14511583\n",
      "Iteration 30, loss = 0.14259421\n",
      "Iteration 31, loss = 0.13847250\n",
      "Iteration 32, loss = 0.13540302\n",
      "Iteration 33, loss = 0.13327335\n",
      "Iteration 34, loss = 0.13097354\n",
      "Iteration 35, loss = 0.12866558\n",
      "Iteration 36, loss = 0.12590667\n",
      "Iteration 37, loss = 0.12239277\n",
      "Iteration 38, loss = 0.12076932\n",
      "Iteration 39, loss = 0.11907401\n",
      "Iteration 40, loss = 0.11764229\n",
      "Iteration 41, loss = 0.11553560\n",
      "Iteration 42, loss = 0.11344602\n",
      "Iteration 43, loss = 0.11131037\n",
      "Iteration 44, loss = 0.10937329\n",
      "Iteration 45, loss = 0.10708640\n",
      "Iteration 46, loss = 0.10708888\n",
      "Iteration 47, loss = 0.10529455\n",
      "Iteration 48, loss = 0.10238278\n",
      "Iteration 49, loss = 0.10102945\n",
      "Iteration 50, loss = 0.10057132\n",
      "Iteration 51, loss = 0.09831381\n",
      "Iteration 52, loss = 0.09658239\n",
      "Iteration 53, loss = 0.09621629\n",
      "Iteration 54, loss = 0.09465787\n",
      "Iteration 55, loss = 0.09387531\n",
      "Iteration 56, loss = 0.09239810\n",
      "Iteration 57, loss = 0.09092286\n",
      "Iteration 58, loss = 0.08979073\n",
      "Iteration 59, loss = 0.08858941\n",
      "Iteration 60, loss = 0.08907982\n",
      "Iteration 61, loss = 0.08747867\n",
      "Iteration 62, loss = 0.08557326\n",
      "Iteration 63, loss = 0.08425338\n",
      "Iteration 64, loss = 0.08401039\n",
      "Iteration 65, loss = 0.08229614\n",
      "Iteration 66, loss = 0.08179469\n",
      "Iteration 67, loss = 0.08023440\n",
      "Iteration 68, loss = 0.07988230\n",
      "Iteration 69, loss = 0.07892006\n",
      "Iteration 70, loss = 0.07746839\n",
      "Iteration 71, loss = 0.07739962\n",
      "Iteration 72, loss = 0.07650759\n",
      "Iteration 73, loss = 0.07571359\n",
      "Iteration 74, loss = 0.07461541\n",
      "Iteration 75, loss = 0.07336294\n",
      "Iteration 76, loss = 0.07333056\n",
      "Iteration 77, loss = 0.07197396\n",
      "Iteration 78, loss = 0.07235172\n",
      "Iteration 79, loss = 0.07056655\n",
      "Iteration 80, loss = 0.07095431\n",
      "Iteration 81, loss = 0.06951204\n",
      "Iteration 82, loss = 0.06862762\n",
      "Iteration 83, loss = 0.06839593\n",
      "Iteration 84, loss = 0.06729402\n",
      "Iteration 85, loss = 0.06603662\n",
      "Iteration 86, loss = 0.06568560\n",
      "Iteration 87, loss = 0.06509332\n",
      "Iteration 88, loss = 0.06502096\n",
      "Iteration 89, loss = 0.06429180\n",
      "Iteration 90, loss = 0.06419809\n",
      "Iteration 91, loss = 0.06327113\n",
      "Iteration 92, loss = 0.06236513\n",
      "Iteration 93, loss = 0.06180175\n",
      "Iteration 94, loss = 0.06132965\n",
      "Iteration 95, loss = 0.06107310\n",
      "Iteration 96, loss = 0.06025700\n",
      "Iteration 97, loss = 0.05961185\n",
      "Iteration 98, loss = 0.05944942\n",
      "Iteration 99, loss = 0.05848761\n",
      "Iteration 100, loss = 0.05829811\n",
      "Iteration 101, loss = 0.05754788\n",
      "Iteration 102, loss = 0.05754779\n",
      "Iteration 103, loss = 0.05608570\n",
      "Iteration 104, loss = 0.05725867\n",
      "Iteration 105, loss = 0.05527524\n",
      "Iteration 106, loss = 0.05532556\n",
      "Iteration 107, loss = 0.05491274\n",
      "Iteration 108, loss = 0.05439134\n",
      "Iteration 109, loss = 0.05381726\n",
      "Iteration 110, loss = 0.05333293\n",
      "Iteration 111, loss = 0.05273940\n",
      "Iteration 112, loss = 0.05301174\n",
      "Iteration 113, loss = 0.05200888\n",
      "Iteration 114, loss = 0.05194732\n",
      "Iteration 115, loss = 0.05090401\n",
      "Iteration 116, loss = 0.05104885\n",
      "Iteration 117, loss = 0.05016337\n",
      "Iteration 118, loss = 0.04981476\n",
      "Iteration 119, loss = 0.04963180\n",
      "Iteration 120, loss = 0.04998483\n",
      "Iteration 121, loss = 0.04857417\n",
      "Iteration 122, loss = 0.04882359\n",
      "Iteration 123, loss = 0.04759870\n",
      "Iteration 124, loss = 0.04742696\n",
      "Iteration 125, loss = 0.04807861\n",
      "Iteration 126, loss = 0.04707977\n",
      "Iteration 127, loss = 0.04679688\n",
      "Iteration 128, loss = 0.04655974\n",
      "Iteration 129, loss = 0.04535563\n",
      "Iteration 130, loss = 0.04563025\n",
      "Iteration 131, loss = 0.04594915\n",
      "Iteration 132, loss = 0.04553307\n",
      "Iteration 133, loss = 0.04483690\n",
      "Iteration 134, loss = 0.04475029\n",
      "Iteration 135, loss = 0.04439603\n",
      "Iteration 136, loss = 0.04330217\n",
      "Iteration 137, loss = 0.04334214\n",
      "Iteration 138, loss = 0.04306304\n",
      "Iteration 139, loss = 0.04310811\n",
      "Iteration 140, loss = 0.04224584\n",
      "Iteration 141, loss = 0.04226891\n",
      "Iteration 142, loss = 0.04191759\n",
      "Iteration 143, loss = 0.04163256\n",
      "Iteration 144, loss = 0.04120347\n",
      "Iteration 145, loss = 0.04158778\n",
      "Iteration 146, loss = 0.04048230\n",
      "Iteration 147, loss = 0.04095239\n",
      "Iteration 148, loss = 0.04033507\n",
      "Iteration 149, loss = 0.03967420\n",
      "Iteration 150, loss = 0.03944944\n",
      "Iteration 151, loss = 0.03945216\n",
      "Iteration 152, loss = 0.03947306\n",
      "Iteration 153, loss = 0.03969199\n",
      "Iteration 154, loss = 0.03908281\n",
      "Iteration 155, loss = 0.03842460\n",
      "Iteration 156, loss = 0.03796942\n",
      "Iteration 157, loss = 0.03757053\n",
      "Iteration 158, loss = 0.03784392\n",
      "Iteration 159, loss = 0.03762265\n",
      "Iteration 160, loss = 0.03724375\n",
      "Iteration 161, loss = 0.03726417\n",
      "Iteration 162, loss = 0.03680841\n",
      "Iteration 163, loss = 0.03640311\n",
      "Iteration 164, loss = 0.03648826\n",
      "Iteration 165, loss = 0.03583253\n",
      "Iteration 166, loss = 0.03573202\n",
      "Iteration 167, loss = 0.03512175\n",
      "Iteration 168, loss = 0.03532971\n",
      "Iteration 169, loss = 0.03487716\n",
      "Iteration 170, loss = 0.03499794\n",
      "Iteration 171, loss = 0.03468134\n",
      "Iteration 172, loss = 0.03458787\n",
      "Iteration 173, loss = 0.03418060\n",
      "Iteration 174, loss = 0.03435586\n",
      "Iteration 175, loss = 0.03401404\n",
      "Iteration 176, loss = 0.03339063\n",
      "Iteration 177, loss = 0.03363726\n",
      "Iteration 178, loss = 0.03314638\n",
      "Iteration 179, loss = 0.03290184\n",
      "Iteration 180, loss = 0.03275944\n",
      "Iteration 181, loss = 0.03271316\n",
      "Iteration 182, loss = 0.03223870\n",
      "Iteration 183, loss = 0.03200183\n",
      "Iteration 184, loss = 0.03222657\n",
      "Iteration 185, loss = 0.03197111\n",
      "Iteration 186, loss = 0.03149742\n",
      "Iteration 187, loss = 0.03184922\n",
      "Iteration 188, loss = 0.03144920\n",
      "Iteration 189, loss = 0.03118975\n",
      "Iteration 190, loss = 0.03113611\n",
      "Iteration 191, loss = 0.03092330\n",
      "Iteration 192, loss = 0.03053712\n",
      "Iteration 193, loss = 0.03028015\n",
      "Iteration 194, loss = 0.03027096\n",
      "Iteration 195, loss = 0.03007165\n",
      "Iteration 196, loss = 0.02978221\n",
      "Iteration 197, loss = 0.02982907\n",
      "Iteration 198, loss = 0.02955864\n",
      "Iteration 199, loss = 0.02911117\n",
      "Iteration 200, loss = 0.02960542\n",
      "Iteration 201, loss = 0.02900652\n",
      "Iteration 202, loss = 0.02882300\n",
      "Iteration 203, loss = 0.02861926\n",
      "Iteration 204, loss = 0.02870810\n",
      "Iteration 205, loss = 0.02864185\n",
      "Iteration 206, loss = 0.02851413\n",
      "Iteration 207, loss = 0.02797513\n",
      "Iteration 208, loss = 0.02804296\n",
      "Iteration 209, loss = 0.02778233\n",
      "Iteration 210, loss = 0.02736573\n",
      "Iteration 211, loss = 0.02768752\n",
      "Iteration 212, loss = 0.02742222\n",
      "Iteration 213, loss = 0.02720636\n",
      "Iteration 214, loss = 0.02720448\n",
      "Iteration 215, loss = 0.02727963\n",
      "Iteration 216, loss = 0.02683233\n",
      "Iteration 217, loss = 0.02662574\n",
      "Iteration 218, loss = 0.02656884\n",
      "Iteration 219, loss = 0.02639615\n",
      "Iteration 220, loss = 0.02642345\n",
      "Iteration 221, loss = 0.02619636\n",
      "Iteration 222, loss = 0.02643170\n",
      "Iteration 223, loss = 0.02571742\n",
      "Iteration 224, loss = 0.02582322\n",
      "Iteration 225, loss = 0.02552162\n",
      "Iteration 226, loss = 0.02531354\n",
      "Iteration 227, loss = 0.02521255\n",
      "Iteration 228, loss = 0.02509752\n",
      "Iteration 229, loss = 0.02499280\n",
      "Iteration 230, loss = 0.02489727\n",
      "Iteration 231, loss = 0.02476278\n",
      "Iteration 232, loss = 0.02475142\n",
      "Iteration 233, loss = 0.02446200\n",
      "Iteration 234, loss = 0.02453657\n",
      "Iteration 235, loss = 0.02448618\n",
      "Iteration 236, loss = 0.02408102\n",
      "Iteration 237, loss = 0.02400327\n",
      "Iteration 238, loss = 0.02383260\n",
      "Iteration 239, loss = 0.02387853\n",
      "Iteration 240, loss = 0.02381829\n",
      "Iteration 241, loss = 0.02385824\n",
      "Iteration 242, loss = 0.02350918\n",
      "Iteration 243, loss = 0.02383921\n",
      "Iteration 244, loss = 0.02316803\n",
      "Iteration 245, loss = 0.02322943\n",
      "Iteration 246, loss = 0.02309459\n",
      "Iteration 247, loss = 0.02303311\n",
      "Iteration 248, loss = 0.02278186\n",
      "Iteration 249, loss = 0.02279671\n",
      "Iteration 250, loss = 0.02280479\n",
      "Iteration 251, loss = 0.02275956\n",
      "Iteration 252, loss = 0.02247327\n",
      "Iteration 253, loss = 0.02240053\n",
      "Iteration 254, loss = 0.02204907\n",
      "Iteration 255, loss = 0.02215677\n",
      "Iteration 256, loss = 0.02243945\n",
      "Iteration 257, loss = 0.02211744\n",
      "Iteration 258, loss = 0.02178905\n",
      "Iteration 259, loss = 0.02168546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 260, loss = 0.02152832\n",
      "Iteration 261, loss = 0.02130605\n",
      "Iteration 262, loss = 0.02122868\n",
      "Iteration 263, loss = 0.02129158\n",
      "Iteration 264, loss = 0.02109658\n",
      "Iteration 265, loss = 0.02096417\n",
      "Iteration 266, loss = 0.02082436\n",
      "Iteration 267, loss = 0.02112102\n",
      "Iteration 268, loss = 0.02075650\n",
      "Iteration 269, loss = 0.02074914\n",
      "Iteration 270, loss = 0.02048330\n",
      "Iteration 271, loss = 0.02075509\n",
      "Iteration 272, loss = 0.02031076\n",
      "Iteration 273, loss = 0.02044365\n",
      "Iteration 274, loss = 0.02017100\n",
      "Iteration 275, loss = 0.02019884\n",
      "Iteration 276, loss = 0.02026111\n",
      "Iteration 277, loss = 0.02013594\n",
      "Iteration 278, loss = 0.02026186\n",
      "Iteration 279, loss = 0.02003610\n",
      "Iteration 280, loss = 0.01956865\n",
      "Iteration 281, loss = 0.01971913\n",
      "Iteration 282, loss = 0.01958778\n",
      "Iteration 283, loss = 0.01911345\n",
      "Iteration 284, loss = 0.01945197\n",
      "Iteration 285, loss = 0.01937069\n",
      "Iteration 286, loss = 0.01915067\n",
      "Iteration 287, loss = 0.01929086\n",
      "Iteration 288, loss = 0.01920142\n",
      "Iteration 289, loss = 0.01901817\n",
      "Iteration 290, loss = 0.01890359\n",
      "Iteration 291, loss = 0.01873483\n",
      "Iteration 292, loss = 0.01860464\n",
      "Iteration 293, loss = 0.01851124\n",
      "Iteration 294, loss = 0.01849434\n",
      "Iteration 295, loss = 0.01861113\n",
      "Iteration 296, loss = 0.01824453\n",
      "Iteration 297, loss = 0.01825420\n",
      "Iteration 298, loss = 0.01836762\n",
      "Iteration 299, loss = 0.01818377\n",
      "Iteration 300, loss = 0.01801968\n",
      "Iteration 301, loss = 0.01804129\n",
      "Iteration 302, loss = 0.01791751\n",
      "Iteration 303, loss = 0.01804687\n",
      "Iteration 304, loss = 0.01781527\n",
      "Iteration 305, loss = 0.01773428\n",
      "Iteration 306, loss = 0.01760659\n",
      "Iteration 307, loss = 0.01754036\n",
      "Iteration 308, loss = 0.01735476\n",
      "Iteration 309, loss = 0.01752930\n",
      "Iteration 310, loss = 0.01723961\n",
      "Iteration 311, loss = 0.01731658\n",
      "Iteration 312, loss = 0.01706340\n",
      "Iteration 313, loss = 0.01713817\n",
      "Iteration 314, loss = 0.01686948\n",
      "Iteration 315, loss = 0.01709471\n",
      "Iteration 316, loss = 0.01690659\n",
      "Iteration 317, loss = 0.01700995\n",
      "Iteration 318, loss = 0.01675899\n",
      "Iteration 319, loss = 0.01691208\n",
      "Iteration 320, loss = 0.01668516\n",
      "Iteration 321, loss = 0.01669953\n",
      "Iteration 322, loss = 0.01652758\n",
      "Iteration 323, loss = 0.01626349\n",
      "Iteration 324, loss = 0.01645705\n",
      "Iteration 325, loss = 0.01646350\n",
      "Iteration 326, loss = 0.01623070\n",
      "Iteration 327, loss = 0.01620146\n",
      "Iteration 328, loss = 0.01624381\n",
      "Iteration 329, loss = 0.01617312\n",
      "Iteration 330, loss = 0.01603730\n",
      "Iteration 331, loss = 0.01594505\n",
      "Iteration 332, loss = 0.01586500\n",
      "Iteration 333, loss = 0.01575196\n",
      "Iteration 334, loss = 0.01574995\n",
      "Iteration 335, loss = 0.01562385\n",
      "Iteration 336, loss = 0.01574151\n",
      "Iteration 337, loss = 0.01587791\n",
      "Iteration 338, loss = 0.01541523\n",
      "Iteration 339, loss = 0.01550392\n",
      "Iteration 340, loss = 0.01534337\n",
      "Iteration 341, loss = 0.01532537\n",
      "Iteration 342, loss = 0.01520869\n",
      "Iteration 343, loss = 0.01518758\n",
      "Iteration 344, loss = 0.01512355\n",
      "Iteration 345, loss = 0.01502611\n",
      "Iteration 346, loss = 0.01489961\n",
      "Iteration 347, loss = 0.01496480\n",
      "Iteration 348, loss = 0.01490130\n",
      "Iteration 349, loss = 0.01501452\n",
      "Iteration 350, loss = 0.01489118\n",
      "Iteration 351, loss = 0.01478542\n",
      "Iteration 352, loss = 0.01482021\n",
      "Iteration 353, loss = 0.01467023\n",
      "Iteration 354, loss = 0.01461795\n",
      "Iteration 355, loss = 0.01457543\n",
      "Iteration 356, loss = 0.01450448\n",
      "Iteration 357, loss = 0.01428273\n",
      "Iteration 358, loss = 0.01412620\n",
      "Iteration 359, loss = 0.01441596\n",
      "Iteration 360, loss = 0.01414592\n",
      "Iteration 361, loss = 0.01433804\n",
      "Iteration 362, loss = 0.01411213\n",
      "Iteration 363, loss = 0.01415772\n",
      "Iteration 364, loss = 0.01410194\n",
      "Iteration 365, loss = 0.01419327\n",
      "Iteration 366, loss = 0.01401318\n",
      "Iteration 367, loss = 0.01385976\n",
      "Iteration 368, loss = 0.01391991\n",
      "Iteration 369, loss = 0.01382997\n",
      "Iteration 370, loss = 0.01362431\n",
      "Iteration 371, loss = 0.01371622\n",
      "Iteration 372, loss = 0.01367463\n",
      "Iteration 373, loss = 0.01359612\n",
      "Iteration 374, loss = 0.01349286\n",
      "Iteration 375, loss = 0.01350906\n",
      "Iteration 376, loss = 0.01352188\n",
      "Iteration 377, loss = 0.01340880\n",
      "Iteration 378, loss = 0.01348195\n",
      "Iteration 379, loss = 0.01323400\n",
      "Iteration 380, loss = 0.01318361\n",
      "Iteration 381, loss = 0.01336841\n",
      "Iteration 382, loss = 0.01328835\n",
      "Iteration 383, loss = 0.01305042\n",
      "Iteration 384, loss = 0.01316021\n",
      "Iteration 385, loss = 0.01321811\n",
      "Iteration 386, loss = 0.01294491\n",
      "Iteration 387, loss = 0.01303133\n",
      "Iteration 388, loss = 0.01293885\n",
      "Iteration 389, loss = 0.01292839\n",
      "Iteration 390, loss = 0.01286880\n",
      "Iteration 391, loss = 0.01272003\n",
      "Iteration 392, loss = 0.01285663\n",
      "Iteration 393, loss = 0.01273363\n",
      "Iteration 394, loss = 0.01267148\n",
      "Iteration 395, loss = 0.01265703\n",
      "Iteration 396, loss = 0.01256597\n",
      "Iteration 397, loss = 0.01252497\n",
      "Iteration 398, loss = 0.01251868\n",
      "Iteration 399, loss = 0.01247307\n",
      "Iteration 400, loss = 0.01250816\n",
      "Iteration 401, loss = 0.01237097\n",
      "Iteration 402, loss = 0.01237136\n",
      "Iteration 403, loss = 0.01232586\n",
      "Iteration 404, loss = 0.01237544\n",
      "Iteration 405, loss = 0.01216009\n",
      "Iteration 406, loss = 0.01217362\n",
      "Iteration 407, loss = 0.01220245\n",
      "Iteration 408, loss = 0.01218060\n",
      "Iteration 409, loss = 0.01199371\n",
      "Iteration 410, loss = 0.01206826\n",
      "Iteration 411, loss = 0.01195327\n",
      "Iteration 412, loss = 0.01186773\n",
      "Iteration 413, loss = 0.01190948\n",
      "Iteration 414, loss = 0.01184905\n",
      "Iteration 415, loss = 0.01190274\n",
      "Iteration 416, loss = 0.01173942\n",
      "Iteration 417, loss = 0.01164648\n",
      "Iteration 418, loss = 0.01163008\n",
      "Iteration 419, loss = 0.01172901\n",
      "Iteration 420, loss = 0.01157425\n",
      "Iteration 421, loss = 0.01156236\n",
      "Iteration 422, loss = 0.01155746\n",
      "Iteration 423, loss = 0.01161785\n",
      "Iteration 424, loss = 0.01154908\n",
      "Iteration 425, loss = 0.01142369\n",
      "Iteration 426, loss = 0.01143741\n",
      "Iteration 427, loss = 0.01146162\n",
      "Iteration 428, loss = 0.01132494\n",
      "Iteration 429, loss = 0.01131299\n",
      "Iteration 430, loss = 0.01129559\n",
      "Iteration 431, loss = 0.01131205\n",
      "Iteration 432, loss = 0.01125442\n",
      "Iteration 433, loss = 0.01114795\n",
      "Iteration 434, loss = 0.01109577\n",
      "Iteration 435, loss = 0.01101839\n",
      "Iteration 436, loss = 0.01103933\n",
      "Iteration 437, loss = 0.01106968\n",
      "Iteration 438, loss = 0.01097765\n",
      "Iteration 439, loss = 0.01093152\n",
      "Iteration 440, loss = 0.01092551\n",
      "Iteration 441, loss = 0.01078054\n",
      "Iteration 442, loss = 0.01081067\n",
      "Iteration 443, loss = 0.01092431\n",
      "Iteration 444, loss = 0.01065785\n",
      "Iteration 445, loss = 0.01077053\n",
      "Iteration 446, loss = 0.01081972\n",
      "Iteration 447, loss = 0.01063999\n",
      "Iteration 448, loss = 0.01069416\n",
      "Iteration 449, loss = 0.01064727\n",
      "Iteration 450, loss = 0.01058582\n",
      "Iteration 451, loss = 0.01050140\n",
      "Iteration 452, loss = 0.01049049\n",
      "Iteration 453, loss = 0.01049436\n",
      "Iteration 454, loss = 0.01040999\n",
      "Iteration 455, loss = 0.01047323\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 1.000000\n",
      "Training set loss: 0.010473\n",
      "training: constant with momentum\n",
      "Iteration 1, loss = 1.94238221\n",
      "Iteration 2, loss = 0.67913298\n",
      "Iteration 3, loss = 0.28130619\n",
      "Iteration 4, loss = 0.19674528\n",
      "Iteration 5, loss = 0.15501248\n",
      "Iteration 6, loss = 0.12093421\n",
      "Iteration 7, loss = 0.09747044\n",
      "Iteration 8, loss = 0.08059499\n",
      "Iteration 9, loss = 0.07003580\n",
      "Iteration 10, loss = 0.06036287\n",
      "Iteration 11, loss = 0.05587655\n",
      "Iteration 12, loss = 0.05304725\n",
      "Iteration 13, loss = 0.05098537\n",
      "Iteration 14, loss = 0.04729062\n",
      "Iteration 15, loss = 0.04129728\n",
      "Iteration 16, loss = 0.03560138\n",
      "Iteration 17, loss = 0.03526845\n",
      "Iteration 18, loss = 0.03438420\n",
      "Iteration 19, loss = 0.03767033\n",
      "Iteration 20, loss = 0.03117248\n",
      "Iteration 21, loss = 0.03026350\n",
      "Iteration 22, loss = 0.02764813\n",
      "Iteration 23, loss = 0.02546351\n",
      "Iteration 24, loss = 0.02132695\n",
      "Iteration 25, loss = 0.02062711\n",
      "Iteration 26, loss = 0.01932537\n",
      "Iteration 27, loss = 0.01873234\n",
      "Iteration 28, loss = 0.01820819\n",
      "Iteration 29, loss = 0.01819218\n",
      "Iteration 30, loss = 0.01707523\n",
      "Iteration 31, loss = 0.01614730\n",
      "Iteration 32, loss = 0.01488048\n",
      "Iteration 33, loss = 0.01442725\n",
      "Iteration 34, loss = 0.01469079\n",
      "Iteration 35, loss = 0.01316747\n",
      "Iteration 36, loss = 0.01324977\n",
      "Iteration 37, loss = 0.01235918\n",
      "Iteration 38, loss = 0.01226651\n",
      "Iteration 39, loss = 0.01220291\n",
      "Iteration 40, loss = 0.01159487\n",
      "Iteration 41, loss = 0.01179890\n",
      "Iteration 42, loss = 0.01171029\n",
      "Iteration 43, loss = 0.01172573\n",
      "Iteration 44, loss = 0.01045805\n",
      "Iteration 45, loss = 0.01016403\n",
      "Iteration 46, loss = 0.01020108\n",
      "Iteration 47, loss = 0.00941769\n",
      "Iteration 48, loss = 0.00838996\n",
      "Iteration 49, loss = 0.00839287\n",
      "Iteration 50, loss = 0.00819317\n",
      "Iteration 51, loss = 0.00774008\n",
      "Iteration 52, loss = 0.00755298\n",
      "Iteration 53, loss = 0.00725135\n",
      "Iteration 54, loss = 0.00776415\n",
      "Iteration 55, loss = 0.00691098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56, loss = 0.00694474\n",
      "Iteration 57, loss = 0.00678754\n",
      "Iteration 58, loss = 0.00692740\n",
      "Iteration 59, loss = 0.00647471\n",
      "Iteration 60, loss = 0.00611788\n",
      "Iteration 61, loss = 0.00596831\n",
      "Iteration 62, loss = 0.00572886\n",
      "Iteration 63, loss = 0.00578571\n",
      "Iteration 64, loss = 0.00567181\n",
      "Iteration 65, loss = 0.00537117\n",
      "Iteration 66, loss = 0.00522246\n",
      "Iteration 67, loss = 0.00517043\n",
      "Iteration 68, loss = 0.00504730\n",
      "Iteration 69, loss = 0.00485173\n",
      "Iteration 70, loss = 0.00513171\n",
      "Iteration 71, loss = 0.00491846\n",
      "Iteration 72, loss = 0.00467479\n",
      "Iteration 73, loss = 0.00452780\n",
      "Iteration 74, loss = 0.00439970\n",
      "Iteration 75, loss = 0.00447187\n",
      "Iteration 76, loss = 0.00431771\n",
      "Iteration 77, loss = 0.00439819\n",
      "Iteration 78, loss = 0.00430839\n",
      "Iteration 79, loss = 0.00410993\n",
      "Iteration 80, loss = 0.00403998\n",
      "Iteration 81, loss = 0.00404303\n",
      "Iteration 82, loss = 0.00387875\n",
      "Iteration 83, loss = 0.00385048\n",
      "Iteration 84, loss = 0.00388751\n",
      "Iteration 85, loss = 0.00373039\n",
      "Iteration 86, loss = 0.00366336\n",
      "Iteration 87, loss = 0.00361291\n",
      "Iteration 88, loss = 0.00361577\n",
      "Iteration 89, loss = 0.00371165\n",
      "Iteration 90, loss = 0.00352901\n",
      "Iteration 91, loss = 0.00350651\n",
      "Iteration 92, loss = 0.00338928\n",
      "Iteration 93, loss = 0.00341287\n",
      "Iteration 94, loss = 0.00329155\n",
      "Iteration 95, loss = 0.00326604\n",
      "Iteration 96, loss = 0.00321307\n",
      "Iteration 97, loss = 0.00313163\n",
      "Iteration 98, loss = 0.00317417\n",
      "Iteration 99, loss = 0.00305119\n",
      "Iteration 100, loss = 0.00302550\n",
      "Iteration 101, loss = 0.00302139\n",
      "Iteration 102, loss = 0.00297283\n",
      "Iteration 103, loss = 0.00291672\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 1.000000\n",
      "Training set loss: 0.002917\n",
      "training: constant with Nesterov's momentum\n",
      "Iteration 1, loss = 1.80716619\n",
      "Iteration 2, loss = 0.52351487\n",
      "Iteration 3, loss = 0.22289129\n",
      "Iteration 4, loss = 0.15015899\n",
      "Iteration 5, loss = 0.11903042\n",
      "Iteration 6, loss = 0.10084898\n",
      "Iteration 7, loss = 0.09181560\n",
      "Iteration 8, loss = 0.07722248\n",
      "Iteration 9, loss = 0.06639360\n",
      "Iteration 10, loss = 0.06426221\n",
      "Iteration 11, loss = 0.05592063\n",
      "Iteration 12, loss = 0.04981325\n",
      "Iteration 13, loss = 0.04807141\n",
      "Iteration 14, loss = 0.04414123\n",
      "Iteration 15, loss = 0.04289821\n",
      "Iteration 16, loss = 0.03870224\n",
      "Iteration 17, loss = 0.03594533\n",
      "Iteration 18, loss = 0.03386465\n",
      "Iteration 19, loss = 0.03322058\n",
      "Iteration 20, loss = 0.03222873\n",
      "Iteration 21, loss = 0.02859296\n",
      "Iteration 22, loss = 0.02693005\n",
      "Iteration 23, loss = 0.02654318\n",
      "Iteration 24, loss = 0.02449836\n",
      "Iteration 25, loss = 0.02379817\n",
      "Iteration 26, loss = 0.02197868\n",
      "Iteration 27, loss = 0.02127737\n",
      "Iteration 28, loss = 0.02106824\n",
      "Iteration 29, loss = 0.01987292\n",
      "Iteration 30, loss = 0.01848127\n",
      "Iteration 31, loss = 0.01780117\n",
      "Iteration 32, loss = 0.01674050\n",
      "Iteration 33, loss = 0.01599648\n",
      "Iteration 34, loss = 0.01570591\n",
      "Iteration 35, loss = 0.01489586\n",
      "Iteration 36, loss = 0.01451466\n",
      "Iteration 37, loss = 0.01351594\n",
      "Iteration 38, loss = 0.01355057\n",
      "Iteration 39, loss = 0.01300040\n",
      "Iteration 40, loss = 0.01293277\n",
      "Iteration 41, loss = 0.01272983\n",
      "Iteration 42, loss = 0.01138628\n",
      "Iteration 43, loss = 0.01113758\n",
      "Iteration 44, loss = 0.01083067\n",
      "Iteration 45, loss = 0.01012067\n",
      "Iteration 46, loss = 0.01041462\n",
      "Iteration 47, loss = 0.00983660\n",
      "Iteration 48, loss = 0.00921491\n",
      "Iteration 49, loss = 0.00894779\n",
      "Iteration 50, loss = 0.00897813\n",
      "Iteration 51, loss = 0.00859326\n",
      "Iteration 52, loss = 0.00847525\n",
      "Iteration 53, loss = 0.00814966\n",
      "Iteration 54, loss = 0.00832979\n",
      "Iteration 55, loss = 0.00790984\n",
      "Iteration 56, loss = 0.00762892\n",
      "Iteration 57, loss = 0.00729059\n",
      "Iteration 58, loss = 0.00727643\n",
      "Iteration 59, loss = 0.00695939\n",
      "Iteration 60, loss = 0.00683185\n",
      "Iteration 61, loss = 0.00671484\n",
      "Iteration 62, loss = 0.00637854\n",
      "Iteration 63, loss = 0.00627511\n",
      "Iteration 64, loss = 0.00622654\n",
      "Iteration 65, loss = 0.00619005\n",
      "Iteration 66, loss = 0.00582342\n",
      "Iteration 67, loss = 0.00575442\n",
      "Iteration 68, loss = 0.00567624\n",
      "Iteration 69, loss = 0.00550452\n",
      "Iteration 70, loss = 0.00562141\n",
      "Iteration 71, loss = 0.00553506\n",
      "Iteration 72, loss = 0.00522682\n",
      "Iteration 73, loss = 0.00509408\n",
      "Iteration 74, loss = 0.00495889\n",
      "Iteration 75, loss = 0.00495071\n",
      "Iteration 76, loss = 0.00485350\n",
      "Iteration 77, loss = 0.00479913\n",
      "Iteration 78, loss = 0.00472752\n",
      "Iteration 79, loss = 0.00461603\n",
      "Iteration 80, loss = 0.00447737\n",
      "Iteration 81, loss = 0.00443442\n",
      "Iteration 82, loss = 0.00438026\n",
      "Iteration 83, loss = 0.00431030\n",
      "Iteration 84, loss = 0.00430504\n",
      "Iteration 85, loss = 0.00419877\n",
      "Iteration 86, loss = 0.00407762\n",
      "Iteration 87, loss = 0.00399076\n",
      "Iteration 88, loss = 0.00405316\n",
      "Iteration 89, loss = 0.00406563\n",
      "Iteration 90, loss = 0.00402517\n",
      "Iteration 91, loss = 0.00386382\n",
      "Iteration 92, loss = 0.00373269\n",
      "Iteration 93, loss = 0.00380193\n",
      "Iteration 94, loss = 0.00369387\n",
      "Iteration 95, loss = 0.00362287\n",
      "Iteration 96, loss = 0.00352923\n",
      "Iteration 97, loss = 0.00345706\n",
      "Iteration 98, loss = 0.00347761\n",
      "Iteration 99, loss = 0.00337685\n",
      "Iteration 100, loss = 0.00332075\n",
      "Iteration 101, loss = 0.00334605\n",
      "Iteration 102, loss = 0.00325211\n",
      "Iteration 103, loss = 0.00321132\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 1.000000\n",
      "Training set loss: 0.003211\n",
      "training: inv-scaling learning-rate\n",
      "Iteration 1, loss = 2.16498699\n",
      "Iteration 2, loss = 1.90870076\n",
      "Iteration 3, loss = 1.90020042\n",
      "Iteration 4, loss = 1.89396007\n",
      "Iteration 5, loss = 1.88874063\n",
      "Iteration 6, loss = 1.88420884\n",
      "Iteration 7, loss = 1.88008198\n",
      "Iteration 8, loss = 1.87632970\n",
      "Iteration 9, loss = 1.87283244\n",
      "Iteration 10, loss = 1.86956168\n",
      "Iteration 11, loss = 1.86646120\n",
      "Iteration 12, loss = 1.86352830\n",
      "Iteration 13, loss = 1.86073380\n",
      "Iteration 14, loss = 1.85802879\n",
      "Iteration 15, loss = 1.85546541\n",
      "Iteration 16, loss = 1.85296455\n",
      "Iteration 17, loss = 1.85055307\n",
      "Iteration 18, loss = 1.84823971\n",
      "Iteration 19, loss = 1.84597284\n",
      "Iteration 20, loss = 1.84375621\n",
      "Iteration 21, loss = 1.84162808\n",
      "Iteration 22, loss = 1.83952686\n",
      "Iteration 23, loss = 1.83749175\n",
      "Iteration 24, loss = 1.83548885\n",
      "Iteration 25, loss = 1.83354117\n",
      "Iteration 26, loss = 1.83163157\n",
      "Iteration 27, loss = 1.82975799\n",
      "Iteration 28, loss = 1.82792425\n",
      "Iteration 29, loss = 1.82612052\n",
      "Iteration 30, loss = 1.82434522\n",
      "Iteration 31, loss = 1.82260947\n",
      "Iteration 32, loss = 1.82089044\n",
      "Iteration 33, loss = 1.81921385\n",
      "Iteration 34, loss = 1.81755508\n",
      "Iteration 35, loss = 1.81591149\n",
      "Iteration 36, loss = 1.81430143\n",
      "Iteration 37, loss = 1.81271937\n",
      "Iteration 38, loss = 1.81115748\n",
      "Iteration 39, loss = 1.80961505\n",
      "Iteration 40, loss = 1.80807800\n",
      "Iteration 41, loss = 1.80657755\n",
      "Iteration 42, loss = 1.80509236\n",
      "Iteration 43, loss = 1.80362457\n",
      "Iteration 44, loss = 1.80217673\n",
      "Iteration 45, loss = 1.80073744\n",
      "Iteration 46, loss = 1.79932099\n",
      "Iteration 47, loss = 1.79791841\n",
      "Iteration 48, loss = 1.79653333\n",
      "Iteration 49, loss = 1.79515679\n",
      "Iteration 50, loss = 1.79379562\n",
      "Iteration 51, loss = 1.79244900\n",
      "Iteration 52, loss = 1.79111982\n",
      "Iteration 53, loss = 1.78980221\n",
      "Iteration 54, loss = 1.78849656\n",
      "Iteration 55, loss = 1.78720038\n",
      "Iteration 56, loss = 1.78591643\n",
      "Iteration 57, loss = 1.78465102\n",
      "Iteration 58, loss = 1.78338176\n",
      "Iteration 59, loss = 1.78213472\n",
      "Iteration 60, loss = 1.78090498\n",
      "Iteration 61, loss = 1.77967285\n",
      "Iteration 62, loss = 1.77844750\n",
      "Iteration 63, loss = 1.77723861\n",
      "Iteration 64, loss = 1.77604526\n",
      "Iteration 65, loss = 1.77485570\n",
      "Iteration 66, loss = 1.77366849\n",
      "Iteration 67, loss = 1.77249900\n",
      "Iteration 68, loss = 1.77133538\n",
      "Iteration 69, loss = 1.77017246\n",
      "Iteration 70, loss = 1.76902925\n",
      "Iteration 71, loss = 1.76788966\n",
      "Iteration 72, loss = 1.76675757\n",
      "Iteration 73, loss = 1.76563542\n",
      "Iteration 74, loss = 1.76451627\n",
      "Iteration 75, loss = 1.76340284\n",
      "Iteration 76, loss = 1.76230331\n",
      "Iteration 77, loss = 1.76120621\n",
      "Iteration 78, loss = 1.76011935\n",
      "Iteration 79, loss = 1.75903650\n",
      "Iteration 80, loss = 1.75796089\n",
      "Iteration 81, loss = 1.75689492\n",
      "Iteration 82, loss = 1.75583046\n",
      "Iteration 83, loss = 1.75477146\n",
      "Iteration 84, loss = 1.75372171\n",
      "Iteration 85, loss = 1.75267681\n",
      "Iteration 86, loss = 1.75164496\n",
      "Iteration 87, loss = 1.75061311\n",
      "Iteration 88, loss = 1.74958655\n",
      "Iteration 89, loss = 1.74856215\n",
      "Iteration 90, loss = 1.74755564\n",
      "Iteration 91, loss = 1.74654579\n",
      "Iteration 92, loss = 1.74553883\n",
      "Iteration 93, loss = 1.74454888\n",
      "Iteration 94, loss = 1.74355099\n",
      "Iteration 95, loss = 1.74256480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 96, loss = 1.74158415\n",
      "Iteration 97, loss = 1.74060671\n",
      "Iteration 98, loss = 1.73963496\n",
      "Iteration 99, loss = 1.73867134\n",
      "Iteration 100, loss = 1.73770373\n",
      "Iteration 101, loss = 1.73675242\n",
      "Iteration 102, loss = 1.73579705\n",
      "Iteration 103, loss = 1.73484656\n",
      "Iteration 104, loss = 1.73390272\n",
      "Iteration 105, loss = 1.73296570\n",
      "Iteration 106, loss = 1.73202927\n",
      "Iteration 107, loss = 1.73110500\n",
      "Iteration 108, loss = 1.73017292\n",
      "Iteration 109, loss = 1.72924995\n",
      "Iteration 110, loss = 1.72833766\n",
      "Iteration 111, loss = 1.72742023\n",
      "Iteration 112, loss = 1.72651042\n",
      "Iteration 113, loss = 1.72560933\n",
      "Iteration 114, loss = 1.72471141\n",
      "Iteration 115, loss = 1.72381654\n",
      "Iteration 116, loss = 1.72292001\n",
      "Iteration 117, loss = 1.72202817\n",
      "Iteration 118, loss = 1.72114131\n",
      "Iteration 119, loss = 1.72025877\n",
      "Iteration 120, loss = 1.71938683\n",
      "Iteration 121, loss = 1.71851043\n",
      "Iteration 122, loss = 1.71764045\n",
      "Iteration 123, loss = 1.71677214\n",
      "Iteration 124, loss = 1.71591252\n",
      "Iteration 125, loss = 1.71505229\n",
      "Iteration 126, loss = 1.71419832\n",
      "Iteration 127, loss = 1.71334119\n",
      "Iteration 128, loss = 1.71249343\n",
      "Iteration 129, loss = 1.71164699\n",
      "Iteration 130, loss = 1.71080489\n",
      "Iteration 131, loss = 1.70996200\n",
      "Iteration 132, loss = 1.70912387\n",
      "Iteration 133, loss = 1.70829309\n",
      "Iteration 134, loss = 1.70746397\n",
      "Iteration 135, loss = 1.70663507\n",
      "Iteration 136, loss = 1.70580856\n",
      "Iteration 137, loss = 1.70498833\n",
      "Iteration 138, loss = 1.70417188\n",
      "Iteration 139, loss = 1.70335643\n",
      "Iteration 140, loss = 1.70254260\n",
      "Iteration 141, loss = 1.70173589\n",
      "Iteration 142, loss = 1.70093217\n",
      "Iteration 143, loss = 1.70012325\n",
      "Iteration 144, loss = 1.69932313\n",
      "Iteration 145, loss = 1.69852049\n",
      "Iteration 146, loss = 1.69772446\n",
      "Iteration 147, loss = 1.69693438\n",
      "Iteration 148, loss = 1.69614193\n",
      "Iteration 149, loss = 1.69535399\n",
      "Iteration 150, loss = 1.69456560\n",
      "Iteration 151, loss = 1.69378370\n",
      "Iteration 152, loss = 1.69300156\n",
      "Iteration 153, loss = 1.69222078\n",
      "Iteration 154, loss = 1.69144454\n",
      "Iteration 155, loss = 1.69067404\n",
      "Iteration 156, loss = 1.68989985\n",
      "Iteration 157, loss = 1.68913366\n",
      "Iteration 158, loss = 1.68836444\n",
      "Iteration 159, loss = 1.68760238\n",
      "Iteration 160, loss = 1.68683804\n",
      "Iteration 161, loss = 1.68608277\n",
      "Iteration 162, loss = 1.68532307\n",
      "Iteration 163, loss = 1.68456905\n",
      "Iteration 164, loss = 1.68381655\n",
      "Iteration 165, loss = 1.68306602\n",
      "Iteration 166, loss = 1.68232662\n",
      "Iteration 167, loss = 1.68157301\n",
      "Iteration 168, loss = 1.68083213\n",
      "Iteration 169, loss = 1.68009001\n",
      "Iteration 170, loss = 1.67934837\n",
      "Iteration 171, loss = 1.67861070\n",
      "Iteration 172, loss = 1.67787531\n",
      "Iteration 173, loss = 1.67714090\n",
      "Iteration 174, loss = 1.67641429\n",
      "Iteration 175, loss = 1.67568295\n",
      "Iteration 176, loss = 1.67495646\n",
      "Iteration 177, loss = 1.67423232\n",
      "Iteration 178, loss = 1.67350737\n",
      "Iteration 179, loss = 1.67278890\n",
      "Iteration 180, loss = 1.67206728\n",
      "Iteration 181, loss = 1.67135148\n",
      "Iteration 182, loss = 1.67063675\n",
      "Iteration 183, loss = 1.66992838\n",
      "Iteration 184, loss = 1.66921162\n",
      "Iteration 185, loss = 1.66850679\n",
      "Iteration 186, loss = 1.66779601\n",
      "Iteration 187, loss = 1.66708924\n",
      "Iteration 188, loss = 1.66638609\n",
      "Iteration 189, loss = 1.66568903\n",
      "Iteration 190, loss = 1.66498554\n",
      "Iteration 191, loss = 1.66429147\n",
      "Iteration 192, loss = 1.66359448\n",
      "Iteration 193, loss = 1.66289981\n",
      "Iteration 194, loss = 1.66220525\n",
      "Iteration 195, loss = 1.66151817\n",
      "Iteration 196, loss = 1.66082877\n",
      "Iteration 197, loss = 1.66013905\n",
      "Iteration 198, loss = 1.65945611\n",
      "Iteration 199, loss = 1.65877108\n",
      "Iteration 200, loss = 1.65809053\n",
      "Iteration 201, loss = 1.65740969\n",
      "Iteration 202, loss = 1.65673013\n",
      "Iteration 203, loss = 1.65605459\n",
      "Iteration 204, loss = 1.65537974\n",
      "Iteration 205, loss = 1.65470937\n",
      "Iteration 206, loss = 1.65403330\n",
      "Iteration 207, loss = 1.65336652\n",
      "Iteration 208, loss = 1.65269652\n",
      "Iteration 209, loss = 1.65203078\n",
      "Iteration 210, loss = 1.65136618\n",
      "Iteration 211, loss = 1.65070211\n",
      "Iteration 212, loss = 1.65003678\n",
      "Iteration 213, loss = 1.64937675\n",
      "Iteration 214, loss = 1.64871815\n",
      "Iteration 215, loss = 1.64806042\n",
      "Iteration 216, loss = 1.64740142\n",
      "Iteration 217, loss = 1.64674590\n",
      "Iteration 218, loss = 1.64609587\n",
      "Iteration 219, loss = 1.64544380\n",
      "Iteration 220, loss = 1.64479009\n",
      "Iteration 221, loss = 1.64414481\n",
      "Iteration 222, loss = 1.64349564\n",
      "Iteration 223, loss = 1.64285101\n",
      "Iteration 224, loss = 1.64220816\n",
      "Iteration 225, loss = 1.64156313\n",
      "Iteration 226, loss = 1.64091714\n",
      "Iteration 227, loss = 1.64027747\n",
      "Iteration 228, loss = 1.63964121\n",
      "Iteration 229, loss = 1.63900109\n",
      "Iteration 230, loss = 1.63836524\n",
      "Iteration 231, loss = 1.63772965\n",
      "Iteration 232, loss = 1.63709675\n",
      "Iteration 233, loss = 1.63646380\n",
      "Iteration 234, loss = 1.63583157\n",
      "Iteration 235, loss = 1.63520508\n",
      "Iteration 236, loss = 1.63457548\n",
      "Iteration 237, loss = 1.63394598\n",
      "Iteration 238, loss = 1.63332093\n",
      "Iteration 239, loss = 1.63269652\n",
      "Iteration 240, loss = 1.63207284\n",
      "Iteration 241, loss = 1.63145158\n",
      "Iteration 242, loss = 1.63083518\n",
      "Iteration 243, loss = 1.63021448\n",
      "Iteration 244, loss = 1.62959370\n",
      "Iteration 245, loss = 1.62897848\n",
      "Iteration 246, loss = 1.62836435\n",
      "Iteration 247, loss = 1.62774870\n",
      "Iteration 248, loss = 1.62713445\n",
      "Iteration 249, loss = 1.62652621\n",
      "Iteration 250, loss = 1.62591875\n",
      "Iteration 251, loss = 1.62530934\n",
      "Iteration 252, loss = 1.62470310\n",
      "Iteration 253, loss = 1.62409489\n",
      "Iteration 254, loss = 1.62349000\n",
      "Iteration 255, loss = 1.62288661\n",
      "Iteration 256, loss = 1.62228519\n",
      "Iteration 257, loss = 1.62168578\n",
      "Iteration 258, loss = 1.62108098\n",
      "Iteration 259, loss = 1.62048127\n",
      "Iteration 260, loss = 1.61988297\n",
      "Iteration 261, loss = 1.61928805\n",
      "Iteration 262, loss = 1.61869245\n",
      "Iteration 263, loss = 1.61809786\n",
      "Iteration 264, loss = 1.61750768\n",
      "Iteration 265, loss = 1.61691374\n",
      "Iteration 266, loss = 1.61632250\n",
      "Iteration 267, loss = 1.61573079\n",
      "Iteration 268, loss = 1.61513976\n",
      "Iteration 269, loss = 1.61455130\n",
      "Iteration 270, loss = 1.61396546\n",
      "Iteration 271, loss = 1.61337962\n",
      "Iteration 272, loss = 1.61279544\n",
      "Iteration 273, loss = 1.61221208\n",
      "Iteration 274, loss = 1.61162685\n",
      "Iteration 275, loss = 1.61104978\n",
      "Iteration 276, loss = 1.61046678\n",
      "Iteration 277, loss = 1.60988712\n",
      "Iteration 278, loss = 1.60930944\n",
      "Iteration 279, loss = 1.60873074\n",
      "Iteration 280, loss = 1.60815322\n",
      "Iteration 281, loss = 1.60757857\n",
      "Iteration 282, loss = 1.60700453\n",
      "Iteration 283, loss = 1.60642973\n",
      "Iteration 284, loss = 1.60585850\n",
      "Iteration 285, loss = 1.60528700\n",
      "Iteration 286, loss = 1.60471622\n",
      "Iteration 287, loss = 1.60414728\n",
      "Iteration 288, loss = 1.60358122\n",
      "Iteration 289, loss = 1.60300967\n",
      "Iteration 290, loss = 1.60244502\n",
      "Iteration 291, loss = 1.60188200\n",
      "Iteration 292, loss = 1.60131889\n",
      "Iteration 293, loss = 1.60075550\n",
      "Iteration 294, loss = 1.60019306\n",
      "Iteration 295, loss = 1.59963158\n",
      "Iteration 296, loss = 1.59906884\n",
      "Iteration 297, loss = 1.59851026\n",
      "Iteration 298, loss = 1.59795318\n",
      "Iteration 299, loss = 1.59739319\n",
      "Iteration 300, loss = 1.59683715\n",
      "Iteration 301, loss = 1.59628108\n",
      "Iteration 302, loss = 1.59572367\n",
      "Iteration 303, loss = 1.59517265\n",
      "Iteration 304, loss = 1.59461872\n",
      "Iteration 305, loss = 1.59406876\n",
      "Iteration 306, loss = 1.59351576\n",
      "Iteration 307, loss = 1.59296245\n",
      "Iteration 308, loss = 1.59241619\n",
      "Iteration 309, loss = 1.59186548\n",
      "Iteration 310, loss = 1.59131958\n",
      "Iteration 311, loss = 1.59077439\n",
      "Iteration 312, loss = 1.59022581\n",
      "Iteration 313, loss = 1.58968201\n",
      "Iteration 314, loss = 1.58913713\n",
      "Iteration 315, loss = 1.58859455\n",
      "Iteration 316, loss = 1.58805289\n",
      "Iteration 317, loss = 1.58751400\n",
      "Iteration 318, loss = 1.58697179\n",
      "Iteration 319, loss = 1.58643253\n",
      "Iteration 320, loss = 1.58589117\n",
      "Iteration 321, loss = 1.58535331\n",
      "Iteration 322, loss = 1.58481866\n",
      "Iteration 323, loss = 1.58428031\n",
      "Iteration 324, loss = 1.58374783\n",
      "Iteration 325, loss = 1.58321133\n",
      "Iteration 326, loss = 1.58267574\n",
      "Iteration 327, loss = 1.58214550\n",
      "Iteration 328, loss = 1.58161218\n",
      "Iteration 329, loss = 1.58108169\n",
      "Iteration 330, loss = 1.58055215\n",
      "Iteration 331, loss = 1.58001974\n",
      "Iteration 332, loss = 1.57949037\n",
      "Iteration 333, loss = 1.57896173\n",
      "Iteration 334, loss = 1.57843595\n",
      "Iteration 335, loss = 1.57790944\n",
      "Iteration 336, loss = 1.57738367\n",
      "Iteration 337, loss = 1.57686354\n",
      "Iteration 338, loss = 1.57633740\n",
      "Iteration 339, loss = 1.57581348\n",
      "Iteration 340, loss = 1.57529094\n",
      "Iteration 341, loss = 1.57477224\n",
      "Iteration 342, loss = 1.57425097\n",
      "Iteration 343, loss = 1.57373230\n",
      "Iteration 344, loss = 1.57321294\n",
      "Iteration 345, loss = 1.57269424\n",
      "Iteration 346, loss = 1.57217594\n",
      "Iteration 347, loss = 1.57166154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 348, loss = 1.57114842\n",
      "Iteration 349, loss = 1.57063308\n",
      "Iteration 350, loss = 1.57011989\n",
      "Iteration 351, loss = 1.56960637\n",
      "Iteration 352, loss = 1.56909525\n",
      "Iteration 353, loss = 1.56858229\n",
      "Iteration 354, loss = 1.56807312\n",
      "Iteration 355, loss = 1.56756350\n",
      "Iteration 356, loss = 1.56705407\n",
      "Iteration 357, loss = 1.56654768\n",
      "Iteration 358, loss = 1.56603912\n",
      "Iteration 359, loss = 1.56553577\n",
      "Iteration 360, loss = 1.56502946\n",
      "Iteration 361, loss = 1.56452293\n",
      "Iteration 362, loss = 1.56401946\n",
      "Iteration 363, loss = 1.56351503\n",
      "Iteration 364, loss = 1.56300976\n",
      "Iteration 365, loss = 1.56251181\n",
      "Iteration 366, loss = 1.56200867\n",
      "Iteration 367, loss = 1.56150813\n",
      "Iteration 368, loss = 1.56100839\n",
      "Iteration 369, loss = 1.56050804\n",
      "Iteration 370, loss = 1.56000862\n",
      "Iteration 371, loss = 1.55951196\n",
      "Iteration 372, loss = 1.55901283\n",
      "Iteration 373, loss = 1.55852012\n",
      "Iteration 374, loss = 1.55802139\n",
      "Iteration 375, loss = 1.55752660\n",
      "Iteration 376, loss = 1.55703589\n",
      "Iteration 377, loss = 1.55653873\n",
      "Iteration 378, loss = 1.55605078\n",
      "Iteration 379, loss = 1.55555477\n",
      "Iteration 380, loss = 1.55506403\n",
      "Iteration 381, loss = 1.55457309\n",
      "Iteration 382, loss = 1.55408523\n",
      "Iteration 383, loss = 1.55359525\n",
      "Iteration 384, loss = 1.55310523\n",
      "Iteration 385, loss = 1.55261811\n",
      "Iteration 386, loss = 1.55213270\n",
      "Iteration 387, loss = 1.55164445\n",
      "Iteration 388, loss = 1.55116268\n",
      "Iteration 389, loss = 1.55067303\n",
      "Iteration 390, loss = 1.55018813\n",
      "Iteration 391, loss = 1.54970557\n",
      "Iteration 392, loss = 1.54922236\n",
      "Iteration 393, loss = 1.54874075\n",
      "Iteration 394, loss = 1.54825718\n",
      "Iteration 395, loss = 1.54777598\n",
      "Iteration 396, loss = 1.54729669\n",
      "Iteration 397, loss = 1.54681537\n",
      "Iteration 398, loss = 1.54633583\n",
      "Iteration 399, loss = 1.54585785\n",
      "Iteration 400, loss = 1.54538045\n",
      "Iteration 401, loss = 1.54490169\n",
      "Iteration 402, loss = 1.54442487\n",
      "Iteration 403, loss = 1.54394949\n",
      "Iteration 404, loss = 1.54347404\n",
      "Iteration 405, loss = 1.54299923\n",
      "Iteration 406, loss = 1.54252551\n",
      "Iteration 407, loss = 1.54205185\n",
      "Iteration 408, loss = 1.54158165\n",
      "Iteration 409, loss = 1.54110854\n",
      "Iteration 410, loss = 1.54063772\n",
      "Iteration 411, loss = 1.54016520\n",
      "Iteration 412, loss = 1.53969432\n",
      "Iteration 413, loss = 1.53922390\n",
      "Iteration 414, loss = 1.53875586\n",
      "Iteration 415, loss = 1.53828710\n",
      "Iteration 416, loss = 1.53781954\n",
      "Iteration 417, loss = 1.53735205\n",
      "Iteration 418, loss = 1.53688441\n",
      "Iteration 419, loss = 1.53641876\n",
      "Iteration 420, loss = 1.53595378\n",
      "Iteration 421, loss = 1.53549058\n",
      "Iteration 422, loss = 1.53502233\n",
      "Iteration 423, loss = 1.53456083\n",
      "Iteration 424, loss = 1.53409714\n",
      "Iteration 425, loss = 1.53363398\n",
      "Iteration 426, loss = 1.53317458\n",
      "Iteration 427, loss = 1.53271286\n",
      "Iteration 428, loss = 1.53224902\n",
      "Iteration 429, loss = 1.53179250\n",
      "Iteration 430, loss = 1.53132876\n",
      "Iteration 431, loss = 1.53087348\n",
      "Iteration 432, loss = 1.53041398\n",
      "Iteration 433, loss = 1.52995503\n",
      "Iteration 434, loss = 1.52950017\n",
      "Iteration 435, loss = 1.52904186\n",
      "Iteration 436, loss = 1.52858521\n",
      "Iteration 437, loss = 1.52813439\n",
      "Iteration 438, loss = 1.52767478\n",
      "Iteration 439, loss = 1.52722129\n",
      "Iteration 440, loss = 1.52676771\n",
      "Iteration 441, loss = 1.52631470\n",
      "Iteration 442, loss = 1.52586274\n",
      "Iteration 443, loss = 1.52541269\n",
      "Iteration 444, loss = 1.52495758\n",
      "Iteration 445, loss = 1.52450465\n",
      "Iteration 446, loss = 1.52405708\n",
      "Iteration 447, loss = 1.52360589\n",
      "Iteration 448, loss = 1.52315826\n",
      "Iteration 449, loss = 1.52270790\n",
      "Iteration 450, loss = 1.52226042\n",
      "Iteration 451, loss = 1.52181221\n",
      "Iteration 452, loss = 1.52136741\n",
      "Iteration 453, loss = 1.52092278\n",
      "Iteration 454, loss = 1.52047481\n",
      "Iteration 455, loss = 1.52003004\n",
      "Iteration 456, loss = 1.51958440\n",
      "Iteration 457, loss = 1.51913901\n",
      "Iteration 458, loss = 1.51869703\n",
      "Iteration 459, loss = 1.51825207\n",
      "Iteration 460, loss = 1.51780730\n",
      "Iteration 461, loss = 1.51736959\n",
      "Iteration 462, loss = 1.51692644\n",
      "Iteration 463, loss = 1.51648737\n",
      "Iteration 464, loss = 1.51604737\n",
      "Iteration 465, loss = 1.51560420\n",
      "Iteration 466, loss = 1.51516336\n",
      "Iteration 467, loss = 1.51472663\n",
      "Iteration 468, loss = 1.51428815\n",
      "Iteration 469, loss = 1.51385024\n",
      "Iteration 470, loss = 1.51341222\n",
      "Iteration 471, loss = 1.51297778\n",
      "Iteration 472, loss = 1.51254048\n",
      "Iteration 473, loss = 1.51210600\n",
      "Iteration 474, loss = 1.51166900\n",
      "Iteration 475, loss = 1.51123390\n",
      "Iteration 476, loss = 1.51079948\n",
      "Iteration 477, loss = 1.51036410\n",
      "Iteration 478, loss = 1.50993126\n",
      "Iteration 479, loss = 1.50949904\n",
      "Iteration 480, loss = 1.50906474\n",
      "Iteration 481, loss = 1.50863502\n",
      "Iteration 482, loss = 1.50820118\n",
      "Iteration 483, loss = 1.50776929\n",
      "Iteration 484, loss = 1.50734039\n",
      "Iteration 485, loss = 1.50691215\n",
      "Iteration 486, loss = 1.50647875\n",
      "Iteration 487, loss = 1.50605287\n",
      "Iteration 488, loss = 1.50562321\n",
      "Iteration 489, loss = 1.50519651\n",
      "Iteration 490, loss = 1.50476657\n",
      "Iteration 491, loss = 1.50434013\n",
      "Iteration 492, loss = 1.50391469\n",
      "Iteration 493, loss = 1.50348892\n",
      "Iteration 494, loss = 1.50306235\n",
      "Iteration 495, loss = 1.50263644\n",
      "Iteration 496, loss = 1.50221360\n",
      "Iteration 497, loss = 1.50179016\n",
      "Iteration 498, loss = 1.50136558\n",
      "Iteration 499, loss = 1.50094210\n",
      "Iteration 500, loss = 1.50052246\n",
      "Iteration 501, loss = 1.50009617\n",
      "Iteration 502, loss = 1.49967672\n",
      "Iteration 503, loss = 1.49925555\n",
      "Iteration 504, loss = 1.49883166\n",
      "Iteration 505, loss = 1.49840900\n",
      "Iteration 506, loss = 1.49799049\n",
      "Iteration 507, loss = 1.49756983\n",
      "Iteration 508, loss = 1.49715289\n",
      "Iteration 509, loss = 1.49673026\n",
      "Iteration 510, loss = 1.49631270\n",
      "Iteration 511, loss = 1.49589410\n",
      "Iteration 512, loss = 1.49547815\n",
      "Iteration 513, loss = 1.49506232\n",
      "Iteration 514, loss = 1.49464514\n",
      "Iteration 515, loss = 1.49422710\n",
      "Iteration 516, loss = 1.49381040\n",
      "Iteration 517, loss = 1.49339476\n",
      "Iteration 518, loss = 1.49298012\n",
      "Iteration 519, loss = 1.49256463\n",
      "Iteration 520, loss = 1.49215036\n",
      "Iteration 521, loss = 1.49173946\n",
      "Iteration 522, loss = 1.49132472\n",
      "Iteration 523, loss = 1.49090864\n",
      "Iteration 524, loss = 1.49049674\n",
      "Iteration 525, loss = 1.49008463\n",
      "Iteration 526, loss = 1.48967370\n",
      "Iteration 527, loss = 1.48926206\n",
      "Iteration 528, loss = 1.48885121\n",
      "Iteration 529, loss = 1.48844032\n",
      "Iteration 530, loss = 1.48803042\n",
      "Iteration 531, loss = 1.48762098\n",
      "Iteration 532, loss = 1.48721243\n",
      "Iteration 533, loss = 1.48680427\n",
      "Iteration 534, loss = 1.48639544\n",
      "Iteration 535, loss = 1.48598533\n",
      "Iteration 536, loss = 1.48558065\n",
      "Iteration 537, loss = 1.48517278\n",
      "Iteration 538, loss = 1.48476718\n",
      "Iteration 539, loss = 1.48436090\n",
      "Iteration 540, loss = 1.48395405\n",
      "Iteration 541, loss = 1.48354878\n",
      "Iteration 542, loss = 1.48314296\n",
      "Iteration 543, loss = 1.48274140\n",
      "Iteration 544, loss = 1.48233665\n",
      "Iteration 545, loss = 1.48193526\n",
      "Iteration 546, loss = 1.48153172\n",
      "Iteration 547, loss = 1.48112775\n",
      "Iteration 548, loss = 1.48072511\n",
      "Iteration 549, loss = 1.48032477\n",
      "Iteration 550, loss = 1.47992090\n",
      "Iteration 551, loss = 1.47952332\n",
      "Iteration 552, loss = 1.47912029\n",
      "Iteration 553, loss = 1.47871908\n",
      "Iteration 554, loss = 1.47832210\n",
      "Iteration 555, loss = 1.47792024\n",
      "Iteration 556, loss = 1.47752296\n",
      "Iteration 557, loss = 1.47712574\n",
      "Iteration 558, loss = 1.47672693\n",
      "Iteration 559, loss = 1.47632935\n",
      "Iteration 560, loss = 1.47593337\n",
      "Iteration 561, loss = 1.47553650\n",
      "Iteration 562, loss = 1.47514216\n",
      "Iteration 563, loss = 1.47474638\n",
      "Iteration 564, loss = 1.47435124\n",
      "Iteration 565, loss = 1.47395328\n",
      "Iteration 566, loss = 1.47355930\n",
      "Iteration 567, loss = 1.47316782\n",
      "Iteration 568, loss = 1.47277305\n",
      "Iteration 569, loss = 1.47237866\n",
      "Iteration 570, loss = 1.47198753\n",
      "Iteration 571, loss = 1.47159527\n",
      "Iteration 572, loss = 1.47120248\n",
      "Iteration 573, loss = 1.47080908\n",
      "Iteration 574, loss = 1.47041818\n",
      "Iteration 575, loss = 1.47002957\n",
      "Iteration 576, loss = 1.46963708\n",
      "Iteration 577, loss = 1.46924678\n",
      "Iteration 578, loss = 1.46885758\n",
      "Iteration 579, loss = 1.46846718\n",
      "Iteration 580, loss = 1.46807788\n",
      "Iteration 581, loss = 1.46769218\n",
      "Iteration 582, loss = 1.46730150\n",
      "Iteration 583, loss = 1.46691436\n",
      "Iteration 584, loss = 1.46652854\n",
      "Iteration 585, loss = 1.46613811\n",
      "Iteration 586, loss = 1.46575245\n",
      "Iteration 587, loss = 1.46536592\n",
      "Iteration 588, loss = 1.46498110\n",
      "Iteration 589, loss = 1.46459519\n",
      "Iteration 590, loss = 1.46420782\n",
      "Iteration 591, loss = 1.46382348\n",
      "Iteration 592, loss = 1.46343804\n",
      "Iteration 593, loss = 1.46305581\n",
      "Iteration 594, loss = 1.46267233\n",
      "Iteration 595, loss = 1.46228787\n",
      "Iteration 596, loss = 1.46190542\n",
      "Iteration 597, loss = 1.46152353\n",
      "Iteration 598, loss = 1.46113919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 599, loss = 1.46075670\n",
      "Iteration 600, loss = 1.46037540\n",
      "Iteration 601, loss = 1.45999654\n",
      "Iteration 602, loss = 1.45961329\n",
      "Iteration 603, loss = 1.45923150\n",
      "Iteration 604, loss = 1.45885048\n",
      "Iteration 605, loss = 1.45847475\n",
      "Iteration 606, loss = 1.45809255\n",
      "Iteration 607, loss = 1.45771411\n",
      "Iteration 608, loss = 1.45733398\n",
      "Iteration 609, loss = 1.45695718\n",
      "Iteration 610, loss = 1.45657922\n",
      "Iteration 611, loss = 1.45620101\n",
      "Iteration 612, loss = 1.45582277\n",
      "Iteration 613, loss = 1.45544454\n",
      "Iteration 614, loss = 1.45506859\n",
      "Iteration 615, loss = 1.45469319\n",
      "Iteration 616, loss = 1.45431698\n",
      "Iteration 617, loss = 1.45394014\n",
      "Iteration 618, loss = 1.45356549\n",
      "Iteration 619, loss = 1.45318955\n",
      "Iteration 620, loss = 1.45281646\n",
      "Iteration 621, loss = 1.45244384\n",
      "Iteration 622, loss = 1.45206940\n",
      "Iteration 623, loss = 1.45169536\n",
      "Iteration 624, loss = 1.45132248\n",
      "Iteration 625, loss = 1.45094816\n",
      "Iteration 626, loss = 1.45057803\n",
      "Iteration 627, loss = 1.45020701\n",
      "Iteration 628, loss = 1.44983367\n",
      "Iteration 629, loss = 1.44946226\n",
      "Iteration 630, loss = 1.44909054\n",
      "Iteration 631, loss = 1.44872048\n",
      "Iteration 632, loss = 1.44835188\n",
      "Iteration 633, loss = 1.44798037\n",
      "Iteration 634, loss = 1.44761119\n",
      "Iteration 635, loss = 1.44724017\n",
      "Iteration 636, loss = 1.44687282\n",
      "Iteration 637, loss = 1.44650257\n",
      "Iteration 638, loss = 1.44613534\n",
      "Iteration 639, loss = 1.44576760\n",
      "Iteration 640, loss = 1.44540005\n",
      "Iteration 641, loss = 1.44503020\n",
      "Iteration 642, loss = 1.44466698\n",
      "Iteration 643, loss = 1.44429887\n",
      "Iteration 644, loss = 1.44393121\n",
      "Iteration 645, loss = 1.44356630\n",
      "Iteration 646, loss = 1.44320136\n",
      "Iteration 647, loss = 1.44283504\n",
      "Iteration 648, loss = 1.44246907\n",
      "Iteration 649, loss = 1.44210331\n",
      "Iteration 650, loss = 1.44174016\n",
      "Iteration 651, loss = 1.44137507\n",
      "Iteration 652, loss = 1.44101280\n",
      "Iteration 653, loss = 1.44064913\n",
      "Iteration 654, loss = 1.44028391\n",
      "Iteration 655, loss = 1.43992181\n",
      "Iteration 656, loss = 1.43955949\n",
      "Iteration 657, loss = 1.43919526\n",
      "Iteration 658, loss = 1.43883284\n",
      "Iteration 659, loss = 1.43847285\n",
      "Iteration 660, loss = 1.43810992\n",
      "Iteration 661, loss = 1.43775102\n",
      "Iteration 662, loss = 1.43739041\n",
      "Iteration 663, loss = 1.43702853\n",
      "Iteration 664, loss = 1.43666921\n",
      "Iteration 665, loss = 1.43630785\n",
      "Iteration 666, loss = 1.43595181\n",
      "Iteration 667, loss = 1.43558896\n",
      "Iteration 668, loss = 1.43523070\n",
      "Iteration 669, loss = 1.43487348\n",
      "Iteration 670, loss = 1.43451420\n",
      "Iteration 671, loss = 1.43415661\n",
      "Iteration 672, loss = 1.43380044\n",
      "Iteration 673, loss = 1.43344152\n",
      "Iteration 674, loss = 1.43308454\n",
      "Iteration 675, loss = 1.43272959\n",
      "Iteration 676, loss = 1.43237551\n",
      "Iteration 677, loss = 1.43201704\n",
      "Iteration 678, loss = 1.43166270\n",
      "Iteration 679, loss = 1.43130868\n",
      "Iteration 680, loss = 1.43095196\n",
      "Iteration 681, loss = 1.43059848\n",
      "Iteration 682, loss = 1.43024473\n",
      "Iteration 683, loss = 1.42988915\n",
      "Iteration 684, loss = 1.42953616\n",
      "Iteration 685, loss = 1.42918225\n",
      "Iteration 686, loss = 1.42883195\n",
      "Iteration 687, loss = 1.42847720\n",
      "Iteration 688, loss = 1.42812797\n",
      "Iteration 689, loss = 1.42777068\n",
      "Iteration 690, loss = 1.42742182\n",
      "Iteration 691, loss = 1.42707063\n",
      "Iteration 692, loss = 1.42671895\n",
      "Iteration 693, loss = 1.42636832\n",
      "Iteration 694, loss = 1.42601746\n",
      "Iteration 695, loss = 1.42566849\n",
      "Iteration 696, loss = 1.42531745\n",
      "Iteration 697, loss = 1.42496826\n",
      "Iteration 698, loss = 1.42461781\n",
      "Iteration 699, loss = 1.42427127\n",
      "Iteration 700, loss = 1.42392075\n",
      "Iteration 701, loss = 1.42357320\n",
      "Iteration 702, loss = 1.42322574\n",
      "Iteration 703, loss = 1.42287685\n",
      "Iteration 704, loss = 1.42252941\n",
      "Iteration 705, loss = 1.42218271\n",
      "Iteration 706, loss = 1.42183451\n",
      "Iteration 707, loss = 1.42148775\n",
      "Iteration 708, loss = 1.42114371\n",
      "Iteration 709, loss = 1.42079599\n",
      "Iteration 710, loss = 1.42044979\n",
      "Iteration 711, loss = 1.42010534\n",
      "Iteration 712, loss = 1.41975810\n",
      "Iteration 713, loss = 1.41941431\n",
      "Iteration 714, loss = 1.41906932\n",
      "Iteration 715, loss = 1.41872434\n",
      "Iteration 716, loss = 1.41838155\n",
      "Iteration 717, loss = 1.41803583\n",
      "Iteration 718, loss = 1.41769291\n",
      "Iteration 719, loss = 1.41735152\n",
      "Iteration 720, loss = 1.41700661\n",
      "Iteration 721, loss = 1.41666372\n",
      "Iteration 722, loss = 1.41631992\n",
      "Iteration 723, loss = 1.41597681\n",
      "Iteration 724, loss = 1.41563357\n",
      "Iteration 725, loss = 1.41529310\n",
      "Iteration 726, loss = 1.41495372\n",
      "Iteration 727, loss = 1.41460838\n",
      "Iteration 728, loss = 1.41426835\n",
      "Iteration 729, loss = 1.41392556\n",
      "Iteration 730, loss = 1.41358718\n",
      "Iteration 731, loss = 1.41324712\n",
      "Iteration 732, loss = 1.41290644\n",
      "Iteration 733, loss = 1.41256771\n",
      "Iteration 734, loss = 1.41222734\n",
      "Iteration 735, loss = 1.41188770\n",
      "Iteration 736, loss = 1.41154709\n",
      "Iteration 737, loss = 1.41121132\n",
      "Iteration 738, loss = 1.41087375\n",
      "Iteration 739, loss = 1.41053349\n",
      "Iteration 740, loss = 1.41019459\n",
      "Iteration 741, loss = 1.40985668\n",
      "Iteration 742, loss = 1.40952302\n",
      "Iteration 743, loss = 1.40918417\n",
      "Iteration 744, loss = 1.40884699\n",
      "Iteration 745, loss = 1.40850964\n",
      "Iteration 746, loss = 1.40817229\n",
      "Iteration 747, loss = 1.40783742\n",
      "Iteration 748, loss = 1.40750129\n",
      "Iteration 749, loss = 1.40716506\n",
      "Iteration 750, loss = 1.40683336\n",
      "Iteration 751, loss = 1.40649663\n",
      "Iteration 752, loss = 1.40616278\n",
      "Iteration 753, loss = 1.40582870\n",
      "Iteration 754, loss = 1.40549459\n",
      "Iteration 755, loss = 1.40515883\n",
      "Iteration 756, loss = 1.40482775\n",
      "Iteration 757, loss = 1.40449120\n",
      "Iteration 758, loss = 1.40415860\n",
      "Iteration 759, loss = 1.40382559\n",
      "Iteration 760, loss = 1.40349510\n",
      "Iteration 761, loss = 1.40316191\n",
      "Iteration 762, loss = 1.40283022\n",
      "Iteration 763, loss = 1.40249708\n",
      "Iteration 764, loss = 1.40216859\n",
      "Iteration 765, loss = 1.40183492\n",
      "Iteration 766, loss = 1.40150720\n",
      "Iteration 767, loss = 1.40117496\n",
      "Iteration 768, loss = 1.40084172\n",
      "Iteration 769, loss = 1.40051488\n",
      "Iteration 770, loss = 1.40018374\n",
      "Iteration 771, loss = 1.39985449\n",
      "Iteration 772, loss = 1.39952466\n",
      "Iteration 773, loss = 1.39919802\n",
      "Iteration 774, loss = 1.39886818\n",
      "Iteration 775, loss = 1.39853845\n",
      "Iteration 776, loss = 1.39821193\n",
      "Iteration 777, loss = 1.39788294\n",
      "Iteration 778, loss = 1.39755468\n",
      "Iteration 779, loss = 1.39722834\n",
      "Iteration 780, loss = 1.39690084\n",
      "Iteration 781, loss = 1.39657279\n",
      "Iteration 782, loss = 1.39624619\n",
      "Iteration 783, loss = 1.39592036\n",
      "Iteration 784, loss = 1.39559263\n",
      "Iteration 785, loss = 1.39526844\n",
      "Iteration 786, loss = 1.39493996\n",
      "Iteration 787, loss = 1.39461542\n",
      "Iteration 788, loss = 1.39429148\n",
      "Iteration 789, loss = 1.39396466\n",
      "Iteration 790, loss = 1.39364176\n",
      "Iteration 791, loss = 1.39331560\n",
      "Iteration 792, loss = 1.39299180\n",
      "Iteration 793, loss = 1.39266838\n",
      "Iteration 794, loss = 1.39234536\n",
      "Iteration 795, loss = 1.39201885\n",
      "Iteration 796, loss = 1.39169778\n",
      "Iteration 797, loss = 1.39137404\n",
      "Iteration 798, loss = 1.39105096\n",
      "Iteration 799, loss = 1.39072863\n",
      "Iteration 800, loss = 1.39040653\n",
      "Iteration 801, loss = 1.39008346\n",
      "Iteration 802, loss = 1.38976282\n",
      "Iteration 803, loss = 1.38944218\n",
      "Iteration 804, loss = 1.38911876\n",
      "Iteration 805, loss = 1.38880008\n",
      "Iteration 806, loss = 1.38847961\n",
      "Iteration 807, loss = 1.38815782\n",
      "Iteration 808, loss = 1.38783850\n",
      "Iteration 809, loss = 1.38751680\n",
      "Iteration 810, loss = 1.38719903\n",
      "Iteration 811, loss = 1.38687863\n",
      "Iteration 812, loss = 1.38655866\n",
      "Iteration 813, loss = 1.38624073\n",
      "Iteration 814, loss = 1.38592104\n",
      "Iteration 815, loss = 1.38560318\n",
      "Iteration 816, loss = 1.38528356\n",
      "Iteration 817, loss = 1.38496596\n",
      "Iteration 818, loss = 1.38464892\n",
      "Iteration 819, loss = 1.38432887\n",
      "Iteration 820, loss = 1.38401193\n",
      "Iteration 821, loss = 1.38369637\n",
      "Iteration 822, loss = 1.38337801\n",
      "Iteration 823, loss = 1.38306107\n",
      "Iteration 824, loss = 1.38274477\n",
      "Iteration 825, loss = 1.38242811\n",
      "Iteration 826, loss = 1.38211303\n",
      "Iteration 827, loss = 1.38179585\n",
      "Iteration 828, loss = 1.38147960\n",
      "Iteration 829, loss = 1.38116519\n",
      "Iteration 830, loss = 1.38085114\n",
      "Iteration 831, loss = 1.38053628\n",
      "Iteration 832, loss = 1.38022172\n",
      "Iteration 833, loss = 1.37990735\n",
      "Iteration 834, loss = 1.37959203\n",
      "Iteration 835, loss = 1.37928142\n",
      "Iteration 836, loss = 1.37896662\n",
      "Iteration 837, loss = 1.37865106\n",
      "Iteration 838, loss = 1.37833920\n",
      "Iteration 839, loss = 1.37802528\n",
      "Iteration 840, loss = 1.37771533\n",
      "Iteration 841, loss = 1.37740036\n",
      "Iteration 842, loss = 1.37708972\n",
      "Iteration 843, loss = 1.37677796\n",
      "Iteration 844, loss = 1.37646573\n",
      "Iteration 845, loss = 1.37615511\n",
      "Iteration 846, loss = 1.37584149\n",
      "Iteration 847, loss = 1.37552882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 848, loss = 1.37522149\n",
      "Iteration 849, loss = 1.37490985\n",
      "Iteration 850, loss = 1.37459900\n",
      "Iteration 851, loss = 1.37428869\n",
      "Iteration 852, loss = 1.37397943\n",
      "Iteration 853, loss = 1.37366978\n",
      "Iteration 854, loss = 1.37335837\n",
      "Iteration 855, loss = 1.37304907\n",
      "Iteration 856, loss = 1.37274007\n",
      "Iteration 857, loss = 1.37243201\n",
      "Iteration 858, loss = 1.37212275\n",
      "Iteration 859, loss = 1.37181390\n",
      "Iteration 860, loss = 1.37150494\n",
      "Iteration 861, loss = 1.37119767\n",
      "Iteration 862, loss = 1.37089027\n",
      "Iteration 863, loss = 1.37058185\n",
      "Iteration 864, loss = 1.37027604\n",
      "Iteration 865, loss = 1.36996939\n",
      "Iteration 866, loss = 1.36966262\n",
      "Iteration 867, loss = 1.36935225\n",
      "Iteration 868, loss = 1.36904829\n",
      "Iteration 869, loss = 1.36874154\n",
      "Iteration 870, loss = 1.36843487\n",
      "Iteration 871, loss = 1.36812959\n",
      "Iteration 872, loss = 1.36782352\n",
      "Iteration 873, loss = 1.36751809\n",
      "Iteration 874, loss = 1.36721190\n",
      "Iteration 875, loss = 1.36690637\n",
      "Iteration 876, loss = 1.36660324\n",
      "Iteration 877, loss = 1.36629851\n",
      "Iteration 878, loss = 1.36599260\n",
      "Iteration 879, loss = 1.36568816\n",
      "Iteration 880, loss = 1.36538569\n",
      "Iteration 881, loss = 1.36508097\n",
      "Iteration 882, loss = 1.36477704\n",
      "Iteration 883, loss = 1.36447277\n",
      "Iteration 884, loss = 1.36417075\n",
      "Iteration 885, loss = 1.36387004\n",
      "Iteration 886, loss = 1.36356505\n",
      "Iteration 887, loss = 1.36326451\n",
      "Iteration 888, loss = 1.36296232\n",
      "Iteration 889, loss = 1.36265886\n",
      "Iteration 890, loss = 1.36235947\n",
      "Iteration 891, loss = 1.36205958\n",
      "Iteration 892, loss = 1.36175713\n",
      "Iteration 893, loss = 1.36145483\n",
      "Iteration 894, loss = 1.36115430\n",
      "Iteration 895, loss = 1.36085302\n",
      "Iteration 896, loss = 1.36055215\n",
      "Iteration 897, loss = 1.36025248\n",
      "Iteration 898, loss = 1.35995450\n",
      "Iteration 899, loss = 1.35965298\n",
      "Iteration 900, loss = 1.35935492\n",
      "Iteration 901, loss = 1.35905652\n",
      "Iteration 902, loss = 1.35875902\n",
      "Iteration 903, loss = 1.35845818\n",
      "Iteration 904, loss = 1.35816092\n",
      "Iteration 905, loss = 1.35786209\n",
      "Iteration 906, loss = 1.35756171\n",
      "Iteration 907, loss = 1.35726584\n",
      "Iteration 908, loss = 1.35696757\n",
      "Iteration 909, loss = 1.35667085\n",
      "Iteration 910, loss = 1.35637239\n",
      "Iteration 911, loss = 1.35607611\n",
      "Iteration 912, loss = 1.35578008\n",
      "Iteration 913, loss = 1.35548505\n",
      "Iteration 914, loss = 1.35518863\n",
      "Iteration 915, loss = 1.35489319\n",
      "Iteration 916, loss = 1.35459545\n",
      "Iteration 917, loss = 1.35429984\n",
      "Iteration 918, loss = 1.35400403\n",
      "Iteration 919, loss = 1.35370811\n",
      "Iteration 920, loss = 1.35341243\n",
      "Iteration 921, loss = 1.35311970\n",
      "Iteration 922, loss = 1.35282332\n",
      "Iteration 923, loss = 1.35253112\n",
      "Iteration 924, loss = 1.35223427\n",
      "Iteration 925, loss = 1.35194088\n",
      "Iteration 926, loss = 1.35164766\n",
      "Iteration 927, loss = 1.35135360\n",
      "Iteration 928, loss = 1.35105850\n",
      "Iteration 929, loss = 1.35076780\n",
      "Iteration 930, loss = 1.35047326\n",
      "Iteration 931, loss = 1.35017930\n",
      "Iteration 932, loss = 1.34988800\n",
      "Iteration 933, loss = 1.34959553\n",
      "Iteration 934, loss = 1.34930336\n",
      "Iteration 935, loss = 1.34901144\n",
      "Iteration 936, loss = 1.34872002\n",
      "Iteration 937, loss = 1.34842774\n",
      "Iteration 938, loss = 1.34813415\n",
      "Iteration 939, loss = 1.34784644\n",
      "Iteration 940, loss = 1.34755382\n",
      "Iteration 941, loss = 1.34726352\n",
      "Iteration 942, loss = 1.34697314\n",
      "Iteration 943, loss = 1.34667925\n",
      "Iteration 944, loss = 1.34639125\n",
      "Iteration 945, loss = 1.34609975\n",
      "Iteration 946, loss = 1.34581017\n",
      "Iteration 947, loss = 1.34551950\n",
      "Iteration 948, loss = 1.34522898\n",
      "Iteration 949, loss = 1.34494084\n",
      "Iteration 950, loss = 1.34465090\n",
      "Iteration 951, loss = 1.34436207\n",
      "Iteration 952, loss = 1.34407363\n",
      "Iteration 953, loss = 1.34378616\n",
      "Iteration 954, loss = 1.34349514\n",
      "Iteration 955, loss = 1.34320799\n",
      "Iteration 956, loss = 1.34291928\n",
      "Iteration 957, loss = 1.34263145\n",
      "Iteration 958, loss = 1.34234309\n",
      "Iteration 959, loss = 1.34205693\n",
      "Iteration 960, loss = 1.34176863\n",
      "Iteration 961, loss = 1.34148272\n",
      "Iteration 962, loss = 1.34119538\n",
      "Iteration 963, loss = 1.34090934\n",
      "Iteration 964, loss = 1.34062113\n",
      "Iteration 965, loss = 1.34033550\n",
      "Iteration 966, loss = 1.34004950\n",
      "Iteration 967, loss = 1.33976293\n",
      "Iteration 968, loss = 1.33947882\n",
      "Iteration 969, loss = 1.33919035\n",
      "Iteration 970, loss = 1.33890675\n",
      "Iteration 971, loss = 1.33862193\n",
      "Iteration 972, loss = 1.33833573\n",
      "Iteration 973, loss = 1.33805235\n",
      "Iteration 974, loss = 1.33776603\n",
      "Iteration 975, loss = 1.33748216\n",
      "Iteration 976, loss = 1.33719687\n",
      "Iteration 977, loss = 1.33691374\n",
      "Iteration 978, loss = 1.33662896\n",
      "Iteration 979, loss = 1.33634701\n",
      "Iteration 980, loss = 1.33606118\n",
      "Iteration 981, loss = 1.33577833\n",
      "Iteration 982, loss = 1.33549553\n",
      "Iteration 983, loss = 1.33521290\n",
      "Iteration 984, loss = 1.33493044\n",
      "Iteration 985, loss = 1.33464699\n",
      "Iteration 986, loss = 1.33436516\n",
      "Iteration 987, loss = 1.33408312\n",
      "Iteration 988, loss = 1.33380005\n",
      "Iteration 989, loss = 1.33351921\n",
      "Iteration 990, loss = 1.33323606\n",
      "Iteration 991, loss = 1.33295508\n",
      "Iteration 992, loss = 1.33267373\n",
      "Iteration 993, loss = 1.33239433\n",
      "Iteration 994, loss = 1.33211296\n",
      "Iteration 995, loss = 1.33183128\n",
      "Iteration 996, loss = 1.33155090\n",
      "Iteration 997, loss = 1.33127241\n",
      "Iteration 998, loss = 1.33099106\n",
      "Iteration 999, loss = 1.33071271\n",
      "Iteration 1000, loss = 1.33043041\n",
      "Iteration 1001, loss = 1.33015164\n",
      "Iteration 1002, loss = 1.32987215\n",
      "Iteration 1003, loss = 1.32959473\n",
      "Iteration 1004, loss = 1.32931512\n",
      "Iteration 1005, loss = 1.32903573\n",
      "Iteration 1006, loss = 1.32875604\n",
      "Iteration 1007, loss = 1.32848073\n",
      "Iteration 1008, loss = 1.32820148\n",
      "Iteration 1009, loss = 1.32792041\n",
      "Iteration 1010, loss = 1.32764295\n",
      "Iteration 1011, loss = 1.32736605\n",
      "Iteration 1012, loss = 1.32708818\n",
      "Iteration 1013, loss = 1.32681378\n",
      "Iteration 1014, loss = 1.32653166\n",
      "Iteration 1015, loss = 1.32625536\n",
      "Iteration 1016, loss = 1.32597914\n",
      "Iteration 1017, loss = 1.32570271\n",
      "Iteration 1018, loss = 1.32542609\n",
      "Iteration 1019, loss = 1.32514935\n",
      "Iteration 1020, loss = 1.32487386\n",
      "Iteration 1021, loss = 1.32459784\n",
      "Iteration 1022, loss = 1.32432192\n",
      "Iteration 1023, loss = 1.32404679\n",
      "Iteration 1024, loss = 1.32377148\n",
      "Iteration 1025, loss = 1.32349750\n",
      "Iteration 1026, loss = 1.32322142\n",
      "Iteration 1027, loss = 1.32294686\n",
      "Iteration 1028, loss = 1.32267054\n",
      "Iteration 1029, loss = 1.32239574\n",
      "Iteration 1030, loss = 1.32212152\n",
      "Iteration 1031, loss = 1.32184831\n",
      "Iteration 1032, loss = 1.32157421\n",
      "Iteration 1033, loss = 1.32129750\n",
      "Iteration 1034, loss = 1.32102461\n",
      "Iteration 1035, loss = 1.32075165\n",
      "Iteration 1036, loss = 1.32047757\n",
      "Iteration 1037, loss = 1.32020351\n",
      "Iteration 1038, loss = 1.31993013\n",
      "Iteration 1039, loss = 1.31965810\n",
      "Iteration 1040, loss = 1.31938507\n",
      "Iteration 1041, loss = 1.31911462\n",
      "Iteration 1042, loss = 1.31883841\n",
      "Iteration 1043, loss = 1.31856772\n",
      "Iteration 1044, loss = 1.31829621\n",
      "Iteration 1045, loss = 1.31802409\n",
      "Iteration 1046, loss = 1.31774933\n",
      "Iteration 1047, loss = 1.31747906\n",
      "Iteration 1048, loss = 1.31720761\n",
      "Iteration 1049, loss = 1.31693803\n",
      "Iteration 1050, loss = 1.31666518\n",
      "Iteration 1051, loss = 1.31639615\n",
      "Iteration 1052, loss = 1.31612479\n",
      "Iteration 1053, loss = 1.31585221\n",
      "Iteration 1054, loss = 1.31558321\n",
      "Iteration 1055, loss = 1.31531151\n",
      "Iteration 1056, loss = 1.31504095\n",
      "Iteration 1057, loss = 1.31477119\n",
      "Iteration 1058, loss = 1.31450172\n",
      "Iteration 1059, loss = 1.31423253\n",
      "Iteration 1060, loss = 1.31396124\n",
      "Iteration 1061, loss = 1.31369306\n",
      "Iteration 1062, loss = 1.31342224\n",
      "Iteration 1063, loss = 1.31315461\n",
      "Iteration 1064, loss = 1.31288441\n",
      "Iteration 1065, loss = 1.31261742\n",
      "Iteration 1066, loss = 1.31234877\n",
      "Iteration 1067, loss = 1.31207964\n",
      "Iteration 1068, loss = 1.31181219\n",
      "Iteration 1069, loss = 1.31154495\n",
      "Iteration 1070, loss = 1.31127665\n",
      "Iteration 1071, loss = 1.31100921\n",
      "Iteration 1072, loss = 1.31074131\n",
      "Iteration 1073, loss = 1.31047263\n",
      "Iteration 1074, loss = 1.31020767\n",
      "Iteration 1075, loss = 1.30993904\n",
      "Iteration 1076, loss = 1.30967230\n",
      "Iteration 1077, loss = 1.30940736\n",
      "Iteration 1078, loss = 1.30914052\n",
      "Iteration 1079, loss = 1.30887255\n",
      "Iteration 1080, loss = 1.30860812\n",
      "Iteration 1081, loss = 1.30834292\n",
      "Iteration 1082, loss = 1.30807584\n",
      "Iteration 1083, loss = 1.30780924\n",
      "Iteration 1084, loss = 1.30754485\n",
      "Iteration 1085, loss = 1.30727984\n",
      "Iteration 1086, loss = 1.30701244\n",
      "Iteration 1087, loss = 1.30674804\n",
      "Iteration 1088, loss = 1.30648317\n",
      "Iteration 1089, loss = 1.30622004\n",
      "Iteration 1090, loss = 1.30595650\n",
      "Iteration 1091, loss = 1.30569032\n",
      "Iteration 1092, loss = 1.30542709\n",
      "Iteration 1093, loss = 1.30516358\n",
      "Iteration 1094, loss = 1.30490056\n",
      "Iteration 1095, loss = 1.30463633\n",
      "Iteration 1096, loss = 1.30437344\n",
      "Iteration 1097, loss = 1.30411241\n",
      "Iteration 1098, loss = 1.30384631\n",
      "Iteration 1099, loss = 1.30358313\n",
      "Iteration 1100, loss = 1.30331989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1101, loss = 1.30305746\n",
      "Iteration 1102, loss = 1.30279555\n",
      "Iteration 1103, loss = 1.30253308\n",
      "Iteration 1104, loss = 1.30227268\n",
      "Iteration 1105, loss = 1.30200782\n",
      "Iteration 1106, loss = 1.30174754\n",
      "Iteration 1107, loss = 1.30148433\n",
      "Iteration 1108, loss = 1.30122242\n",
      "Iteration 1109, loss = 1.30096195\n",
      "Iteration 1110, loss = 1.30070127\n",
      "Iteration 1111, loss = 1.30044010\n",
      "Iteration 1112, loss = 1.30017841\n",
      "Iteration 1113, loss = 1.29991792\n",
      "Iteration 1114, loss = 1.29965781\n",
      "Iteration 1115, loss = 1.29939724\n",
      "Iteration 1116, loss = 1.29913784\n",
      "Iteration 1117, loss = 1.29887574\n",
      "Iteration 1118, loss = 1.29861624\n",
      "Iteration 1119, loss = 1.29835444\n",
      "Iteration 1120, loss = 1.29809666\n",
      "Iteration 1121, loss = 1.29783602\n",
      "Iteration 1122, loss = 1.29757452\n",
      "Iteration 1123, loss = 1.29731610\n",
      "Iteration 1124, loss = 1.29705594\n",
      "Iteration 1125, loss = 1.29679896\n",
      "Iteration 1126, loss = 1.29653791\n",
      "Iteration 1127, loss = 1.29627949\n",
      "Iteration 1128, loss = 1.29602019\n",
      "Iteration 1129, loss = 1.29576234\n",
      "Iteration 1130, loss = 1.29550469\n",
      "Iteration 1131, loss = 1.29524587\n",
      "Iteration 1132, loss = 1.29498874\n",
      "Iteration 1133, loss = 1.29472975\n",
      "Iteration 1134, loss = 1.29447061\n",
      "Iteration 1135, loss = 1.29421464\n",
      "Iteration 1136, loss = 1.29395712\n",
      "Iteration 1137, loss = 1.29369895\n",
      "Iteration 1138, loss = 1.29344273\n",
      "Iteration 1139, loss = 1.29318494\n",
      "Iteration 1140, loss = 1.29292828\n",
      "Iteration 1141, loss = 1.29267245\n",
      "Iteration 1142, loss = 1.29241573\n",
      "Iteration 1143, loss = 1.29215887\n",
      "Iteration 1144, loss = 1.29190326\n",
      "Iteration 1145, loss = 1.29164811\n",
      "Iteration 1146, loss = 1.29139183\n",
      "Iteration 1147, loss = 1.29113568\n",
      "Iteration 1148, loss = 1.29088012\n",
      "Iteration 1149, loss = 1.29062572\n",
      "Iteration 1150, loss = 1.29037149\n",
      "Iteration 1151, loss = 1.29011613\n",
      "Iteration 1152, loss = 1.28986023\n",
      "Iteration 1153, loss = 1.28960620\n",
      "Iteration 1154, loss = 1.28935106\n",
      "Iteration 1155, loss = 1.28909556\n",
      "Iteration 1156, loss = 1.28884244\n",
      "Iteration 1157, loss = 1.28858984\n",
      "Iteration 1158, loss = 1.28833657\n",
      "Iteration 1159, loss = 1.28808037\n",
      "Iteration 1160, loss = 1.28782717\n",
      "Iteration 1161, loss = 1.28757470\n",
      "Iteration 1162, loss = 1.28732054\n",
      "Iteration 1163, loss = 1.28706722\n",
      "Iteration 1164, loss = 1.28681381\n",
      "Iteration 1165, loss = 1.28656249\n",
      "Iteration 1166, loss = 1.28630985\n",
      "Iteration 1167, loss = 1.28605565\n",
      "Iteration 1168, loss = 1.28580474\n",
      "Iteration 1169, loss = 1.28555083\n",
      "Iteration 1170, loss = 1.28529903\n",
      "Iteration 1171, loss = 1.28504846\n",
      "Iteration 1172, loss = 1.28479381\n",
      "Iteration 1173, loss = 1.28454289\n",
      "Iteration 1174, loss = 1.28429201\n",
      "Iteration 1175, loss = 1.28404047\n",
      "Iteration 1176, loss = 1.28378958\n",
      "Iteration 1177, loss = 1.28353826\n",
      "Iteration 1178, loss = 1.28328758\n",
      "Iteration 1179, loss = 1.28303549\n",
      "Iteration 1180, loss = 1.28278604\n",
      "Iteration 1181, loss = 1.28253568\n",
      "Iteration 1182, loss = 1.28228495\n",
      "Iteration 1183, loss = 1.28203327\n",
      "Iteration 1184, loss = 1.28178613\n",
      "Iteration 1185, loss = 1.28153374\n",
      "Iteration 1186, loss = 1.28128426\n",
      "Iteration 1187, loss = 1.28103277\n",
      "Iteration 1188, loss = 1.28078553\n",
      "Iteration 1189, loss = 1.28053519\n",
      "Iteration 1190, loss = 1.28028827\n",
      "Iteration 1191, loss = 1.28003605\n",
      "Iteration 1192, loss = 1.27978669\n",
      "Iteration 1193, loss = 1.27953755\n",
      "Iteration 1194, loss = 1.27929055\n",
      "Iteration 1195, loss = 1.27904040\n",
      "Iteration 1196, loss = 1.27879150\n",
      "Iteration 1197, loss = 1.27854392\n",
      "Iteration 1198, loss = 1.27829589\n",
      "Iteration 1199, loss = 1.27804795\n",
      "Iteration 1200, loss = 1.27779942\n",
      "Iteration 1201, loss = 1.27755147\n",
      "Iteration 1202, loss = 1.27730321\n",
      "Iteration 1203, loss = 1.27705487\n",
      "Iteration 1204, loss = 1.27680803\n",
      "Iteration 1205, loss = 1.27656099\n",
      "Iteration 1206, loss = 1.27631499\n",
      "Iteration 1207, loss = 1.27606875\n",
      "Iteration 1208, loss = 1.27581934\n",
      "Iteration 1209, loss = 1.27557250\n",
      "Iteration 1210, loss = 1.27532568\n",
      "Iteration 1211, loss = 1.27508014\n",
      "Iteration 1212, loss = 1.27483201\n",
      "Iteration 1213, loss = 1.27458608\n",
      "Iteration 1214, loss = 1.27434039\n",
      "Iteration 1215, loss = 1.27409470\n",
      "Iteration 1216, loss = 1.27384928\n",
      "Iteration 1217, loss = 1.27360075\n",
      "Iteration 1218, loss = 1.27335680\n",
      "Iteration 1219, loss = 1.27311141\n",
      "Iteration 1220, loss = 1.27286631\n",
      "Iteration 1221, loss = 1.27262034\n",
      "Iteration 1222, loss = 1.27237706\n",
      "Iteration 1223, loss = 1.27213228\n",
      "Iteration 1224, loss = 1.27188497\n",
      "Iteration 1225, loss = 1.27164173\n",
      "Iteration 1226, loss = 1.27139729\n",
      "Iteration 1227, loss = 1.27115469\n",
      "Iteration 1228, loss = 1.27090823\n",
      "Iteration 1229, loss = 1.27066351\n",
      "Iteration 1230, loss = 1.27042159\n",
      "Iteration 1231, loss = 1.27017897\n",
      "Iteration 1232, loss = 1.26993483\n",
      "Iteration 1233, loss = 1.26969128\n",
      "Iteration 1234, loss = 1.26944746\n",
      "Iteration 1235, loss = 1.26920496\n",
      "Iteration 1236, loss = 1.26896154\n",
      "Iteration 1237, loss = 1.26871853\n",
      "Iteration 1238, loss = 1.26847710\n",
      "Iteration 1239, loss = 1.26823537\n",
      "Iteration 1240, loss = 1.26799000\n",
      "Iteration 1241, loss = 1.26774807\n",
      "Iteration 1242, loss = 1.26750613\n",
      "Iteration 1243, loss = 1.26726359\n",
      "Iteration 1244, loss = 1.26702276\n",
      "Iteration 1245, loss = 1.26678191\n",
      "Iteration 1246, loss = 1.26653830\n",
      "Iteration 1247, loss = 1.26629771\n",
      "Iteration 1248, loss = 1.26605618\n",
      "Iteration 1249, loss = 1.26581399\n",
      "Iteration 1250, loss = 1.26557223\n",
      "Iteration 1251, loss = 1.26533177\n",
      "Iteration 1252, loss = 1.26509080\n",
      "Iteration 1253, loss = 1.26484987\n",
      "Iteration 1254, loss = 1.26460813\n",
      "Iteration 1255, loss = 1.26436896\n",
      "Iteration 1256, loss = 1.26412817\n",
      "Iteration 1257, loss = 1.26388641\n",
      "Iteration 1258, loss = 1.26364719\n",
      "Iteration 1259, loss = 1.26340649\n",
      "Iteration 1260, loss = 1.26316974\n",
      "Iteration 1261, loss = 1.26292929\n",
      "Iteration 1262, loss = 1.26268747\n",
      "Iteration 1263, loss = 1.26244812\n",
      "Iteration 1264, loss = 1.26220726\n",
      "Iteration 1265, loss = 1.26197139\n",
      "Iteration 1266, loss = 1.26172924\n",
      "Iteration 1267, loss = 1.26148925\n",
      "Iteration 1268, loss = 1.26125301\n",
      "Iteration 1269, loss = 1.26101286\n",
      "Iteration 1270, loss = 1.26077483\n",
      "Iteration 1271, loss = 1.26053570\n",
      "Iteration 1272, loss = 1.26029778\n",
      "Iteration 1273, loss = 1.26006000\n",
      "Iteration 1274, loss = 1.25982083\n",
      "Iteration 1275, loss = 1.25958270\n",
      "Iteration 1276, loss = 1.25934416\n",
      "Iteration 1277, loss = 1.25910706\n",
      "Iteration 1278, loss = 1.25886974\n",
      "Iteration 1279, loss = 1.25863308\n",
      "Iteration 1280, loss = 1.25839498\n",
      "Iteration 1281, loss = 1.25815659\n",
      "Iteration 1282, loss = 1.25792071\n",
      "Iteration 1283, loss = 1.25768290\n",
      "Iteration 1284, loss = 1.25744666\n",
      "Iteration 1285, loss = 1.25721067\n",
      "Iteration 1286, loss = 1.25697346\n",
      "Iteration 1287, loss = 1.25673754\n",
      "Iteration 1288, loss = 1.25649933\n",
      "Iteration 1289, loss = 1.25626520\n",
      "Iteration 1290, loss = 1.25602858\n",
      "Iteration 1291, loss = 1.25579369\n",
      "Iteration 1292, loss = 1.25555654\n",
      "Iteration 1293, loss = 1.25532306\n",
      "Iteration 1294, loss = 1.25508599\n",
      "Iteration 1295, loss = 1.25485023\n",
      "Iteration 1296, loss = 1.25461407\n",
      "Iteration 1297, loss = 1.25438126\n",
      "Iteration 1298, loss = 1.25414405\n",
      "Iteration 1299, loss = 1.25391081\n",
      "Iteration 1300, loss = 1.25367529\n",
      "Iteration 1301, loss = 1.25343996\n",
      "Iteration 1302, loss = 1.25320505\n",
      "Iteration 1303, loss = 1.25297310\n",
      "Iteration 1304, loss = 1.25273718\n",
      "Iteration 1305, loss = 1.25250383\n",
      "Iteration 1306, loss = 1.25226992\n",
      "Iteration 1307, loss = 1.25203542\n",
      "Iteration 1308, loss = 1.25180163\n",
      "Iteration 1309, loss = 1.25156923\n",
      "Iteration 1310, loss = 1.25133409\n",
      "Iteration 1311, loss = 1.25110156\n",
      "Iteration 1312, loss = 1.25086696\n",
      "Iteration 1313, loss = 1.25063551\n",
      "Iteration 1314, loss = 1.25040325\n",
      "Iteration 1315, loss = 1.25016883\n",
      "Iteration 1316, loss = 1.24993631\n",
      "Iteration 1317, loss = 1.24970480\n",
      "Iteration 1318, loss = 1.24947073\n",
      "Iteration 1319, loss = 1.24923831\n",
      "Iteration 1320, loss = 1.24900624\n",
      "Iteration 1321, loss = 1.24877544\n",
      "Iteration 1322, loss = 1.24854139\n",
      "Iteration 1323, loss = 1.24830967\n",
      "Iteration 1324, loss = 1.24807798\n",
      "Iteration 1325, loss = 1.24784618\n",
      "Iteration 1326, loss = 1.24761381\n",
      "Iteration 1327, loss = 1.24738324\n",
      "Iteration 1328, loss = 1.24715203\n",
      "Iteration 1329, loss = 1.24692052\n",
      "Iteration 1330, loss = 1.24668844\n",
      "Iteration 1331, loss = 1.24645739\n",
      "Iteration 1332, loss = 1.24622940\n",
      "Iteration 1333, loss = 1.24599835\n",
      "Iteration 1334, loss = 1.24576724\n",
      "Iteration 1335, loss = 1.24553567\n",
      "Iteration 1336, loss = 1.24530563\n",
      "Iteration 1337, loss = 1.24507561\n",
      "Iteration 1338, loss = 1.24484558\n",
      "Iteration 1339, loss = 1.24461650\n",
      "Iteration 1340, loss = 1.24438608\n",
      "Iteration 1341, loss = 1.24415702\n",
      "Iteration 1342, loss = 1.24392726\n",
      "Iteration 1343, loss = 1.24369671\n",
      "Iteration 1344, loss = 1.24346794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1345, loss = 1.24323928\n",
      "Iteration 1346, loss = 1.24301079\n",
      "Iteration 1347, loss = 1.24278145\n",
      "Iteration 1348, loss = 1.24255280\n",
      "Iteration 1349, loss = 1.24232465\n",
      "Iteration 1350, loss = 1.24209712\n",
      "Iteration 1351, loss = 1.24186731\n",
      "Iteration 1352, loss = 1.24163815\n",
      "Iteration 1353, loss = 1.24141130\n",
      "Iteration 1354, loss = 1.24118297\n",
      "Iteration 1355, loss = 1.24095393\n",
      "Iteration 1356, loss = 1.24072857\n",
      "Iteration 1357, loss = 1.24050034\n",
      "Iteration 1358, loss = 1.24027232\n",
      "Iteration 1359, loss = 1.24004494\n",
      "Iteration 1360, loss = 1.23981689\n",
      "Iteration 1361, loss = 1.23958925\n",
      "Iteration 1362, loss = 1.23936410\n",
      "Iteration 1363, loss = 1.23913698\n",
      "Iteration 1364, loss = 1.23890995\n",
      "Iteration 1365, loss = 1.23868376\n",
      "Iteration 1366, loss = 1.23845680\n",
      "Iteration 1367, loss = 1.23823352\n",
      "Iteration 1368, loss = 1.23800535\n",
      "Iteration 1369, loss = 1.23777769\n",
      "Iteration 1370, loss = 1.23755178\n",
      "Iteration 1371, loss = 1.23732576\n",
      "Iteration 1372, loss = 1.23709889\n",
      "Iteration 1373, loss = 1.23687390\n",
      "Iteration 1374, loss = 1.23664884\n",
      "Iteration 1375, loss = 1.23642354\n",
      "Iteration 1376, loss = 1.23619909\n",
      "Iteration 1377, loss = 1.23597393\n",
      "Iteration 1378, loss = 1.23574756\n",
      "Iteration 1379, loss = 1.23552420\n",
      "Iteration 1380, loss = 1.23529821\n",
      "Iteration 1381, loss = 1.23507385\n",
      "Iteration 1382, loss = 1.23485036\n",
      "Iteration 1383, loss = 1.23462499\n",
      "Iteration 1384, loss = 1.23440102\n",
      "Iteration 1385, loss = 1.23417714\n",
      "Iteration 1386, loss = 1.23395230\n",
      "Iteration 1387, loss = 1.23372894\n",
      "Iteration 1388, loss = 1.23350527\n",
      "Iteration 1389, loss = 1.23328204\n",
      "Iteration 1390, loss = 1.23305764\n",
      "Iteration 1391, loss = 1.23283471\n",
      "Iteration 1392, loss = 1.23260916\n",
      "Iteration 1393, loss = 1.23238704\n",
      "Iteration 1394, loss = 1.23216350\n",
      "Iteration 1395, loss = 1.23194049\n",
      "Iteration 1396, loss = 1.23171909\n",
      "Iteration 1397, loss = 1.23149515\n",
      "Iteration 1398, loss = 1.23127157\n",
      "Iteration 1399, loss = 1.23104976\n",
      "Iteration 1400, loss = 1.23082720\n",
      "Iteration 1401, loss = 1.23060599\n",
      "Iteration 1402, loss = 1.23038162\n",
      "Iteration 1403, loss = 1.23015876\n",
      "Iteration 1404, loss = 1.22993815\n",
      "Iteration 1405, loss = 1.22971603\n",
      "Iteration 1406, loss = 1.22949305\n",
      "Iteration 1407, loss = 1.22927214\n",
      "Iteration 1408, loss = 1.22905008\n",
      "Iteration 1409, loss = 1.22882954\n",
      "Iteration 1410, loss = 1.22860714\n",
      "Iteration 1411, loss = 1.22838683\n",
      "Iteration 1412, loss = 1.22816442\n",
      "Iteration 1413, loss = 1.22794437\n",
      "Iteration 1414, loss = 1.22772501\n",
      "Iteration 1415, loss = 1.22750222\n",
      "Iteration 1416, loss = 1.22728487\n",
      "Iteration 1417, loss = 1.22706183\n",
      "Iteration 1418, loss = 1.22684289\n",
      "Iteration 1419, loss = 1.22662152\n",
      "Iteration 1420, loss = 1.22639920\n",
      "Iteration 1421, loss = 1.22618103\n",
      "Iteration 1422, loss = 1.22596125\n",
      "Iteration 1423, loss = 1.22574136\n",
      "Iteration 1424, loss = 1.22552019\n",
      "Iteration 1425, loss = 1.22530132\n",
      "Iteration 1426, loss = 1.22508109\n",
      "Iteration 1427, loss = 1.22486277\n",
      "Iteration 1428, loss = 1.22464424\n",
      "Iteration 1429, loss = 1.22442353\n",
      "Iteration 1430, loss = 1.22420506\n",
      "Iteration 1431, loss = 1.22398539\n",
      "Iteration 1432, loss = 1.22376678\n",
      "Iteration 1433, loss = 1.22354895\n",
      "Iteration 1434, loss = 1.22333058\n",
      "Iteration 1435, loss = 1.22311153\n",
      "Iteration 1436, loss = 1.22289317\n",
      "Iteration 1437, loss = 1.22267483\n",
      "Iteration 1438, loss = 1.22245656\n",
      "Iteration 1439, loss = 1.22224077\n",
      "Iteration 1440, loss = 1.22202231\n",
      "Iteration 1441, loss = 1.22180308\n",
      "Iteration 1442, loss = 1.22158593\n",
      "Iteration 1443, loss = 1.22137074\n",
      "Iteration 1444, loss = 1.22115052\n",
      "Iteration 1445, loss = 1.22093394\n",
      "Iteration 1446, loss = 1.22071536\n",
      "Iteration 1447, loss = 1.22049987\n",
      "Iteration 1448, loss = 1.22028259\n",
      "Iteration 1449, loss = 1.22006549\n",
      "Iteration 1450, loss = 1.21984775\n",
      "Iteration 1451, loss = 1.21963182\n",
      "Iteration 1452, loss = 1.21941562\n",
      "Iteration 1453, loss = 1.21919917\n",
      "Iteration 1454, loss = 1.21898368\n",
      "Iteration 1455, loss = 1.21876753\n",
      "Iteration 1456, loss = 1.21855167\n",
      "Iteration 1457, loss = 1.21833498\n",
      "Iteration 1458, loss = 1.21812036\n",
      "Iteration 1459, loss = 1.21790426\n",
      "Iteration 1460, loss = 1.21768806\n",
      "Iteration 1461, loss = 1.21747248\n",
      "Iteration 1462, loss = 1.21725899\n",
      "Iteration 1463, loss = 1.21704102\n",
      "Iteration 1464, loss = 1.21682749\n",
      "Iteration 1465, loss = 1.21661289\n",
      "Iteration 1466, loss = 1.21639729\n",
      "Iteration 1467, loss = 1.21618301\n",
      "Iteration 1468, loss = 1.21596909\n",
      "Iteration 1469, loss = 1.21575419\n",
      "Iteration 1470, loss = 1.21553862\n",
      "Iteration 1471, loss = 1.21532506\n",
      "Iteration 1472, loss = 1.21511120\n",
      "Iteration 1473, loss = 1.21489842\n",
      "Iteration 1474, loss = 1.21468323\n",
      "Iteration 1475, loss = 1.21446892\n",
      "Iteration 1476, loss = 1.21425643\n",
      "Iteration 1477, loss = 1.21404031\n",
      "Iteration 1478, loss = 1.21382823\n",
      "Iteration 1479, loss = 1.21361459\n",
      "Iteration 1480, loss = 1.21340073\n",
      "Iteration 1481, loss = 1.21318746\n",
      "Iteration 1482, loss = 1.21297469\n",
      "Iteration 1483, loss = 1.21276195\n",
      "Iteration 1484, loss = 1.21254867\n",
      "Iteration 1485, loss = 1.21233472\n",
      "Iteration 1486, loss = 1.21212277\n",
      "Iteration 1487, loss = 1.21191024\n",
      "Iteration 1488, loss = 1.21169775\n",
      "Iteration 1489, loss = 1.21148545\n",
      "Iteration 1490, loss = 1.21127337\n",
      "Iteration 1491, loss = 1.21106208\n",
      "Iteration 1492, loss = 1.21084875\n",
      "Iteration 1493, loss = 1.21063618\n",
      "Iteration 1494, loss = 1.21042349\n",
      "Iteration 1495, loss = 1.21021266\n",
      "Iteration 1496, loss = 1.21000202\n",
      "Iteration 1497, loss = 1.20978987\n",
      "Iteration 1498, loss = 1.20957785\n",
      "Iteration 1499, loss = 1.20936690\n",
      "Iteration 1500, loss = 1.20915477\n",
      "Iteration 1501, loss = 1.20894413\n",
      "Iteration 1502, loss = 1.20873350\n",
      "Iteration 1503, loss = 1.20852217\n",
      "Iteration 1504, loss = 1.20831106\n",
      "Iteration 1505, loss = 1.20810151\n",
      "Iteration 1506, loss = 1.20789052\n",
      "Iteration 1507, loss = 1.20767934\n",
      "Iteration 1508, loss = 1.20746810\n",
      "Iteration 1509, loss = 1.20725842\n",
      "Iteration 1510, loss = 1.20704983\n",
      "Iteration 1511, loss = 1.20683762\n",
      "Iteration 1512, loss = 1.20662809\n",
      "Iteration 1513, loss = 1.20641946\n",
      "Iteration 1514, loss = 1.20620797\n",
      "Iteration 1515, loss = 1.20600020\n",
      "Iteration 1516, loss = 1.20578879\n",
      "Iteration 1517, loss = 1.20558042\n",
      "Iteration 1518, loss = 1.20537087\n",
      "Iteration 1519, loss = 1.20516086\n",
      "Iteration 1520, loss = 1.20495269\n",
      "Iteration 1521, loss = 1.20474189\n",
      "Iteration 1522, loss = 1.20453443\n",
      "Iteration 1523, loss = 1.20432389\n",
      "Iteration 1524, loss = 1.20411574\n",
      "Iteration 1525, loss = 1.20390745\n",
      "Iteration 1526, loss = 1.20369856\n",
      "Iteration 1527, loss = 1.20349175\n",
      "Iteration 1528, loss = 1.20328104\n",
      "Iteration 1529, loss = 1.20307489\n",
      "Iteration 1530, loss = 1.20286522\n",
      "Iteration 1531, loss = 1.20265828\n",
      "Iteration 1532, loss = 1.20244916\n",
      "Iteration 1533, loss = 1.20224196\n",
      "Iteration 1534, loss = 1.20203446\n",
      "Iteration 1535, loss = 1.20182514\n",
      "Iteration 1536, loss = 1.20161882\n",
      "Iteration 1537, loss = 1.20141089\n",
      "Iteration 1538, loss = 1.20120419\n",
      "Iteration 1539, loss = 1.20099691\n",
      "Iteration 1540, loss = 1.20079146\n",
      "Iteration 1541, loss = 1.20058378\n",
      "Iteration 1542, loss = 1.20037515\n",
      "Iteration 1543, loss = 1.20016901\n",
      "Iteration 1544, loss = 1.19996154\n",
      "Iteration 1545, loss = 1.19975541\n",
      "Iteration 1546, loss = 1.19954768\n",
      "Iteration 1547, loss = 1.19934182\n",
      "Iteration 1548, loss = 1.19913535\n",
      "Iteration 1549, loss = 1.19893020\n",
      "Iteration 1550, loss = 1.19872213\n",
      "Iteration 1551, loss = 1.19851751\n",
      "Iteration 1552, loss = 1.19831175\n",
      "Iteration 1553, loss = 1.19810688\n",
      "Iteration 1554, loss = 1.19789909\n",
      "Iteration 1555, loss = 1.19769342\n",
      "Iteration 1556, loss = 1.19748889\n",
      "Iteration 1557, loss = 1.19728419\n",
      "Iteration 1558, loss = 1.19707835\n",
      "Iteration 1559, loss = 1.19687252\n",
      "Iteration 1560, loss = 1.19666704\n",
      "Iteration 1561, loss = 1.19646264\n",
      "Iteration 1562, loss = 1.19625705\n",
      "Iteration 1563, loss = 1.19605444\n",
      "Iteration 1564, loss = 1.19584948\n",
      "Iteration 1565, loss = 1.19564426\n",
      "Iteration 1566, loss = 1.19544011\n",
      "Iteration 1567, loss = 1.19523613\n",
      "Iteration 1568, loss = 1.19503136\n",
      "Iteration 1569, loss = 1.19482728\n",
      "Iteration 1570, loss = 1.19462241\n",
      "Iteration 1571, loss = 1.19441951\n",
      "Iteration 1572, loss = 1.19421646\n",
      "Iteration 1573, loss = 1.19401008\n",
      "Iteration 1574, loss = 1.19380816\n",
      "Iteration 1575, loss = 1.19360454\n",
      "Iteration 1576, loss = 1.19340071\n",
      "Iteration 1577, loss = 1.19319757\n",
      "Iteration 1578, loss = 1.19299310\n",
      "Iteration 1579, loss = 1.19279097\n",
      "Iteration 1580, loss = 1.19258724\n",
      "Iteration 1581, loss = 1.19238511\n",
      "Iteration 1582, loss = 1.19218161\n",
      "Iteration 1583, loss = 1.19197944\n",
      "Iteration 1584, loss = 1.19177643\n",
      "Iteration 1585, loss = 1.19157355\n",
      "Iteration 1586, loss = 1.19137079\n",
      "Iteration 1587, loss = 1.19116861\n",
      "Iteration 1588, loss = 1.19096731\n",
      "Iteration 1589, loss = 1.19076554\n",
      "Iteration 1590, loss = 1.19056349\n",
      "Iteration 1591, loss = 1.19036137\n",
      "Iteration 1592, loss = 1.19016018\n",
      "Iteration 1593, loss = 1.18995799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1594, loss = 1.18975626\n",
      "Iteration 1595, loss = 1.18955475\n",
      "Iteration 1596, loss = 1.18935382\n",
      "Iteration 1597, loss = 1.18915226\n",
      "Iteration 1598, loss = 1.18895072\n",
      "Iteration 1599, loss = 1.18874948\n",
      "Iteration 1600, loss = 1.18854922\n",
      "Iteration 1601, loss = 1.18834938\n",
      "Iteration 1602, loss = 1.18814653\n",
      "Iteration 1603, loss = 1.18794535\n",
      "Iteration 1604, loss = 1.18774458\n",
      "Iteration 1605, loss = 1.18754381\n",
      "Iteration 1606, loss = 1.18734563\n",
      "Iteration 1607, loss = 1.18714467\n",
      "Iteration 1608, loss = 1.18694269\n",
      "Iteration 1609, loss = 1.18674244\n",
      "Iteration 1610, loss = 1.18654302\n",
      "Iteration 1611, loss = 1.18634292\n",
      "Iteration 1612, loss = 1.18614305\n",
      "Iteration 1613, loss = 1.18594279\n",
      "Iteration 1614, loss = 1.18574418\n",
      "Iteration 1615, loss = 1.18554327\n",
      "Iteration 1616, loss = 1.18534468\n",
      "Iteration 1617, loss = 1.18514509\n",
      "Iteration 1618, loss = 1.18494680\n",
      "Iteration 1619, loss = 1.18474600\n",
      "Iteration 1620, loss = 1.18454689\n",
      "Iteration 1621, loss = 1.18434800\n",
      "Iteration 1622, loss = 1.18414994\n",
      "Iteration 1623, loss = 1.18395096\n",
      "Iteration 1624, loss = 1.18375315\n",
      "Iteration 1625, loss = 1.18355281\n",
      "Iteration 1626, loss = 1.18335485\n",
      "Iteration 1627, loss = 1.18315655\n",
      "Iteration 1628, loss = 1.18295861\n",
      "Iteration 1629, loss = 1.18276049\n",
      "Iteration 1630, loss = 1.18256198\n",
      "Iteration 1631, loss = 1.18236522\n",
      "Iteration 1632, loss = 1.18216586\n",
      "Iteration 1633, loss = 1.18196736\n",
      "Iteration 1634, loss = 1.18176987\n",
      "Iteration 1635, loss = 1.18157146\n",
      "Iteration 1636, loss = 1.18137463\n",
      "Iteration 1637, loss = 1.18117741\n",
      "Iteration 1638, loss = 1.18098021\n",
      "Iteration 1639, loss = 1.18078259\n",
      "Iteration 1640, loss = 1.18058597\n",
      "Iteration 1641, loss = 1.18038801\n",
      "Iteration 1642, loss = 1.18019065\n",
      "Iteration 1643, loss = 1.17999449\n",
      "Iteration 1644, loss = 1.17979824\n",
      "Iteration 1645, loss = 1.17960104\n",
      "Iteration 1646, loss = 1.17940304\n",
      "Iteration 1647, loss = 1.17920749\n",
      "Iteration 1648, loss = 1.17901149\n",
      "Iteration 1649, loss = 1.17881509\n",
      "Iteration 1650, loss = 1.17861906\n",
      "Iteration 1651, loss = 1.17842279\n",
      "Iteration 1652, loss = 1.17822724\n",
      "Iteration 1653, loss = 1.17803016\n",
      "Iteration 1654, loss = 1.17783382\n",
      "Iteration 1655, loss = 1.17763793\n",
      "Iteration 1656, loss = 1.17744243\n",
      "Iteration 1657, loss = 1.17724812\n",
      "Iteration 1658, loss = 1.17705179\n",
      "Iteration 1659, loss = 1.17685649\n",
      "Iteration 1660, loss = 1.17666043\n",
      "Iteration 1661, loss = 1.17646666\n",
      "Iteration 1662, loss = 1.17626966\n",
      "Iteration 1663, loss = 1.17607508\n",
      "Iteration 1664, loss = 1.17588030\n",
      "Iteration 1665, loss = 1.17568450\n",
      "Iteration 1666, loss = 1.17549059\n",
      "Iteration 1667, loss = 1.17529548\n",
      "Iteration 1668, loss = 1.17510041\n",
      "Iteration 1669, loss = 1.17490654\n",
      "Iteration 1670, loss = 1.17471171\n",
      "Iteration 1671, loss = 1.17451861\n",
      "Iteration 1672, loss = 1.17432301\n",
      "Iteration 1673, loss = 1.17413024\n",
      "Iteration 1674, loss = 1.17393455\n",
      "Iteration 1675, loss = 1.17374198\n",
      "Iteration 1676, loss = 1.17354706\n",
      "Iteration 1677, loss = 1.17335518\n",
      "Iteration 1678, loss = 1.17316078\n",
      "Iteration 1679, loss = 1.17296565\n",
      "Iteration 1680, loss = 1.17277278\n",
      "Iteration 1681, loss = 1.17257892\n",
      "Iteration 1682, loss = 1.17238611\n",
      "Iteration 1683, loss = 1.17219287\n",
      "Iteration 1684, loss = 1.17199963\n",
      "Iteration 1685, loss = 1.17180559\n",
      "Iteration 1686, loss = 1.17161337\n",
      "Iteration 1687, loss = 1.17141952\n",
      "Iteration 1688, loss = 1.17122767\n",
      "Iteration 1689, loss = 1.17103459\n",
      "Iteration 1690, loss = 1.17084382\n",
      "Iteration 1691, loss = 1.17064893\n",
      "Iteration 1692, loss = 1.17045733\n",
      "Iteration 1693, loss = 1.17026460\n",
      "Iteration 1694, loss = 1.17007264\n",
      "Iteration 1695, loss = 1.16988150\n",
      "Iteration 1696, loss = 1.16968946\n",
      "Iteration 1697, loss = 1.16949786\n",
      "Iteration 1698, loss = 1.16930587\n",
      "Iteration 1699, loss = 1.16911495\n",
      "Iteration 1700, loss = 1.16892165\n",
      "Iteration 1701, loss = 1.16872985\n",
      "Iteration 1702, loss = 1.16853841\n",
      "Iteration 1703, loss = 1.16834697\n",
      "Iteration 1704, loss = 1.16815765\n",
      "Iteration 1705, loss = 1.16796389\n",
      "Iteration 1706, loss = 1.16777369\n",
      "Iteration 1707, loss = 1.16758330\n",
      "Iteration 1708, loss = 1.16739202\n",
      "Iteration 1709, loss = 1.16720134\n",
      "Iteration 1710, loss = 1.16700993\n",
      "Iteration 1711, loss = 1.16681965\n",
      "Iteration 1712, loss = 1.16662980\n",
      "Iteration 1713, loss = 1.16643699\n",
      "Iteration 1714, loss = 1.16624947\n",
      "Iteration 1715, loss = 1.16605813\n",
      "Iteration 1716, loss = 1.16586974\n",
      "Iteration 1717, loss = 1.16567921\n",
      "Iteration 1718, loss = 1.16548836\n",
      "Iteration 1719, loss = 1.16529848\n",
      "Iteration 1720, loss = 1.16510684\n",
      "Iteration 1721, loss = 1.16491753\n",
      "Iteration 1722, loss = 1.16472861\n",
      "Iteration 1723, loss = 1.16453873\n",
      "Iteration 1724, loss = 1.16435001\n",
      "Iteration 1725, loss = 1.16416186\n",
      "Iteration 1726, loss = 1.16397026\n",
      "Iteration 1727, loss = 1.16378069\n",
      "Iteration 1728, loss = 1.16359163\n",
      "Iteration 1729, loss = 1.16340266\n",
      "Iteration 1730, loss = 1.16321438\n",
      "Iteration 1731, loss = 1.16302403\n",
      "Iteration 1732, loss = 1.16283522\n",
      "Iteration 1733, loss = 1.16264773\n",
      "Iteration 1734, loss = 1.16246050\n",
      "Iteration 1735, loss = 1.16227025\n",
      "Iteration 1736, loss = 1.16208188\n",
      "Iteration 1737, loss = 1.16189258\n",
      "Iteration 1738, loss = 1.16170490\n",
      "Iteration 1739, loss = 1.16151689\n",
      "Iteration 1740, loss = 1.16132844\n",
      "Iteration 1741, loss = 1.16114141\n",
      "Iteration 1742, loss = 1.16095345\n",
      "Iteration 1743, loss = 1.16076634\n",
      "Iteration 1744, loss = 1.16057818\n",
      "Iteration 1745, loss = 1.16039009\n",
      "Iteration 1746, loss = 1.16020269\n",
      "Iteration 1747, loss = 1.16001544\n",
      "Iteration 1748, loss = 1.15982719\n",
      "Iteration 1749, loss = 1.15964089\n",
      "Iteration 1750, loss = 1.15945280\n",
      "Iteration 1751, loss = 1.15926723\n",
      "Iteration 1752, loss = 1.15907967\n",
      "Iteration 1753, loss = 1.15889310\n",
      "Iteration 1754, loss = 1.15870597\n",
      "Iteration 1755, loss = 1.15851707\n",
      "Iteration 1756, loss = 1.15833155\n",
      "Iteration 1757, loss = 1.15814592\n",
      "Iteration 1758, loss = 1.15795930\n",
      "Iteration 1759, loss = 1.15777295\n",
      "Iteration 1760, loss = 1.15758654\n",
      "Iteration 1761, loss = 1.15740086\n",
      "Iteration 1762, loss = 1.15721317\n",
      "Iteration 1763, loss = 1.15702820\n",
      "Iteration 1764, loss = 1.15684117\n",
      "Iteration 1765, loss = 1.15665527\n",
      "Iteration 1766, loss = 1.15647011\n",
      "Iteration 1767, loss = 1.15628334\n",
      "Iteration 1768, loss = 1.15609843\n",
      "Iteration 1769, loss = 1.15591313\n",
      "Iteration 1770, loss = 1.15572744\n",
      "Iteration 1771, loss = 1.15554208\n",
      "Iteration 1772, loss = 1.15535663\n",
      "Iteration 1773, loss = 1.15517315\n",
      "Iteration 1774, loss = 1.15498746\n",
      "Iteration 1775, loss = 1.15480221\n",
      "Iteration 1776, loss = 1.15461663\n",
      "Iteration 1777, loss = 1.15443108\n",
      "Iteration 1778, loss = 1.15424559\n",
      "Iteration 1779, loss = 1.15406367\n",
      "Iteration 1780, loss = 1.15387782\n",
      "Iteration 1781, loss = 1.15369306\n",
      "Iteration 1782, loss = 1.15350860\n",
      "Iteration 1783, loss = 1.15332273\n",
      "Iteration 1784, loss = 1.15313927\n",
      "Iteration 1785, loss = 1.15295428\n",
      "Iteration 1786, loss = 1.15276952\n",
      "Iteration 1787, loss = 1.15258696\n",
      "Iteration 1788, loss = 1.15240158\n",
      "Iteration 1789, loss = 1.15222089\n",
      "Iteration 1790, loss = 1.15203635\n",
      "Iteration 1791, loss = 1.15185132\n",
      "Iteration 1792, loss = 1.15166789\n",
      "Iteration 1793, loss = 1.15148607\n",
      "Iteration 1794, loss = 1.15130102\n",
      "Iteration 1795, loss = 1.15111811\n",
      "Iteration 1796, loss = 1.15093310\n",
      "Iteration 1797, loss = 1.15075085\n",
      "Iteration 1798, loss = 1.15056781\n",
      "Iteration 1799, loss = 1.15038392\n",
      "Iteration 1800, loss = 1.15020144\n",
      "Iteration 1801, loss = 1.15001852\n",
      "Iteration 1802, loss = 1.14983573\n",
      "Iteration 1803, loss = 1.14965120\n",
      "Iteration 1804, loss = 1.14946854\n",
      "Iteration 1805, loss = 1.14928643\n",
      "Iteration 1806, loss = 1.14910329\n",
      "Iteration 1807, loss = 1.14892016\n",
      "Iteration 1808, loss = 1.14873682\n",
      "Iteration 1809, loss = 1.14855623\n",
      "Iteration 1810, loss = 1.14837293\n",
      "Iteration 1811, loss = 1.14819065\n",
      "Iteration 1812, loss = 1.14800980\n",
      "Iteration 1813, loss = 1.14782688\n",
      "Iteration 1814, loss = 1.14764514\n",
      "Iteration 1815, loss = 1.14746385\n",
      "Iteration 1816, loss = 1.14728215\n",
      "Iteration 1817, loss = 1.14709923\n",
      "Iteration 1818, loss = 1.14691776\n",
      "Iteration 1819, loss = 1.14673659\n",
      "Iteration 1820, loss = 1.14655557\n",
      "Iteration 1821, loss = 1.14637268\n",
      "Iteration 1822, loss = 1.14619217\n",
      "Iteration 1823, loss = 1.14601054\n",
      "Iteration 1824, loss = 1.14583072\n",
      "Iteration 1825, loss = 1.14564827\n",
      "Iteration 1826, loss = 1.14546849\n",
      "Iteration 1827, loss = 1.14528562\n",
      "Iteration 1828, loss = 1.14510477\n",
      "Iteration 1829, loss = 1.14492327\n",
      "Iteration 1830, loss = 1.14474322\n",
      "Iteration 1831, loss = 1.14456324\n",
      "Iteration 1832, loss = 1.14438222\n",
      "Iteration 1833, loss = 1.14420301\n",
      "Iteration 1834, loss = 1.14402154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1835, loss = 1.14384074\n",
      "Iteration 1836, loss = 1.14366240\n",
      "Iteration 1837, loss = 1.14347981\n",
      "Iteration 1838, loss = 1.14330136\n",
      "Iteration 1839, loss = 1.14312110\n",
      "Iteration 1840, loss = 1.14293957\n",
      "Iteration 1841, loss = 1.14276071\n",
      "Iteration 1842, loss = 1.14257990\n",
      "Iteration 1843, loss = 1.14240018\n",
      "Iteration 1844, loss = 1.14222098\n",
      "Iteration 1845, loss = 1.14204147\n",
      "Iteration 1846, loss = 1.14186301\n",
      "Iteration 1847, loss = 1.14168353\n",
      "Iteration 1848, loss = 1.14150271\n",
      "Iteration 1849, loss = 1.14132536\n",
      "Iteration 1850, loss = 1.14114392\n",
      "Iteration 1851, loss = 1.14096521\n",
      "Iteration 1852, loss = 1.14078511\n",
      "Iteration 1853, loss = 1.14060761\n",
      "Iteration 1854, loss = 1.14042816\n",
      "Iteration 1855, loss = 1.14025133\n",
      "Iteration 1856, loss = 1.14007078\n",
      "Iteration 1857, loss = 1.13989145\n",
      "Iteration 1858, loss = 1.13971501\n",
      "Iteration 1859, loss = 1.13953613\n",
      "Iteration 1860, loss = 1.13935636\n",
      "Iteration 1861, loss = 1.13917804\n",
      "Iteration 1862, loss = 1.13900063\n",
      "Iteration 1863, loss = 1.13882126\n",
      "Iteration 1864, loss = 1.13864346\n",
      "Iteration 1865, loss = 1.13846631\n",
      "Iteration 1866, loss = 1.13828804\n",
      "Iteration 1867, loss = 1.13811021\n",
      "Iteration 1868, loss = 1.13793108\n",
      "Iteration 1869, loss = 1.13775427\n",
      "Iteration 1870, loss = 1.13757699\n",
      "Iteration 1871, loss = 1.13739729\n",
      "Iteration 1872, loss = 1.13722028\n",
      "Iteration 1873, loss = 1.13704406\n",
      "Iteration 1874, loss = 1.13686664\n",
      "Iteration 1875, loss = 1.13668932\n",
      "Iteration 1876, loss = 1.13651183\n",
      "Iteration 1877, loss = 1.13633533\n",
      "Iteration 1878, loss = 1.13615699\n",
      "Iteration 1879, loss = 1.13598015\n",
      "Iteration 1880, loss = 1.13580276\n",
      "Iteration 1881, loss = 1.13562665\n",
      "Iteration 1882, loss = 1.13545032\n",
      "Iteration 1883, loss = 1.13527265\n",
      "Iteration 1884, loss = 1.13509656\n",
      "Iteration 1885, loss = 1.13491982\n",
      "Iteration 1886, loss = 1.13474266\n",
      "Iteration 1887, loss = 1.13456783\n",
      "Iteration 1888, loss = 1.13439068\n",
      "Iteration 1889, loss = 1.13421426\n",
      "Iteration 1890, loss = 1.13403926\n",
      "Iteration 1891, loss = 1.13386145\n",
      "Iteration 1892, loss = 1.13368682\n",
      "Iteration 1893, loss = 1.13351016\n",
      "Iteration 1894, loss = 1.13333469\n",
      "Iteration 1895, loss = 1.13315983\n",
      "Iteration 1896, loss = 1.13298480\n",
      "Iteration 1897, loss = 1.13280683\n",
      "Iteration 1898, loss = 1.13263272\n",
      "Iteration 1899, loss = 1.13245698\n",
      "Iteration 1900, loss = 1.13228151\n",
      "Iteration 1901, loss = 1.13210637\n",
      "Iteration 1902, loss = 1.13193118\n",
      "Iteration 1903, loss = 1.13175624\n",
      "Iteration 1904, loss = 1.13158069\n",
      "Iteration 1905, loss = 1.13140497\n",
      "Iteration 1906, loss = 1.13123073\n",
      "Iteration 1907, loss = 1.13105658\n",
      "Iteration 1908, loss = 1.13088167\n",
      "Iteration 1909, loss = 1.13070665\n",
      "Iteration 1910, loss = 1.13053177\n",
      "Iteration 1911, loss = 1.13035791\n",
      "Iteration 1912, loss = 1.13018205\n",
      "Iteration 1913, loss = 1.13000806\n",
      "Iteration 1914, loss = 1.12983442\n",
      "Iteration 1915, loss = 1.12965985\n",
      "Iteration 1916, loss = 1.12948690\n",
      "Iteration 1917, loss = 1.12931233\n",
      "Iteration 1918, loss = 1.12913870\n",
      "Iteration 1919, loss = 1.12896445\n",
      "Iteration 1920, loss = 1.12879079\n",
      "Iteration 1921, loss = 1.12861742\n",
      "Iteration 1922, loss = 1.12844336\n",
      "Iteration 1923, loss = 1.12827033\n",
      "Iteration 1924, loss = 1.12809688\n",
      "Iteration 1925, loss = 1.12792290\n",
      "Iteration 1926, loss = 1.12775022\n",
      "Iteration 1927, loss = 1.12757622\n",
      "Iteration 1928, loss = 1.12740255\n",
      "Iteration 1929, loss = 1.12722913\n",
      "Iteration 1930, loss = 1.12706014\n",
      "Iteration 1931, loss = 1.12688470\n",
      "Iteration 1932, loss = 1.12671083\n",
      "Iteration 1933, loss = 1.12653950\n",
      "Iteration 1934, loss = 1.12636572\n",
      "Iteration 1935, loss = 1.12619220\n",
      "Iteration 1936, loss = 1.12602079\n",
      "Iteration 1937, loss = 1.12584866\n",
      "Iteration 1938, loss = 1.12567577\n",
      "Iteration 1939, loss = 1.12550182\n",
      "Iteration 1940, loss = 1.12533005\n",
      "Iteration 1941, loss = 1.12515892\n",
      "Iteration 1942, loss = 1.12498774\n",
      "Iteration 1943, loss = 1.12481387\n",
      "Iteration 1944, loss = 1.12464238\n",
      "Iteration 1945, loss = 1.12447001\n",
      "Iteration 1946, loss = 1.12430025\n",
      "Iteration 1947, loss = 1.12412704\n",
      "Iteration 1948, loss = 1.12395495\n",
      "Iteration 1949, loss = 1.12378329\n",
      "Iteration 1950, loss = 1.12361249\n",
      "Iteration 1951, loss = 1.12344022\n",
      "Iteration 1952, loss = 1.12326859\n",
      "Iteration 1953, loss = 1.12309791\n",
      "Iteration 1954, loss = 1.12292684\n",
      "Iteration 1955, loss = 1.12275502\n",
      "Iteration 1956, loss = 1.12258339\n",
      "Iteration 1957, loss = 1.12241214\n",
      "Iteration 1958, loss = 1.12224061\n",
      "Iteration 1959, loss = 1.12207105\n",
      "Iteration 1960, loss = 1.12189987\n",
      "Iteration 1961, loss = 1.12172987\n",
      "Iteration 1962, loss = 1.12155802\n",
      "Iteration 1963, loss = 1.12138803\n",
      "Iteration 1964, loss = 1.12121592\n",
      "Iteration 1965, loss = 1.12104653\n",
      "Iteration 1966, loss = 1.12087558\n",
      "Iteration 1967, loss = 1.12070505\n",
      "Iteration 1968, loss = 1.12053479\n",
      "Iteration 1969, loss = 1.12036597\n",
      "Iteration 1970, loss = 1.12019679\n",
      "Iteration 1971, loss = 1.12002522\n",
      "Iteration 1972, loss = 1.11985497\n",
      "Iteration 1973, loss = 1.11968473\n",
      "Iteration 1974, loss = 1.11951526\n",
      "Iteration 1975, loss = 1.11934571\n",
      "Iteration 1976, loss = 1.11917641\n",
      "Iteration 1977, loss = 1.11900622\n",
      "Iteration 1978, loss = 1.11883582\n",
      "Iteration 1979, loss = 1.11866725\n",
      "Iteration 1980, loss = 1.11849766\n",
      "Iteration 1981, loss = 1.11832905\n",
      "Iteration 1982, loss = 1.11815853\n",
      "Iteration 1983, loss = 1.11799017\n",
      "Iteration 1984, loss = 1.11782067\n",
      "Iteration 1985, loss = 1.11765181\n",
      "Iteration 1986, loss = 1.11748292\n",
      "Iteration 1987, loss = 1.11731488\n",
      "Iteration 1988, loss = 1.11714539\n",
      "Iteration 1989, loss = 1.11697608\n",
      "Iteration 1990, loss = 1.11680869\n",
      "Iteration 1991, loss = 1.11663990\n",
      "Iteration 1992, loss = 1.11647134\n",
      "Iteration 1993, loss = 1.11630213\n",
      "Iteration 1994, loss = 1.11613453\n",
      "Iteration 1995, loss = 1.11596725\n",
      "Iteration 1996, loss = 1.11579710\n",
      "Iteration 1997, loss = 1.11562961\n",
      "Iteration 1998, loss = 1.11546106\n",
      "Iteration 1999, loss = 1.11529247\n",
      "Iteration 2000, loss = 1.11512658\n",
      "Iteration 2001, loss = 1.11495824\n",
      "Iteration 2002, loss = 1.11479084\n",
      "Iteration 2003, loss = 1.11462115\n",
      "Iteration 2004, loss = 1.11445407\n",
      "Iteration 2005, loss = 1.11428665\n",
      "Iteration 2006, loss = 1.11412053\n",
      "Iteration 2007, loss = 1.11395250\n",
      "Iteration 2008, loss = 1.11378436\n",
      "Iteration 2009, loss = 1.11361617\n",
      "Iteration 2010, loss = 1.11345053\n",
      "Iteration 2011, loss = 1.11328289\n",
      "Iteration 2012, loss = 1.11311612\n",
      "Iteration 2013, loss = 1.11294828\n",
      "Iteration 2014, loss = 1.11278175\n",
      "Iteration 2015, loss = 1.11261389\n",
      "Iteration 2016, loss = 1.11244725\n",
      "Iteration 2017, loss = 1.11228133\n",
      "Iteration 2018, loss = 1.11211481\n",
      "Iteration 2019, loss = 1.11194726\n",
      "Iteration 2020, loss = 1.11178166\n",
      "Iteration 2021, loss = 1.11161479\n",
      "Iteration 2022, loss = 1.11144760\n",
      "Iteration 2023, loss = 1.11128117\n",
      "Iteration 2024, loss = 1.11111435\n",
      "Iteration 2025, loss = 1.11094828\n",
      "Iteration 2026, loss = 1.11078240\n",
      "Iteration 2027, loss = 1.11061669\n",
      "Iteration 2028, loss = 1.11045242\n",
      "Iteration 2029, loss = 1.11028514\n",
      "Iteration 2030, loss = 1.11011871\n",
      "Iteration 2031, loss = 1.10995465\n",
      "Iteration 2032, loss = 1.10978803\n",
      "Iteration 2033, loss = 1.10962281\n",
      "Iteration 2034, loss = 1.10945838\n",
      "Iteration 2035, loss = 1.10928997\n",
      "Iteration 2036, loss = 1.10912561\n",
      "Iteration 2037, loss = 1.10895941\n",
      "Iteration 2038, loss = 1.10879600\n",
      "Iteration 2039, loss = 1.10863041\n",
      "Iteration 2040, loss = 1.10846510\n",
      "Iteration 2041, loss = 1.10830084\n",
      "Iteration 2042, loss = 1.10813466\n",
      "Iteration 2043, loss = 1.10796999\n",
      "Iteration 2044, loss = 1.10780604\n",
      "Iteration 2045, loss = 1.10764078\n",
      "Iteration 2046, loss = 1.10747721\n",
      "Iteration 2047, loss = 1.10731287\n",
      "Iteration 2048, loss = 1.10714692\n",
      "Iteration 2049, loss = 1.10698297\n",
      "Iteration 2050, loss = 1.10681939\n",
      "Iteration 2051, loss = 1.10665393\n",
      "Iteration 2052, loss = 1.10648874\n",
      "Iteration 2053, loss = 1.10632545\n",
      "Iteration 2054, loss = 1.10616104\n",
      "Iteration 2055, loss = 1.10599749\n",
      "Iteration 2056, loss = 1.10583358\n",
      "Iteration 2057, loss = 1.10566959\n",
      "Iteration 2058, loss = 1.10550421\n",
      "Iteration 2059, loss = 1.10534087\n",
      "Iteration 2060, loss = 1.10517758\n",
      "Iteration 2061, loss = 1.10501562\n",
      "Iteration 2062, loss = 1.10484965\n",
      "Iteration 2063, loss = 1.10468661\n",
      "Iteration 2064, loss = 1.10452296\n",
      "Iteration 2065, loss = 1.10436006\n",
      "Iteration 2066, loss = 1.10419738\n",
      "Iteration 2067, loss = 1.10403328\n",
      "Iteration 2068, loss = 1.10387047\n",
      "Iteration 2069, loss = 1.10370639\n",
      "Iteration 2070, loss = 1.10354352\n",
      "Iteration 2071, loss = 1.10338023\n",
      "Iteration 2072, loss = 1.10321697\n",
      "Iteration 2073, loss = 1.10305441\n",
      "Iteration 2074, loss = 1.10289080\n",
      "Iteration 2075, loss = 1.10272875\n",
      "Iteration 2076, loss = 1.10256572\n",
      "Iteration 2077, loss = 1.10240427\n",
      "Iteration 2078, loss = 1.10223964\n",
      "Iteration 2079, loss = 1.10207912\n",
      "Iteration 2080, loss = 1.10191611\n",
      "Iteration 2081, loss = 1.10175321\n",
      "Iteration 2082, loss = 1.10159090\n",
      "Iteration 2083, loss = 1.10142929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2084, loss = 1.10126739\n",
      "Iteration 2085, loss = 1.10110437\n",
      "Iteration 2086, loss = 1.10094258\n",
      "Iteration 2087, loss = 1.10078053\n",
      "Iteration 2088, loss = 1.10061917\n",
      "Iteration 2089, loss = 1.10045673\n",
      "Iteration 2090, loss = 1.10029520\n",
      "Iteration 2091, loss = 1.10013435\n",
      "Iteration 2092, loss = 1.09997293\n",
      "Iteration 2093, loss = 1.09981068\n",
      "Iteration 2094, loss = 1.09964847\n",
      "Iteration 2095, loss = 1.09948834\n",
      "Iteration 2096, loss = 1.09932690\n",
      "Iteration 2097, loss = 1.09916579\n",
      "Iteration 2098, loss = 1.09900375\n",
      "Iteration 2099, loss = 1.09884155\n",
      "Iteration 2100, loss = 1.09868256\n",
      "Iteration 2101, loss = 1.09852040\n",
      "Iteration 2102, loss = 1.09835951\n",
      "Iteration 2103, loss = 1.09819894\n",
      "Iteration 2104, loss = 1.09803898\n",
      "Iteration 2105, loss = 1.09787860\n",
      "Iteration 2106, loss = 1.09771649\n",
      "Iteration 2107, loss = 1.09755643\n",
      "Iteration 2108, loss = 1.09739608\n",
      "Iteration 2109, loss = 1.09723543\n",
      "Iteration 2110, loss = 1.09707420\n",
      "Iteration 2111, loss = 1.09691547\n",
      "Iteration 2112, loss = 1.09675482\n",
      "Iteration 2113, loss = 1.09659472\n",
      "Iteration 2114, loss = 1.09643478\n",
      "Iteration 2115, loss = 1.09627585\n",
      "Iteration 2116, loss = 1.09611503\n",
      "Iteration 2117, loss = 1.09595470\n",
      "Iteration 2118, loss = 1.09579526\n",
      "Iteration 2119, loss = 1.09563614\n",
      "Iteration 2120, loss = 1.09547520\n",
      "Iteration 2121, loss = 1.09531495\n",
      "Iteration 2122, loss = 1.09515543\n",
      "Iteration 2123, loss = 1.09499697\n",
      "Iteration 2124, loss = 1.09483737\n",
      "Iteration 2125, loss = 1.09467780\n",
      "Iteration 2126, loss = 1.09451763\n",
      "Iteration 2127, loss = 1.09435885\n",
      "Iteration 2128, loss = 1.09420053\n",
      "Iteration 2129, loss = 1.09404090\n",
      "Iteration 2130, loss = 1.09388183\n",
      "Iteration 2131, loss = 1.09372208\n",
      "Iteration 2132, loss = 1.09356329\n",
      "Iteration 2133, loss = 1.09340478\n",
      "Iteration 2134, loss = 1.09324659\n",
      "Iteration 2135, loss = 1.09308761\n",
      "Iteration 2136, loss = 1.09292869\n",
      "Iteration 2137, loss = 1.09277002\n",
      "Iteration 2138, loss = 1.09261138\n",
      "Iteration 2139, loss = 1.09245283\n",
      "Iteration 2140, loss = 1.09229597\n",
      "Iteration 2141, loss = 1.09213544\n",
      "Iteration 2142, loss = 1.09197834\n",
      "Iteration 2143, loss = 1.09181976\n",
      "Iteration 2144, loss = 1.09166133\n",
      "Iteration 2145, loss = 1.09150365\n",
      "Iteration 2146, loss = 1.09134542\n",
      "Iteration 2147, loss = 1.09118772\n",
      "Iteration 2148, loss = 1.09102969\n",
      "Iteration 2149, loss = 1.09087179\n",
      "Iteration 2150, loss = 1.09071355\n",
      "Iteration 2151, loss = 1.09055598\n",
      "Iteration 2152, loss = 1.09039743\n",
      "Iteration 2153, loss = 1.09024073\n",
      "Iteration 2154, loss = 1.09008384\n",
      "Iteration 2155, loss = 1.08992555\n",
      "Iteration 2156, loss = 1.08976808\n",
      "Iteration 2157, loss = 1.08961126\n",
      "Iteration 2158, loss = 1.08945304\n",
      "Iteration 2159, loss = 1.08929664\n",
      "Iteration 2160, loss = 1.08913973\n",
      "Iteration 2161, loss = 1.08898185\n",
      "Iteration 2162, loss = 1.08882537\n",
      "Iteration 2163, loss = 1.08866915\n",
      "Iteration 2164, loss = 1.08851128\n",
      "Iteration 2165, loss = 1.08835360\n",
      "Iteration 2166, loss = 1.08819711\n",
      "Iteration 2167, loss = 1.08804140\n",
      "Iteration 2168, loss = 1.08788287\n",
      "Iteration 2169, loss = 1.08772654\n",
      "Iteration 2170, loss = 1.08757189\n",
      "Iteration 2171, loss = 1.08741355\n",
      "Iteration 2172, loss = 1.08725789\n",
      "Iteration 2173, loss = 1.08710162\n",
      "Iteration 2174, loss = 1.08694412\n",
      "Iteration 2175, loss = 1.08678884\n",
      "Iteration 2176, loss = 1.08663277\n",
      "Iteration 2177, loss = 1.08647658\n",
      "Iteration 2178, loss = 1.08632205\n",
      "Iteration 2179, loss = 1.08616410\n",
      "Iteration 2180, loss = 1.08600837\n",
      "Iteration 2181, loss = 1.08585243\n",
      "Iteration 2182, loss = 1.08569567\n",
      "Iteration 2183, loss = 1.08554169\n",
      "Iteration 2184, loss = 1.08538514\n",
      "Iteration 2185, loss = 1.08523101\n",
      "Iteration 2186, loss = 1.08507322\n",
      "Iteration 2187, loss = 1.08491869\n",
      "Iteration 2188, loss = 1.08476286\n",
      "Iteration 2189, loss = 1.08460780\n",
      "Iteration 2190, loss = 1.08445212\n",
      "Iteration 2191, loss = 1.08429780\n",
      "Iteration 2192, loss = 1.08414107\n",
      "Iteration 2193, loss = 1.08398681\n",
      "Iteration 2194, loss = 1.08383218\n",
      "Iteration 2195, loss = 1.08367653\n",
      "Iteration 2196, loss = 1.08352280\n",
      "Iteration 2197, loss = 1.08336651\n",
      "Iteration 2198, loss = 1.08321394\n",
      "Iteration 2199, loss = 1.08305730\n",
      "Iteration 2200, loss = 1.08290386\n",
      "Iteration 2201, loss = 1.08274771\n",
      "Iteration 2202, loss = 1.08259415\n",
      "Iteration 2203, loss = 1.08243865\n",
      "Iteration 2204, loss = 1.08228439\n",
      "Iteration 2205, loss = 1.08213020\n",
      "Iteration 2206, loss = 1.08197641\n",
      "Iteration 2207, loss = 1.08182319\n",
      "Iteration 2208, loss = 1.08166756\n",
      "Iteration 2209, loss = 1.08151285\n",
      "Iteration 2210, loss = 1.08135936\n",
      "Iteration 2211, loss = 1.08120631\n",
      "Iteration 2212, loss = 1.08105060\n",
      "Iteration 2213, loss = 1.08089846\n",
      "Iteration 2214, loss = 1.08074355\n",
      "Iteration 2215, loss = 1.08059011\n",
      "Iteration 2216, loss = 1.08043660\n",
      "Iteration 2217, loss = 1.08028220\n",
      "Iteration 2218, loss = 1.08012828\n",
      "Iteration 2219, loss = 1.07997488\n",
      "Iteration 2220, loss = 1.07982214\n",
      "Iteration 2221, loss = 1.07966885\n",
      "Iteration 2222, loss = 1.07951486\n",
      "Iteration 2223, loss = 1.07936204\n",
      "Iteration 2224, loss = 1.07920904\n",
      "Iteration 2225, loss = 1.07905455\n",
      "Iteration 2226, loss = 1.07890153\n",
      "Iteration 2227, loss = 1.07874887\n",
      "Iteration 2228, loss = 1.07859516\n",
      "Iteration 2229, loss = 1.07844123\n",
      "Iteration 2230, loss = 1.07829070\n",
      "Iteration 2231, loss = 1.07813712\n",
      "Iteration 2232, loss = 1.07798412\n",
      "Iteration 2233, loss = 1.07783181\n",
      "Iteration 2234, loss = 1.07767860\n",
      "Iteration 2235, loss = 1.07752602\n",
      "Iteration 2236, loss = 1.07737439\n",
      "Iteration 2237, loss = 1.07722203\n",
      "Iteration 2238, loss = 1.07706742\n",
      "Iteration 2239, loss = 1.07691586\n",
      "Iteration 2240, loss = 1.07676364\n",
      "Iteration 2241, loss = 1.07661140\n",
      "Iteration 2242, loss = 1.07645891\n",
      "Iteration 2243, loss = 1.07630743\n",
      "Iteration 2244, loss = 1.07615534\n",
      "Iteration 2245, loss = 1.07600323\n",
      "Iteration 2246, loss = 1.07585166\n",
      "Iteration 2247, loss = 1.07570092\n",
      "Iteration 2248, loss = 1.07554756\n",
      "Iteration 2249, loss = 1.07539531\n",
      "Iteration 2250, loss = 1.07524413\n",
      "Iteration 2251, loss = 1.07509175\n",
      "Iteration 2252, loss = 1.07494098\n",
      "Iteration 2253, loss = 1.07478819\n",
      "Iteration 2254, loss = 1.07463746\n",
      "Iteration 2255, loss = 1.07448736\n",
      "Iteration 2256, loss = 1.07433436\n",
      "Iteration 2257, loss = 1.07418279\n",
      "Iteration 2258, loss = 1.07403229\n",
      "Iteration 2259, loss = 1.07388001\n",
      "Iteration 2260, loss = 1.07373000\n",
      "Iteration 2261, loss = 1.07357729\n",
      "Iteration 2262, loss = 1.07342859\n",
      "Iteration 2263, loss = 1.07327607\n",
      "Iteration 2264, loss = 1.07312451\n",
      "Iteration 2265, loss = 1.07297404\n",
      "Iteration 2266, loss = 1.07282261\n",
      "Iteration 2267, loss = 1.07267351\n",
      "Iteration 2268, loss = 1.07252248\n",
      "Iteration 2269, loss = 1.07237287\n",
      "Iteration 2270, loss = 1.07222087\n",
      "Iteration 2271, loss = 1.07207088\n",
      "Iteration 2272, loss = 1.07192005\n",
      "Iteration 2273, loss = 1.07176968\n",
      "Iteration 2274, loss = 1.07161930\n",
      "Iteration 2275, loss = 1.07146988\n",
      "Iteration 2276, loss = 1.07131975\n",
      "Iteration 2277, loss = 1.07116865\n",
      "Iteration 2278, loss = 1.07101856\n",
      "Iteration 2279, loss = 1.07086925\n",
      "Iteration 2280, loss = 1.07071842\n",
      "Iteration 2281, loss = 1.07057058\n",
      "Iteration 2282, loss = 1.07041959\n",
      "Iteration 2283, loss = 1.07026928\n",
      "Iteration 2284, loss = 1.07011938\n",
      "Iteration 2285, loss = 1.06996990\n",
      "Iteration 2286, loss = 1.06982297\n",
      "Iteration 2287, loss = 1.06967144\n",
      "Iteration 2288, loss = 1.06952137\n",
      "Iteration 2289, loss = 1.06937260\n",
      "Iteration 2290, loss = 1.06922407\n",
      "Iteration 2291, loss = 1.06907456\n",
      "Iteration 2292, loss = 1.06892536\n",
      "Iteration 2293, loss = 1.06877608\n",
      "Iteration 2294, loss = 1.06862645\n",
      "Iteration 2295, loss = 1.06847750\n",
      "Iteration 2296, loss = 1.06832794\n",
      "Iteration 2297, loss = 1.06818052\n",
      "Iteration 2298, loss = 1.06803143\n",
      "Iteration 2299, loss = 1.06788208\n",
      "Iteration 2300, loss = 1.06773332\n",
      "Iteration 2301, loss = 1.06758527\n",
      "Iteration 2302, loss = 1.06743694\n",
      "Iteration 2303, loss = 1.06728715\n",
      "Iteration 2304, loss = 1.06713909\n",
      "Iteration 2305, loss = 1.06699146\n",
      "Iteration 2306, loss = 1.06684155\n",
      "Iteration 2307, loss = 1.06669470\n",
      "Iteration 2308, loss = 1.06654518\n",
      "Iteration 2309, loss = 1.06639771\n",
      "Iteration 2310, loss = 1.06624910\n",
      "Iteration 2311, loss = 1.06610119\n",
      "Iteration 2312, loss = 1.06595345\n",
      "Iteration 2313, loss = 1.06580616\n",
      "Iteration 2314, loss = 1.06565682\n",
      "Iteration 2315, loss = 1.06550942\n",
      "Iteration 2316, loss = 1.06536218\n",
      "Iteration 2317, loss = 1.06521389\n",
      "Iteration 2318, loss = 1.06506715\n",
      "Iteration 2319, loss = 1.06491913\n",
      "Iteration 2320, loss = 1.06477121\n",
      "Iteration 2321, loss = 1.06462619\n",
      "Iteration 2322, loss = 1.06447714\n",
      "Iteration 2323, loss = 1.06432847\n",
      "Iteration 2324, loss = 1.06418275\n",
      "Iteration 2325, loss = 1.06403568\n",
      "Iteration 2326, loss = 1.06388737\n",
      "Iteration 2327, loss = 1.06374039\n",
      "Iteration 2328, loss = 1.06359414\n",
      "Iteration 2329, loss = 1.06344737\n",
      "Iteration 2330, loss = 1.06329999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2331, loss = 1.06315212\n",
      "Iteration 2332, loss = 1.06300551\n",
      "Iteration 2333, loss = 1.06286112\n",
      "Iteration 2334, loss = 1.06271198\n",
      "Iteration 2335, loss = 1.06256716\n",
      "Iteration 2336, loss = 1.06241903\n",
      "Iteration 2337, loss = 1.06227222\n",
      "Iteration 2338, loss = 1.06212588\n",
      "Iteration 2339, loss = 1.06197953\n",
      "Iteration 2340, loss = 1.06183323\n",
      "Iteration 2341, loss = 1.06168736\n",
      "Iteration 2342, loss = 1.06154271\n",
      "Iteration 2343, loss = 1.06139527\n",
      "Iteration 2344, loss = 1.06124736\n",
      "Iteration 2345, loss = 1.06110236\n",
      "Iteration 2346, loss = 1.06095761\n",
      "Iteration 2347, loss = 1.06080947\n",
      "Iteration 2348, loss = 1.06066422\n",
      "Iteration 2349, loss = 1.06051841\n",
      "Iteration 2350, loss = 1.06037281\n",
      "Iteration 2351, loss = 1.06022751\n",
      "Iteration 2352, loss = 1.06008164\n",
      "Iteration 2353, loss = 1.05993522\n",
      "Iteration 2354, loss = 1.05978976\n",
      "Iteration 2355, loss = 1.05964502\n",
      "Iteration 2356, loss = 1.05949868\n",
      "Iteration 2357, loss = 1.05935319\n",
      "Iteration 2358, loss = 1.05920773\n",
      "Iteration 2359, loss = 1.05906313\n",
      "Iteration 2360, loss = 1.05891683\n",
      "Iteration 2361, loss = 1.05877268\n",
      "Iteration 2362, loss = 1.05862607\n",
      "Iteration 2363, loss = 1.05848204\n",
      "Iteration 2364, loss = 1.05833831\n",
      "Iteration 2365, loss = 1.05819200\n",
      "Iteration 2366, loss = 1.05804624\n",
      "Iteration 2367, loss = 1.05790151\n",
      "Iteration 2368, loss = 1.05775742\n",
      "Iteration 2369, loss = 1.05761158\n",
      "Iteration 2370, loss = 1.05746682\n",
      "Iteration 2371, loss = 1.05732220\n",
      "Iteration 2372, loss = 1.05717796\n",
      "Iteration 2373, loss = 1.05703376\n",
      "Iteration 2374, loss = 1.05688862\n",
      "Iteration 2375, loss = 1.05674348\n",
      "Iteration 2376, loss = 1.05660074\n",
      "Iteration 2377, loss = 1.05645489\n",
      "Iteration 2378, loss = 1.05631238\n",
      "Iteration 2379, loss = 1.05616645\n",
      "Iteration 2380, loss = 1.05602272\n",
      "Iteration 2381, loss = 1.05587909\n",
      "Iteration 2382, loss = 1.05573397\n",
      "Iteration 2383, loss = 1.05559113\n",
      "Iteration 2384, loss = 1.05544638\n",
      "Iteration 2385, loss = 1.05530484\n",
      "Iteration 2386, loss = 1.05515826\n",
      "Iteration 2387, loss = 1.05501609\n",
      "Iteration 2388, loss = 1.05487271\n",
      "Iteration 2389, loss = 1.05472759\n",
      "Iteration 2390, loss = 1.05458417\n",
      "Iteration 2391, loss = 1.05444014\n",
      "Iteration 2392, loss = 1.05429700\n",
      "Iteration 2393, loss = 1.05415325\n",
      "Iteration 2394, loss = 1.05401037\n",
      "Iteration 2395, loss = 1.05386779\n",
      "Iteration 2396, loss = 1.05372355\n",
      "Iteration 2397, loss = 1.05358104\n",
      "Iteration 2398, loss = 1.05343709\n",
      "Iteration 2399, loss = 1.05329469\n",
      "Iteration 2400, loss = 1.05315113\n",
      "Iteration 2401, loss = 1.05300725\n",
      "Iteration 2402, loss = 1.05286603\n",
      "Iteration 2403, loss = 1.05272270\n",
      "Iteration 2404, loss = 1.05258022\n",
      "Iteration 2405, loss = 1.05243652\n",
      "Iteration 2406, loss = 1.05229417\n",
      "Iteration 2407, loss = 1.05215248\n",
      "Iteration 2408, loss = 1.05200851\n",
      "Iteration 2409, loss = 1.05186611\n",
      "Iteration 2410, loss = 1.05172367\n",
      "Iteration 2411, loss = 1.05158184\n",
      "Iteration 2412, loss = 1.05143876\n",
      "Iteration 2413, loss = 1.05129643\n",
      "Iteration 2414, loss = 1.05115531\n",
      "Iteration 2415, loss = 1.05101213\n",
      "Iteration 2416, loss = 1.05086955\n",
      "Iteration 2417, loss = 1.05072810\n",
      "Iteration 2418, loss = 1.05058497\n",
      "Iteration 2419, loss = 1.05044422\n",
      "Iteration 2420, loss = 1.05030144\n",
      "Iteration 2421, loss = 1.05015917\n",
      "Iteration 2422, loss = 1.05001812\n",
      "Iteration 2423, loss = 1.04987603\n",
      "Iteration 2424, loss = 1.04973475\n",
      "Iteration 2425, loss = 1.04959350\n",
      "Iteration 2426, loss = 1.04945216\n",
      "Iteration 2427, loss = 1.04930966\n",
      "Iteration 2428, loss = 1.04916806\n",
      "Iteration 2429, loss = 1.04902694\n",
      "Iteration 2430, loss = 1.04888607\n",
      "Iteration 2431, loss = 1.04874376\n",
      "Iteration 2432, loss = 1.04860176\n",
      "Iteration 2433, loss = 1.04846204\n",
      "Iteration 2434, loss = 1.04831998\n",
      "Iteration 2435, loss = 1.04817878\n",
      "Iteration 2436, loss = 1.04803692\n",
      "Iteration 2437, loss = 1.04789727\n",
      "Iteration 2438, loss = 1.04775533\n",
      "Iteration 2439, loss = 1.04761440\n",
      "Iteration 2440, loss = 1.04747382\n",
      "Iteration 2441, loss = 1.04733287\n",
      "Iteration 2442, loss = 1.04719210\n",
      "Iteration 2443, loss = 1.04705090\n",
      "Iteration 2444, loss = 1.04691163\n",
      "Iteration 2445, loss = 1.04677003\n",
      "Iteration 2446, loss = 1.04662930\n",
      "Iteration 2447, loss = 1.04648893\n",
      "Iteration 2448, loss = 1.04634849\n",
      "Iteration 2449, loss = 1.04620741\n",
      "Iteration 2450, loss = 1.04606688\n",
      "Iteration 2451, loss = 1.04592754\n",
      "Iteration 2452, loss = 1.04578626\n",
      "Iteration 2453, loss = 1.04564667\n",
      "Iteration 2454, loss = 1.04550596\n",
      "Iteration 2455, loss = 1.04536599\n",
      "Iteration 2456, loss = 1.04522672\n",
      "Iteration 2457, loss = 1.04508607\n",
      "Iteration 2458, loss = 1.04494626\n",
      "Iteration 2459, loss = 1.04480644\n",
      "Iteration 2460, loss = 1.04466671\n",
      "Iteration 2461, loss = 1.04452664\n",
      "Iteration 2462, loss = 1.04438699\n",
      "Iteration 2463, loss = 1.04424839\n",
      "Iteration 2464, loss = 1.04410919\n",
      "Iteration 2465, loss = 1.04396886\n",
      "Iteration 2466, loss = 1.04382810\n",
      "Iteration 2467, loss = 1.04369010\n",
      "Iteration 2468, loss = 1.04354982\n",
      "Iteration 2469, loss = 1.04341090\n",
      "Iteration 2470, loss = 1.04327211\n",
      "Iteration 2471, loss = 1.04313175\n",
      "Iteration 2472, loss = 1.04299185\n",
      "Iteration 2473, loss = 1.04285400\n",
      "Iteration 2474, loss = 1.04271355\n",
      "Iteration 2475, loss = 1.04257438\n",
      "Iteration 2476, loss = 1.04243679\n",
      "Iteration 2477, loss = 1.04229638\n",
      "Iteration 2478, loss = 1.04215886\n",
      "Iteration 2479, loss = 1.04201757\n",
      "Iteration 2480, loss = 1.04188083\n",
      "Iteration 2481, loss = 1.04174185\n",
      "Iteration 2482, loss = 1.04160231\n",
      "Iteration 2483, loss = 1.04146429\n",
      "Iteration 2484, loss = 1.04132508\n",
      "Iteration 2485, loss = 1.04118680\n",
      "Iteration 2486, loss = 1.04104756\n",
      "Iteration 2487, loss = 1.04090820\n",
      "Iteration 2488, loss = 1.04077041\n",
      "Iteration 2489, loss = 1.04063164\n",
      "Iteration 2490, loss = 1.04049543\n",
      "Iteration 2491, loss = 1.04035768\n",
      "Iteration 2492, loss = 1.04021782\n",
      "Iteration 2493, loss = 1.04007996\n",
      "Iteration 2494, loss = 1.03994193\n",
      "Iteration 2495, loss = 1.03980288\n",
      "Iteration 2496, loss = 1.03966496\n",
      "Iteration 2497, loss = 1.03952682\n",
      "Iteration 2498, loss = 1.03939004\n",
      "Iteration 2499, loss = 1.03925102\n",
      "Iteration 2500, loss = 1.03911405\n",
      "Iteration 2501, loss = 1.03897631\n",
      "Iteration 2502, loss = 1.03883641\n",
      "Iteration 2503, loss = 1.03870135\n",
      "Iteration 2504, loss = 1.03856268\n",
      "Iteration 2505, loss = 1.03842429\n",
      "Iteration 2506, loss = 1.03828693\n",
      "Iteration 2507, loss = 1.03814921\n",
      "Iteration 2508, loss = 1.03801176\n",
      "Iteration 2509, loss = 1.03787477\n",
      "Iteration 2510, loss = 1.03773878\n",
      "Iteration 2511, loss = 1.03760031\n",
      "Iteration 2512, loss = 1.03746301\n",
      "Iteration 2513, loss = 1.03732533\n",
      "Iteration 2514, loss = 1.03718817\n",
      "Iteration 2515, loss = 1.03705313\n",
      "Iteration 2516, loss = 1.03691388\n",
      "Iteration 2517, loss = 1.03677789\n",
      "Iteration 2518, loss = 1.03663974\n",
      "Iteration 2519, loss = 1.03650362\n",
      "Iteration 2520, loss = 1.03636618\n",
      "Iteration 2521, loss = 1.03623034\n",
      "Iteration 2522, loss = 1.03609260\n",
      "Iteration 2523, loss = 1.03595666\n",
      "Iteration 2524, loss = 1.03581985\n",
      "Iteration 2525, loss = 1.03568278\n",
      "Iteration 2526, loss = 1.03554734\n",
      "Iteration 2527, loss = 1.03541049\n",
      "Iteration 2528, loss = 1.03527390\n",
      "Iteration 2529, loss = 1.03513805\n",
      "Iteration 2530, loss = 1.03500209\n",
      "Iteration 2531, loss = 1.03486579\n",
      "Iteration 2532, loss = 1.03472876\n",
      "Iteration 2533, loss = 1.03459228\n",
      "Iteration 2534, loss = 1.03445535\n",
      "Iteration 2535, loss = 1.03431996\n",
      "Iteration 2536, loss = 1.03418389\n",
      "Iteration 2537, loss = 1.03404836\n",
      "Iteration 2538, loss = 1.03391165\n",
      "Iteration 2539, loss = 1.03377785\n",
      "Iteration 2540, loss = 1.03364048\n",
      "Iteration 2541, loss = 1.03350407\n",
      "Iteration 2542, loss = 1.03336919\n",
      "Iteration 2543, loss = 1.03323210\n",
      "Iteration 2544, loss = 1.03309660\n",
      "Iteration 2545, loss = 1.03296114\n",
      "Iteration 2546, loss = 1.03282644\n",
      "Iteration 2547, loss = 1.03269162\n",
      "Iteration 2548, loss = 1.03255606\n",
      "Iteration 2549, loss = 1.03242057\n",
      "Iteration 2550, loss = 1.03228506\n",
      "Iteration 2551, loss = 1.03214969\n",
      "Iteration 2552, loss = 1.03201439\n",
      "Iteration 2553, loss = 1.03187928\n",
      "Iteration 2554, loss = 1.03174506\n",
      "Iteration 2555, loss = 1.03160903\n",
      "Iteration 2556, loss = 1.03147363\n",
      "Iteration 2557, loss = 1.03133846\n",
      "Iteration 2558, loss = 1.03120267\n",
      "Iteration 2559, loss = 1.03106907\n",
      "Iteration 2560, loss = 1.03093406\n",
      "Iteration 2561, loss = 1.03079958\n",
      "Iteration 2562, loss = 1.03066482\n",
      "Iteration 2563, loss = 1.03052965\n",
      "Iteration 2564, loss = 1.03039616\n",
      "Iteration 2565, loss = 1.03026157\n",
      "Iteration 2566, loss = 1.03012701\n",
      "Iteration 2567, loss = 1.02999276\n",
      "Iteration 2568, loss = 1.02985827\n",
      "Iteration 2569, loss = 1.02972199\n",
      "Iteration 2570, loss = 1.02958804\n",
      "Iteration 2571, loss = 1.02945424\n",
      "Iteration 2572, loss = 1.02931964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2573, loss = 1.02918573\n",
      "Iteration 2574, loss = 1.02905078\n",
      "Iteration 2575, loss = 1.02891761\n",
      "Iteration 2576, loss = 1.02878356\n",
      "Iteration 2577, loss = 1.02864925\n",
      "Iteration 2578, loss = 1.02851470\n",
      "Iteration 2579, loss = 1.02838239\n",
      "Iteration 2580, loss = 1.02824707\n",
      "Iteration 2581, loss = 1.02811409\n",
      "Iteration 2582, loss = 1.02798103\n",
      "Iteration 2583, loss = 1.02784594\n",
      "Iteration 2584, loss = 1.02771250\n",
      "Iteration 2585, loss = 1.02757942\n",
      "Iteration 2586, loss = 1.02744540\n",
      "Iteration 2587, loss = 1.02731178\n",
      "Iteration 2588, loss = 1.02717809\n",
      "Iteration 2589, loss = 1.02704588\n",
      "Iteration 2590, loss = 1.02691188\n",
      "Iteration 2591, loss = 1.02677786\n",
      "Iteration 2592, loss = 1.02664584\n",
      "Iteration 2593, loss = 1.02651265\n",
      "Iteration 2594, loss = 1.02637804\n",
      "Iteration 2595, loss = 1.02624592\n",
      "Iteration 2596, loss = 1.02611252\n",
      "Iteration 2597, loss = 1.02597910\n",
      "Iteration 2598, loss = 1.02584675\n",
      "Iteration 2599, loss = 1.02571364\n",
      "Iteration 2600, loss = 1.02558061\n",
      "Iteration 2601, loss = 1.02544723\n",
      "Iteration 2602, loss = 1.02531536\n",
      "Iteration 2603, loss = 1.02518302\n",
      "Iteration 2604, loss = 1.02504940\n",
      "Iteration 2605, loss = 1.02491733\n",
      "Iteration 2606, loss = 1.02478431\n",
      "Iteration 2607, loss = 1.02465196\n",
      "Iteration 2608, loss = 1.02451908\n",
      "Iteration 2609, loss = 1.02438811\n",
      "Iteration 2610, loss = 1.02425444\n",
      "Iteration 2611, loss = 1.02412337\n",
      "Iteration 2612, loss = 1.02399070\n",
      "Iteration 2613, loss = 1.02385651\n",
      "Iteration 2614, loss = 1.02372468\n",
      "Iteration 2615, loss = 1.02359404\n",
      "Iteration 2616, loss = 1.02346166\n",
      "Iteration 2617, loss = 1.02332977\n",
      "Iteration 2618, loss = 1.02319712\n",
      "Iteration 2619, loss = 1.02306492\n",
      "Iteration 2620, loss = 1.02293449\n",
      "Iteration 2621, loss = 1.02280190\n",
      "Iteration 2622, loss = 1.02266988\n",
      "Iteration 2623, loss = 1.02253933\n",
      "Iteration 2624, loss = 1.02240755\n",
      "Iteration 2625, loss = 1.02227488\n",
      "Iteration 2626, loss = 1.02214335\n",
      "Iteration 2627, loss = 1.02201130\n",
      "Iteration 2628, loss = 1.02188052\n",
      "Iteration 2629, loss = 1.02174952\n",
      "Iteration 2630, loss = 1.02161674\n",
      "Iteration 2631, loss = 1.02148595\n",
      "Iteration 2632, loss = 1.02135533\n",
      "Iteration 2633, loss = 1.02122333\n",
      "Iteration 2634, loss = 1.02109208\n",
      "Iteration 2635, loss = 1.02095999\n",
      "Iteration 2636, loss = 1.02082973\n",
      "Iteration 2637, loss = 1.02069917\n",
      "Iteration 2638, loss = 1.02056845\n",
      "Iteration 2639, loss = 1.02043660\n",
      "Iteration 2640, loss = 1.02030592\n",
      "Iteration 2641, loss = 1.02017494\n",
      "Iteration 2642, loss = 1.02004408\n",
      "Iteration 2643, loss = 1.01991363\n",
      "Iteration 2644, loss = 1.01978311\n",
      "Iteration 2645, loss = 1.01965208\n",
      "Iteration 2646, loss = 1.01952266\n",
      "Iteration 2647, loss = 1.01939096\n",
      "Iteration 2648, loss = 1.01925943\n",
      "Iteration 2649, loss = 1.01912907\n",
      "Iteration 2650, loss = 1.01899960\n",
      "Iteration 2651, loss = 1.01886856\n",
      "Iteration 2652, loss = 1.01873800\n",
      "Iteration 2653, loss = 1.01860917\n",
      "Iteration 2654, loss = 1.01847869\n",
      "Iteration 2655, loss = 1.01834682\n",
      "Iteration 2656, loss = 1.01821787\n",
      "Iteration 2657, loss = 1.01808783\n",
      "Iteration 2658, loss = 1.01795718\n",
      "Iteration 2659, loss = 1.01782662\n",
      "Iteration 2660, loss = 1.01769778\n",
      "Iteration 2661, loss = 1.01756671\n",
      "Iteration 2662, loss = 1.01743736\n",
      "Iteration 2663, loss = 1.01730676\n",
      "Iteration 2664, loss = 1.01717782\n",
      "Iteration 2665, loss = 1.01704860\n",
      "Iteration 2666, loss = 1.01691847\n",
      "Iteration 2667, loss = 1.01678669\n",
      "Iteration 2668, loss = 1.01665787\n",
      "Iteration 2669, loss = 1.01652992\n",
      "Iteration 2670, loss = 1.01640050\n",
      "Iteration 2671, loss = 1.01627026\n",
      "Iteration 2672, loss = 1.01613937\n",
      "Iteration 2673, loss = 1.01601114\n",
      "Iteration 2674, loss = 1.01588038\n",
      "Iteration 2675, loss = 1.01575222\n",
      "Iteration 2676, loss = 1.01562234\n",
      "Iteration 2677, loss = 1.01549464\n",
      "Iteration 2678, loss = 1.01536406\n",
      "Iteration 2679, loss = 1.01523426\n",
      "Iteration 2680, loss = 1.01510585\n",
      "Iteration 2681, loss = 1.01497741\n",
      "Iteration 2682, loss = 1.01484844\n",
      "Iteration 2683, loss = 1.01471906\n",
      "Iteration 2684, loss = 1.01458996\n",
      "Iteration 2685, loss = 1.01446119\n",
      "Iteration 2686, loss = 1.01433289\n",
      "Iteration 2687, loss = 1.01420389\n",
      "Iteration 2688, loss = 1.01407518\n",
      "Iteration 2689, loss = 1.01394608\n",
      "Iteration 2690, loss = 1.01381799\n",
      "Iteration 2691, loss = 1.01368996\n",
      "Iteration 2692, loss = 1.01355985\n",
      "Iteration 2693, loss = 1.01343278\n",
      "Iteration 2694, loss = 1.01330290\n",
      "Iteration 2695, loss = 1.01317485\n",
      "Iteration 2696, loss = 1.01304734\n",
      "Iteration 2697, loss = 1.01291934\n",
      "Iteration 2698, loss = 1.01279066\n",
      "Iteration 2699, loss = 1.01266291\n",
      "Iteration 2700, loss = 1.01253492\n",
      "Iteration 2701, loss = 1.01240543\n",
      "Iteration 2702, loss = 1.01227831\n",
      "Iteration 2703, loss = 1.01214897\n",
      "Iteration 2704, loss = 1.01202318\n",
      "Iteration 2705, loss = 1.01189456\n",
      "Iteration 2706, loss = 1.01176612\n",
      "Iteration 2707, loss = 1.01163823\n",
      "Iteration 2708, loss = 1.01151027\n",
      "Iteration 2709, loss = 1.01138278\n",
      "Iteration 2710, loss = 1.01125413\n",
      "Iteration 2711, loss = 1.01112658\n",
      "Iteration 2712, loss = 1.01099949\n",
      "Iteration 2713, loss = 1.01087178\n",
      "Iteration 2714, loss = 1.01074375\n",
      "Iteration 2715, loss = 1.01061668\n",
      "Iteration 2716, loss = 1.01049047\n",
      "Iteration 2717, loss = 1.01036158\n",
      "Iteration 2718, loss = 1.01023390\n",
      "Iteration 2719, loss = 1.01010730\n",
      "Iteration 2720, loss = 1.00998059\n",
      "Iteration 2721, loss = 1.00985230\n",
      "Iteration 2722, loss = 1.00972666\n",
      "Iteration 2723, loss = 1.00959803\n",
      "Iteration 2724, loss = 1.00947091\n",
      "Iteration 2725, loss = 1.00934461\n",
      "Iteration 2726, loss = 1.00921742\n",
      "Iteration 2727, loss = 1.00908984\n",
      "Iteration 2728, loss = 1.00896263\n",
      "Iteration 2729, loss = 1.00883581\n",
      "Iteration 2730, loss = 1.00870941\n",
      "Iteration 2731, loss = 1.00858274\n",
      "Iteration 2732, loss = 1.00845652\n",
      "Iteration 2733, loss = 1.00833007\n",
      "Iteration 2734, loss = 1.00820177\n",
      "Iteration 2735, loss = 1.00807593\n",
      "Iteration 2736, loss = 1.00794994\n",
      "Iteration 2737, loss = 1.00782343\n",
      "Iteration 2738, loss = 1.00769654\n",
      "Iteration 2739, loss = 1.00756940\n",
      "Iteration 2740, loss = 1.00744365\n",
      "Iteration 2741, loss = 1.00731701\n",
      "Iteration 2742, loss = 1.00719054\n",
      "Iteration 2743, loss = 1.00706444\n",
      "Iteration 2744, loss = 1.00693812\n",
      "Iteration 2745, loss = 1.00681327\n",
      "Iteration 2746, loss = 1.00668575\n",
      "Iteration 2747, loss = 1.00656056\n",
      "Iteration 2748, loss = 1.00643321\n",
      "Iteration 2749, loss = 1.00630809\n",
      "Iteration 2750, loss = 1.00618164\n",
      "Iteration 2751, loss = 1.00605695\n",
      "Iteration 2752, loss = 1.00593008\n",
      "Iteration 2753, loss = 1.00580423\n",
      "Iteration 2754, loss = 1.00567936\n",
      "Iteration 2755, loss = 1.00555436\n",
      "Iteration 2756, loss = 1.00542725\n",
      "Iteration 2757, loss = 1.00530182\n",
      "Iteration 2758, loss = 1.00517768\n",
      "Iteration 2759, loss = 1.00504948\n",
      "Iteration 2760, loss = 1.00492590\n",
      "Iteration 2761, loss = 1.00480071\n",
      "Iteration 2762, loss = 1.00467548\n",
      "Iteration 2763, loss = 1.00454985\n",
      "Iteration 2764, loss = 1.00442312\n",
      "Iteration 2765, loss = 1.00429924\n",
      "Iteration 2766, loss = 1.00417215\n",
      "Iteration 2767, loss = 1.00404772\n",
      "Iteration 2768, loss = 1.00392329\n",
      "Iteration 2769, loss = 1.00379798\n",
      "Iteration 2770, loss = 1.00367338\n",
      "Iteration 2771, loss = 1.00354788\n",
      "Iteration 2772, loss = 1.00342170\n",
      "Iteration 2773, loss = 1.00329754\n",
      "Iteration 2774, loss = 1.00317328\n",
      "Iteration 2775, loss = 1.00304767\n",
      "Iteration 2776, loss = 1.00292355\n",
      "Iteration 2777, loss = 1.00279920\n",
      "Iteration 2778, loss = 1.00267447\n",
      "Iteration 2779, loss = 1.00254909\n",
      "Iteration 2780, loss = 1.00242406\n",
      "Iteration 2781, loss = 1.00229924\n",
      "Iteration 2782, loss = 1.00217502\n",
      "Iteration 2783, loss = 1.00205105\n",
      "Iteration 2784, loss = 1.00192737\n",
      "Iteration 2785, loss = 1.00180196\n",
      "Iteration 2786, loss = 1.00167850\n",
      "Iteration 2787, loss = 1.00155384\n",
      "Iteration 2788, loss = 1.00143005\n",
      "Iteration 2789, loss = 1.00130474\n",
      "Iteration 2790, loss = 1.00118094\n",
      "Iteration 2791, loss = 1.00105664\n",
      "Iteration 2792, loss = 1.00093318\n",
      "Iteration 2793, loss = 1.00080863\n",
      "Iteration 2794, loss = 1.00068435\n",
      "Iteration 2795, loss = 1.00056157\n",
      "Iteration 2796, loss = 1.00043654\n",
      "Iteration 2797, loss = 1.00031389\n",
      "Iteration 2798, loss = 1.00018991\n",
      "Iteration 2799, loss = 1.00006485\n",
      "Iteration 2800, loss = 0.99994210\n",
      "Iteration 2801, loss = 0.99981897\n",
      "Iteration 2802, loss = 0.99969629\n",
      "Iteration 2803, loss = 0.99957154\n",
      "Iteration 2804, loss = 0.99944812\n",
      "Iteration 2805, loss = 0.99932467\n",
      "Iteration 2806, loss = 0.99920069\n",
      "Iteration 2807, loss = 0.99907804\n",
      "Iteration 2808, loss = 0.99895488\n",
      "Iteration 2809, loss = 0.99883104\n",
      "Iteration 2810, loss = 0.99870708\n",
      "Iteration 2811, loss = 0.99858470\n",
      "Iteration 2812, loss = 0.99846156\n",
      "Iteration 2813, loss = 0.99833739\n",
      "Iteration 2814, loss = 0.99821511\n",
      "Iteration 2815, loss = 0.99809213\n",
      "Iteration 2816, loss = 0.99796835\n",
      "Iteration 2817, loss = 0.99784518\n",
      "Iteration 2818, loss = 0.99772299\n",
      "Iteration 2819, loss = 0.99759988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2820, loss = 0.99747735\n",
      "Iteration 2821, loss = 0.99735390\n",
      "Iteration 2822, loss = 0.99723097\n",
      "Iteration 2823, loss = 0.99710850\n",
      "Iteration 2824, loss = 0.99698598\n",
      "Iteration 2825, loss = 0.99686538\n",
      "Iteration 2826, loss = 0.99674138\n",
      "Iteration 2827, loss = 0.99661851\n",
      "Iteration 2828, loss = 0.99649626\n",
      "Iteration 2829, loss = 0.99637316\n",
      "Iteration 2830, loss = 0.99625018\n",
      "Iteration 2831, loss = 0.99612873\n",
      "Iteration 2832, loss = 0.99600600\n",
      "Iteration 2833, loss = 0.99588390\n",
      "Iteration 2834, loss = 0.99576060\n",
      "Iteration 2835, loss = 0.99563811\n",
      "Iteration 2836, loss = 0.99551591\n",
      "Iteration 2837, loss = 0.99539450\n",
      "Iteration 2838, loss = 0.99527237\n",
      "Iteration 2839, loss = 0.99515063\n",
      "Iteration 2840, loss = 0.99502823\n",
      "Iteration 2841, loss = 0.99490594\n",
      "Iteration 2842, loss = 0.99478405\n",
      "Iteration 2843, loss = 0.99466181\n",
      "Iteration 2844, loss = 0.99454051\n",
      "Iteration 2845, loss = 0.99441855\n",
      "Iteration 2846, loss = 0.99429693\n",
      "Iteration 2847, loss = 0.99417459\n",
      "Iteration 2848, loss = 0.99405289\n",
      "Iteration 2849, loss = 0.99393243\n",
      "Iteration 2850, loss = 0.99381011\n",
      "Iteration 2851, loss = 0.99368850\n",
      "Iteration 2852, loss = 0.99356683\n",
      "Iteration 2853, loss = 0.99344575\n",
      "Iteration 2854, loss = 0.99332478\n",
      "Iteration 2855, loss = 0.99320274\n",
      "Iteration 2856, loss = 0.99308112\n",
      "Iteration 2857, loss = 0.99295949\n",
      "Iteration 2858, loss = 0.99283778\n",
      "Iteration 2859, loss = 0.99271689\n",
      "Iteration 2860, loss = 0.99259594\n",
      "Iteration 2861, loss = 0.99247584\n",
      "Iteration 2862, loss = 0.99235340\n",
      "Iteration 2863, loss = 0.99223274\n",
      "Iteration 2864, loss = 0.99211103\n",
      "Iteration 2865, loss = 0.99199018\n",
      "Iteration 2866, loss = 0.99186853\n",
      "Iteration 2867, loss = 0.99174901\n",
      "Iteration 2868, loss = 0.99162707\n",
      "Iteration 2869, loss = 0.99150817\n",
      "Iteration 2870, loss = 0.99138602\n",
      "Iteration 2871, loss = 0.99126381\n",
      "Iteration 2872, loss = 0.99114418\n",
      "Iteration 2873, loss = 0.99102337\n",
      "Iteration 2874, loss = 0.99090201\n",
      "Iteration 2875, loss = 0.99078202\n",
      "Iteration 2876, loss = 0.99066255\n",
      "Iteration 2877, loss = 0.99054126\n",
      "Iteration 2878, loss = 0.99042141\n",
      "Iteration 2879, loss = 0.99030111\n",
      "Iteration 2880, loss = 0.99017975\n",
      "Iteration 2881, loss = 0.99006045\n",
      "Iteration 2882, loss = 0.98993917\n",
      "Iteration 2883, loss = 0.98981966\n",
      "Iteration 2884, loss = 0.98969855\n",
      "Iteration 2885, loss = 0.98957784\n",
      "Iteration 2886, loss = 0.98945794\n",
      "Iteration 2887, loss = 0.98933898\n",
      "Iteration 2888, loss = 0.98921765\n",
      "Iteration 2889, loss = 0.98909843\n",
      "Iteration 2890, loss = 0.98897780\n",
      "Iteration 2891, loss = 0.98885811\n",
      "Iteration 2892, loss = 0.98873874\n",
      "Iteration 2893, loss = 0.98861801\n",
      "Iteration 2894, loss = 0.98849871\n",
      "Iteration 2895, loss = 0.98837969\n",
      "Iteration 2896, loss = 0.98825919\n",
      "Iteration 2897, loss = 0.98813924\n",
      "Iteration 2898, loss = 0.98802058\n",
      "Iteration 2899, loss = 0.98789933\n",
      "Iteration 2900, loss = 0.98778029\n",
      "Iteration 2901, loss = 0.98766109\n",
      "Iteration 2902, loss = 0.98754103\n",
      "Iteration 2903, loss = 0.98742128\n",
      "Iteration 2904, loss = 0.98730197\n",
      "Iteration 2905, loss = 0.98718434\n",
      "Iteration 2906, loss = 0.98706378\n",
      "Iteration 2907, loss = 0.98694422\n",
      "Iteration 2908, loss = 0.98682601\n",
      "Iteration 2909, loss = 0.98670537\n",
      "Iteration 2910, loss = 0.98658716\n",
      "Iteration 2911, loss = 0.98646683\n",
      "Iteration 2912, loss = 0.98634836\n",
      "Iteration 2913, loss = 0.98622974\n",
      "Iteration 2914, loss = 0.98611040\n",
      "Iteration 2915, loss = 0.98599172\n",
      "Iteration 2916, loss = 0.98587278\n",
      "Iteration 2917, loss = 0.98575430\n",
      "Iteration 2918, loss = 0.98563457\n",
      "Iteration 2919, loss = 0.98551673\n",
      "Iteration 2920, loss = 0.98539661\n",
      "Iteration 2921, loss = 0.98527857\n",
      "Iteration 2922, loss = 0.98516038\n",
      "Iteration 2923, loss = 0.98504149\n",
      "Iteration 2924, loss = 0.98492250\n",
      "Iteration 2925, loss = 0.98480427\n",
      "Iteration 2926, loss = 0.98468514\n",
      "Iteration 2927, loss = 0.98456659\n",
      "Iteration 2928, loss = 0.98444863\n",
      "Iteration 2929, loss = 0.98432947\n",
      "Iteration 2930, loss = 0.98421152\n",
      "Iteration 2931, loss = 0.98409256\n",
      "Iteration 2932, loss = 0.98397569\n",
      "Iteration 2933, loss = 0.98385653\n",
      "Iteration 2934, loss = 0.98373869\n",
      "Iteration 2935, loss = 0.98362066\n",
      "Iteration 2936, loss = 0.98350215\n",
      "Iteration 2937, loss = 0.98338400\n",
      "Iteration 2938, loss = 0.98326534\n",
      "Iteration 2939, loss = 0.98314868\n",
      "Iteration 2940, loss = 0.98303082\n",
      "Iteration 2941, loss = 0.98291263\n",
      "Iteration 2942, loss = 0.98279548\n",
      "Iteration 2943, loss = 0.98267715\n",
      "Iteration 2944, loss = 0.98255814\n",
      "Iteration 2945, loss = 0.98244038\n",
      "Iteration 2946, loss = 0.98232372\n",
      "Iteration 2947, loss = 0.98220577\n",
      "Iteration 2948, loss = 0.98208786\n",
      "Iteration 2949, loss = 0.98197110\n",
      "Iteration 2950, loss = 0.98185355\n",
      "Iteration 2951, loss = 0.98173585\n",
      "Iteration 2952, loss = 0.98161749\n",
      "Iteration 2953, loss = 0.98150070\n",
      "Iteration 2954, loss = 0.98138293\n",
      "Iteration 2955, loss = 0.98126596\n",
      "Iteration 2956, loss = 0.98114790\n",
      "Iteration 2957, loss = 0.98103158\n",
      "Iteration 2958, loss = 0.98091436\n",
      "Iteration 2959, loss = 0.98079734\n",
      "Iteration 2960, loss = 0.98067899\n",
      "Iteration 2961, loss = 0.98056225\n",
      "Iteration 2962, loss = 0.98044546\n",
      "Iteration 2963, loss = 0.98032930\n",
      "Iteration 2964, loss = 0.98021171\n",
      "Iteration 2965, loss = 0.98009379\n",
      "Iteration 2966, loss = 0.97997662\n",
      "Iteration 2967, loss = 0.97986133\n",
      "Iteration 2968, loss = 0.97974452\n",
      "Iteration 2969, loss = 0.97962701\n",
      "Iteration 2970, loss = 0.97951008\n",
      "Iteration 2971, loss = 0.97939340\n",
      "Iteration 2972, loss = 0.97927740\n",
      "Iteration 2973, loss = 0.97916042\n",
      "Iteration 2974, loss = 0.97904372\n",
      "Iteration 2975, loss = 0.97892712\n",
      "Iteration 2976, loss = 0.97880939\n",
      "Iteration 2977, loss = 0.97869322\n",
      "Iteration 2978, loss = 0.97857648\n",
      "Iteration 2979, loss = 0.97846095\n",
      "Iteration 2980, loss = 0.97834377\n",
      "Iteration 2981, loss = 0.97822723\n",
      "Iteration 2982, loss = 0.97811164\n",
      "Iteration 2983, loss = 0.97799470\n",
      "Iteration 2984, loss = 0.97787930\n",
      "Iteration 2985, loss = 0.97776280\n",
      "Iteration 2986, loss = 0.97764682\n",
      "Iteration 2987, loss = 0.97753093\n",
      "Iteration 2988, loss = 0.97741419\n",
      "Iteration 2989, loss = 0.97729824\n",
      "Iteration 2990, loss = 0.97718329\n",
      "Iteration 2991, loss = 0.97706689\n",
      "Iteration 2992, loss = 0.97695105\n",
      "Iteration 2993, loss = 0.97683349\n",
      "Iteration 2994, loss = 0.97671842\n",
      "Iteration 2995, loss = 0.97660218\n",
      "Iteration 2996, loss = 0.97648670\n",
      "Iteration 2997, loss = 0.97637039\n",
      "Iteration 2998, loss = 0.97625532\n",
      "Iteration 2999, loss = 0.97613952\n",
      "Iteration 3000, loss = 0.97602420\n",
      "Iteration 3001, loss = 0.97590820\n",
      "Iteration 3002, loss = 0.97579174\n",
      "Iteration 3003, loss = 0.97567706\n",
      "Iteration 3004, loss = 0.97556100\n",
      "Iteration 3005, loss = 0.97544651\n",
      "Iteration 3006, loss = 0.97533124\n",
      "Iteration 3007, loss = 0.97521537\n",
      "Iteration 3008, loss = 0.97509991\n",
      "Iteration 3009, loss = 0.97498491\n",
      "Iteration 3010, loss = 0.97486874\n",
      "Iteration 3011, loss = 0.97475523\n",
      "Iteration 3012, loss = 0.97463875\n",
      "Iteration 3013, loss = 0.97452460\n",
      "Iteration 3014, loss = 0.97440781\n",
      "Iteration 3015, loss = 0.97429295\n",
      "Iteration 3016, loss = 0.97417796\n",
      "Iteration 3017, loss = 0.97406334\n",
      "Iteration 3018, loss = 0.97394841\n",
      "Iteration 3019, loss = 0.97383407\n",
      "Iteration 3020, loss = 0.97371855\n",
      "Iteration 3021, loss = 0.97360391\n",
      "Iteration 3022, loss = 0.97348890\n",
      "Iteration 3023, loss = 0.97337339\n",
      "Iteration 3024, loss = 0.97325950\n",
      "Iteration 3025, loss = 0.97314399\n",
      "Iteration 3026, loss = 0.97302964\n",
      "Iteration 3027, loss = 0.97291588\n",
      "Iteration 3028, loss = 0.97280170\n",
      "Iteration 3029, loss = 0.97268529\n",
      "Iteration 3030, loss = 0.97257100\n",
      "Iteration 3031, loss = 0.97245611\n",
      "Iteration 3032, loss = 0.97234164\n",
      "Iteration 3033, loss = 0.97222715\n",
      "Iteration 3034, loss = 0.97211321\n",
      "Iteration 3035, loss = 0.97199838\n",
      "Iteration 3036, loss = 0.97188432\n",
      "Iteration 3037, loss = 0.97177010\n",
      "Iteration 3038, loss = 0.97165506\n",
      "Iteration 3039, loss = 0.97154222\n",
      "Iteration 3040, loss = 0.97142718\n",
      "Iteration 3041, loss = 0.97131241\n",
      "Iteration 3042, loss = 0.97120076\n",
      "Iteration 3043, loss = 0.97108487\n",
      "Iteration 3044, loss = 0.97097117\n",
      "Iteration 3045, loss = 0.97085746\n",
      "Iteration 3046, loss = 0.97074290\n",
      "Iteration 3047, loss = 0.97063041\n",
      "Iteration 3048, loss = 0.97051523\n",
      "Iteration 3049, loss = 0.97040122\n",
      "Iteration 3050, loss = 0.97028751\n",
      "Iteration 3051, loss = 0.97017403\n",
      "Iteration 3052, loss = 0.97006049\n",
      "Iteration 3053, loss = 0.96994703\n",
      "Iteration 3054, loss = 0.96983312\n",
      "Iteration 3055, loss = 0.96971939\n",
      "Iteration 3056, loss = 0.96960620\n",
      "Iteration 3057, loss = 0.96949252\n",
      "Iteration 3058, loss = 0.96937903\n",
      "Iteration 3059, loss = 0.96926450\n",
      "Iteration 3060, loss = 0.96915090\n",
      "Iteration 3061, loss = 0.96903863\n",
      "Iteration 3062, loss = 0.96892486\n",
      "Iteration 3063, loss = 0.96881194\n",
      "Iteration 3064, loss = 0.96869855\n",
      "Iteration 3065, loss = 0.96858603\n",
      "Iteration 3066, loss = 0.96847317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3067, loss = 0.96835883\n",
      "Iteration 3068, loss = 0.96824512\n",
      "Iteration 3069, loss = 0.96813270\n",
      "Iteration 3070, loss = 0.96802048\n",
      "Iteration 3071, loss = 0.96790734\n",
      "Iteration 3072, loss = 0.96779460\n",
      "Iteration 3073, loss = 0.96768120\n",
      "Iteration 3074, loss = 0.96756759\n",
      "Iteration 3075, loss = 0.96745582\n",
      "Iteration 3076, loss = 0.96734245\n",
      "Iteration 3077, loss = 0.96722935\n",
      "Iteration 3078, loss = 0.96711757\n",
      "Iteration 3079, loss = 0.96700423\n",
      "Iteration 3080, loss = 0.96689090\n",
      "Iteration 3081, loss = 0.96677842\n",
      "Iteration 3082, loss = 0.96666535\n",
      "Iteration 3083, loss = 0.96655280\n",
      "Iteration 3084, loss = 0.96644031\n",
      "Iteration 3085, loss = 0.96632750\n",
      "Iteration 3086, loss = 0.96621711\n",
      "Iteration 3087, loss = 0.96610357\n",
      "Iteration 3088, loss = 0.96599094\n",
      "Iteration 3089, loss = 0.96587906\n",
      "Iteration 3090, loss = 0.96576648\n",
      "Iteration 3091, loss = 0.96565355\n",
      "Iteration 3092, loss = 0.96554141\n",
      "Iteration 3093, loss = 0.96542918\n",
      "Iteration 3094, loss = 0.96531672\n",
      "Iteration 3095, loss = 0.96520463\n",
      "Iteration 3096, loss = 0.96509240\n",
      "Iteration 3097, loss = 0.96498041\n",
      "Iteration 3098, loss = 0.96486852\n",
      "Iteration 3099, loss = 0.96475610\n",
      "Iteration 3100, loss = 0.96464445\n",
      "Iteration 3101, loss = 0.96453273\n",
      "Iteration 3102, loss = 0.96441980\n",
      "Iteration 3103, loss = 0.96430729\n",
      "Iteration 3104, loss = 0.96419709\n",
      "Iteration 3105, loss = 0.96408390\n",
      "Iteration 3106, loss = 0.96397293\n",
      "Iteration 3107, loss = 0.96386050\n",
      "Iteration 3108, loss = 0.96375010\n",
      "Iteration 3109, loss = 0.96363743\n",
      "Iteration 3110, loss = 0.96352578\n",
      "Iteration 3111, loss = 0.96341406\n",
      "Iteration 3112, loss = 0.96330202\n",
      "Iteration 3113, loss = 0.96319000\n",
      "Iteration 3114, loss = 0.96307901\n",
      "Iteration 3115, loss = 0.96296830\n",
      "Iteration 3116, loss = 0.96285684\n",
      "Iteration 3117, loss = 0.96274503\n",
      "Iteration 3118, loss = 0.96263325\n",
      "Iteration 3119, loss = 0.96252157\n",
      "Iteration 3120, loss = 0.96241092\n",
      "Iteration 3121, loss = 0.96229966\n",
      "Iteration 3122, loss = 0.96218901\n",
      "Iteration 3123, loss = 0.96207679\n",
      "Iteration 3124, loss = 0.96196575\n",
      "Iteration 3125, loss = 0.96185447\n",
      "Iteration 3126, loss = 0.96174435\n",
      "Iteration 3127, loss = 0.96163260\n",
      "Iteration 3128, loss = 0.96152119\n",
      "Iteration 3129, loss = 0.96141017\n",
      "Iteration 3130, loss = 0.96129987\n",
      "Iteration 3131, loss = 0.96118748\n",
      "Iteration 3132, loss = 0.96107747\n",
      "Iteration 3133, loss = 0.96096629\n",
      "Iteration 3134, loss = 0.96085474\n",
      "Iteration 3135, loss = 0.96074434\n",
      "Iteration 3136, loss = 0.96063471\n",
      "Iteration 3137, loss = 0.96052316\n",
      "Iteration 3138, loss = 0.96041187\n",
      "Iteration 3139, loss = 0.96030178\n",
      "Iteration 3140, loss = 0.96019169\n",
      "Iteration 3141, loss = 0.96007971\n",
      "Iteration 3142, loss = 0.95996988\n",
      "Iteration 3143, loss = 0.95985962\n",
      "Iteration 3144, loss = 0.95974826\n",
      "Iteration 3145, loss = 0.95963938\n",
      "Iteration 3146, loss = 0.95952802\n",
      "Iteration 3147, loss = 0.95941742\n",
      "Iteration 3148, loss = 0.95930689\n",
      "Iteration 3149, loss = 0.95919884\n",
      "Iteration 3150, loss = 0.95908639\n",
      "Iteration 3151, loss = 0.95897638\n",
      "Iteration 3152, loss = 0.95886621\n",
      "Iteration 3153, loss = 0.95875539\n",
      "Iteration 3154, loss = 0.95864549\n",
      "Iteration 3155, loss = 0.95853513\n",
      "Iteration 3156, loss = 0.95842548\n",
      "Iteration 3157, loss = 0.95831558\n",
      "Iteration 3158, loss = 0.95820575\n",
      "Iteration 3159, loss = 0.95809551\n",
      "Iteration 3160, loss = 0.95798578\n",
      "Iteration 3161, loss = 0.95787467\n",
      "Iteration 3162, loss = 0.95776647\n",
      "Iteration 3163, loss = 0.95765590\n",
      "Iteration 3164, loss = 0.95754640\n",
      "Iteration 3165, loss = 0.95743605\n",
      "Iteration 3166, loss = 0.95732620\n",
      "Iteration 3167, loss = 0.95721688\n",
      "Iteration 3168, loss = 0.95710781\n",
      "Iteration 3169, loss = 0.95699790\n",
      "Iteration 3170, loss = 0.95688755\n",
      "Iteration 3171, loss = 0.95677847\n",
      "Iteration 3172, loss = 0.95666906\n",
      "Iteration 3173, loss = 0.95655915\n",
      "Iteration 3174, loss = 0.95644971\n",
      "Iteration 3175, loss = 0.95634028\n",
      "Iteration 3176, loss = 0.95623029\n",
      "Iteration 3177, loss = 0.95612174\n",
      "Iteration 3178, loss = 0.95601320\n",
      "Iteration 3179, loss = 0.95590329\n",
      "Iteration 3180, loss = 0.95579395\n",
      "Iteration 3181, loss = 0.95568461\n",
      "Iteration 3182, loss = 0.95557587\n",
      "Iteration 3183, loss = 0.95546668\n",
      "Iteration 3184, loss = 0.95535774\n",
      "Iteration 3185, loss = 0.95524852\n",
      "Iteration 3186, loss = 0.95513923\n",
      "Iteration 3187, loss = 0.95503113\n",
      "Iteration 3188, loss = 0.95492127\n",
      "Iteration 3189, loss = 0.95481314\n",
      "Iteration 3190, loss = 0.95470341\n",
      "Iteration 3191, loss = 0.95459479\n",
      "Iteration 3192, loss = 0.95448598\n",
      "Iteration 3193, loss = 0.95437698\n",
      "Iteration 3194, loss = 0.95426854\n",
      "Iteration 3195, loss = 0.95416041\n",
      "Iteration 3196, loss = 0.95405221\n",
      "Iteration 3197, loss = 0.95394312\n",
      "Iteration 3198, loss = 0.95383397\n",
      "Iteration 3199, loss = 0.95372545\n",
      "Iteration 3200, loss = 0.95361656\n",
      "Iteration 3201, loss = 0.95350778\n",
      "Iteration 3202, loss = 0.95339972\n",
      "Iteration 3203, loss = 0.95329130\n",
      "Iteration 3204, loss = 0.95318281\n",
      "Iteration 3205, loss = 0.95307526\n",
      "Iteration 3206, loss = 0.95296660\n",
      "Iteration 3207, loss = 0.95285814\n",
      "Iteration 3208, loss = 0.95275004\n",
      "Iteration 3209, loss = 0.95264243\n",
      "Iteration 3210, loss = 0.95253295\n",
      "Iteration 3211, loss = 0.95242530\n",
      "Iteration 3212, loss = 0.95231774\n",
      "Iteration 3213, loss = 0.95220995\n",
      "Iteration 3214, loss = 0.95210158\n",
      "Iteration 3215, loss = 0.95199366\n",
      "Iteration 3216, loss = 0.95188601\n",
      "Iteration 3217, loss = 0.95177814\n",
      "Iteration 3218, loss = 0.95167003\n",
      "Iteration 3219, loss = 0.95156293\n",
      "Iteration 3220, loss = 0.95145443\n",
      "Iteration 3221, loss = 0.95134706\n",
      "Iteration 3222, loss = 0.95123954\n",
      "Iteration 3223, loss = 0.95113102\n",
      "Iteration 3224, loss = 0.95102442\n",
      "Iteration 3225, loss = 0.95091594\n",
      "Iteration 3226, loss = 0.95080750\n",
      "Iteration 3227, loss = 0.95070128\n",
      "Iteration 3228, loss = 0.95059328\n",
      "Iteration 3229, loss = 0.95048539\n",
      "Iteration 3230, loss = 0.95037891\n",
      "Iteration 3231, loss = 0.95027201\n",
      "Iteration 3232, loss = 0.95016369\n",
      "Iteration 3233, loss = 0.95005572\n",
      "Iteration 3234, loss = 0.94994880\n",
      "Iteration 3235, loss = 0.94984212\n",
      "Iteration 3236, loss = 0.94973363\n",
      "Iteration 3237, loss = 0.94962713\n",
      "Iteration 3238, loss = 0.94951948\n",
      "Iteration 3239, loss = 0.94941291\n",
      "Iteration 3240, loss = 0.94930531\n",
      "Iteration 3241, loss = 0.94919940\n",
      "Iteration 3242, loss = 0.94909142\n",
      "Iteration 3243, loss = 0.94898431\n",
      "Iteration 3244, loss = 0.94887770\n",
      "Iteration 3245, loss = 0.94877068\n",
      "Iteration 3246, loss = 0.94866431\n",
      "Iteration 3247, loss = 0.94855700\n",
      "Iteration 3248, loss = 0.94844993\n",
      "Iteration 3249, loss = 0.94834363\n",
      "Iteration 3250, loss = 0.94823621\n",
      "Iteration 3251, loss = 0.94813039\n",
      "Iteration 3252, loss = 0.94802304\n",
      "Iteration 3253, loss = 0.94791596\n",
      "Iteration 3254, loss = 0.94781060\n",
      "Iteration 3255, loss = 0.94770438\n",
      "Iteration 3256, loss = 0.94759666\n",
      "Iteration 3257, loss = 0.94749043\n",
      "Iteration 3258, loss = 0.94738416\n",
      "Iteration 3259, loss = 0.94727733\n",
      "Iteration 3260, loss = 0.94717134\n",
      "Iteration 3261, loss = 0.94706486\n",
      "Iteration 3262, loss = 0.94695771\n",
      "Iteration 3263, loss = 0.94685267\n",
      "Iteration 3264, loss = 0.94674635\n",
      "Iteration 3265, loss = 0.94663883\n",
      "Iteration 3266, loss = 0.94653452\n",
      "Iteration 3267, loss = 0.94642878\n",
      "Iteration 3268, loss = 0.94632083\n",
      "Iteration 3269, loss = 0.94621478\n",
      "Iteration 3270, loss = 0.94610893\n",
      "Iteration 3271, loss = 0.94600317\n",
      "Iteration 3272, loss = 0.94589630\n",
      "Iteration 3273, loss = 0.94578982\n",
      "Iteration 3274, loss = 0.94568480\n",
      "Iteration 3275, loss = 0.94557774\n",
      "Iteration 3276, loss = 0.94547271\n",
      "Iteration 3277, loss = 0.94536731\n",
      "Iteration 3278, loss = 0.94526152\n",
      "Iteration 3279, loss = 0.94515605\n",
      "Iteration 3280, loss = 0.94505017\n",
      "Iteration 3281, loss = 0.94494295\n",
      "Iteration 3282, loss = 0.94483749\n",
      "Iteration 3283, loss = 0.94473209\n",
      "Iteration 3284, loss = 0.94462694\n",
      "Iteration 3285, loss = 0.94452040\n",
      "Iteration 3286, loss = 0.94441621\n",
      "Iteration 3287, loss = 0.94431016\n",
      "Iteration 3288, loss = 0.94420470\n",
      "Iteration 3289, loss = 0.94409852\n",
      "Iteration 3290, loss = 0.94399403\n",
      "Iteration 3291, loss = 0.94388748\n",
      "Iteration 3292, loss = 0.94378298\n",
      "Iteration 3293, loss = 0.94367673\n",
      "Iteration 3294, loss = 0.94357175\n",
      "Iteration 3295, loss = 0.94346616\n",
      "Iteration 3296, loss = 0.94336251\n",
      "Iteration 3297, loss = 0.94325603\n",
      "Iteration 3298, loss = 0.94315064\n",
      "Iteration 3299, loss = 0.94304566\n",
      "Iteration 3300, loss = 0.94294085\n",
      "Iteration 3301, loss = 0.94283602\n",
      "Iteration 3302, loss = 0.94273020\n",
      "Iteration 3303, loss = 0.94262637\n",
      "Iteration 3304, loss = 0.94252052\n",
      "Iteration 3305, loss = 0.94241509\n",
      "Iteration 3306, loss = 0.94231041\n",
      "Iteration 3307, loss = 0.94220597\n",
      "Iteration 3308, loss = 0.94210092\n",
      "Iteration 3309, loss = 0.94199714\n",
      "Iteration 3310, loss = 0.94189184\n",
      "Iteration 3311, loss = 0.94178693\n",
      "Iteration 3312, loss = 0.94168188\n",
      "Iteration 3313, loss = 0.94157732\n",
      "Iteration 3314, loss = 0.94147358\n",
      "Iteration 3315, loss = 0.94136840\n",
      "Iteration 3316, loss = 0.94126336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3317, loss = 0.94115973\n",
      "Iteration 3318, loss = 0.94105613\n",
      "Iteration 3319, loss = 0.94095009\n",
      "Iteration 3320, loss = 0.94084584\n",
      "Iteration 3321, loss = 0.94074174\n",
      "Iteration 3322, loss = 0.94063615\n",
      "Iteration 3323, loss = 0.94053245\n",
      "Iteration 3324, loss = 0.94042781\n",
      "Iteration 3325, loss = 0.94032509\n",
      "Iteration 3326, loss = 0.94021831\n",
      "Iteration 3327, loss = 0.94011602\n",
      "Iteration 3328, loss = 0.94001173\n",
      "Iteration 3329, loss = 0.93990670\n",
      "Iteration 3330, loss = 0.93980279\n",
      "Iteration 3331, loss = 0.93969867\n",
      "Iteration 3332, loss = 0.93959522\n",
      "Iteration 3333, loss = 0.93949066\n",
      "Iteration 3334, loss = 0.93938680\n",
      "Iteration 3335, loss = 0.93928249\n",
      "Iteration 3336, loss = 0.93917880\n",
      "Iteration 3337, loss = 0.93907555\n",
      "Iteration 3338, loss = 0.93897075\n",
      "Iteration 3339, loss = 0.93886710\n",
      "Iteration 3340, loss = 0.93876213\n",
      "Iteration 3341, loss = 0.93865879\n",
      "Iteration 3342, loss = 0.93855567\n",
      "Iteration 3343, loss = 0.93845171\n",
      "Iteration 3344, loss = 0.93834783\n",
      "Iteration 3345, loss = 0.93824432\n",
      "Iteration 3346, loss = 0.93814081\n",
      "Iteration 3347, loss = 0.93803756\n",
      "Iteration 3348, loss = 0.93793382\n",
      "Iteration 3349, loss = 0.93782974\n",
      "Iteration 3350, loss = 0.93772702\n",
      "Iteration 3351, loss = 0.93762283\n",
      "Iteration 3352, loss = 0.93752066\n",
      "Iteration 3353, loss = 0.93741592\n",
      "Iteration 3354, loss = 0.93731274\n",
      "Iteration 3355, loss = 0.93720939\n",
      "Iteration 3356, loss = 0.93710583\n",
      "Iteration 3357, loss = 0.93700305\n",
      "Iteration 3358, loss = 0.93690027\n",
      "Iteration 3359, loss = 0.93679635\n",
      "Iteration 3360, loss = 0.93669394\n",
      "Iteration 3361, loss = 0.93659085\n",
      "Iteration 3362, loss = 0.93648664\n",
      "Iteration 3363, loss = 0.93638468\n",
      "Iteration 3364, loss = 0.93628043\n",
      "Iteration 3365, loss = 0.93617838\n",
      "Iteration 3366, loss = 0.93607597\n",
      "Iteration 3367, loss = 0.93597262\n",
      "Iteration 3368, loss = 0.93586943\n",
      "Iteration 3369, loss = 0.93576667\n",
      "Iteration 3370, loss = 0.93566336\n",
      "Iteration 3371, loss = 0.93556153\n",
      "Iteration 3372, loss = 0.93545861\n",
      "Iteration 3373, loss = 0.93535574\n",
      "Iteration 3374, loss = 0.93525264\n",
      "Iteration 3375, loss = 0.93515068\n",
      "Iteration 3376, loss = 0.93504771\n",
      "Iteration 3377, loss = 0.93494512\n",
      "Iteration 3378, loss = 0.93484220\n",
      "Iteration 3379, loss = 0.93473953\n",
      "Iteration 3380, loss = 0.93463715\n",
      "Iteration 3381, loss = 0.93453451\n",
      "Iteration 3382, loss = 0.93443205\n",
      "Iteration 3383, loss = 0.93433054\n",
      "Iteration 3384, loss = 0.93422698\n",
      "Iteration 3385, loss = 0.93412481\n",
      "Iteration 3386, loss = 0.93402339\n",
      "Iteration 3387, loss = 0.93392111\n",
      "Iteration 3388, loss = 0.93381941\n",
      "Iteration 3389, loss = 0.93371622\n",
      "Iteration 3390, loss = 0.93361425\n",
      "Iteration 3391, loss = 0.93351196\n",
      "Iteration 3392, loss = 0.93341036\n",
      "Iteration 3393, loss = 0.93330696\n",
      "Iteration 3394, loss = 0.93320546\n",
      "Iteration 3395, loss = 0.93310282\n",
      "Iteration 3396, loss = 0.93300079\n",
      "Iteration 3397, loss = 0.93290031\n",
      "Iteration 3398, loss = 0.93279738\n",
      "Iteration 3399, loss = 0.93269489\n",
      "Iteration 3400, loss = 0.93259426\n",
      "Iteration 3401, loss = 0.93249191\n",
      "Iteration 3402, loss = 0.93239055\n",
      "Iteration 3403, loss = 0.93228777\n",
      "Iteration 3404, loss = 0.93218578\n",
      "Iteration 3405, loss = 0.93208503\n",
      "Iteration 3406, loss = 0.93198219\n",
      "Iteration 3407, loss = 0.93188181\n",
      "Iteration 3408, loss = 0.93178096\n",
      "Iteration 3409, loss = 0.93167795\n",
      "Iteration 3410, loss = 0.93157690\n",
      "Iteration 3411, loss = 0.93147519\n",
      "Iteration 3412, loss = 0.93137430\n",
      "Iteration 3413, loss = 0.93127218\n",
      "Iteration 3414, loss = 0.93117161\n",
      "Iteration 3415, loss = 0.93107031\n",
      "Iteration 3416, loss = 0.93096762\n",
      "Iteration 3417, loss = 0.93086654\n",
      "Iteration 3418, loss = 0.93076522\n",
      "Iteration 3419, loss = 0.93066397\n",
      "Iteration 3420, loss = 0.93056257\n",
      "Iteration 3421, loss = 0.93046148\n",
      "Iteration 3422, loss = 0.93036008\n",
      "Iteration 3423, loss = 0.93025898\n",
      "Iteration 3424, loss = 0.93015824\n",
      "Iteration 3425, loss = 0.93005635\n",
      "Iteration 3426, loss = 0.92995521\n",
      "Iteration 3427, loss = 0.92985426\n",
      "Iteration 3428, loss = 0.92975259\n",
      "Iteration 3429, loss = 0.92965248\n",
      "Iteration 3430, loss = 0.92955173\n",
      "Iteration 3431, loss = 0.92945158\n",
      "Iteration 3432, loss = 0.92934971\n",
      "Iteration 3433, loss = 0.92924869\n",
      "Iteration 3434, loss = 0.92914821\n",
      "Iteration 3435, loss = 0.92904753\n",
      "Iteration 3436, loss = 0.92894602\n",
      "Iteration 3437, loss = 0.92884617\n",
      "Iteration 3438, loss = 0.92874538\n",
      "Iteration 3439, loss = 0.92864442\n",
      "Iteration 3440, loss = 0.92854364\n",
      "Iteration 3441, loss = 0.92844331\n",
      "Iteration 3442, loss = 0.92834277\n",
      "Iteration 3443, loss = 0.92824151\n",
      "Iteration 3444, loss = 0.92814095\n",
      "Iteration 3445, loss = 0.92804120\n",
      "Iteration 3446, loss = 0.92794051\n",
      "Iteration 3447, loss = 0.92783997\n",
      "Iteration 3448, loss = 0.92773922\n",
      "Iteration 3449, loss = 0.92763945\n",
      "Iteration 3450, loss = 0.92753901\n",
      "Iteration 3451, loss = 0.92743808\n",
      "Iteration 3452, loss = 0.92733849\n",
      "Iteration 3453, loss = 0.92723769\n",
      "Iteration 3454, loss = 0.92713829\n",
      "Iteration 3455, loss = 0.92703764\n",
      "Iteration 3456, loss = 0.92693873\n",
      "Iteration 3457, loss = 0.92683854\n",
      "Iteration 3458, loss = 0.92673835\n",
      "Iteration 3459, loss = 0.92663732\n",
      "Iteration 3460, loss = 0.92653774\n",
      "Iteration 3461, loss = 0.92643719\n",
      "Iteration 3462, loss = 0.92633721\n",
      "Iteration 3463, loss = 0.92623710\n",
      "Iteration 3464, loss = 0.92613774\n",
      "Iteration 3465, loss = 0.92603725\n",
      "Iteration 3466, loss = 0.92593870\n",
      "Iteration 3467, loss = 0.92583848\n",
      "Iteration 3468, loss = 0.92573951\n",
      "Iteration 3469, loss = 0.92563905\n",
      "Iteration 3470, loss = 0.92553890\n",
      "Iteration 3471, loss = 0.92543987\n",
      "Iteration 3472, loss = 0.92534019\n",
      "Iteration 3473, loss = 0.92524036\n",
      "Iteration 3474, loss = 0.92514110\n",
      "Iteration 3475, loss = 0.92504154\n",
      "Iteration 3476, loss = 0.92494230\n",
      "Iteration 3477, loss = 0.92484240\n",
      "Iteration 3478, loss = 0.92474268\n",
      "Iteration 3479, loss = 0.92464331\n",
      "Iteration 3480, loss = 0.92454472\n",
      "Iteration 3481, loss = 0.92444404\n",
      "Iteration 3482, loss = 0.92434489\n",
      "Iteration 3483, loss = 0.92424647\n",
      "Iteration 3484, loss = 0.92414701\n",
      "Iteration 3485, loss = 0.92404897\n",
      "Iteration 3486, loss = 0.92394858\n",
      "Iteration 3487, loss = 0.92385028\n",
      "Iteration 3488, loss = 0.92375107\n",
      "Iteration 3489, loss = 0.92365125\n",
      "Iteration 3490, loss = 0.92355162\n",
      "Iteration 3491, loss = 0.92345386\n",
      "Iteration 3492, loss = 0.92335380\n",
      "Iteration 3493, loss = 0.92325587\n",
      "Iteration 3494, loss = 0.92315657\n",
      "Iteration 3495, loss = 0.92305788\n",
      "Iteration 3496, loss = 0.92295829\n",
      "Iteration 3497, loss = 0.92286015\n",
      "Iteration 3498, loss = 0.92276125\n",
      "Iteration 3499, loss = 0.92266288\n",
      "Iteration 3500, loss = 0.92256406\n",
      "Iteration 3501, loss = 0.92246484\n",
      "Iteration 3502, loss = 0.92236625\n",
      "Iteration 3503, loss = 0.92226752\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.895381\n",
      "Training set loss: 0.922268\n",
      "training: inv-scaling with momentum\n",
      "Iteration 1, loss = 1.80716619\n",
      "Iteration 2, loss = 0.68805802\n",
      "Iteration 3, loss = 0.47428264\n",
      "Iteration 4, loss = 0.36684535\n",
      "Iteration 5, loss = 0.33149494\n",
      "Iteration 6, loss = 0.31995923\n",
      "Iteration 7, loss = 0.31330918\n",
      "Iteration 8, loss = 0.30837950\n",
      "Iteration 9, loss = 0.30427265\n",
      "Iteration 10, loss = 0.30138966\n",
      "Iteration 11, loss = 0.29860726\n",
      "Iteration 12, loss = 0.29635282\n",
      "Iteration 13, loss = 0.29423215\n",
      "Iteration 14, loss = 0.29232278\n",
      "Iteration 15, loss = 0.29058362\n",
      "Iteration 16, loss = 0.28889084\n",
      "Iteration 17, loss = 0.28734680\n",
      "Iteration 18, loss = 0.28582486\n",
      "Iteration 19, loss = 0.28447234\n",
      "Iteration 20, loss = 0.28302181\n",
      "Iteration 21, loss = 0.28165825\n",
      "Iteration 22, loss = 0.28043458\n",
      "Iteration 23, loss = 0.27922620\n",
      "Iteration 24, loss = 0.27808806\n",
      "Iteration 25, loss = 0.27700274\n",
      "Iteration 26, loss = 0.27586753\n",
      "Iteration 27, loss = 0.27488050\n",
      "Iteration 28, loss = 0.27386070\n",
      "Iteration 29, loss = 0.27286720\n",
      "Iteration 30, loss = 0.27188322\n",
      "Iteration 31, loss = 0.27093301\n",
      "Iteration 32, loss = 0.27006159\n",
      "Iteration 33, loss = 0.26920838\n",
      "Iteration 34, loss = 0.26839206\n",
      "Iteration 35, loss = 0.26753582\n",
      "Iteration 36, loss = 0.26667732\n",
      "Iteration 37, loss = 0.26591110\n",
      "Iteration 38, loss = 0.26513661\n",
      "Iteration 39, loss = 0.26436236\n",
      "Iteration 40, loss = 0.26363482\n",
      "Iteration 41, loss = 0.26290252\n",
      "Iteration 42, loss = 0.26220313\n",
      "Iteration 43, loss = 0.26150953\n",
      "Iteration 44, loss = 0.26083187\n",
      "Iteration 45, loss = 0.26019542\n",
      "Iteration 46, loss = 0.25953843\n",
      "Iteration 47, loss = 0.25884908\n",
      "Iteration 48, loss = 0.25828356\n",
      "Iteration 49, loss = 0.25762361\n",
      "Iteration 50, loss = 0.25700892\n",
      "Iteration 51, loss = 0.25644325\n",
      "Iteration 52, loss = 0.25583965\n",
      "Iteration 53, loss = 0.25525115\n",
      "Iteration 54, loss = 0.25472982\n",
      "Iteration 55, loss = 0.25411803\n",
      "Iteration 56, loss = 0.25358920\n",
      "Iteration 57, loss = 0.25307170\n",
      "Iteration 58, loss = 0.25251505\n",
      "Iteration 59, loss = 0.25203965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60, loss = 0.25148937\n",
      "Iteration 61, loss = 0.25097772\n",
      "Iteration 62, loss = 0.25047680\n",
      "Iteration 63, loss = 0.24998717\n",
      "Iteration 64, loss = 0.24948462\n",
      "Iteration 65, loss = 0.24901780\n",
      "Iteration 66, loss = 0.24856692\n",
      "Iteration 67, loss = 0.24806933\n",
      "Iteration 68, loss = 0.24762462\n",
      "Iteration 69, loss = 0.24718758\n",
      "Iteration 70, loss = 0.24675236\n",
      "Iteration 71, loss = 0.24627741\n",
      "Iteration 72, loss = 0.24584588\n",
      "Iteration 73, loss = 0.24543967\n",
      "Iteration 74, loss = 0.24501185\n",
      "Iteration 75, loss = 0.24458492\n",
      "Iteration 76, loss = 0.24418241\n",
      "Iteration 77, loss = 0.24374778\n",
      "Iteration 78, loss = 0.24333016\n",
      "Iteration 79, loss = 0.24296056\n",
      "Iteration 80, loss = 0.24256060\n",
      "Iteration 81, loss = 0.24216720\n",
      "Iteration 82, loss = 0.24177245\n",
      "Iteration 83, loss = 0.24138718\n",
      "Iteration 84, loss = 0.24100383\n",
      "Iteration 85, loss = 0.24062605\n",
      "Iteration 86, loss = 0.24028276\n",
      "Iteration 87, loss = 0.23989702\n",
      "Iteration 88, loss = 0.23955355\n",
      "Iteration 89, loss = 0.23918882\n",
      "Iteration 90, loss = 0.23886914\n",
      "Iteration 91, loss = 0.23849134\n",
      "Iteration 92, loss = 0.23813797\n",
      "Iteration 93, loss = 0.23781725\n",
      "Iteration 94, loss = 0.23744053\n",
      "Iteration 95, loss = 0.23711774\n",
      "Iteration 96, loss = 0.23679324\n",
      "Iteration 97, loss = 0.23644990\n",
      "Iteration 98, loss = 0.23612711\n",
      "Iteration 99, loss = 0.23578795\n",
      "Iteration 100, loss = 0.23547657\n",
      "Iteration 101, loss = 0.23515741\n",
      "Iteration 102, loss = 0.23484290\n",
      "Iteration 103, loss = 0.23452081\n",
      "Iteration 104, loss = 0.23423684\n",
      "Iteration 105, loss = 0.23393270\n",
      "Iteration 106, loss = 0.23361132\n",
      "Iteration 107, loss = 0.23329990\n",
      "Iteration 108, loss = 0.23299604\n",
      "Iteration 109, loss = 0.23271123\n",
      "Iteration 110, loss = 0.23242101\n",
      "Iteration 111, loss = 0.23213009\n",
      "Iteration 112, loss = 0.23184845\n",
      "Iteration 113, loss = 0.23155564\n",
      "Iteration 114, loss = 0.23126343\n",
      "Iteration 115, loss = 0.23099572\n",
      "Iteration 116, loss = 0.23069527\n",
      "Iteration 117, loss = 0.23042531\n",
      "Iteration 118, loss = 0.23016110\n",
      "Iteration 119, loss = 0.22988932\n",
      "Iteration 120, loss = 0.22960721\n",
      "Iteration 121, loss = 0.22935031\n",
      "Iteration 122, loss = 0.22909077\n",
      "Iteration 123, loss = 0.22881625\n",
      "Iteration 124, loss = 0.22854442\n",
      "Iteration 125, loss = 0.22828442\n",
      "Iteration 126, loss = 0.22803639\n",
      "Iteration 127, loss = 0.22779023\n",
      "Iteration 128, loss = 0.22751666\n",
      "Iteration 129, loss = 0.22728733\n",
      "Iteration 130, loss = 0.22702828\n",
      "Iteration 131, loss = 0.22677925\n",
      "Iteration 132, loss = 0.22652273\n",
      "Iteration 133, loss = 0.22630966\n",
      "Iteration 134, loss = 0.22603555\n",
      "Iteration 135, loss = 0.22577989\n",
      "Iteration 136, loss = 0.22555874\n",
      "Iteration 137, loss = 0.22530402\n",
      "Iteration 138, loss = 0.22510504\n",
      "Iteration 139, loss = 0.22484577\n",
      "Iteration 140, loss = 0.22461483\n",
      "Iteration 141, loss = 0.22438100\n",
      "Iteration 142, loss = 0.22414245\n",
      "Iteration 143, loss = 0.22390176\n",
      "Iteration 144, loss = 0.22369736\n",
      "Iteration 145, loss = 0.22346936\n",
      "Iteration 146, loss = 0.22324001\n",
      "Iteration 147, loss = 0.22302349\n",
      "Iteration 148, loss = 0.22279907\n",
      "Iteration 149, loss = 0.22256237\n",
      "Iteration 150, loss = 0.22234526\n",
      "Iteration 151, loss = 0.22215191\n",
      "Iteration 152, loss = 0.22192931\n",
      "Iteration 153, loss = 0.22169996\n",
      "Iteration 154, loss = 0.22149528\n",
      "Iteration 155, loss = 0.22127429\n",
      "Iteration 156, loss = 0.22107075\n",
      "Iteration 157, loss = 0.22085169\n",
      "Iteration 158, loss = 0.22064689\n",
      "Iteration 159, loss = 0.22044623\n",
      "Iteration 160, loss = 0.22024467\n",
      "Iteration 161, loss = 0.22003817\n",
      "Iteration 162, loss = 0.21982297\n",
      "Iteration 163, loss = 0.21965037\n",
      "Iteration 164, loss = 0.21941995\n",
      "Iteration 165, loss = 0.21921931\n",
      "Iteration 166, loss = 0.21904615\n",
      "Iteration 167, loss = 0.21884203\n",
      "Iteration 168, loss = 0.21864084\n",
      "Iteration 169, loss = 0.21844102\n",
      "Iteration 170, loss = 0.21823799\n",
      "Iteration 171, loss = 0.21805703\n",
      "Iteration 172, loss = 0.21786277\n",
      "Iteration 173, loss = 0.21767364\n",
      "Iteration 174, loss = 0.21749957\n",
      "Iteration 175, loss = 0.21730296\n",
      "Iteration 176, loss = 0.21711041\n",
      "Iteration 177, loss = 0.21694063\n",
      "Iteration 178, loss = 0.21674177\n",
      "Iteration 179, loss = 0.21654600\n",
      "Iteration 180, loss = 0.21637658\n",
      "Iteration 181, loss = 0.21618363\n",
      "Iteration 182, loss = 0.21600080\n",
      "Iteration 183, loss = 0.21583770\n",
      "Iteration 184, loss = 0.21564434\n",
      "Iteration 185, loss = 0.21546825\n",
      "Iteration 186, loss = 0.21529945\n",
      "Iteration 187, loss = 0.21511660\n",
      "Iteration 188, loss = 0.21492975\n",
      "Iteration 189, loss = 0.21478211\n",
      "Iteration 190, loss = 0.21458470\n",
      "Iteration 191, loss = 0.21441650\n",
      "Iteration 192, loss = 0.21424750\n",
      "Iteration 193, loss = 0.21407481\n",
      "Iteration 194, loss = 0.21390987\n",
      "Iteration 195, loss = 0.21373978\n",
      "Iteration 196, loss = 0.21356565\n",
      "Iteration 197, loss = 0.21338865\n",
      "Iteration 198, loss = 0.21322225\n",
      "Iteration 199, loss = 0.21305502\n",
      "Iteration 200, loss = 0.21289990\n",
      "Iteration 201, loss = 0.21272290\n",
      "Iteration 202, loss = 0.21256403\n",
      "Iteration 203, loss = 0.21240761\n",
      "Iteration 204, loss = 0.21223723\n",
      "Iteration 205, loss = 0.21207791\n",
      "Iteration 206, loss = 0.21192525\n",
      "Iteration 207, loss = 0.21176300\n",
      "Iteration 208, loss = 0.21160067\n",
      "Iteration 209, loss = 0.21145199\n",
      "Iteration 210, loss = 0.21127689\n",
      "Iteration 211, loss = 0.21114581\n",
      "Iteration 212, loss = 0.21097644\n",
      "Iteration 213, loss = 0.21081795\n",
      "Iteration 214, loss = 0.21066688\n",
      "Iteration 215, loss = 0.21050746\n",
      "Iteration 216, loss = 0.21035338\n",
      "Iteration 217, loss = 0.21020257\n",
      "Iteration 218, loss = 0.21004174\n",
      "Iteration 219, loss = 0.20990111\n",
      "Iteration 220, loss = 0.20975524\n",
      "Iteration 221, loss = 0.20959398\n",
      "Iteration 222, loss = 0.20945321\n",
      "Iteration 223, loss = 0.20929085\n",
      "Iteration 224, loss = 0.20915024\n",
      "Iteration 225, loss = 0.20900140\n",
      "Iteration 226, loss = 0.20885511\n",
      "Iteration 227, loss = 0.20872620\n",
      "Iteration 228, loss = 0.20856864\n",
      "Iteration 229, loss = 0.20842288\n",
      "Iteration 230, loss = 0.20827085\n",
      "Iteration 231, loss = 0.20814131\n",
      "Iteration 232, loss = 0.20799960\n",
      "Iteration 233, loss = 0.20786078\n",
      "Iteration 234, loss = 0.20770092\n",
      "Iteration 235, loss = 0.20756903\n",
      "Iteration 236, loss = 0.20742251\n",
      "Iteration 237, loss = 0.20728322\n",
      "Iteration 238, loss = 0.20715312\n",
      "Iteration 239, loss = 0.20700213\n",
      "Iteration 240, loss = 0.20686248\n",
      "Iteration 241, loss = 0.20672893\n",
      "Iteration 242, loss = 0.20659188\n",
      "Iteration 243, loss = 0.20645748\n",
      "Iteration 244, loss = 0.20631165\n",
      "Iteration 245, loss = 0.20617744\n",
      "Iteration 246, loss = 0.20605690\n",
      "Iteration 247, loss = 0.20590555\n",
      "Iteration 248, loss = 0.20576987\n",
      "Iteration 249, loss = 0.20564303\n",
      "Iteration 250, loss = 0.20550542\n",
      "Iteration 251, loss = 0.20537351\n",
      "Iteration 252, loss = 0.20526696\n",
      "Iteration 253, loss = 0.20511170\n",
      "Iteration 254, loss = 0.20497806\n",
      "Iteration 255, loss = 0.20486070\n",
      "Iteration 256, loss = 0.20473176\n",
      "Iteration 257, loss = 0.20459047\n",
      "Iteration 258, loss = 0.20446182\n",
      "Iteration 259, loss = 0.20434648\n",
      "Iteration 260, loss = 0.20420773\n",
      "Iteration 261, loss = 0.20407636\n",
      "Iteration 262, loss = 0.20394792\n",
      "Iteration 263, loss = 0.20382434\n",
      "Iteration 264, loss = 0.20370880\n",
      "Iteration 265, loss = 0.20356974\n",
      "Iteration 266, loss = 0.20344335\n",
      "Iteration 267, loss = 0.20331917\n",
      "Iteration 268, loss = 0.20320791\n",
      "Iteration 269, loss = 0.20309148\n",
      "Iteration 270, loss = 0.20295164\n",
      "Iteration 271, loss = 0.20282884\n",
      "Iteration 272, loss = 0.20270806\n",
      "Iteration 273, loss = 0.20259152\n",
      "Iteration 274, loss = 0.20246017\n",
      "Iteration 275, loss = 0.20234896\n",
      "Iteration 276, loss = 0.20222312\n",
      "Iteration 277, loss = 0.20210741\n",
      "Iteration 278, loss = 0.20198675\n",
      "Iteration 279, loss = 0.20186556\n",
      "Iteration 280, loss = 0.20175395\n",
      "Iteration 281, loss = 0.20162413\n",
      "Iteration 282, loss = 0.20152487\n",
      "Iteration 283, loss = 0.20140914\n",
      "Iteration 284, loss = 0.20127137\n",
      "Iteration 285, loss = 0.20116388\n",
      "Iteration 286, loss = 0.20104365\n",
      "Iteration 287, loss = 0.20092760\n",
      "Iteration 288, loss = 0.20081718\n",
      "Iteration 289, loss = 0.20070087\n",
      "Iteration 290, loss = 0.20057559\n",
      "Iteration 291, loss = 0.20046124\n",
      "Iteration 292, loss = 0.20035224\n",
      "Iteration 293, loss = 0.20022839\n",
      "Iteration 294, loss = 0.20013142\n",
      "Iteration 295, loss = 0.20002684\n",
      "Iteration 296, loss = 0.19989589\n",
      "Iteration 297, loss = 0.19979226\n",
      "Iteration 298, loss = 0.19969273\n",
      "Iteration 299, loss = 0.19957269\n",
      "Iteration 300, loss = 0.19944681\n",
      "Iteration 301, loss = 0.19934128\n",
      "Iteration 302, loss = 0.19924064\n",
      "Iteration 303, loss = 0.19913434\n",
      "Iteration 304, loss = 0.19900989\n",
      "Iteration 305, loss = 0.19890641\n",
      "Iteration 306, loss = 0.19879216\n",
      "Iteration 307, loss = 0.19868242\n",
      "Iteration 308, loss = 0.19857882\n",
      "Iteration 309, loss = 0.19847150\n",
      "Iteration 310, loss = 0.19835451\n",
      "Iteration 311, loss = 0.19825269\n",
      "Iteration 312, loss = 0.19815688\n",
      "Iteration 313, loss = 0.19803782\n",
      "Iteration 314, loss = 0.19792533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 315, loss = 0.19782646\n",
      "Iteration 316, loss = 0.19772840\n",
      "Iteration 317, loss = 0.19762035\n",
      "Iteration 318, loss = 0.19751808\n",
      "Iteration 319, loss = 0.19740472\n",
      "Iteration 320, loss = 0.19729758\n",
      "Iteration 321, loss = 0.19719646\n",
      "Iteration 322, loss = 0.19710031\n",
      "Iteration 323, loss = 0.19700832\n",
      "Iteration 324, loss = 0.19688411\n",
      "Iteration 325, loss = 0.19677984\n",
      "Iteration 326, loss = 0.19669412\n",
      "Iteration 327, loss = 0.19658463\n",
      "Iteration 328, loss = 0.19647780\n",
      "Iteration 329, loss = 0.19637494\n",
      "Iteration 330, loss = 0.19627573\n",
      "Iteration 331, loss = 0.19618061\n",
      "Iteration 332, loss = 0.19606877\n",
      "Iteration 333, loss = 0.19597205\n",
      "Iteration 334, loss = 0.19588269\n",
      "Iteration 335, loss = 0.19577933\n",
      "Iteration 336, loss = 0.19567944\n",
      "Iteration 337, loss = 0.19558614\n",
      "Iteration 338, loss = 0.19548073\n",
      "Iteration 339, loss = 0.19538901\n",
      "Iteration 340, loss = 0.19527809\n",
      "Iteration 341, loss = 0.19518206\n",
      "Iteration 342, loss = 0.19508178\n",
      "Iteration 343, loss = 0.19499930\n",
      "Iteration 344, loss = 0.19489059\n",
      "Iteration 345, loss = 0.19479364\n",
      "Iteration 346, loss = 0.19469143\n",
      "Iteration 347, loss = 0.19460072\n",
      "Iteration 348, loss = 0.19450305\n",
      "Iteration 349, loss = 0.19442431\n",
      "Iteration 350, loss = 0.19432213\n",
      "Iteration 351, loss = 0.19421837\n",
      "Iteration 352, loss = 0.19412108\n",
      "Iteration 353, loss = 0.19402418\n",
      "Iteration 354, loss = 0.19394746\n",
      "Iteration 355, loss = 0.19383594\n",
      "Iteration 356, loss = 0.19375189\n",
      "Iteration 357, loss = 0.19364839\n",
      "Iteration 358, loss = 0.19355929\n",
      "Iteration 359, loss = 0.19346942\n",
      "Iteration 360, loss = 0.19337805\n",
      "Iteration 361, loss = 0.19327834\n",
      "Iteration 362, loss = 0.19319188\n",
      "Iteration 363, loss = 0.19309932\n",
      "Iteration 364, loss = 0.19300661\n",
      "Iteration 365, loss = 0.19291816\n",
      "Iteration 366, loss = 0.19283173\n",
      "Iteration 367, loss = 0.19272893\n",
      "Iteration 368, loss = 0.19264748\n",
      "Iteration 369, loss = 0.19255101\n",
      "Iteration 370, loss = 0.19245816\n",
      "Iteration 371, loss = 0.19236919\n",
      "Iteration 372, loss = 0.19228206\n",
      "Iteration 373, loss = 0.19219763\n",
      "Iteration 374, loss = 0.19209959\n",
      "Iteration 375, loss = 0.19200505\n",
      "Iteration 376, loss = 0.19192884\n",
      "Iteration 377, loss = 0.19183447\n",
      "Iteration 378, loss = 0.19175455\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.948247\n",
      "Training set loss: 0.191755\n",
      "training: inv-scaling with Nesterov's momentum\n",
      "Iteration 1, loss = 1.94238221\n",
      "Iteration 2, loss = 1.02966209\n",
      "Iteration 3, loss = 0.65086024\n",
      "Iteration 4, loss = 0.40808694\n",
      "Iteration 5, loss = 0.37051408\n",
      "Iteration 6, loss = 0.36274491\n",
      "Iteration 7, loss = 0.35222370\n",
      "Iteration 8, loss = 0.34292204\n",
      "Iteration 9, loss = 0.33616468\n",
      "Iteration 10, loss = 0.33181677\n",
      "Iteration 11, loss = 0.32827739\n",
      "Iteration 12, loss = 0.32538427\n",
      "Iteration 13, loss = 0.32278729\n",
      "Iteration 14, loss = 0.32053006\n",
      "Iteration 15, loss = 0.31838693\n",
      "Iteration 16, loss = 0.31635300\n",
      "Iteration 17, loss = 0.31447224\n",
      "Iteration 18, loss = 0.31263545\n",
      "Iteration 19, loss = 0.31101187\n",
      "Iteration 20, loss = 0.30928120\n",
      "Iteration 21, loss = 0.30765133\n",
      "Iteration 22, loss = 0.30616853\n",
      "Iteration 23, loss = 0.30469398\n",
      "Iteration 24, loss = 0.30331228\n",
      "Iteration 25, loss = 0.30200013\n",
      "Iteration 26, loss = 0.30063288\n",
      "Iteration 27, loss = 0.29942739\n",
      "Iteration 28, loss = 0.29819457\n",
      "Iteration 29, loss = 0.29699021\n",
      "Iteration 30, loss = 0.29580395\n",
      "Iteration 31, loss = 0.29466031\n",
      "Iteration 32, loss = 0.29359904\n",
      "Iteration 33, loss = 0.29254924\n",
      "Iteration 34, loss = 0.29156222\n",
      "Iteration 35, loss = 0.29052087\n",
      "Iteration 36, loss = 0.28950587\n",
      "Iteration 37, loss = 0.28857824\n",
      "Iteration 38, loss = 0.28763730\n",
      "Iteration 39, loss = 0.28670694\n",
      "Iteration 40, loss = 0.28581995\n",
      "Iteration 41, loss = 0.28493580\n",
      "Iteration 42, loss = 0.28409004\n",
      "Iteration 43, loss = 0.28325880\n",
      "Iteration 44, loss = 0.28244846\n",
      "Iteration 45, loss = 0.28168483\n",
      "Iteration 46, loss = 0.28089006\n",
      "Iteration 47, loss = 0.28006964\n",
      "Iteration 48, loss = 0.27938992\n",
      "Iteration 49, loss = 0.27859873\n",
      "Iteration 50, loss = 0.27785976\n",
      "Iteration 51, loss = 0.27718421\n",
      "Iteration 52, loss = 0.27646166\n",
      "Iteration 53, loss = 0.27575224\n",
      "Iteration 54, loss = 0.27512075\n",
      "Iteration 55, loss = 0.27439791\n",
      "Iteration 56, loss = 0.27376673\n",
      "Iteration 57, loss = 0.27314275\n",
      "Iteration 58, loss = 0.27248081\n",
      "Iteration 59, loss = 0.27190288\n",
      "Iteration 60, loss = 0.27124896\n",
      "Iteration 61, loss = 0.27064347\n",
      "Iteration 62, loss = 0.27004518\n",
      "Iteration 63, loss = 0.26946296\n",
      "Iteration 64, loss = 0.26885479\n",
      "Iteration 65, loss = 0.26830383\n",
      "Iteration 66, loss = 0.26775641\n",
      "Iteration 67, loss = 0.26717565\n",
      "Iteration 68, loss = 0.26664512\n",
      "Iteration 69, loss = 0.26611589\n",
      "Iteration 70, loss = 0.26559443\n",
      "Iteration 71, loss = 0.26502838\n",
      "Iteration 72, loss = 0.26451924\n",
      "Iteration 73, loss = 0.26403331\n",
      "Iteration 74, loss = 0.26352652\n",
      "Iteration 75, loss = 0.26302194\n",
      "Iteration 76, loss = 0.26253615\n",
      "Iteration 77, loss = 0.26201942\n",
      "Iteration 78, loss = 0.26152466\n",
      "Iteration 79, loss = 0.26108939\n",
      "Iteration 80, loss = 0.26060617\n",
      "Iteration 81, loss = 0.26014397\n",
      "Iteration 82, loss = 0.25967421\n",
      "Iteration 83, loss = 0.25921572\n",
      "Iteration 84, loss = 0.25876090\n",
      "Iteration 85, loss = 0.25831743\n",
      "Iteration 86, loss = 0.25790983\n",
      "Iteration 87, loss = 0.25744874\n",
      "Iteration 88, loss = 0.25704228\n",
      "Iteration 89, loss = 0.25660723\n",
      "Iteration 90, loss = 0.25622982\n",
      "Iteration 91, loss = 0.25577611\n",
      "Iteration 92, loss = 0.25536648\n",
      "Iteration 93, loss = 0.25497528\n",
      "Iteration 94, loss = 0.25453856\n",
      "Iteration 95, loss = 0.25415287\n",
      "Iteration 96, loss = 0.25377211\n",
      "Iteration 97, loss = 0.25336228\n",
      "Iteration 98, loss = 0.25298445\n",
      "Iteration 99, loss = 0.25258232\n",
      "Iteration 100, loss = 0.25221367\n",
      "Iteration 101, loss = 0.25183794\n",
      "Iteration 102, loss = 0.25146088\n",
      "Iteration 103, loss = 0.25108646\n",
      "Iteration 104, loss = 0.25074317\n",
      "Iteration 105, loss = 0.25038219\n",
      "Iteration 106, loss = 0.25000328\n",
      "Iteration 107, loss = 0.24963415\n",
      "Iteration 108, loss = 0.24928611\n",
      "Iteration 109, loss = 0.24894302\n",
      "Iteration 110, loss = 0.24859603\n",
      "Iteration 111, loss = 0.24825542\n",
      "Iteration 112, loss = 0.24791904\n",
      "Iteration 113, loss = 0.24757186\n",
      "Iteration 114, loss = 0.24722333\n",
      "Iteration 115, loss = 0.24691007\n",
      "Iteration 116, loss = 0.24655773\n",
      "Iteration 117, loss = 0.24623629\n",
      "Iteration 118, loss = 0.24592382\n",
      "Iteration 119, loss = 0.24559769\n",
      "Iteration 120, loss = 0.24527066\n",
      "Iteration 121, loss = 0.24496682\n",
      "Iteration 122, loss = 0.24465773\n",
      "Iteration 123, loss = 0.24433663\n",
      "Iteration 124, loss = 0.24401814\n",
      "Iteration 125, loss = 0.24371026\n",
      "Iteration 126, loss = 0.24341098\n",
      "Iteration 127, loss = 0.24312440\n",
      "Iteration 128, loss = 0.24280502\n",
      "Iteration 129, loss = 0.24253599\n",
      "Iteration 130, loss = 0.24222767\n",
      "Iteration 131, loss = 0.24193030\n",
      "Iteration 132, loss = 0.24162911\n",
      "Iteration 133, loss = 0.24137703\n",
      "Iteration 134, loss = 0.24105711\n",
      "Iteration 135, loss = 0.24075612\n",
      "Iteration 136, loss = 0.24049718\n",
      "Iteration 137, loss = 0.24019686\n",
      "Iteration 138, loss = 0.23995652\n",
      "Iteration 139, loss = 0.23965091\n",
      "Iteration 140, loss = 0.23938242\n",
      "Iteration 141, loss = 0.23910623\n",
      "Iteration 142, loss = 0.23882344\n",
      "Iteration 143, loss = 0.23854427\n",
      "Iteration 144, loss = 0.23829712\n",
      "Iteration 145, loss = 0.23803212\n",
      "Iteration 146, loss = 0.23776090\n",
      "Iteration 147, loss = 0.23750083\n",
      "Iteration 148, loss = 0.23723941\n",
      "Iteration 149, loss = 0.23696038\n",
      "Iteration 150, loss = 0.23670291\n",
      "Iteration 151, loss = 0.23646901\n",
      "Iteration 152, loss = 0.23620374\n",
      "Iteration 153, loss = 0.23593600\n",
      "Iteration 154, loss = 0.23569252\n",
      "Iteration 155, loss = 0.23543512\n",
      "Iteration 156, loss = 0.23519628\n",
      "Iteration 157, loss = 0.23493675\n",
      "Iteration 158, loss = 0.23469303\n",
      "Iteration 159, loss = 0.23445294\n",
      "Iteration 160, loss = 0.23421757\n",
      "Iteration 161, loss = 0.23397152\n",
      "Iteration 162, loss = 0.23371793\n",
      "Iteration 163, loss = 0.23350957\n",
      "Iteration 164, loss = 0.23324198\n",
      "Iteration 165, loss = 0.23300611\n",
      "Iteration 166, loss = 0.23279640\n",
      "Iteration 167, loss = 0.23256057\n",
      "Iteration 168, loss = 0.23232052\n",
      "Iteration 169, loss = 0.23208580\n",
      "Iteration 170, loss = 0.23184738\n",
      "Iteration 171, loss = 0.23163013\n",
      "Iteration 172, loss = 0.23140037\n",
      "Iteration 173, loss = 0.23117808\n",
      "Iteration 174, loss = 0.23096785\n",
      "Iteration 175, loss = 0.23074342\n",
      "Iteration 176, loss = 0.23051676\n",
      "Iteration 177, loss = 0.23031034\n",
      "Iteration 178, loss = 0.23007988\n",
      "Iteration 179, loss = 0.22985097\n",
      "Iteration 180, loss = 0.22965055\n",
      "Iteration 181, loss = 0.22942381\n",
      "Iteration 182, loss = 0.22920799\n",
      "Iteration 183, loss = 0.22901406\n",
      "Iteration 184, loss = 0.22878523\n",
      "Iteration 185, loss = 0.22857968\n",
      "Iteration 186, loss = 0.22838086\n",
      "Iteration 187, loss = 0.22816648\n",
      "Iteration 188, loss = 0.22794771\n",
      "Iteration 189, loss = 0.22777158\n",
      "Iteration 190, loss = 0.22754146\n",
      "Iteration 191, loss = 0.22734427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 192, loss = 0.22714771\n",
      "Iteration 193, loss = 0.22694550\n",
      "Iteration 194, loss = 0.22675119\n",
      "Iteration 195, loss = 0.22654963\n",
      "Iteration 196, loss = 0.22634561\n",
      "Iteration 197, loss = 0.22614061\n",
      "Iteration 198, loss = 0.22594638\n",
      "Iteration 199, loss = 0.22575143\n",
      "Iteration 200, loss = 0.22556787\n",
      "Iteration 201, loss = 0.22535859\n",
      "Iteration 202, loss = 0.22517565\n",
      "Iteration 203, loss = 0.22499188\n",
      "Iteration 204, loss = 0.22478806\n",
      "Iteration 205, loss = 0.22460399\n",
      "Iteration 206, loss = 0.22441952\n",
      "Iteration 207, loss = 0.22423296\n",
      "Iteration 208, loss = 0.22404368\n",
      "Iteration 209, loss = 0.22386724\n",
      "Iteration 210, loss = 0.22366539\n",
      "Iteration 211, loss = 0.22350811\n",
      "Iteration 212, loss = 0.22330668\n",
      "Iteration 213, loss = 0.22312557\n",
      "Iteration 214, loss = 0.22294702\n",
      "Iteration 215, loss = 0.22276064\n",
      "Iteration 216, loss = 0.22257927\n",
      "Iteration 217, loss = 0.22240084\n",
      "Iteration 218, loss = 0.22221340\n",
      "Iteration 219, loss = 0.22204599\n",
      "Iteration 220, loss = 0.22187820\n",
      "Iteration 221, loss = 0.22169086\n",
      "Iteration 222, loss = 0.22152267\n",
      "Iteration 223, loss = 0.22133421\n",
      "Iteration 224, loss = 0.22116844\n",
      "Iteration 225, loss = 0.22099675\n",
      "Iteration 226, loss = 0.22082474\n",
      "Iteration 227, loss = 0.22067245\n",
      "Iteration 228, loss = 0.22048959\n",
      "Iteration 229, loss = 0.22031852\n",
      "Iteration 230, loss = 0.22013998\n",
      "Iteration 231, loss = 0.21999002\n",
      "Iteration 232, loss = 0.21982115\n",
      "Iteration 233, loss = 0.21966121\n",
      "Iteration 234, loss = 0.21947518\n",
      "Iteration 235, loss = 0.21931742\n",
      "Iteration 236, loss = 0.21914894\n",
      "Iteration 237, loss = 0.21898470\n",
      "Iteration 238, loss = 0.21883374\n",
      "Iteration 239, loss = 0.21865617\n",
      "Iteration 240, loss = 0.21849299\n",
      "Iteration 241, loss = 0.21833541\n",
      "Iteration 242, loss = 0.21817655\n",
      "Iteration 243, loss = 0.21801900\n",
      "Iteration 244, loss = 0.21784986\n",
      "Iteration 245, loss = 0.21769308\n",
      "Iteration 246, loss = 0.21755070\n",
      "Iteration 247, loss = 0.21737514\n",
      "Iteration 248, loss = 0.21721877\n",
      "Iteration 249, loss = 0.21706778\n",
      "Iteration 250, loss = 0.21690700\n",
      "Iteration 251, loss = 0.21675404\n",
      "Iteration 252, loss = 0.21662619\n",
      "Iteration 253, loss = 0.21645060\n",
      "Iteration 254, loss = 0.21629392\n",
      "Iteration 255, loss = 0.21615371\n",
      "Iteration 256, loss = 0.21600277\n",
      "Iteration 257, loss = 0.21583927\n",
      "Iteration 258, loss = 0.21569179\n",
      "Iteration 259, loss = 0.21555412\n",
      "Iteration 260, loss = 0.21539571\n",
      "Iteration 261, loss = 0.21524234\n",
      "Iteration 262, loss = 0.21509218\n",
      "Iteration 263, loss = 0.21494812\n",
      "Iteration 264, loss = 0.21481270\n",
      "Iteration 265, loss = 0.21465317\n",
      "Iteration 266, loss = 0.21450477\n",
      "Iteration 267, loss = 0.21436065\n",
      "Iteration 268, loss = 0.21422772\n",
      "Iteration 269, loss = 0.21409304\n",
      "Iteration 270, loss = 0.21393064\n",
      "Iteration 271, loss = 0.21378857\n",
      "Iteration 272, loss = 0.21364912\n",
      "Iteration 273, loss = 0.21351006\n",
      "Iteration 274, loss = 0.21336035\n",
      "Iteration 275, loss = 0.21322761\n",
      "Iteration 276, loss = 0.21308005\n",
      "Iteration 277, loss = 0.21294719\n",
      "Iteration 278, loss = 0.21280603\n",
      "Iteration 279, loss = 0.21266317\n",
      "Iteration 280, loss = 0.21253328\n",
      "Iteration 281, loss = 0.21238295\n",
      "Iteration 282, loss = 0.21226638\n",
      "Iteration 283, loss = 0.21213119\n",
      "Iteration 284, loss = 0.21197237\n",
      "Iteration 285, loss = 0.21184606\n",
      "Iteration 286, loss = 0.21170736\n",
      "Iteration 287, loss = 0.21156972\n",
      "Iteration 288, loss = 0.21144333\n",
      "Iteration 289, loss = 0.21130897\n",
      "Iteration 290, loss = 0.21116390\n",
      "Iteration 291, loss = 0.21102951\n",
      "Iteration 292, loss = 0.21090233\n",
      "Iteration 293, loss = 0.21076067\n",
      "Iteration 294, loss = 0.21064871\n",
      "Iteration 295, loss = 0.21052177\n",
      "Iteration 296, loss = 0.21037240\n",
      "Iteration 297, loss = 0.21025115\n",
      "Iteration 298, loss = 0.21013371\n",
      "Iteration 299, loss = 0.20999615\n",
      "Iteration 300, loss = 0.20985237\n",
      "Iteration 301, loss = 0.20972774\n",
      "Iteration 302, loss = 0.20960990\n",
      "Iteration 303, loss = 0.20948521\n",
      "Iteration 304, loss = 0.20934241\n",
      "Iteration 305, loss = 0.20922313\n",
      "Iteration 306, loss = 0.20909040\n",
      "Iteration 307, loss = 0.20896435\n",
      "Iteration 308, loss = 0.20884587\n",
      "Iteration 309, loss = 0.20871577\n",
      "Iteration 310, loss = 0.20858468\n",
      "Iteration 311, loss = 0.20846421\n",
      "Iteration 312, loss = 0.20835192\n",
      "Iteration 313, loss = 0.20821580\n",
      "Iteration 314, loss = 0.20808690\n",
      "Iteration 315, loss = 0.20797090\n",
      "Iteration 316, loss = 0.20785561\n",
      "Iteration 317, loss = 0.20773239\n",
      "Iteration 318, loss = 0.20761203\n",
      "Iteration 319, loss = 0.20748222\n",
      "Iteration 320, loss = 0.20735893\n",
      "Iteration 321, loss = 0.20724082\n",
      "Iteration 322, loss = 0.20712854\n",
      "Iteration 323, loss = 0.20702238\n",
      "Iteration 324, loss = 0.20687943\n",
      "Iteration 325, loss = 0.20676003\n",
      "Iteration 326, loss = 0.20665975\n",
      "Iteration 327, loss = 0.20653293\n",
      "Iteration 328, loss = 0.20641063\n",
      "Iteration 329, loss = 0.20629069\n",
      "Iteration 330, loss = 0.20617730\n",
      "Iteration 331, loss = 0.20606351\n",
      "Iteration 332, loss = 0.20593911\n",
      "Iteration 333, loss = 0.20582650\n",
      "Iteration 334, loss = 0.20572157\n",
      "Iteration 335, loss = 0.20560535\n",
      "Iteration 336, loss = 0.20548871\n",
      "Iteration 337, loss = 0.20538089\n",
      "Iteration 338, loss = 0.20526119\n",
      "Iteration 339, loss = 0.20515377\n",
      "Iteration 340, loss = 0.20502548\n",
      "Iteration 341, loss = 0.20491511\n",
      "Iteration 342, loss = 0.20479944\n",
      "Iteration 343, loss = 0.20470360\n",
      "Iteration 344, loss = 0.20457763\n",
      "Iteration 345, loss = 0.20446715\n",
      "Iteration 346, loss = 0.20435066\n",
      "Iteration 347, loss = 0.20424455\n",
      "Iteration 348, loss = 0.20413413\n",
      "Iteration 349, loss = 0.20404036\n",
      "Iteration 350, loss = 0.20392398\n",
      "Iteration 351, loss = 0.20380487\n",
      "Iteration 352, loss = 0.20369274\n",
      "Iteration 353, loss = 0.20358038\n",
      "Iteration 354, loss = 0.20349170\n",
      "Iteration 355, loss = 0.20336401\n",
      "Iteration 356, loss = 0.20326513\n",
      "Iteration 357, loss = 0.20314897\n",
      "Iteration 358, loss = 0.20304538\n",
      "Iteration 359, loss = 0.20293978\n",
      "Iteration 360, loss = 0.20283614\n",
      "Iteration 361, loss = 0.20272210\n",
      "Iteration 362, loss = 0.20262347\n",
      "Iteration 363, loss = 0.20251367\n",
      "Iteration 364, loss = 0.20240959\n",
      "Iteration 365, loss = 0.20230503\n",
      "Iteration 366, loss = 0.20220767\n",
      "Iteration 367, loss = 0.20209152\n",
      "Iteration 368, loss = 0.20199359\n",
      "Iteration 369, loss = 0.20188436\n",
      "Iteration 370, loss = 0.20177910\n",
      "Iteration 371, loss = 0.20167565\n",
      "Iteration 372, loss = 0.20157571\n",
      "Iteration 373, loss = 0.20147814\n",
      "Iteration 374, loss = 0.20136763\n",
      "Iteration 375, loss = 0.20125870\n",
      "Iteration 376, loss = 0.20116857\n",
      "Iteration 377, loss = 0.20106109\n",
      "Iteration 378, loss = 0.20096739\n",
      "Iteration 379, loss = 0.20086950\n",
      "Iteration 380, loss = 0.20075762\n",
      "Iteration 381, loss = 0.20066101\n",
      "Iteration 382, loss = 0.20055546\n",
      "Iteration 383, loss = 0.20045686\n",
      "Iteration 384, loss = 0.20036182\n",
      "Iteration 385, loss = 0.20026581\n",
      "Iteration 386, loss = 0.20015733\n",
      "Iteration 387, loss = 0.20006764\n",
      "Iteration 388, loss = 0.19996861\n",
      "Iteration 389, loss = 0.19985968\n",
      "Iteration 390, loss = 0.19976860\n",
      "Iteration 391, loss = 0.19966623\n",
      "Iteration 392, loss = 0.19958406\n",
      "Iteration 393, loss = 0.19947398\n",
      "Iteration 394, loss = 0.19937605\n",
      "Iteration 395, loss = 0.19927792\n",
      "Iteration 396, loss = 0.19919214\n",
      "Iteration 397, loss = 0.19909603\n",
      "Iteration 398, loss = 0.19900290\n",
      "Iteration 399, loss = 0.19888659\n",
      "Iteration 400, loss = 0.19879679\n",
      "Iteration 401, loss = 0.19870125\n",
      "Iteration 402, loss = 0.19861544\n",
      "Iteration 403, loss = 0.19850817\n",
      "Iteration 404, loss = 0.19841259\n",
      "Iteration 405, loss = 0.19832692\n",
      "Iteration 406, loss = 0.19822906\n",
      "Iteration 407, loss = 0.19813694\n",
      "Iteration 408, loss = 0.19804331\n",
      "Iteration 409, loss = 0.19794230\n",
      "Iteration 410, loss = 0.19786765\n",
      "Iteration 411, loss = 0.19775887\n",
      "Iteration 412, loss = 0.19767228\n",
      "Iteration 413, loss = 0.19757364\n",
      "Iteration 414, loss = 0.19747871\n",
      "Iteration 415, loss = 0.19740062\n",
      "Iteration 416, loss = 0.19729880\n",
      "Iteration 417, loss = 0.19720289\n",
      "Iteration 418, loss = 0.19712400\n",
      "Iteration 419, loss = 0.19702327\n",
      "Iteration 420, loss = 0.19693920\n",
      "Iteration 421, loss = 0.19684363\n",
      "Iteration 422, loss = 0.19675966\n",
      "Iteration 423, loss = 0.19666409\n",
      "Iteration 424, loss = 0.19658305\n",
      "Iteration 425, loss = 0.19649642\n",
      "Iteration 426, loss = 0.19639219\n",
      "Iteration 427, loss = 0.19630618\n",
      "Iteration 428, loss = 0.19621343\n",
      "Iteration 429, loss = 0.19612545\n",
      "Iteration 430, loss = 0.19603537\n",
      "Iteration 431, loss = 0.19594969\n",
      "Iteration 432, loss = 0.19586322\n",
      "Iteration 433, loss = 0.19577256\n",
      "Iteration 434, loss = 0.19568572\n",
      "Iteration 435, loss = 0.19559924\n",
      "Iteration 436, loss = 0.19552333\n",
      "Iteration 437, loss = 0.19542346\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.949917\n",
      "Training set loss: 0.195423\n",
      "training: adam\n",
      "Iteration 1, loss = 1.79299026\n",
      "Iteration 2, loss = 0.70576785\n",
      "Iteration 3, loss = 0.32082894\n",
      "Iteration 4, loss = 0.20531281\n",
      "Iteration 5, loss = 0.15483935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.13064054\n",
      "Iteration 7, loss = 0.11579721\n",
      "Iteration 8, loss = 0.09939453\n",
      "Iteration 9, loss = 0.08412694\n",
      "Iteration 10, loss = 0.07732560\n",
      "Iteration 11, loss = 0.07169006\n",
      "Iteration 12, loss = 0.06331308\n",
      "Iteration 13, loss = 0.05730114\n",
      "Iteration 14, loss = 0.05329983\n",
      "Iteration 15, loss = 0.04593450\n",
      "Iteration 16, loss = 0.04249543\n",
      "Iteration 17, loss = 0.03997710\n",
      "Iteration 18, loss = 0.03626572\n",
      "Iteration 19, loss = 0.03605921\n",
      "Iteration 20, loss = 0.03171875\n",
      "Iteration 21, loss = 0.02947410\n",
      "Iteration 22, loss = 0.02660658\n",
      "Iteration 23, loss = 0.02429873\n",
      "Iteration 24, loss = 0.02011003\n",
      "Iteration 25, loss = 0.01864453\n",
      "Iteration 26, loss = 0.01749772\n",
      "Iteration 27, loss = 0.01622076\n",
      "Iteration 28, loss = 0.01607448\n",
      "Iteration 29, loss = 0.01590500\n",
      "Iteration 30, loss = 0.01473511\n",
      "Iteration 31, loss = 0.01274196\n",
      "Iteration 32, loss = 0.01098187\n",
      "Iteration 33, loss = 0.01040021\n",
      "Iteration 34, loss = 0.01023764\n",
      "Iteration 35, loss = 0.00897877\n",
      "Iteration 36, loss = 0.00872924\n",
      "Iteration 37, loss = 0.00811416\n",
      "Iteration 38, loss = 0.00779988\n",
      "Iteration 39, loss = 0.00740746\n",
      "Iteration 40, loss = 0.00695684\n",
      "Iteration 41, loss = 0.00675809\n",
      "Iteration 42, loss = 0.00678919\n",
      "Iteration 43, loss = 0.00588270\n",
      "Iteration 44, loss = 0.00578085\n",
      "Iteration 45, loss = 0.00556886\n",
      "Iteration 46, loss = 0.00545432\n",
      "Iteration 47, loss = 0.00497094\n",
      "Iteration 48, loss = 0.00462583\n",
      "Iteration 49, loss = 0.00430916\n",
      "Iteration 50, loss = 0.00409276\n",
      "Iteration 51, loss = 0.00391729\n",
      "Iteration 52, loss = 0.00376940\n",
      "Iteration 53, loss = 0.00349451\n",
      "Iteration 54, loss = 0.00367591\n",
      "Iteration 55, loss = 0.00336975\n",
      "Iteration 56, loss = 0.00324305\n",
      "Iteration 57, loss = 0.00318373\n",
      "Iteration 58, loss = 0.00317282\n",
      "Iteration 59, loss = 0.00295666\n",
      "Iteration 60, loss = 0.00279235\n",
      "Iteration 61, loss = 0.00269615\n",
      "Iteration 62, loss = 0.00254248\n",
      "Iteration 63, loss = 0.00251892\n",
      "Iteration 64, loss = 0.00255487\n",
      "Iteration 65, loss = 0.00237797\n",
      "Iteration 66, loss = 0.00225238\n",
      "Iteration 67, loss = 0.00218249\n",
      "Iteration 68, loss = 0.00210973\n",
      "Iteration 69, loss = 0.00206163\n",
      "Iteration 70, loss = 0.00215144\n",
      "Iteration 71, loss = 0.00205416\n",
      "Iteration 72, loss = 0.00195937\n",
      "Iteration 73, loss = 0.00186411\n",
      "Iteration 74, loss = 0.00180556\n",
      "Iteration 75, loss = 0.00178158\n",
      "Iteration 76, loss = 0.00173693\n",
      "Iteration 77, loss = 0.00172891\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 1.000000\n",
      "Training set loss: 0.001729\n",
      "\n",
      "learning on dataset circles\n",
      "training: constant learning-rate\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.69336674\n",
      "Iteration 3, loss = 0.69183412\n",
      "Iteration 4, loss = 0.69072503\n",
      "Iteration 5, loss = 0.68991576\n",
      "Iteration 6, loss = 0.68928297\n",
      "Iteration 7, loss = 0.68877282\n",
      "Iteration 8, loss = 0.68834795\n",
      "Iteration 9, loss = 0.68799632\n",
      "Iteration 10, loss = 0.68769577\n",
      "Iteration 11, loss = 0.68742717\n",
      "Iteration 12, loss = 0.68718825\n",
      "Iteration 13, loss = 0.68696762\n",
      "Iteration 14, loss = 0.68675326\n",
      "Iteration 15, loss = 0.68653513\n",
      "Iteration 16, loss = 0.68631577\n",
      "Iteration 17, loss = 0.68610482\n",
      "Iteration 18, loss = 0.68588712\n",
      "Iteration 19, loss = 0.68566580\n",
      "Iteration 20, loss = 0.68543962\n",
      "Iteration 21, loss = 0.68522084\n",
      "Iteration 22, loss = 0.68501041\n",
      "Iteration 23, loss = 0.68481008\n",
      "Iteration 24, loss = 0.68461552\n",
      "Iteration 25, loss = 0.68441030\n",
      "Iteration 26, loss = 0.68421279\n",
      "Iteration 27, loss = 0.68402724\n",
      "Iteration 28, loss = 0.68386305\n",
      "Iteration 29, loss = 0.68371924\n",
      "Iteration 30, loss = 0.68358544\n",
      "Iteration 31, loss = 0.68345139\n",
      "Iteration 32, loss = 0.68332397\n",
      "Iteration 33, loss = 0.68319834\n",
      "Iteration 34, loss = 0.68307718\n",
      "Iteration 35, loss = 0.68295747\n",
      "Iteration 36, loss = 0.68284132\n",
      "Iteration 37, loss = 0.68273132\n",
      "Iteration 38, loss = 0.68262330\n",
      "Iteration 39, loss = 0.68251852\n",
      "Iteration 40, loss = 0.68241398\n",
      "Iteration 41, loss = 0.68231056\n",
      "Iteration 42, loss = 0.68220776\n",
      "Iteration 43, loss = 0.68210447\n",
      "Iteration 44, loss = 0.68200034\n",
      "Iteration 45, loss = 0.68189766\n",
      "Iteration 46, loss = 0.68179759\n",
      "Iteration 47, loss = 0.68169821\n",
      "Iteration 48, loss = 0.68159827\n",
      "Iteration 49, loss = 0.68149842\n",
      "Iteration 50, loss = 0.68139856\n",
      "Iteration 51, loss = 0.68129925\n",
      "Iteration 52, loss = 0.68119983\n",
      "Iteration 53, loss = 0.68109923\n",
      "Iteration 54, loss = 0.68099749\n",
      "Iteration 55, loss = 0.68089509\n",
      "Iteration 56, loss = 0.68079308\n",
      "Iteration 57, loss = 0.68069103\n",
      "Iteration 58, loss = 0.68058818\n",
      "Iteration 59, loss = 0.68048382\n",
      "Iteration 60, loss = 0.68037928\n",
      "Iteration 61, loss = 0.68027431\n",
      "Iteration 62, loss = 0.68016889\n",
      "Iteration 63, loss = 0.68006299\n",
      "Iteration 64, loss = 0.67995854\n",
      "Iteration 65, loss = 0.67985320\n",
      "Iteration 66, loss = 0.67974814\n",
      "Iteration 67, loss = 0.67964317\n",
      "Iteration 68, loss = 0.67953760\n",
      "Iteration 69, loss = 0.67943116\n",
      "Iteration 70, loss = 0.67932354\n",
      "Iteration 71, loss = 0.67921457\n",
      "Iteration 72, loss = 0.67910528\n",
      "Iteration 73, loss = 0.67899456\n",
      "Iteration 74, loss = 0.67888263\n",
      "Iteration 75, loss = 0.67877001\n",
      "Iteration 76, loss = 0.67865730\n",
      "Iteration 77, loss = 0.67854484\n",
      "Iteration 78, loss = 0.67843164\n",
      "Iteration 79, loss = 0.67831836\n",
      "Iteration 80, loss = 0.67820403\n",
      "Iteration 81, loss = 0.67809000\n",
      "Iteration 82, loss = 0.67797463\n",
      "Iteration 83, loss = 0.67785920\n",
      "Iteration 84, loss = 0.67774169\n",
      "Iteration 85, loss = 0.67762228\n",
      "Iteration 86, loss = 0.67750156\n",
      "Iteration 87, loss = 0.67737882\n",
      "Iteration 88, loss = 0.67725328\n",
      "Iteration 89, loss = 0.67712794\n",
      "Iteration 90, loss = 0.67700293\n",
      "Iteration 91, loss = 0.67687813\n",
      "Iteration 92, loss = 0.67675278\n",
      "Iteration 93, loss = 0.67662632\n",
      "Iteration 94, loss = 0.67649915\n",
      "Iteration 95, loss = 0.67637199\n",
      "Iteration 96, loss = 0.67624150\n",
      "Iteration 97, loss = 0.67610839\n",
      "Iteration 98, loss = 0.67597534\n",
      "Iteration 99, loss = 0.67584136\n",
      "Iteration 100, loss = 0.67570553\n",
      "Iteration 101, loss = 0.67556876\n",
      "Iteration 102, loss = 0.67542436\n",
      "Iteration 103, loss = 0.67527494\n",
      "Iteration 104, loss = 0.67512423\n",
      "Iteration 105, loss = 0.67497003\n",
      "Iteration 106, loss = 0.67479976\n",
      "Iteration 107, loss = 0.67461642\n",
      "Iteration 108, loss = 0.67443003\n",
      "Iteration 109, loss = 0.67424585\n",
      "Iteration 110, loss = 0.67407829\n",
      "Iteration 111, loss = 0.67392181\n",
      "Iteration 112, loss = 0.67376151\n",
      "Iteration 113, loss = 0.67359601\n",
      "Iteration 114, loss = 0.67343570\n",
      "Iteration 115, loss = 0.67328177\n",
      "Iteration 116, loss = 0.67312572\n",
      "Iteration 117, loss = 0.67296889\n",
      "Iteration 118, loss = 0.67281165\n",
      "Iteration 119, loss = 0.67265954\n",
      "Iteration 120, loss = 0.67251258\n",
      "Iteration 121, loss = 0.67236702\n",
      "Iteration 122, loss = 0.67222041\n",
      "Iteration 123, loss = 0.67207168\n",
      "Iteration 124, loss = 0.67192025\n",
      "Iteration 125, loss = 0.67176919\n",
      "Iteration 126, loss = 0.67161748\n",
      "Iteration 127, loss = 0.67146679\n",
      "Iteration 128, loss = 0.67131746\n",
      "Iteration 129, loss = 0.67116793\n",
      "Iteration 130, loss = 0.67101909\n",
      "Iteration 131, loss = 0.67087130\n",
      "Iteration 132, loss = 0.67072382\n",
      "Iteration 133, loss = 0.67057633\n",
      "Iteration 134, loss = 0.67042891\n",
      "Iteration 135, loss = 0.67028230\n",
      "Iteration 136, loss = 0.67013585\n",
      "Iteration 137, loss = 0.66998853\n",
      "Iteration 138, loss = 0.66984043\n",
      "Iteration 139, loss = 0.66969217\n",
      "Iteration 140, loss = 0.66954345\n",
      "Iteration 141, loss = 0.66939493\n",
      "Iteration 142, loss = 0.66924573\n",
      "Iteration 143, loss = 0.66909547\n",
      "Iteration 144, loss = 0.66894460\n",
      "Iteration 145, loss = 0.66879302\n",
      "Iteration 146, loss = 0.66864094\n",
      "Iteration 147, loss = 0.66848911\n",
      "Iteration 148, loss = 0.66833638\n",
      "Iteration 149, loss = 0.66818271\n",
      "Iteration 150, loss = 0.66802794\n",
      "Iteration 151, loss = 0.66787277\n",
      "Iteration 152, loss = 0.66771682\n",
      "Iteration 153, loss = 0.66756019\n",
      "Iteration 154, loss = 0.66740353\n",
      "Iteration 155, loss = 0.66724618\n",
      "Iteration 156, loss = 0.66708836\n",
      "Iteration 157, loss = 0.66692973\n",
      "Iteration 158, loss = 0.66677085\n",
      "Iteration 159, loss = 0.66661128\n",
      "Iteration 160, loss = 0.66645182\n",
      "Iteration 161, loss = 0.66629119\n",
      "Iteration 162, loss = 0.66612963\n",
      "Iteration 163, loss = 0.66596748\n",
      "Iteration 164, loss = 0.66580493\n",
      "Iteration 165, loss = 0.66564160\n",
      "Iteration 166, loss = 0.66547769\n",
      "Iteration 167, loss = 0.66531304\n",
      "Iteration 168, loss = 0.66514670\n",
      "Iteration 169, loss = 0.66497947\n",
      "Iteration 170, loss = 0.66481217\n",
      "Iteration 171, loss = 0.66464341\n",
      "Iteration 172, loss = 0.66447480\n",
      "Iteration 173, loss = 0.66430521\n",
      "Iteration 174, loss = 0.66413515\n",
      "Iteration 175, loss = 0.66396445\n",
      "Iteration 176, loss = 0.66379254\n",
      "Iteration 177, loss = 0.66361952\n",
      "Iteration 178, loss = 0.66344692\n",
      "Iteration 179, loss = 0.66327248\n",
      "Iteration 180, loss = 0.66309799\n",
      "Iteration 181, loss = 0.66292225\n",
      "Iteration 182, loss = 0.66274593\n",
      "Iteration 183, loss = 0.66256884\n",
      "Iteration 184, loss = 0.66239176\n",
      "Iteration 185, loss = 0.66221310\n",
      "Iteration 186, loss = 0.66203470\n",
      "Iteration 187, loss = 0.66185483\n",
      "Iteration 188, loss = 0.66167497\n",
      "Iteration 189, loss = 0.66149367\n",
      "Iteration 190, loss = 0.66131137\n",
      "Iteration 191, loss = 0.66112911\n",
      "Iteration 192, loss = 0.66094547\n",
      "Iteration 193, loss = 0.66076158\n",
      "Iteration 194, loss = 0.66057661\n",
      "Iteration 195, loss = 0.66039145\n",
      "Iteration 196, loss = 0.66020504\n",
      "Iteration 197, loss = 0.66001724\n",
      "Iteration 198, loss = 0.65982910\n",
      "Iteration 199, loss = 0.65963918\n",
      "Iteration 200, loss = 0.65944976\n",
      "Iteration 201, loss = 0.65925796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 202, loss = 0.65906532\n",
      "Iteration 203, loss = 0.65887273\n",
      "Iteration 204, loss = 0.65867780\n",
      "Iteration 205, loss = 0.65848197\n",
      "Iteration 206, loss = 0.65828588\n",
      "Iteration 207, loss = 0.65808854\n",
      "Iteration 208, loss = 0.65788997\n",
      "Iteration 209, loss = 0.65769167\n",
      "Iteration 210, loss = 0.65749202\n",
      "Iteration 211, loss = 0.65729139\n",
      "Iteration 212, loss = 0.65709098\n",
      "Iteration 213, loss = 0.65688834\n",
      "Iteration 214, loss = 0.65668453\n",
      "Iteration 215, loss = 0.65647900\n",
      "Iteration 216, loss = 0.65627233\n",
      "Iteration 217, loss = 0.65606466\n",
      "Iteration 218, loss = 0.65585671\n",
      "Iteration 219, loss = 0.65564701\n",
      "Iteration 220, loss = 0.65543712\n",
      "Iteration 221, loss = 0.65522491\n",
      "Iteration 222, loss = 0.65501380\n",
      "Iteration 223, loss = 0.65480029\n",
      "Iteration 224, loss = 0.65458608\n",
      "Iteration 225, loss = 0.65437029\n",
      "Iteration 226, loss = 0.65415502\n",
      "Iteration 227, loss = 0.65393795\n",
      "Iteration 228, loss = 0.65371971\n",
      "Iteration 229, loss = 0.65350065\n",
      "Iteration 230, loss = 0.65328055\n",
      "Iteration 231, loss = 0.65306085\n",
      "Iteration 232, loss = 0.65283963\n",
      "Iteration 233, loss = 0.65261795\n",
      "Iteration 234, loss = 0.65239656\n",
      "Iteration 235, loss = 0.65217187\n",
      "Iteration 236, loss = 0.65194770\n",
      "Iteration 237, loss = 0.65172085\n",
      "Iteration 238, loss = 0.65149422\n",
      "Iteration 239, loss = 0.65126587\n",
      "Iteration 240, loss = 0.65103619\n",
      "Iteration 241, loss = 0.65080725\n",
      "Iteration 242, loss = 0.65057744\n",
      "Iteration 243, loss = 0.65034748\n",
      "Iteration 244, loss = 0.65011605\n",
      "Iteration 245, loss = 0.64988332\n",
      "Iteration 246, loss = 0.64965013\n",
      "Iteration 247, loss = 0.64941616\n",
      "Iteration 248, loss = 0.64918124\n",
      "Iteration 249, loss = 0.64894573\n",
      "Iteration 250, loss = 0.64870981\n",
      "Iteration 251, loss = 0.64847309\n",
      "Iteration 252, loss = 0.64823471\n",
      "Iteration 253, loss = 0.64799528\n",
      "Iteration 254, loss = 0.64775456\n",
      "Iteration 255, loss = 0.64751345\n",
      "Iteration 256, loss = 0.64727121\n",
      "Iteration 257, loss = 0.64702861\n",
      "Iteration 258, loss = 0.64678506\n",
      "Iteration 259, loss = 0.64654020\n",
      "Iteration 260, loss = 0.64629520\n",
      "Iteration 261, loss = 0.64604925\n",
      "Iteration 262, loss = 0.64580229\n",
      "Iteration 263, loss = 0.64555337\n",
      "Iteration 264, loss = 0.64530414\n",
      "Iteration 265, loss = 0.64505481\n",
      "Iteration 266, loss = 0.64480446\n",
      "Iteration 267, loss = 0.64455243\n",
      "Iteration 268, loss = 0.64429991\n",
      "Iteration 269, loss = 0.64404663\n",
      "Iteration 270, loss = 0.64379195\n",
      "Iteration 271, loss = 0.64353698\n",
      "Iteration 272, loss = 0.64328052\n",
      "Iteration 273, loss = 0.64302248\n",
      "Iteration 274, loss = 0.64276356\n",
      "Iteration 275, loss = 0.64250364\n",
      "Iteration 276, loss = 0.64224335\n",
      "Iteration 277, loss = 0.64198264\n",
      "Iteration 278, loss = 0.64171991\n",
      "Iteration 279, loss = 0.64145561\n",
      "Iteration 280, loss = 0.64119071\n",
      "Iteration 281, loss = 0.64092541\n",
      "Iteration 282, loss = 0.64065934\n",
      "Iteration 283, loss = 0.64039207\n",
      "Iteration 284, loss = 0.64012369\n",
      "Iteration 285, loss = 0.63985227\n",
      "Iteration 286, loss = 0.63958121\n",
      "Iteration 287, loss = 0.63930693\n",
      "Iteration 288, loss = 0.63903208\n",
      "Iteration 289, loss = 0.63875524\n",
      "Iteration 290, loss = 0.63847841\n",
      "Iteration 291, loss = 0.63820035\n",
      "Iteration 292, loss = 0.63791930\n",
      "Iteration 293, loss = 0.63763370\n",
      "Iteration 294, loss = 0.63734735\n",
      "Iteration 295, loss = 0.63706057\n",
      "Iteration 296, loss = 0.63677124\n",
      "Iteration 297, loss = 0.63647863\n",
      "Iteration 298, loss = 0.63618362\n",
      "Iteration 299, loss = 0.63588869\n",
      "Iteration 300, loss = 0.63559695\n",
      "Iteration 301, loss = 0.63530377\n",
      "Iteration 302, loss = 0.63500902\n",
      "Iteration 303, loss = 0.63471674\n",
      "Iteration 304, loss = 0.63442419\n",
      "Iteration 305, loss = 0.63413151\n",
      "Iteration 306, loss = 0.63383713\n",
      "Iteration 307, loss = 0.63354507\n",
      "Iteration 308, loss = 0.63325019\n",
      "Iteration 309, loss = 0.63295478\n",
      "Iteration 310, loss = 0.63266071\n",
      "Iteration 311, loss = 0.63236383\n",
      "Iteration 312, loss = 0.63206507\n",
      "Iteration 313, loss = 0.63176624\n",
      "Iteration 314, loss = 0.63146668\n",
      "Iteration 315, loss = 0.63116759\n",
      "Iteration 316, loss = 0.63086582\n",
      "Iteration 317, loss = 0.63056247\n",
      "Iteration 318, loss = 0.63025789\n",
      "Iteration 319, loss = 0.62995237\n",
      "Iteration 320, loss = 0.62964756\n",
      "Iteration 321, loss = 0.62933982\n",
      "Iteration 322, loss = 0.62903003\n",
      "Iteration 323, loss = 0.62872065\n",
      "Iteration 324, loss = 0.62841128\n",
      "Iteration 325, loss = 0.62809894\n",
      "Iteration 326, loss = 0.62778441\n",
      "Iteration 327, loss = 0.62747196\n",
      "Iteration 328, loss = 0.62715585\n",
      "Iteration 329, loss = 0.62683975\n",
      "Iteration 330, loss = 0.62652263\n",
      "Iteration 331, loss = 0.62620538\n",
      "Iteration 332, loss = 0.62588579\n",
      "Iteration 333, loss = 0.62556674\n",
      "Iteration 334, loss = 0.62524436\n",
      "Iteration 335, loss = 0.62492199\n",
      "Iteration 336, loss = 0.62460003\n",
      "Iteration 337, loss = 0.62427488\n",
      "Iteration 338, loss = 0.62394908\n",
      "Iteration 339, loss = 0.62362253\n",
      "Iteration 340, loss = 0.62329624\n",
      "Iteration 341, loss = 0.62296701\n",
      "Iteration 342, loss = 0.62263713\n",
      "Iteration 343, loss = 0.62230767\n",
      "Iteration 344, loss = 0.62197310\n",
      "Iteration 345, loss = 0.62163906\n",
      "Iteration 346, loss = 0.62130258\n",
      "Iteration 347, loss = 0.62096656\n",
      "Iteration 348, loss = 0.62062713\n",
      "Iteration 349, loss = 0.62028458\n",
      "Iteration 350, loss = 0.61994364\n",
      "Iteration 351, loss = 0.61960456\n",
      "Iteration 352, loss = 0.61926305\n",
      "Iteration 353, loss = 0.61891803\n",
      "Iteration 354, loss = 0.61857341\n",
      "Iteration 355, loss = 0.61822990\n",
      "Iteration 356, loss = 0.61788167\n",
      "Iteration 357, loss = 0.61753778\n",
      "Iteration 358, loss = 0.61718789\n",
      "Iteration 359, loss = 0.61683758\n",
      "Iteration 360, loss = 0.61648568\n",
      "Iteration 361, loss = 0.61613553\n",
      "Iteration 362, loss = 0.61578121\n",
      "Iteration 363, loss = 0.61542492\n",
      "Iteration 364, loss = 0.61507036\n",
      "Iteration 365, loss = 0.61471248\n",
      "Iteration 366, loss = 0.61435195\n",
      "Iteration 367, loss = 0.61399451\n",
      "Iteration 368, loss = 0.61363053\n",
      "Iteration 369, loss = 0.61326691\n",
      "Iteration 370, loss = 0.61291037\n",
      "Iteration 371, loss = 0.61254049\n",
      "Iteration 372, loss = 0.61217638\n",
      "Iteration 373, loss = 0.61180840\n",
      "Iteration 374, loss = 0.61144489\n",
      "Iteration 375, loss = 0.61107237\n",
      "Iteration 376, loss = 0.61070317\n",
      "Iteration 377, loss = 0.61033322\n",
      "Iteration 378, loss = 0.60995840\n",
      "Iteration 379, loss = 0.60958640\n",
      "Iteration 380, loss = 0.60920880\n",
      "Iteration 381, loss = 0.60883242\n",
      "Iteration 382, loss = 0.60845326\n",
      "Iteration 383, loss = 0.60807729\n",
      "Iteration 384, loss = 0.60769494\n",
      "Iteration 385, loss = 0.60731509\n",
      "Iteration 386, loss = 0.60692876\n",
      "Iteration 387, loss = 0.60654677\n",
      "Iteration 388, loss = 0.60615334\n",
      "Iteration 389, loss = 0.60575936\n",
      "Iteration 390, loss = 0.60535294\n",
      "Iteration 391, loss = 0.60494525\n",
      "Iteration 392, loss = 0.60452614\n",
      "Iteration 393, loss = 0.60411030\n",
      "Iteration 394, loss = 0.60369268\n",
      "Iteration 395, loss = 0.60326536\n",
      "Iteration 396, loss = 0.60283216\n",
      "Iteration 397, loss = 0.60238757\n",
      "Iteration 398, loss = 0.60194306\n",
      "Iteration 399, loss = 0.60148734\n",
      "Iteration 400, loss = 0.60105195\n",
      "Iteration 401, loss = 0.60061484\n",
      "Iteration 402, loss = 0.60017402\n",
      "Iteration 403, loss = 0.59973515\n",
      "Iteration 404, loss = 0.59930263\n",
      "Iteration 405, loss = 0.59886010\n",
      "Iteration 406, loss = 0.59842131\n",
      "Iteration 407, loss = 0.59796830\n",
      "Iteration 408, loss = 0.59752687\n",
      "Iteration 409, loss = 0.59707316\n",
      "Iteration 410, loss = 0.59662845\n",
      "Iteration 411, loss = 0.59618560\n",
      "Iteration 412, loss = 0.59575501\n",
      "Iteration 413, loss = 0.59532494\n",
      "Iteration 414, loss = 0.59489888\n",
      "Iteration 415, loss = 0.59447213\n",
      "Iteration 416, loss = 0.59405507\n",
      "Iteration 417, loss = 0.59363322\n",
      "Iteration 418, loss = 0.59321520\n",
      "Iteration 419, loss = 0.59279363\n",
      "Iteration 420, loss = 0.59237327\n",
      "Iteration 421, loss = 0.59194985\n",
      "Iteration 422, loss = 0.59152833\n",
      "Iteration 423, loss = 0.59110193\n",
      "Iteration 424, loss = 0.59067570\n",
      "Iteration 425, loss = 0.59024873\n",
      "Iteration 426, loss = 0.58982007\n",
      "Iteration 427, loss = 0.58939282\n",
      "Iteration 428, loss = 0.58896147\n",
      "Iteration 429, loss = 0.58852942\n",
      "Iteration 430, loss = 0.58809728\n",
      "Iteration 431, loss = 0.58766407\n",
      "Iteration 432, loss = 0.58722884\n",
      "Iteration 433, loss = 0.58679239\n",
      "Iteration 434, loss = 0.58635649\n",
      "Iteration 435, loss = 0.58591859\n",
      "Iteration 436, loss = 0.58547688\n",
      "Iteration 437, loss = 0.58503564\n",
      "Iteration 438, loss = 0.58459382\n",
      "Iteration 439, loss = 0.58415156\n",
      "Iteration 440, loss = 0.58370757\n",
      "Iteration 441, loss = 0.58326158\n",
      "Iteration 442, loss = 0.58281597\n",
      "Iteration 443, loss = 0.58237288\n",
      "Iteration 444, loss = 0.58192443\n",
      "Iteration 445, loss = 0.58147507\n",
      "Iteration 446, loss = 0.58102742\n",
      "Iteration 447, loss = 0.58057754\n",
      "Iteration 448, loss = 0.58012823\n",
      "Iteration 449, loss = 0.57967666\n",
      "Iteration 450, loss = 0.57922281\n",
      "Iteration 451, loss = 0.57876841\n",
      "Iteration 452, loss = 0.57831600\n",
      "Iteration 453, loss = 0.57785893\n",
      "Iteration 454, loss = 0.57740283\n",
      "Iteration 455, loss = 0.57694543\n",
      "Iteration 456, loss = 0.57648804\n",
      "Iteration 457, loss = 0.57602664\n",
      "Iteration 458, loss = 0.57556530\n",
      "Iteration 459, loss = 0.57510259\n",
      "Iteration 460, loss = 0.57463837\n",
      "Iteration 461, loss = 0.57417339\n",
      "Iteration 462, loss = 0.57370743\n",
      "Iteration 463, loss = 0.57324287\n",
      "Iteration 464, loss = 0.57277656\n",
      "Iteration 465, loss = 0.57230500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 466, loss = 0.57183882\n",
      "Iteration 467, loss = 0.57136518\n",
      "Iteration 468, loss = 0.57089137\n",
      "Iteration 469, loss = 0.57042037\n",
      "Iteration 470, loss = 0.56994859\n",
      "Iteration 471, loss = 0.56947441\n",
      "Iteration 472, loss = 0.56899846\n",
      "Iteration 473, loss = 0.56852202\n",
      "Iteration 474, loss = 0.56804453\n",
      "Iteration 475, loss = 0.56756836\n",
      "Iteration 476, loss = 0.56709018\n",
      "Iteration 477, loss = 0.56660983\n",
      "Iteration 478, loss = 0.56612751\n",
      "Iteration 479, loss = 0.56564331\n",
      "Iteration 480, loss = 0.56516185\n",
      "Iteration 481, loss = 0.56467833\n",
      "Iteration 482, loss = 0.56419195\n",
      "Iteration 483, loss = 0.56370471\n",
      "Iteration 484, loss = 0.56321779\n",
      "Iteration 485, loss = 0.56273236\n",
      "Iteration 486, loss = 0.56224129\n",
      "Iteration 487, loss = 0.56175139\n",
      "Iteration 488, loss = 0.56126015\n",
      "Iteration 489, loss = 0.56076376\n",
      "Iteration 490, loss = 0.56026865\n",
      "Iteration 491, loss = 0.55977301\n",
      "Iteration 492, loss = 0.55926949\n",
      "Iteration 493, loss = 0.55877310\n",
      "Iteration 494, loss = 0.55827595\n",
      "Iteration 495, loss = 0.55777875\n",
      "Iteration 496, loss = 0.55727892\n",
      "Iteration 497, loss = 0.55677915\n",
      "Iteration 498, loss = 0.55628021\n",
      "Iteration 499, loss = 0.55577825\n",
      "Iteration 500, loss = 0.55527098\n",
      "Iteration 501, loss = 0.55476818\n",
      "Iteration 502, loss = 0.55426420\n",
      "Iteration 503, loss = 0.55375737\n",
      "Iteration 504, loss = 0.55325159\n",
      "Iteration 505, loss = 0.55274503\n",
      "Iteration 506, loss = 0.55223922\n",
      "Iteration 507, loss = 0.55173124\n",
      "Iteration 508, loss = 0.55122360\n",
      "Iteration 509, loss = 0.55071253\n",
      "Iteration 510, loss = 0.55020025\n",
      "Iteration 511, loss = 0.54968726\n",
      "Iteration 512, loss = 0.54917595\n",
      "Iteration 513, loss = 0.54866326\n",
      "Iteration 514, loss = 0.54815124\n",
      "Iteration 515, loss = 0.54763477\n",
      "Iteration 516, loss = 0.54712253\n",
      "Iteration 517, loss = 0.54660580\n",
      "Iteration 518, loss = 0.54609388\n",
      "Iteration 519, loss = 0.54557839\n",
      "Iteration 520, loss = 0.54505966\n",
      "Iteration 521, loss = 0.54454879\n",
      "Iteration 522, loss = 0.54403164\n",
      "Iteration 523, loss = 0.54351554\n",
      "Iteration 524, loss = 0.54300086\n",
      "Iteration 525, loss = 0.54248219\n",
      "Iteration 526, loss = 0.54196579\n",
      "Iteration 527, loss = 0.54144709\n",
      "Iteration 528, loss = 0.54093030\n",
      "Iteration 529, loss = 0.54041037\n",
      "Iteration 530, loss = 0.53989065\n",
      "Iteration 531, loss = 0.53937198\n",
      "Iteration 532, loss = 0.53885106\n",
      "Iteration 533, loss = 0.53832965\n",
      "Iteration 534, loss = 0.53780688\n",
      "Iteration 535, loss = 0.53728538\n",
      "Iteration 536, loss = 0.53676289\n",
      "Iteration 537, loss = 0.53623812\n",
      "Iteration 538, loss = 0.53571369\n",
      "Iteration 539, loss = 0.53518930\n",
      "Iteration 540, loss = 0.53466480\n",
      "Iteration 541, loss = 0.53414105\n",
      "Iteration 542, loss = 0.53361130\n",
      "Iteration 543, loss = 0.53308879\n",
      "Iteration 544, loss = 0.53255931\n",
      "Iteration 545, loss = 0.53203587\n",
      "Iteration 546, loss = 0.53150372\n",
      "Iteration 547, loss = 0.53098099\n",
      "Iteration 548, loss = 0.53045193\n",
      "Iteration 549, loss = 0.52992196\n",
      "Iteration 550, loss = 0.52939240\n",
      "Iteration 551, loss = 0.52886372\n",
      "Iteration 552, loss = 0.52833273\n",
      "Iteration 553, loss = 0.52780411\n",
      "Iteration 554, loss = 0.52727201\n",
      "Iteration 555, loss = 0.52674153\n",
      "Iteration 556, loss = 0.52621023\n",
      "Iteration 557, loss = 0.52567838\n",
      "Iteration 558, loss = 0.52514517\n",
      "Iteration 559, loss = 0.52460959\n",
      "Iteration 560, loss = 0.52408041\n",
      "Iteration 561, loss = 0.52354245\n",
      "Iteration 562, loss = 0.52301217\n",
      "Iteration 563, loss = 0.52247558\n",
      "Iteration 564, loss = 0.52193846\n",
      "Iteration 565, loss = 0.52140881\n",
      "Iteration 566, loss = 0.52086767\n",
      "Iteration 567, loss = 0.52033444\n",
      "Iteration 568, loss = 0.51979820\n",
      "Iteration 569, loss = 0.51926177\n",
      "Iteration 570, loss = 0.51872811\n",
      "Iteration 571, loss = 0.51818718\n",
      "Iteration 572, loss = 0.51765145\n",
      "Iteration 573, loss = 0.51711472\n",
      "Iteration 574, loss = 0.51657407\n",
      "Iteration 575, loss = 0.51603925\n",
      "Iteration 576, loss = 0.51550160\n",
      "Iteration 577, loss = 0.51496134\n",
      "Iteration 578, loss = 0.51442064\n",
      "Iteration 579, loss = 0.51388284\n",
      "Iteration 580, loss = 0.51334296\n",
      "Iteration 581, loss = 0.51280201\n",
      "Iteration 582, loss = 0.51226330\n",
      "Iteration 583, loss = 0.51172160\n",
      "Iteration 584, loss = 0.51117699\n",
      "Iteration 585, loss = 0.51063797\n",
      "Iteration 586, loss = 0.51009588\n",
      "Iteration 587, loss = 0.50955017\n",
      "Iteration 588, loss = 0.50900989\n",
      "Iteration 589, loss = 0.50846735\n",
      "Iteration 590, loss = 0.50792153\n",
      "Iteration 591, loss = 0.50737905\n",
      "Iteration 592, loss = 0.50683347\n",
      "Iteration 593, loss = 0.50628014\n",
      "Iteration 594, loss = 0.50573201\n",
      "Iteration 595, loss = 0.50518670\n",
      "Iteration 596, loss = 0.50463749\n",
      "Iteration 597, loss = 0.50408993\n",
      "Iteration 598, loss = 0.50354487\n",
      "Iteration 599, loss = 0.50299695\n",
      "Iteration 600, loss = 0.50244778\n",
      "Iteration 601, loss = 0.50190269\n",
      "Iteration 602, loss = 0.50135993\n",
      "Iteration 603, loss = 0.50081097\n",
      "Iteration 604, loss = 0.50026223\n",
      "Iteration 605, loss = 0.49971142\n",
      "Iteration 606, loss = 0.49915989\n",
      "Iteration 607, loss = 0.49861050\n",
      "Iteration 608, loss = 0.49806318\n",
      "Iteration 609, loss = 0.49751138\n",
      "Iteration 610, loss = 0.49696156\n",
      "Iteration 611, loss = 0.49641562\n",
      "Iteration 612, loss = 0.49587242\n",
      "Iteration 613, loss = 0.49532539\n",
      "Iteration 614, loss = 0.49477178\n",
      "Iteration 615, loss = 0.49422806\n",
      "Iteration 616, loss = 0.49368285\n",
      "Iteration 617, loss = 0.49313855\n",
      "Iteration 618, loss = 0.49259113\n",
      "Iteration 619, loss = 0.49204508\n",
      "Iteration 620, loss = 0.49149802\n",
      "Iteration 621, loss = 0.49095158\n",
      "Iteration 622, loss = 0.49040404\n",
      "Iteration 623, loss = 0.48985978\n",
      "Iteration 624, loss = 0.48931300\n",
      "Iteration 625, loss = 0.48877079\n",
      "Iteration 626, loss = 0.48822103\n",
      "Iteration 627, loss = 0.48767585\n",
      "Iteration 628, loss = 0.48713087\n",
      "Iteration 629, loss = 0.48658701\n",
      "Iteration 630, loss = 0.48603916\n",
      "Iteration 631, loss = 0.48549164\n",
      "Iteration 632, loss = 0.48495059\n",
      "Iteration 633, loss = 0.48440359\n",
      "Iteration 634, loss = 0.48385605\n",
      "Iteration 635, loss = 0.48331230\n",
      "Iteration 636, loss = 0.48276673\n",
      "Iteration 637, loss = 0.48222228\n",
      "Iteration 638, loss = 0.48166999\n",
      "Iteration 639, loss = 0.48111876\n",
      "Iteration 640, loss = 0.48056503\n",
      "Iteration 641, loss = 0.48001777\n",
      "Iteration 642, loss = 0.47947187\n",
      "Iteration 643, loss = 0.47892633\n",
      "Iteration 644, loss = 0.47838465\n",
      "Iteration 645, loss = 0.47783582\n",
      "Iteration 646, loss = 0.47729502\n",
      "Iteration 647, loss = 0.47675133\n",
      "Iteration 648, loss = 0.47621089\n",
      "Iteration 649, loss = 0.47566459\n",
      "Iteration 650, loss = 0.47512040\n",
      "Iteration 651, loss = 0.47458170\n",
      "Iteration 652, loss = 0.47403778\n",
      "Iteration 653, loss = 0.47349450\n",
      "Iteration 654, loss = 0.47294707\n",
      "Iteration 655, loss = 0.47240972\n",
      "Iteration 656, loss = 0.47186583\n",
      "Iteration 657, loss = 0.47132262\n",
      "Iteration 658, loss = 0.47078116\n",
      "Iteration 659, loss = 0.47023754\n",
      "Iteration 660, loss = 0.46969948\n",
      "Iteration 661, loss = 0.46915927\n",
      "Iteration 662, loss = 0.46861586\n",
      "Iteration 663, loss = 0.46808001\n",
      "Iteration 664, loss = 0.46753904\n",
      "Iteration 665, loss = 0.46700105\n",
      "Iteration 666, loss = 0.46645744\n",
      "Iteration 667, loss = 0.46592038\n",
      "Iteration 668, loss = 0.46538286\n",
      "Iteration 669, loss = 0.46484268\n",
      "Iteration 670, loss = 0.46430648\n",
      "Iteration 671, loss = 0.46376584\n",
      "Iteration 672, loss = 0.46322837\n",
      "Iteration 673, loss = 0.46269142\n",
      "Iteration 674, loss = 0.46215264\n",
      "Iteration 675, loss = 0.46161808\n",
      "Iteration 676, loss = 0.46107744\n",
      "Iteration 677, loss = 0.46053823\n",
      "Iteration 678, loss = 0.46000683\n",
      "Iteration 679, loss = 0.45947095\n",
      "Iteration 680, loss = 0.45893112\n",
      "Iteration 681, loss = 0.45839303\n",
      "Iteration 682, loss = 0.45786578\n",
      "Iteration 683, loss = 0.45732603\n",
      "Iteration 684, loss = 0.45678977\n",
      "Iteration 685, loss = 0.45625432\n",
      "Iteration 686, loss = 0.45571643\n",
      "Iteration 687, loss = 0.45518751\n",
      "Iteration 688, loss = 0.45465213\n",
      "Iteration 689, loss = 0.45411459\n",
      "Iteration 690, loss = 0.45358270\n",
      "Iteration 691, loss = 0.45305009\n",
      "Iteration 692, loss = 0.45251377\n",
      "Iteration 693, loss = 0.45197766\n",
      "Iteration 694, loss = 0.45144939\n",
      "Iteration 695, loss = 0.45091231\n",
      "Iteration 696, loss = 0.45037778\n",
      "Iteration 697, loss = 0.44984669\n",
      "Iteration 698, loss = 0.44931016\n",
      "Iteration 699, loss = 0.44878523\n",
      "Iteration 700, loss = 0.44824980\n",
      "Iteration 701, loss = 0.44771190\n",
      "Iteration 702, loss = 0.44718311\n",
      "Iteration 703, loss = 0.44664793\n",
      "Iteration 704, loss = 0.44611055\n",
      "Iteration 705, loss = 0.44557685\n",
      "Iteration 706, loss = 0.44504668\n",
      "Iteration 707, loss = 0.44451110\n",
      "Iteration 708, loss = 0.44397669\n",
      "Iteration 709, loss = 0.44345137\n",
      "Iteration 710, loss = 0.44292169\n",
      "Iteration 711, loss = 0.44239731\n",
      "Iteration 712, loss = 0.44186840\n",
      "Iteration 713, loss = 0.44134327\n",
      "Iteration 714, loss = 0.44082231\n",
      "Iteration 715, loss = 0.44029810\n",
      "Iteration 716, loss = 0.43977111\n",
      "Iteration 717, loss = 0.43924896\n",
      "Iteration 718, loss = 0.43872606\n",
      "Iteration 719, loss = 0.43821374\n",
      "Iteration 720, loss = 0.43768685\n",
      "Iteration 721, loss = 0.43716807\n",
      "Iteration 722, loss = 0.43664416\n",
      "Iteration 723, loss = 0.43612967\n",
      "Iteration 724, loss = 0.43561554\n",
      "Iteration 725, loss = 0.43509300\n",
      "Iteration 726, loss = 0.43457592\n",
      "Iteration 727, loss = 0.43406243\n",
      "Iteration 728, loss = 0.43354759\n",
      "Iteration 729, loss = 0.43303111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 730, loss = 0.43251811\n",
      "Iteration 731, loss = 0.43199909\n",
      "Iteration 732, loss = 0.43149053\n",
      "Iteration 733, loss = 0.43098069\n",
      "Iteration 734, loss = 0.43046134\n",
      "Iteration 735, loss = 0.42995035\n",
      "Iteration 736, loss = 0.42943681\n",
      "Iteration 737, loss = 0.42892821\n",
      "Iteration 738, loss = 0.42841653\n",
      "Iteration 739, loss = 0.42790211\n",
      "Iteration 740, loss = 0.42739035\n",
      "Iteration 741, loss = 0.42688819\n",
      "Iteration 742, loss = 0.42637806\n",
      "Iteration 743, loss = 0.42586912\n",
      "Iteration 744, loss = 0.42535774\n",
      "Iteration 745, loss = 0.42485173\n",
      "Iteration 746, loss = 0.42435350\n",
      "Iteration 747, loss = 0.42384314\n",
      "Iteration 748, loss = 0.42333592\n",
      "Iteration 749, loss = 0.42283144\n",
      "Iteration 750, loss = 0.42232812\n",
      "Iteration 751, loss = 0.42182467\n",
      "Iteration 752, loss = 0.42132795\n",
      "Iteration 753, loss = 0.42082034\n",
      "Iteration 754, loss = 0.42032025\n",
      "Iteration 755, loss = 0.41982439\n",
      "Iteration 756, loss = 0.41933063\n",
      "Iteration 757, loss = 0.41882643\n",
      "Iteration 758, loss = 0.41832692\n",
      "Iteration 759, loss = 0.41782620\n",
      "Iteration 760, loss = 0.41734239\n",
      "Iteration 761, loss = 0.41683885\n",
      "Iteration 762, loss = 0.41634244\n",
      "Iteration 763, loss = 0.41584367\n",
      "Iteration 764, loss = 0.41535530\n",
      "Iteration 765, loss = 0.41486574\n",
      "Iteration 766, loss = 0.41436583\n",
      "Iteration 767, loss = 0.41387616\n",
      "Iteration 768, loss = 0.41338229\n",
      "Iteration 769, loss = 0.41289825\n",
      "Iteration 770, loss = 0.41240675\n",
      "Iteration 771, loss = 0.41191671\n",
      "Iteration 772, loss = 0.41142749\n",
      "Iteration 773, loss = 0.41093805\n",
      "Iteration 774, loss = 0.41045349\n",
      "Iteration 775, loss = 0.40996786\n",
      "Iteration 776, loss = 0.40948532\n",
      "Iteration 777, loss = 0.40899295\n",
      "Iteration 778, loss = 0.40851006\n",
      "Iteration 779, loss = 0.40803350\n",
      "Iteration 780, loss = 0.40755362\n",
      "Iteration 781, loss = 0.40706753\n",
      "Iteration 782, loss = 0.40658299\n",
      "Iteration 783, loss = 0.40609962\n",
      "Iteration 784, loss = 0.40562404\n",
      "Iteration 785, loss = 0.40514941\n",
      "Iteration 786, loss = 0.40466920\n",
      "Iteration 787, loss = 0.40418823\n",
      "Iteration 788, loss = 0.40371280\n",
      "Iteration 789, loss = 0.40324079\n",
      "Iteration 790, loss = 0.40276694\n",
      "Iteration 791, loss = 0.40229309\n",
      "Iteration 792, loss = 0.40181250\n",
      "Iteration 793, loss = 0.40134192\n",
      "Iteration 794, loss = 0.40087166\n",
      "Iteration 795, loss = 0.40040223\n",
      "Iteration 796, loss = 0.39992713\n",
      "Iteration 797, loss = 0.39945583\n",
      "Iteration 798, loss = 0.39898695\n",
      "Iteration 799, loss = 0.39852193\n",
      "Iteration 800, loss = 0.39806336\n",
      "Iteration 801, loss = 0.39759070\n",
      "Iteration 802, loss = 0.39712059\n",
      "Iteration 803, loss = 0.39666298\n",
      "Iteration 804, loss = 0.39619150\n",
      "Iteration 805, loss = 0.39573049\n",
      "Iteration 806, loss = 0.39526898\n",
      "Iteration 807, loss = 0.39480927\n",
      "Iteration 808, loss = 0.39434119\n",
      "Iteration 809, loss = 0.39389516\n",
      "Iteration 810, loss = 0.39343354\n",
      "Iteration 811, loss = 0.39297287\n",
      "Iteration 812, loss = 0.39250791\n",
      "Iteration 813, loss = 0.39205779\n",
      "Iteration 814, loss = 0.39159902\n",
      "Iteration 815, loss = 0.39114205\n",
      "Iteration 816, loss = 0.39068943\n",
      "Iteration 817, loss = 0.39023284\n",
      "Iteration 818, loss = 0.38977909\n",
      "Iteration 819, loss = 0.38932907\n",
      "Iteration 820, loss = 0.38887696\n",
      "Iteration 821, loss = 0.38842331\n",
      "Iteration 822, loss = 0.38797327\n",
      "Iteration 823, loss = 0.38752325\n",
      "Iteration 824, loss = 0.38708184\n",
      "Iteration 825, loss = 0.38662825\n",
      "Iteration 826, loss = 0.38618374\n",
      "Iteration 827, loss = 0.38573806\n",
      "Iteration 828, loss = 0.38529333\n",
      "Iteration 829, loss = 0.38485141\n",
      "Iteration 830, loss = 0.38440608\n",
      "Iteration 831, loss = 0.38396462\n",
      "Iteration 832, loss = 0.38351694\n",
      "Iteration 833, loss = 0.38308302\n",
      "Iteration 834, loss = 0.38264895\n",
      "Iteration 835, loss = 0.38220591\n",
      "Iteration 836, loss = 0.38176492\n",
      "Iteration 837, loss = 0.38132414\n",
      "Iteration 838, loss = 0.38089636\n",
      "Iteration 839, loss = 0.38045717\n",
      "Iteration 840, loss = 0.38001171\n",
      "Iteration 841, loss = 0.37957555\n",
      "Iteration 842, loss = 0.37914113\n",
      "Iteration 843, loss = 0.37870172\n",
      "Iteration 844, loss = 0.37826542\n",
      "Iteration 845, loss = 0.37782781\n",
      "Iteration 846, loss = 0.37739486\n",
      "Iteration 847, loss = 0.37696834\n",
      "Iteration 848, loss = 0.37653478\n",
      "Iteration 849, loss = 0.37610090\n",
      "Iteration 850, loss = 0.37567356\n",
      "Iteration 851, loss = 0.37524845\n",
      "Iteration 852, loss = 0.37481574\n",
      "Iteration 853, loss = 0.37438662\n",
      "Iteration 854, loss = 0.37396769\n",
      "Iteration 855, loss = 0.37354264\n",
      "Iteration 856, loss = 0.37311824\n",
      "Iteration 857, loss = 0.37267424\n",
      "Iteration 858, loss = 0.37224666\n",
      "Iteration 859, loss = 0.37182267\n",
      "Iteration 860, loss = 0.37138692\n",
      "Iteration 861, loss = 0.37096377\n",
      "Iteration 862, loss = 0.37053700\n",
      "Iteration 863, loss = 0.37012087\n",
      "Iteration 864, loss = 0.36970342\n",
      "Iteration 865, loss = 0.36927688\n",
      "Iteration 866, loss = 0.36885852\n",
      "Iteration 867, loss = 0.36844114\n",
      "Iteration 868, loss = 0.36802946\n",
      "Iteration 869, loss = 0.36761744\n",
      "Iteration 870, loss = 0.36719742\n",
      "Iteration 871, loss = 0.36678406\n",
      "Iteration 872, loss = 0.36637673\n",
      "Iteration 873, loss = 0.36597537\n",
      "Iteration 874, loss = 0.36555155\n",
      "Iteration 875, loss = 0.36514086\n",
      "Iteration 876, loss = 0.36472302\n",
      "Iteration 877, loss = 0.36431042\n",
      "Iteration 878, loss = 0.36390844\n",
      "Iteration 879, loss = 0.36349909\n",
      "Iteration 880, loss = 0.36308544\n",
      "Iteration 881, loss = 0.36267477\n",
      "Iteration 882, loss = 0.36226695\n",
      "Iteration 883, loss = 0.36186362\n",
      "Iteration 884, loss = 0.36146584\n",
      "Iteration 885, loss = 0.36105189\n",
      "Iteration 886, loss = 0.36064625\n",
      "Iteration 887, loss = 0.36024660\n",
      "Iteration 888, loss = 0.35984894\n",
      "Iteration 889, loss = 0.35944130\n",
      "Iteration 890, loss = 0.35903567\n",
      "Iteration 891, loss = 0.35863583\n",
      "Iteration 892, loss = 0.35823228\n",
      "Iteration 893, loss = 0.35783917\n",
      "Iteration 894, loss = 0.35742888\n",
      "Iteration 895, loss = 0.35703688\n",
      "Iteration 896, loss = 0.35662788\n",
      "Iteration 897, loss = 0.35623422\n",
      "Iteration 898, loss = 0.35585223\n",
      "Iteration 899, loss = 0.35545069\n",
      "Iteration 900, loss = 0.35505773\n",
      "Iteration 901, loss = 0.35466009\n",
      "Iteration 902, loss = 0.35427157\n",
      "Iteration 903, loss = 0.35388843\n",
      "Iteration 904, loss = 0.35349547\n",
      "Iteration 905, loss = 0.35311177\n",
      "Iteration 906, loss = 0.35271336\n",
      "Iteration 907, loss = 0.35233808\n",
      "Iteration 908, loss = 0.35195622\n",
      "Iteration 909, loss = 0.35156636\n",
      "Iteration 910, loss = 0.35118126\n",
      "Iteration 911, loss = 0.35079342\n",
      "Iteration 912, loss = 0.35041659\n",
      "Iteration 913, loss = 0.35004060\n",
      "Iteration 914, loss = 0.34966609\n",
      "Iteration 915, loss = 0.34928828\n",
      "Iteration 916, loss = 0.34890294\n",
      "Iteration 917, loss = 0.34852677\n",
      "Iteration 918, loss = 0.34815772\n",
      "Iteration 919, loss = 0.34777995\n",
      "Iteration 920, loss = 0.34740142\n",
      "Iteration 921, loss = 0.34703313\n",
      "Iteration 922, loss = 0.34665792\n",
      "Iteration 923, loss = 0.34628703\n",
      "Iteration 924, loss = 0.34592650\n",
      "Iteration 925, loss = 0.34555455\n",
      "Iteration 926, loss = 0.34519137\n",
      "Iteration 927, loss = 0.34481161\n",
      "Iteration 928, loss = 0.34445142\n",
      "Iteration 929, loss = 0.34408914\n",
      "Iteration 930, loss = 0.34372466\n",
      "Iteration 931, loss = 0.34335876\n",
      "Iteration 932, loss = 0.34298737\n",
      "Iteration 933, loss = 0.34263074\n",
      "Iteration 934, loss = 0.34226320\n",
      "Iteration 935, loss = 0.34191113\n",
      "Iteration 936, loss = 0.34155536\n",
      "Iteration 937, loss = 0.34118847\n",
      "Iteration 938, loss = 0.34083460\n",
      "Iteration 939, loss = 0.34046724\n",
      "Iteration 940, loss = 0.34011525\n",
      "Iteration 941, loss = 0.33976288\n",
      "Iteration 942, loss = 0.33940839\n",
      "Iteration 943, loss = 0.33905327\n",
      "Iteration 944, loss = 0.33869821\n",
      "Iteration 945, loss = 0.33834404\n",
      "Iteration 946, loss = 0.33798271\n",
      "Iteration 947, loss = 0.33763475\n",
      "Iteration 948, loss = 0.33727877\n",
      "Iteration 949, loss = 0.33692014\n",
      "Iteration 950, loss = 0.33656306\n",
      "Iteration 951, loss = 0.33620837\n",
      "Iteration 952, loss = 0.33586171\n",
      "Iteration 953, loss = 0.33551918\n",
      "Iteration 954, loss = 0.33515743\n",
      "Iteration 955, loss = 0.33480744\n",
      "Iteration 956, loss = 0.33446786\n",
      "Iteration 957, loss = 0.33411039\n",
      "Iteration 958, loss = 0.33376448\n",
      "Iteration 959, loss = 0.33342441\n",
      "Iteration 960, loss = 0.33307895\n",
      "Iteration 961, loss = 0.33273220\n",
      "Iteration 962, loss = 0.33239021\n",
      "Iteration 963, loss = 0.33205362\n",
      "Iteration 964, loss = 0.33171277\n",
      "Iteration 965, loss = 0.33137514\n",
      "Iteration 966, loss = 0.33102374\n",
      "Iteration 967, loss = 0.33068928\n",
      "Iteration 968, loss = 0.33034544\n",
      "Iteration 969, loss = 0.33000082\n",
      "Iteration 970, loss = 0.32967245\n",
      "Iteration 971, loss = 0.32933929\n",
      "Iteration 972, loss = 0.32899851\n",
      "Iteration 973, loss = 0.32866937\n",
      "Iteration 974, loss = 0.32832162\n",
      "Iteration 975, loss = 0.32801229\n",
      "Iteration 976, loss = 0.32767150\n",
      "Iteration 977, loss = 0.32733172\n",
      "Iteration 978, loss = 0.32700285\n",
      "Iteration 979, loss = 0.32666791\n",
      "Iteration 980, loss = 0.32634637\n",
      "Iteration 981, loss = 0.32602813\n",
      "Iteration 982, loss = 0.32568643\n",
      "Iteration 983, loss = 0.32536233\n",
      "Iteration 984, loss = 0.32504193\n",
      "Iteration 985, loss = 0.32470785\n",
      "Iteration 986, loss = 0.32438845\n",
      "Iteration 987, loss = 0.32406069\n",
      "Iteration 988, loss = 0.32373896\n",
      "Iteration 989, loss = 0.32341623\n",
      "Iteration 990, loss = 0.32309452\n",
      "Iteration 991, loss = 0.32278510\n",
      "Iteration 992, loss = 0.32245664\n",
      "Iteration 993, loss = 0.32214337\n",
      "Iteration 994, loss = 0.32180890\n",
      "Iteration 995, loss = 0.32149844\n",
      "Iteration 996, loss = 0.32117796\n",
      "Iteration 997, loss = 0.32087293\n",
      "Iteration 998, loss = 0.32055552\n",
      "Iteration 999, loss = 0.32024473\n",
      "Iteration 1000, loss = 0.31992282\n",
      "Iteration 1001, loss = 0.31961299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1002, loss = 0.31929687\n",
      "Iteration 1003, loss = 0.31900778\n",
      "Iteration 1004, loss = 0.31867118\n",
      "Iteration 1005, loss = 0.31837659\n",
      "Iteration 1006, loss = 0.31805390\n",
      "Iteration 1007, loss = 0.31774217\n",
      "Iteration 1008, loss = 0.31744306\n",
      "Iteration 1009, loss = 0.31714398\n",
      "Iteration 1010, loss = 0.31682434\n",
      "Iteration 1011, loss = 0.31652534\n",
      "Iteration 1012, loss = 0.31621677\n",
      "Iteration 1013, loss = 0.31591221\n",
      "Iteration 1014, loss = 0.31560687\n",
      "Iteration 1015, loss = 0.31531233\n",
      "Iteration 1016, loss = 0.31500497\n",
      "Iteration 1017, loss = 0.31470149\n",
      "Iteration 1018, loss = 0.31440511\n",
      "Iteration 1019, loss = 0.31409324\n",
      "Iteration 1020, loss = 0.31382050\n",
      "Iteration 1021, loss = 0.31351312\n",
      "Iteration 1022, loss = 0.31321046\n",
      "Iteration 1023, loss = 0.31290637\n",
      "Iteration 1024, loss = 0.31260308\n",
      "Iteration 1025, loss = 0.31233341\n",
      "Iteration 1026, loss = 0.31203216\n",
      "Iteration 1027, loss = 0.31172837\n",
      "Iteration 1028, loss = 0.31144423\n",
      "Iteration 1029, loss = 0.31116594\n",
      "Iteration 1030, loss = 0.31089214\n",
      "Iteration 1031, loss = 0.31056531\n",
      "Iteration 1032, loss = 0.31028827\n",
      "Iteration 1033, loss = 0.30999527\n",
      "Iteration 1034, loss = 0.30973406\n",
      "Iteration 1035, loss = 0.30942071\n",
      "Iteration 1036, loss = 0.30913809\n",
      "Iteration 1037, loss = 0.30885697\n",
      "Iteration 1038, loss = 0.30858262\n",
      "Iteration 1039, loss = 0.30829849\n",
      "Iteration 1040, loss = 0.30799528\n",
      "Iteration 1041, loss = 0.30772806\n",
      "Iteration 1042, loss = 0.30745665\n",
      "Iteration 1043, loss = 0.30716332\n",
      "Iteration 1044, loss = 0.30687281\n",
      "Iteration 1045, loss = 0.30660616\n",
      "Iteration 1046, loss = 0.30632082\n",
      "Iteration 1047, loss = 0.30604131\n",
      "Iteration 1048, loss = 0.30578650\n",
      "Iteration 1049, loss = 0.30549486\n",
      "Iteration 1050, loss = 0.30521030\n",
      "Iteration 1051, loss = 0.30495263\n",
      "Iteration 1052, loss = 0.30468373\n",
      "Iteration 1053, loss = 0.30439929\n",
      "Iteration 1054, loss = 0.30411166\n",
      "Iteration 1055, loss = 0.30384665\n",
      "Iteration 1056, loss = 0.30356425\n",
      "Iteration 1057, loss = 0.30329971\n",
      "Iteration 1058, loss = 0.30305044\n",
      "Iteration 1059, loss = 0.30276841\n",
      "Iteration 1060, loss = 0.30247794\n",
      "Iteration 1061, loss = 0.30223417\n",
      "Iteration 1062, loss = 0.30194148\n",
      "Iteration 1063, loss = 0.30167195\n",
      "Iteration 1064, loss = 0.30140977\n",
      "Iteration 1065, loss = 0.30116482\n",
      "Iteration 1066, loss = 0.30088251\n",
      "Iteration 1067, loss = 0.30062149\n",
      "Iteration 1068, loss = 0.30036338\n",
      "Iteration 1069, loss = 0.30008035\n",
      "Iteration 1070, loss = 0.29982983\n",
      "Iteration 1071, loss = 0.29958377\n",
      "Iteration 1072, loss = 0.29929900\n",
      "Iteration 1073, loss = 0.29902936\n",
      "Iteration 1074, loss = 0.29880989\n",
      "Iteration 1075, loss = 0.29851565\n",
      "Iteration 1076, loss = 0.29826328\n",
      "Iteration 1077, loss = 0.29799268\n",
      "Iteration 1078, loss = 0.29773394\n",
      "Iteration 1079, loss = 0.29747717\n",
      "Iteration 1080, loss = 0.29722680\n",
      "Iteration 1081, loss = 0.29696109\n",
      "Iteration 1082, loss = 0.29672650\n",
      "Iteration 1083, loss = 0.29644691\n",
      "Iteration 1084, loss = 0.29621796\n",
      "Iteration 1085, loss = 0.29593741\n",
      "Iteration 1086, loss = 0.29570683\n",
      "Iteration 1087, loss = 0.29544801\n",
      "Iteration 1088, loss = 0.29518237\n",
      "Iteration 1089, loss = 0.29494530\n",
      "Iteration 1090, loss = 0.29469297\n",
      "Iteration 1091, loss = 0.29441863\n",
      "Iteration 1092, loss = 0.29419293\n",
      "Iteration 1093, loss = 0.29393882\n",
      "Iteration 1094, loss = 0.29368760\n",
      "Iteration 1095, loss = 0.29344795\n",
      "Iteration 1096, loss = 0.29320744\n",
      "Iteration 1097, loss = 0.29293321\n",
      "Iteration 1098, loss = 0.29270702\n",
      "Iteration 1099, loss = 0.29245649\n",
      "Iteration 1100, loss = 0.29219860\n",
      "Iteration 1101, loss = 0.29196468\n",
      "Iteration 1102, loss = 0.29173182\n",
      "Iteration 1103, loss = 0.29147826\n",
      "Iteration 1104, loss = 0.29123600\n",
      "Iteration 1105, loss = 0.29099095\n",
      "Iteration 1106, loss = 0.29075779\n",
      "Iteration 1107, loss = 0.29050341\n",
      "Iteration 1108, loss = 0.29026343\n",
      "Iteration 1109, loss = 0.29002800\n",
      "Iteration 1110, loss = 0.28979436\n",
      "Iteration 1111, loss = 0.28956252\n",
      "Iteration 1112, loss = 0.28932156\n",
      "Iteration 1113, loss = 0.28907263\n",
      "Iteration 1114, loss = 0.28885083\n",
      "Iteration 1115, loss = 0.28860191\n",
      "Iteration 1116, loss = 0.28836180\n",
      "Iteration 1117, loss = 0.28813663\n",
      "Iteration 1118, loss = 0.28791552\n",
      "Iteration 1119, loss = 0.28766792\n",
      "Iteration 1120, loss = 0.28744377\n",
      "Iteration 1121, loss = 0.28720739\n",
      "Iteration 1122, loss = 0.28696252\n",
      "Iteration 1123, loss = 0.28674486\n",
      "Iteration 1124, loss = 0.28652810\n",
      "Iteration 1125, loss = 0.28626624\n",
      "Iteration 1126, loss = 0.28605738\n",
      "Iteration 1127, loss = 0.28581609\n",
      "Iteration 1128, loss = 0.28559813\n",
      "Iteration 1129, loss = 0.28537344\n",
      "Iteration 1130, loss = 0.28513915\n",
      "Iteration 1131, loss = 0.28490112\n",
      "Iteration 1132, loss = 0.28468319\n",
      "Iteration 1133, loss = 0.28447146\n",
      "Iteration 1134, loss = 0.28423714\n",
      "Iteration 1135, loss = 0.28401809\n",
      "Iteration 1136, loss = 0.28376678\n",
      "Iteration 1137, loss = 0.28356580\n",
      "Iteration 1138, loss = 0.28333670\n",
      "Iteration 1139, loss = 0.28310943\n",
      "Iteration 1140, loss = 0.28290569\n",
      "Iteration 1141, loss = 0.28266918\n",
      "Iteration 1142, loss = 0.28245458\n",
      "Iteration 1143, loss = 0.28222242\n",
      "Iteration 1144, loss = 0.28202620\n",
      "Iteration 1145, loss = 0.28179236\n",
      "Iteration 1146, loss = 0.28157029\n",
      "Iteration 1147, loss = 0.28135721\n",
      "Iteration 1148, loss = 0.28112159\n",
      "Iteration 1149, loss = 0.28094230\n",
      "Iteration 1150, loss = 0.28070465\n",
      "Iteration 1151, loss = 0.28047805\n",
      "Iteration 1152, loss = 0.28027918\n",
      "Iteration 1153, loss = 0.28004875\n",
      "Iteration 1154, loss = 0.27985032\n",
      "Iteration 1155, loss = 0.27964210\n",
      "Iteration 1156, loss = 0.27942846\n",
      "Iteration 1157, loss = 0.27919446\n",
      "Iteration 1158, loss = 0.27899131\n",
      "Iteration 1159, loss = 0.27877354\n",
      "Iteration 1160, loss = 0.27856667\n",
      "Iteration 1161, loss = 0.27835804\n",
      "Iteration 1162, loss = 0.27813943\n",
      "Iteration 1163, loss = 0.27792907\n",
      "Iteration 1164, loss = 0.27771447\n",
      "Iteration 1165, loss = 0.27752429\n",
      "Iteration 1166, loss = 0.27729372\n",
      "Iteration 1167, loss = 0.27710723\n",
      "Iteration 1168, loss = 0.27689568\n",
      "Iteration 1169, loss = 0.27667010\n",
      "Iteration 1170, loss = 0.27647806\n",
      "Iteration 1171, loss = 0.27626030\n",
      "Iteration 1172, loss = 0.27606428\n",
      "Iteration 1173, loss = 0.27587202\n",
      "Iteration 1174, loss = 0.27566662\n",
      "Iteration 1175, loss = 0.27545230\n",
      "Iteration 1176, loss = 0.27523368\n",
      "Iteration 1177, loss = 0.27504573\n",
      "Iteration 1178, loss = 0.27482883\n",
      "Iteration 1179, loss = 0.27462224\n",
      "Iteration 1180, loss = 0.27444537\n",
      "Iteration 1181, loss = 0.27421338\n",
      "Iteration 1182, loss = 0.27403005\n",
      "Iteration 1183, loss = 0.27383361\n",
      "Iteration 1184, loss = 0.27361680\n",
      "Iteration 1185, loss = 0.27343455\n",
      "Iteration 1186, loss = 0.27322103\n",
      "Iteration 1187, loss = 0.27302861\n",
      "Iteration 1188, loss = 0.27283220\n",
      "Iteration 1189, loss = 0.27263785\n",
      "Iteration 1190, loss = 0.27242762\n",
      "Iteration 1191, loss = 0.27222807\n",
      "Iteration 1192, loss = 0.27205879\n",
      "Iteration 1193, loss = 0.27183012\n",
      "Iteration 1194, loss = 0.27163860\n",
      "Iteration 1195, loss = 0.27146831\n",
      "Iteration 1196, loss = 0.27123943\n",
      "Iteration 1197, loss = 0.27105780\n",
      "Iteration 1198, loss = 0.27086593\n",
      "Iteration 1199, loss = 0.27067734\n",
      "Iteration 1200, loss = 0.27048503\n",
      "Iteration 1201, loss = 0.27028200\n",
      "Iteration 1202, loss = 0.27009807\n",
      "Iteration 1203, loss = 0.26989985\n",
      "Iteration 1204, loss = 0.26970545\n",
      "Iteration 1205, loss = 0.26951222\n",
      "Iteration 1206, loss = 0.26933481\n",
      "Iteration 1207, loss = 0.26912595\n",
      "Iteration 1208, loss = 0.26894957\n",
      "Iteration 1209, loss = 0.26876089\n",
      "Iteration 1210, loss = 0.26854907\n",
      "Iteration 1211, loss = 0.26838272\n",
      "Iteration 1212, loss = 0.26819347\n",
      "Iteration 1213, loss = 0.26798898\n",
      "Iteration 1214, loss = 0.26781183\n",
      "Iteration 1215, loss = 0.26764013\n",
      "Iteration 1216, loss = 0.26742411\n",
      "Iteration 1217, loss = 0.26725007\n",
      "Iteration 1218, loss = 0.26707874\n",
      "Iteration 1219, loss = 0.26687259\n",
      "Iteration 1220, loss = 0.26668627\n",
      "Iteration 1221, loss = 0.26653221\n",
      "Iteration 1222, loss = 0.26631019\n",
      "Iteration 1223, loss = 0.26614172\n",
      "Iteration 1224, loss = 0.26597325\n",
      "Iteration 1225, loss = 0.26577322\n",
      "Iteration 1226, loss = 0.26558248\n",
      "Iteration 1227, loss = 0.26542991\n",
      "Iteration 1228, loss = 0.26523142\n",
      "Iteration 1229, loss = 0.26503327\n",
      "Iteration 1230, loss = 0.26487825\n",
      "Iteration 1231, loss = 0.26468931\n",
      "Iteration 1232, loss = 0.26450335\n",
      "Iteration 1233, loss = 0.26432809\n",
      "Iteration 1234, loss = 0.26414416\n",
      "Iteration 1235, loss = 0.26398734\n",
      "Iteration 1236, loss = 0.26379693\n",
      "Iteration 1237, loss = 0.26359324\n",
      "Iteration 1238, loss = 0.26344465\n",
      "Iteration 1239, loss = 0.26325545\n",
      "Iteration 1240, loss = 0.26307198\n",
      "Iteration 1241, loss = 0.26290169\n",
      "Iteration 1242, loss = 0.26275450\n",
      "Iteration 1243, loss = 0.26253777\n",
      "Iteration 1244, loss = 0.26237545\n",
      "Iteration 1245, loss = 0.26221191\n",
      "Iteration 1246, loss = 0.26202422\n",
      "Iteration 1247, loss = 0.26184731\n",
      "Iteration 1248, loss = 0.26166127\n",
      "Iteration 1249, loss = 0.26151090\n",
      "Iteration 1250, loss = 0.26132430\n",
      "Iteration 1251, loss = 0.26114807\n",
      "Iteration 1252, loss = 0.26097871\n",
      "Iteration 1253, loss = 0.26079800\n",
      "Iteration 1254, loss = 0.26065732\n",
      "Iteration 1255, loss = 0.26047961\n",
      "Iteration 1256, loss = 0.26028650\n",
      "Iteration 1257, loss = 0.26012116\n",
      "Iteration 1258, loss = 0.25996964\n",
      "Iteration 1259, loss = 0.25976599\n",
      "Iteration 1260, loss = 0.25959482\n",
      "Iteration 1261, loss = 0.25947867\n",
      "Iteration 1262, loss = 0.25927422\n",
      "Iteration 1263, loss = 0.25909419\n",
      "Iteration 1264, loss = 0.25894048\n",
      "Iteration 1265, loss = 0.25878302\n",
      "Iteration 1266, loss = 0.25862271\n",
      "Iteration 1267, loss = 0.25841980\n",
      "Iteration 1268, loss = 0.25828429\n",
      "Iteration 1269, loss = 0.25809702\n",
      "Iteration 1270, loss = 0.25794672\n",
      "Iteration 1271, loss = 0.25778820\n",
      "Iteration 1272, loss = 0.25760899\n",
      "Iteration 1273, loss = 0.25745517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1274, loss = 0.25728836\n",
      "Iteration 1275, loss = 0.25710511\n",
      "Iteration 1276, loss = 0.25694881\n",
      "Iteration 1277, loss = 0.25681896\n",
      "Iteration 1278, loss = 0.25662742\n",
      "Iteration 1279, loss = 0.25646040\n",
      "Iteration 1280, loss = 0.25630890\n",
      "Iteration 1281, loss = 0.25614787\n",
      "Iteration 1282, loss = 0.25597744\n",
      "Iteration 1283, loss = 0.25582725\n",
      "Iteration 1284, loss = 0.25567880\n",
      "Iteration 1285, loss = 0.25550795\n",
      "Iteration 1286, loss = 0.25533863\n",
      "Iteration 1287, loss = 0.25517894\n",
      "Iteration 1288, loss = 0.25502609\n",
      "Iteration 1289, loss = 0.25486932\n",
      "Iteration 1290, loss = 0.25471857\n",
      "Iteration 1291, loss = 0.25453676\n",
      "Iteration 1292, loss = 0.25440230\n",
      "Iteration 1293, loss = 0.25421898\n",
      "Iteration 1294, loss = 0.25408015\n",
      "Iteration 1295, loss = 0.25392376\n",
      "Iteration 1296, loss = 0.25374875\n",
      "Iteration 1297, loss = 0.25362162\n",
      "Iteration 1298, loss = 0.25345922\n",
      "Iteration 1299, loss = 0.25329900\n",
      "Iteration 1300, loss = 0.25314205\n",
      "Iteration 1301, loss = 0.25296387\n",
      "Iteration 1302, loss = 0.25283446\n",
      "Iteration 1303, loss = 0.25271037\n",
      "Iteration 1304, loss = 0.25252722\n",
      "Iteration 1305, loss = 0.25235173\n",
      "Iteration 1306, loss = 0.25221801\n",
      "Iteration 1307, loss = 0.25207587\n",
      "Iteration 1308, loss = 0.25191723\n",
      "Iteration 1309, loss = 0.25175295\n",
      "Iteration 1310, loss = 0.25160896\n",
      "Iteration 1311, loss = 0.25145110\n",
      "Iteration 1312, loss = 0.25129332\n",
      "Iteration 1313, loss = 0.25111192\n",
      "Iteration 1314, loss = 0.25095693\n",
      "Iteration 1315, loss = 0.25079633\n",
      "Iteration 1316, loss = 0.25062112\n",
      "Iteration 1317, loss = 0.25044859\n",
      "Iteration 1318, loss = 0.25029758\n",
      "Iteration 1319, loss = 0.25013834\n",
      "Iteration 1320, loss = 0.24996789\n",
      "Iteration 1321, loss = 0.24978609\n",
      "Iteration 1322, loss = 0.24962216\n",
      "Iteration 1323, loss = 0.24947237\n",
      "Iteration 1324, loss = 0.24927875\n",
      "Iteration 1325, loss = 0.24913720\n",
      "Iteration 1326, loss = 0.24897039\n",
      "Iteration 1327, loss = 0.24881104\n",
      "Iteration 1328, loss = 0.24866179\n",
      "Iteration 1329, loss = 0.24848422\n",
      "Iteration 1330, loss = 0.24832780\n",
      "Iteration 1331, loss = 0.24816990\n",
      "Iteration 1332, loss = 0.24800139\n",
      "Iteration 1333, loss = 0.24784795\n",
      "Iteration 1334, loss = 0.24768100\n",
      "Iteration 1335, loss = 0.24754158\n",
      "Iteration 1336, loss = 0.24737814\n",
      "Iteration 1337, loss = 0.24721959\n",
      "Iteration 1338, loss = 0.24705282\n",
      "Iteration 1339, loss = 0.24691217\n",
      "Iteration 1340, loss = 0.24675534\n",
      "Iteration 1341, loss = 0.24658682\n",
      "Iteration 1342, loss = 0.24645116\n",
      "Iteration 1343, loss = 0.24626655\n",
      "Iteration 1344, loss = 0.24614269\n",
      "Iteration 1345, loss = 0.24597760\n",
      "Iteration 1346, loss = 0.24582648\n",
      "Iteration 1347, loss = 0.24566486\n",
      "Iteration 1348, loss = 0.24552154\n",
      "Iteration 1349, loss = 0.24537291\n",
      "Iteration 1350, loss = 0.24520991\n",
      "Iteration 1351, loss = 0.24505143\n",
      "Iteration 1352, loss = 0.24491811\n",
      "Iteration 1353, loss = 0.24476798\n",
      "Iteration 1354, loss = 0.24462788\n",
      "Iteration 1355, loss = 0.24444147\n",
      "Iteration 1356, loss = 0.24432013\n",
      "Iteration 1357, loss = 0.24416282\n",
      "Iteration 1358, loss = 0.24399818\n",
      "Iteration 1359, loss = 0.24386411\n",
      "Iteration 1360, loss = 0.24368578\n",
      "Iteration 1361, loss = 0.24357221\n",
      "Iteration 1362, loss = 0.24338989\n",
      "Iteration 1363, loss = 0.24326614\n",
      "Iteration 1364, loss = 0.24309341\n",
      "Iteration 1365, loss = 0.24296578\n",
      "Iteration 1366, loss = 0.24281324\n",
      "Iteration 1367, loss = 0.24264811\n",
      "Iteration 1368, loss = 0.24252170\n",
      "Iteration 1369, loss = 0.24237213\n",
      "Iteration 1370, loss = 0.24222219\n",
      "Iteration 1371, loss = 0.24206516\n",
      "Iteration 1372, loss = 0.24193365\n",
      "Iteration 1373, loss = 0.24179827\n",
      "Iteration 1374, loss = 0.24162863\n",
      "Iteration 1375, loss = 0.24149585\n",
      "Iteration 1376, loss = 0.24134962\n",
      "Iteration 1377, loss = 0.24121256\n",
      "Iteration 1378, loss = 0.24104711\n",
      "Iteration 1379, loss = 0.24091380\n",
      "Iteration 1380, loss = 0.24076728\n",
      "Iteration 1381, loss = 0.24061318\n",
      "Iteration 1382, loss = 0.24048689\n",
      "Iteration 1383, loss = 0.24032301\n",
      "Iteration 1384, loss = 0.24019274\n",
      "Iteration 1385, loss = 0.24004704\n",
      "Iteration 1386, loss = 0.23991609\n",
      "Iteration 1387, loss = 0.23976657\n",
      "Iteration 1388, loss = 0.23963942\n",
      "Iteration 1389, loss = 0.23948328\n",
      "Iteration 1390, loss = 0.23933975\n",
      "Iteration 1391, loss = 0.23918371\n",
      "Iteration 1392, loss = 0.23906665\n",
      "Iteration 1393, loss = 0.23891402\n",
      "Iteration 1394, loss = 0.23875275\n",
      "Iteration 1395, loss = 0.23863796\n",
      "Iteration 1396, loss = 0.23849166\n",
      "Iteration 1397, loss = 0.23835887\n",
      "Iteration 1398, loss = 0.23819694\n",
      "Iteration 1399, loss = 0.23809447\n",
      "Iteration 1400, loss = 0.23792155\n",
      "Iteration 1401, loss = 0.23779315\n",
      "Iteration 1402, loss = 0.23765595\n",
      "Iteration 1403, loss = 0.23750640\n",
      "Iteration 1404, loss = 0.23739991\n",
      "Iteration 1405, loss = 0.23722218\n",
      "Iteration 1406, loss = 0.23708792\n",
      "Iteration 1407, loss = 0.23697779\n",
      "Iteration 1408, loss = 0.23681701\n",
      "Iteration 1409, loss = 0.23668882\n",
      "Iteration 1410, loss = 0.23654319\n",
      "Iteration 1411, loss = 0.23644358\n",
      "Iteration 1412, loss = 0.23627285\n",
      "Iteration 1413, loss = 0.23613603\n",
      "Iteration 1414, loss = 0.23602591\n",
      "Iteration 1415, loss = 0.23586491\n",
      "Iteration 1416, loss = 0.23572778\n",
      "Iteration 1417, loss = 0.23558689\n",
      "Iteration 1418, loss = 0.23545366\n",
      "Iteration 1419, loss = 0.23530746\n",
      "Iteration 1420, loss = 0.23519557\n",
      "Iteration 1421, loss = 0.23502936\n",
      "Iteration 1422, loss = 0.23494331\n",
      "Iteration 1423, loss = 0.23476552\n",
      "Iteration 1424, loss = 0.23464957\n",
      "Iteration 1425, loss = 0.23453745\n",
      "Iteration 1426, loss = 0.23436960\n",
      "Iteration 1427, loss = 0.23424509\n",
      "Iteration 1428, loss = 0.23412881\n",
      "Iteration 1429, loss = 0.23397188\n",
      "Iteration 1430, loss = 0.23385799\n",
      "Iteration 1431, loss = 0.23371489\n",
      "Iteration 1432, loss = 0.23359332\n",
      "Iteration 1433, loss = 0.23345399\n",
      "Iteration 1434, loss = 0.23330393\n",
      "Iteration 1435, loss = 0.23322618\n",
      "Iteration 1436, loss = 0.23304930\n",
      "Iteration 1437, loss = 0.23293828\n",
      "Iteration 1438, loss = 0.23282452\n",
      "Iteration 1439, loss = 0.23269091\n",
      "Iteration 1440, loss = 0.23255990\n",
      "Iteration 1441, loss = 0.23242263\n",
      "Iteration 1442, loss = 0.23231379\n",
      "Iteration 1443, loss = 0.23219424\n",
      "Iteration 1444, loss = 0.23204919\n",
      "Iteration 1445, loss = 0.23193281\n",
      "Iteration 1446, loss = 0.23182060\n",
      "Iteration 1447, loss = 0.23169217\n",
      "Iteration 1448, loss = 0.23155120\n",
      "Iteration 1449, loss = 0.23145164\n",
      "Iteration 1450, loss = 0.23133214\n",
      "Iteration 1451, loss = 0.23122643\n",
      "Iteration 1452, loss = 0.23106968\n",
      "Iteration 1453, loss = 0.23099153\n",
      "Iteration 1454, loss = 0.23084033\n",
      "Iteration 1455, loss = 0.23075269\n",
      "Iteration 1456, loss = 0.23062077\n",
      "Iteration 1457, loss = 0.23053077\n",
      "Iteration 1458, loss = 0.23036991\n",
      "Iteration 1459, loss = 0.23025755\n",
      "Iteration 1460, loss = 0.23018258\n",
      "Iteration 1461, loss = 0.23005234\n",
      "Iteration 1462, loss = 0.22990613\n",
      "Iteration 1463, loss = 0.22987260\n",
      "Iteration 1464, loss = 0.22969900\n",
      "Iteration 1465, loss = 0.22957106\n",
      "Iteration 1466, loss = 0.22951299\n",
      "Iteration 1467, loss = 0.22935255\n",
      "Iteration 1468, loss = 0.22925671\n",
      "Iteration 1469, loss = 0.22917655\n",
      "Iteration 1470, loss = 0.22899906\n",
      "Iteration 1471, loss = 0.22890888\n",
      "Iteration 1472, loss = 0.22880438\n",
      "Iteration 1473, loss = 0.22872901\n",
      "Iteration 1474, loss = 0.22857228\n",
      "Iteration 1475, loss = 0.22845000\n",
      "Iteration 1476, loss = 0.22838757\n",
      "Iteration 1477, loss = 0.22824228\n",
      "Iteration 1478, loss = 0.22814988\n",
      "Iteration 1479, loss = 0.22806352\n",
      "Iteration 1480, loss = 0.22791274\n",
      "Iteration 1481, loss = 0.22779224\n",
      "Iteration 1482, loss = 0.22770644\n",
      "Iteration 1483, loss = 0.22759022\n",
      "Iteration 1484, loss = 0.22750084\n",
      "Iteration 1485, loss = 0.22739224\n",
      "Iteration 1486, loss = 0.22727922\n",
      "Iteration 1487, loss = 0.22718911\n",
      "Iteration 1488, loss = 0.22706788\n",
      "Iteration 1489, loss = 0.22703005\n",
      "Iteration 1490, loss = 0.22685246\n",
      "Iteration 1491, loss = 0.22674666\n",
      "Iteration 1492, loss = 0.22662273\n",
      "Iteration 1493, loss = 0.22653381\n",
      "Iteration 1494, loss = 0.22642008\n",
      "Iteration 1495, loss = 0.22633841\n",
      "Iteration 1496, loss = 0.22621575\n",
      "Iteration 1497, loss = 0.22607840\n",
      "Iteration 1498, loss = 0.22599076\n",
      "Iteration 1499, loss = 0.22588928\n",
      "Iteration 1500, loss = 0.22578493\n",
      "Iteration 1501, loss = 0.22568924\n",
      "Iteration 1502, loss = 0.22557966\n",
      "Iteration 1503, loss = 0.22546926\n",
      "Iteration 1504, loss = 0.22539289\n",
      "Iteration 1505, loss = 0.22523246\n",
      "Iteration 1506, loss = 0.22517232\n",
      "Iteration 1507, loss = 0.22506632\n",
      "Iteration 1508, loss = 0.22494943\n",
      "Iteration 1509, loss = 0.22483308\n",
      "Iteration 1510, loss = 0.22479707\n",
      "Iteration 1511, loss = 0.22463909\n",
      "Iteration 1512, loss = 0.22453401\n",
      "Iteration 1513, loss = 0.22446633\n",
      "Iteration 1514, loss = 0.22432417\n",
      "Iteration 1515, loss = 0.22425986\n",
      "Iteration 1516, loss = 0.22415940\n",
      "Iteration 1517, loss = 0.22403728\n",
      "Iteration 1518, loss = 0.22393196\n",
      "Iteration 1519, loss = 0.22386017\n",
      "Iteration 1520, loss = 0.22372844\n",
      "Iteration 1521, loss = 0.22365078\n",
      "Iteration 1522, loss = 0.22359371\n",
      "Iteration 1523, loss = 0.22343965\n",
      "Iteration 1524, loss = 0.22336677\n",
      "Iteration 1525, loss = 0.22323743\n",
      "Iteration 1526, loss = 0.22314281\n",
      "Iteration 1527, loss = 0.22305027\n",
      "Iteration 1528, loss = 0.22299178\n",
      "Iteration 1529, loss = 0.22286147\n",
      "Iteration 1530, loss = 0.22274741\n",
      "Iteration 1531, loss = 0.22267822\n",
      "Iteration 1532, loss = 0.22255609\n",
      "Iteration 1533, loss = 0.22248013\n",
      "Iteration 1534, loss = 0.22241093\n",
      "Iteration 1535, loss = 0.22227473\n",
      "Iteration 1536, loss = 0.22220224\n",
      "Iteration 1537, loss = 0.22207055\n",
      "Iteration 1538, loss = 0.22197320\n",
      "Iteration 1539, loss = 0.22187745\n",
      "Iteration 1540, loss = 0.22178167\n",
      "Iteration 1541, loss = 0.22168667\n",
      "Iteration 1542, loss = 0.22164105\n",
      "Iteration 1543, loss = 0.22148141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1544, loss = 0.22143558\n",
      "Iteration 1545, loss = 0.22133424\n",
      "Iteration 1546, loss = 0.22120388\n",
      "Iteration 1547, loss = 0.22117381\n",
      "Iteration 1548, loss = 0.22103168\n",
      "Iteration 1549, loss = 0.22093899\n",
      "Iteration 1550, loss = 0.22083969\n",
      "Iteration 1551, loss = 0.22078408\n",
      "Iteration 1552, loss = 0.22064942\n",
      "Iteration 1553, loss = 0.22058595\n",
      "Iteration 1554, loss = 0.22044964\n",
      "Iteration 1555, loss = 0.22039376\n",
      "Iteration 1556, loss = 0.22030241\n",
      "Iteration 1557, loss = 0.22017601\n",
      "Iteration 1558, loss = 0.22011381\n",
      "Iteration 1559, loss = 0.22000803\n",
      "Iteration 1560, loss = 0.21993338\n",
      "Iteration 1561, loss = 0.21983684\n",
      "Iteration 1562, loss = 0.21969838\n",
      "Iteration 1563, loss = 0.21964167\n",
      "Iteration 1564, loss = 0.21951202\n",
      "Iteration 1565, loss = 0.21944352\n",
      "Iteration 1566, loss = 0.21931081\n",
      "Iteration 1567, loss = 0.21922842\n",
      "Iteration 1568, loss = 0.21911250\n",
      "Iteration 1569, loss = 0.21904071\n",
      "Iteration 1570, loss = 0.21893248\n",
      "Iteration 1571, loss = 0.21883748\n",
      "Iteration 1572, loss = 0.21875549\n",
      "Iteration 1573, loss = 0.21864970\n",
      "Iteration 1574, loss = 0.21854209\n",
      "Iteration 1575, loss = 0.21847028\n",
      "Iteration 1576, loss = 0.21835812\n",
      "Iteration 1577, loss = 0.21826688\n",
      "Iteration 1578, loss = 0.21818483\n",
      "Iteration 1579, loss = 0.21808721\n",
      "Iteration 1580, loss = 0.21800590\n",
      "Iteration 1581, loss = 0.21791861\n",
      "Iteration 1582, loss = 0.21780670\n",
      "Iteration 1583, loss = 0.21771608\n",
      "Iteration 1584, loss = 0.21763824\n",
      "Iteration 1585, loss = 0.21754161\n",
      "Iteration 1586, loss = 0.21747506\n",
      "Iteration 1587, loss = 0.21735312\n",
      "Iteration 1588, loss = 0.21727472\n",
      "Iteration 1589, loss = 0.21717764\n",
      "Iteration 1590, loss = 0.21708520\n",
      "Iteration 1591, loss = 0.21699946\n",
      "Iteration 1592, loss = 0.21691263\n",
      "Iteration 1593, loss = 0.21682041\n",
      "Iteration 1594, loss = 0.21674574\n",
      "Iteration 1595, loss = 0.21665372\n",
      "Iteration 1596, loss = 0.21655530\n",
      "Iteration 1597, loss = 0.21649702\n",
      "Iteration 1598, loss = 0.21639657\n",
      "Iteration 1599, loss = 0.21633147\n",
      "Iteration 1600, loss = 0.21622248\n",
      "Iteration 1601, loss = 0.21614807\n",
      "Iteration 1602, loss = 0.21600201\n",
      "Iteration 1603, loss = 0.21595515\n",
      "Iteration 1604, loss = 0.21585536\n",
      "Iteration 1605, loss = 0.21583409\n",
      "Iteration 1606, loss = 0.21569413\n",
      "Iteration 1607, loss = 0.21562477\n",
      "Iteration 1608, loss = 0.21548817\n",
      "Iteration 1609, loss = 0.21545896\n",
      "Iteration 1610, loss = 0.21535891\n",
      "Iteration 1611, loss = 0.21527200\n",
      "Iteration 1612, loss = 0.21520968\n",
      "Iteration 1613, loss = 0.21513271\n",
      "Iteration 1614, loss = 0.21499192\n",
      "Iteration 1615, loss = 0.21493225\n",
      "Iteration 1616, loss = 0.21482515\n",
      "Iteration 1617, loss = 0.21475104\n",
      "Iteration 1618, loss = 0.21464806\n",
      "Iteration 1619, loss = 0.21458518\n",
      "Iteration 1620, loss = 0.21448830\n",
      "Iteration 1621, loss = 0.21441272\n",
      "Iteration 1622, loss = 0.21433572\n",
      "Iteration 1623, loss = 0.21427826\n",
      "Iteration 1624, loss = 0.21422104\n",
      "Iteration 1625, loss = 0.21410537\n",
      "Iteration 1626, loss = 0.21402084\n",
      "Iteration 1627, loss = 0.21390799\n",
      "Iteration 1628, loss = 0.21384175\n",
      "Iteration 1629, loss = 0.21377537\n",
      "Iteration 1630, loss = 0.21370434\n",
      "Iteration 1631, loss = 0.21359195\n",
      "Iteration 1632, loss = 0.21352835\n",
      "Iteration 1633, loss = 0.21341716\n",
      "Iteration 1634, loss = 0.21336128\n",
      "Iteration 1635, loss = 0.21326747\n",
      "Iteration 1636, loss = 0.21320244\n",
      "Iteration 1637, loss = 0.21311032\n",
      "Iteration 1638, loss = 0.21303375\n",
      "Iteration 1639, loss = 0.21295499\n",
      "Iteration 1640, loss = 0.21286518\n",
      "Iteration 1641, loss = 0.21280720\n",
      "Iteration 1642, loss = 0.21268335\n",
      "Iteration 1643, loss = 0.21265108\n",
      "Iteration 1644, loss = 0.21258632\n",
      "Iteration 1645, loss = 0.21245139\n",
      "Iteration 1646, loss = 0.21237209\n",
      "Iteration 1647, loss = 0.21227101\n",
      "Iteration 1648, loss = 0.21220099\n",
      "Iteration 1649, loss = 0.21213158\n",
      "Iteration 1650, loss = 0.21206773\n",
      "Iteration 1651, loss = 0.21196034\n",
      "Iteration 1652, loss = 0.21190788\n",
      "Iteration 1653, loss = 0.21180749\n",
      "Iteration 1654, loss = 0.21173831\n",
      "Iteration 1655, loss = 0.21166154\n",
      "Iteration 1656, loss = 0.21156922\n",
      "Iteration 1657, loss = 0.21152424\n",
      "Iteration 1658, loss = 0.21142667\n",
      "Iteration 1659, loss = 0.21136877\n",
      "Iteration 1660, loss = 0.21123979\n",
      "Iteration 1661, loss = 0.21119260\n",
      "Iteration 1662, loss = 0.21108804\n",
      "Iteration 1663, loss = 0.21105473\n",
      "Iteration 1664, loss = 0.21093517\n",
      "Iteration 1665, loss = 0.21091376\n",
      "Iteration 1666, loss = 0.21082868\n",
      "Iteration 1667, loss = 0.21074190\n",
      "Iteration 1668, loss = 0.21067443\n",
      "Iteration 1669, loss = 0.21058154\n",
      "Iteration 1670, loss = 0.21051268\n",
      "Iteration 1671, loss = 0.21041299\n",
      "Iteration 1672, loss = 0.21031429\n",
      "Iteration 1673, loss = 0.21027081\n",
      "Iteration 1674, loss = 0.21018505\n",
      "Iteration 1675, loss = 0.21009961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.920000\n",
      "Training set loss: 0.210100\n",
      "training: constant with momentum\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.69336674\n",
      "Iteration 3, loss = 0.69050049\n",
      "Iteration 4, loss = 0.68849635\n",
      "Iteration 5, loss = 0.68795185\n",
      "Iteration 6, loss = 0.68819456\n",
      "Iteration 7, loss = 0.68820704\n",
      "Iteration 8, loss = 0.68739541\n",
      "Iteration 9, loss = 0.68581003\n",
      "Iteration 10, loss = 0.68419661\n",
      "Iteration 11, loss = 0.68328953\n",
      "Iteration 12, loss = 0.68301378\n",
      "Iteration 13, loss = 0.68291225\n",
      "Iteration 14, loss = 0.68245571\n",
      "Iteration 15, loss = 0.68149674\n",
      "Iteration 16, loss = 0.68016390\n",
      "Iteration 17, loss = 0.67885208\n",
      "Iteration 18, loss = 0.67789901\n",
      "Iteration 19, loss = 0.67722329\n",
      "Iteration 20, loss = 0.67651854\n",
      "Iteration 21, loss = 0.67559645\n",
      "Iteration 22, loss = 0.67444870\n",
      "Iteration 23, loss = 0.67315265\n",
      "Iteration 24, loss = 0.67182081\n",
      "Iteration 25, loss = 0.67059247\n",
      "Iteration 26, loss = 0.66941282\n",
      "Iteration 27, loss = 0.66825697\n",
      "Iteration 28, loss = 0.66699810\n",
      "Iteration 29, loss = 0.66559468\n",
      "Iteration 30, loss = 0.66413948\n",
      "Iteration 31, loss = 0.66281419\n",
      "Iteration 32, loss = 0.66144850\n",
      "Iteration 33, loss = 0.65995742\n",
      "Iteration 34, loss = 0.65836491\n",
      "Iteration 35, loss = 0.65660313\n",
      "Iteration 36, loss = 0.65481723\n",
      "Iteration 37, loss = 0.65312243\n",
      "Iteration 38, loss = 0.65136376\n",
      "Iteration 39, loss = 0.64948894\n",
      "Iteration 40, loss = 0.64751310\n",
      "Iteration 41, loss = 0.64541993\n",
      "Iteration 42, loss = 0.64327631\n",
      "Iteration 43, loss = 0.64108932\n",
      "Iteration 44, loss = 0.63882254\n",
      "Iteration 45, loss = 0.63643918\n",
      "Iteration 46, loss = 0.63398411\n",
      "Iteration 47, loss = 0.63148733\n",
      "Iteration 48, loss = 0.62889645\n",
      "Iteration 49, loss = 0.62620723\n",
      "Iteration 50, loss = 0.62345761\n",
      "Iteration 51, loss = 0.62065455\n",
      "Iteration 52, loss = 0.61775549\n",
      "Iteration 53, loss = 0.61473699\n",
      "Iteration 54, loss = 0.61160744\n",
      "Iteration 55, loss = 0.60838924\n",
      "Iteration 56, loss = 0.60513371\n",
      "Iteration 57, loss = 0.60179004\n",
      "Iteration 58, loss = 0.59834439\n",
      "Iteration 59, loss = 0.59482056\n",
      "Iteration 60, loss = 0.59120563\n",
      "Iteration 61, loss = 0.58747978\n",
      "Iteration 62, loss = 0.58364819\n",
      "Iteration 63, loss = 0.57967412\n",
      "Iteration 64, loss = 0.57554549\n",
      "Iteration 65, loss = 0.57132247\n",
      "Iteration 66, loss = 0.56698925\n",
      "Iteration 67, loss = 0.56259344\n",
      "Iteration 68, loss = 0.55822370\n",
      "Iteration 69, loss = 0.55380330\n",
      "Iteration 70, loss = 0.54935323\n",
      "Iteration 71, loss = 0.54483986\n",
      "Iteration 72, loss = 0.54019056\n",
      "Iteration 73, loss = 0.53546909\n",
      "Iteration 74, loss = 0.53072720\n",
      "Iteration 75, loss = 0.52591451\n",
      "Iteration 76, loss = 0.52102082\n",
      "Iteration 77, loss = 0.51604046\n",
      "Iteration 78, loss = 0.51101775\n",
      "Iteration 79, loss = 0.50595539\n",
      "Iteration 80, loss = 0.50082908\n",
      "Iteration 81, loss = 0.49569104\n",
      "Iteration 82, loss = 0.49057194\n",
      "Iteration 83, loss = 0.48545921\n",
      "Iteration 84, loss = 0.48030221\n",
      "Iteration 85, loss = 0.47512757\n",
      "Iteration 86, loss = 0.46996549\n",
      "Iteration 87, loss = 0.46480063\n",
      "Iteration 88, loss = 0.45961852\n",
      "Iteration 89, loss = 0.45443574\n",
      "Iteration 90, loss = 0.44927808\n",
      "Iteration 91, loss = 0.44413810\n",
      "Iteration 92, loss = 0.43901322\n",
      "Iteration 93, loss = 0.43393955\n",
      "Iteration 94, loss = 0.42887850\n",
      "Iteration 95, loss = 0.42386243\n",
      "Iteration 96, loss = 0.41889424\n",
      "Iteration 97, loss = 0.41397601\n",
      "Iteration 98, loss = 0.40911526\n",
      "Iteration 99, loss = 0.40432553\n",
      "Iteration 100, loss = 0.39956281\n",
      "Iteration 101, loss = 0.39482484\n",
      "Iteration 102, loss = 0.39013901\n",
      "Iteration 103, loss = 0.38549478\n",
      "Iteration 104, loss = 0.38091267\n",
      "Iteration 105, loss = 0.37642805\n",
      "Iteration 106, loss = 0.37202747\n",
      "Iteration 107, loss = 0.36771540\n",
      "Iteration 108, loss = 0.36347798\n",
      "Iteration 109, loss = 0.35925485\n",
      "Iteration 110, loss = 0.35509049\n",
      "Iteration 111, loss = 0.35103345\n",
      "Iteration 112, loss = 0.34701626\n",
      "Iteration 113, loss = 0.34307445\n",
      "Iteration 114, loss = 0.33918571\n",
      "Iteration 115, loss = 0.33544066\n",
      "Iteration 116, loss = 0.33176797\n",
      "Iteration 117, loss = 0.32819243\n",
      "Iteration 118, loss = 0.32467497\n",
      "Iteration 119, loss = 0.32121424\n",
      "Iteration 120, loss = 0.31783775\n",
      "Iteration 121, loss = 0.31450414\n",
      "Iteration 122, loss = 0.31124349\n",
      "Iteration 123, loss = 0.30801618\n",
      "Iteration 124, loss = 0.30487620\n",
      "Iteration 125, loss = 0.30186396\n",
      "Iteration 126, loss = 0.29893102\n",
      "Iteration 127, loss = 0.29607370\n",
      "Iteration 128, loss = 0.29332068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 129, loss = 0.29065415\n",
      "Iteration 130, loss = 0.28802291\n",
      "Iteration 131, loss = 0.28544242\n",
      "Iteration 132, loss = 0.28289621\n",
      "Iteration 133, loss = 0.28044040\n",
      "Iteration 134, loss = 0.27810792\n",
      "Iteration 135, loss = 0.27576380\n",
      "Iteration 136, loss = 0.27348504\n",
      "Iteration 137, loss = 0.27128798\n",
      "Iteration 138, loss = 0.26913765\n",
      "Iteration 139, loss = 0.26707611\n",
      "Iteration 140, loss = 0.26505059\n",
      "Iteration 141, loss = 0.26304724\n",
      "Iteration 142, loss = 0.26110966\n",
      "Iteration 143, loss = 0.25925186\n",
      "Iteration 144, loss = 0.25738433\n",
      "Iteration 145, loss = 0.25552856\n",
      "Iteration 146, loss = 0.25375440\n",
      "Iteration 147, loss = 0.25202459\n",
      "Iteration 148, loss = 0.25027344\n",
      "Iteration 149, loss = 0.24863267\n",
      "Iteration 150, loss = 0.24695851\n",
      "Iteration 151, loss = 0.24534471\n",
      "Iteration 152, loss = 0.24387756\n",
      "Iteration 153, loss = 0.24240937\n",
      "Iteration 154, loss = 0.24094750\n",
      "Iteration 155, loss = 0.23951251\n",
      "Iteration 156, loss = 0.23816486\n",
      "Iteration 157, loss = 0.23682062\n",
      "Iteration 158, loss = 0.23539098\n",
      "Iteration 159, loss = 0.23406729\n",
      "Iteration 160, loss = 0.23279957\n",
      "Iteration 161, loss = 0.23149972\n",
      "Iteration 162, loss = 0.23022656\n",
      "Iteration 163, loss = 0.22894661\n",
      "Iteration 164, loss = 0.22766336\n",
      "Iteration 165, loss = 0.22642017\n",
      "Iteration 166, loss = 0.22519327\n",
      "Iteration 167, loss = 0.22395392\n",
      "Iteration 168, loss = 0.22272423\n",
      "Iteration 169, loss = 0.22151913\n",
      "Iteration 170, loss = 0.22038445\n",
      "Iteration 171, loss = 0.21918804\n",
      "Iteration 172, loss = 0.21801609\n",
      "Iteration 173, loss = 0.21697552\n",
      "Iteration 174, loss = 0.21588546\n",
      "Iteration 175, loss = 0.21474280\n",
      "Iteration 176, loss = 0.21377459\n",
      "Iteration 177, loss = 0.21279945\n",
      "Iteration 178, loss = 0.21178227\n",
      "Iteration 179, loss = 0.21080474\n",
      "Iteration 180, loss = 0.20998232\n",
      "Iteration 181, loss = 0.20911406\n",
      "Iteration 182, loss = 0.20825184\n",
      "Iteration 183, loss = 0.20749144\n",
      "Iteration 184, loss = 0.20666216\n",
      "Iteration 185, loss = 0.20591701\n",
      "Iteration 186, loss = 0.20519976\n",
      "Iteration 187, loss = 0.20432476\n",
      "Iteration 188, loss = 0.20370510\n",
      "Iteration 189, loss = 0.20291695\n",
      "Iteration 190, loss = 0.20222703\n",
      "Iteration 191, loss = 0.20156056\n",
      "Iteration 192, loss = 0.20086734\n",
      "Iteration 193, loss = 0.20027739\n",
      "Iteration 194, loss = 0.19955499\n",
      "Iteration 195, loss = 0.19899269\n",
      "Iteration 196, loss = 0.19835804\n",
      "Iteration 197, loss = 0.19775333\n",
      "Iteration 198, loss = 0.19712513\n",
      "Iteration 199, loss = 0.19651181\n",
      "Iteration 200, loss = 0.19600130\n",
      "Iteration 201, loss = 0.19550534\n",
      "Iteration 202, loss = 0.19485036\n",
      "Iteration 203, loss = 0.19431949\n",
      "Iteration 204, loss = 0.19378136\n",
      "Iteration 205, loss = 0.19324504\n",
      "Iteration 206, loss = 0.19282020\n",
      "Iteration 207, loss = 0.19229325\n",
      "Iteration 208, loss = 0.19176096\n",
      "Iteration 209, loss = 0.19131950\n",
      "Iteration 210, loss = 0.19082673\n",
      "Iteration 211, loss = 0.19036693\n",
      "Iteration 212, loss = 0.18992192\n",
      "Iteration 213, loss = 0.18948607\n",
      "Iteration 214, loss = 0.18904405\n",
      "Iteration 215, loss = 0.18868328\n",
      "Iteration 216, loss = 0.18818885\n",
      "Iteration 217, loss = 0.18770193\n",
      "Iteration 218, loss = 0.18740346\n",
      "Iteration 219, loss = 0.18694114\n",
      "Iteration 220, loss = 0.18656569\n",
      "Iteration 221, loss = 0.18617213\n",
      "Iteration 222, loss = 0.18579575\n",
      "Iteration 223, loss = 0.18536059\n",
      "Iteration 224, loss = 0.18501182\n",
      "Iteration 225, loss = 0.18472561\n",
      "Iteration 226, loss = 0.18434603\n",
      "Iteration 227, loss = 0.18394086\n",
      "Iteration 228, loss = 0.18358713\n",
      "Iteration 229, loss = 0.18321983\n",
      "Iteration 230, loss = 0.18294638\n",
      "Iteration 231, loss = 0.18257737\n",
      "Iteration 232, loss = 0.18221489\n",
      "Iteration 233, loss = 0.18188928\n",
      "Iteration 234, loss = 0.18159280\n",
      "Iteration 235, loss = 0.18126245\n",
      "Iteration 236, loss = 0.18097205\n",
      "Iteration 237, loss = 0.18064779\n",
      "Iteration 238, loss = 0.18039435\n",
      "Iteration 239, loss = 0.18006490\n",
      "Iteration 240, loss = 0.17964327\n",
      "Iteration 241, loss = 0.17932924\n",
      "Iteration 242, loss = 0.17907227\n",
      "Iteration 243, loss = 0.17872614\n",
      "Iteration 244, loss = 0.17833974\n",
      "Iteration 245, loss = 0.17797180\n",
      "Iteration 246, loss = 0.17768946\n",
      "Iteration 247, loss = 0.17736879\n",
      "Iteration 248, loss = 0.17700089\n",
      "Iteration 249, loss = 0.17669452\n",
      "Iteration 250, loss = 0.17627829\n",
      "Iteration 251, loss = 0.17596699\n",
      "Iteration 252, loss = 0.17574671\n",
      "Iteration 253, loss = 0.17543146\n",
      "Iteration 254, loss = 0.17512305\n",
      "Iteration 255, loss = 0.17486619\n",
      "Iteration 256, loss = 0.17460730\n",
      "Iteration 257, loss = 0.17434431\n",
      "Iteration 258, loss = 0.17410463\n",
      "Iteration 259, loss = 0.17379034\n",
      "Iteration 260, loss = 0.17352917\n",
      "Iteration 261, loss = 0.17325005\n",
      "Iteration 262, loss = 0.17300125\n",
      "Iteration 263, loss = 0.17274118\n",
      "Iteration 264, loss = 0.17252946\n",
      "Iteration 265, loss = 0.17230241\n",
      "Iteration 266, loss = 0.17204870\n",
      "Iteration 267, loss = 0.17174400\n",
      "Iteration 268, loss = 0.17161954\n",
      "Iteration 269, loss = 0.17138927\n",
      "Iteration 270, loss = 0.17107810\n",
      "Iteration 271, loss = 0.17083766\n",
      "Iteration 272, loss = 0.17067018\n",
      "Iteration 273, loss = 0.17049788\n",
      "Iteration 274, loss = 0.17034903\n",
      "Iteration 275, loss = 0.17015113\n",
      "Iteration 276, loss = 0.16987316\n",
      "Iteration 277, loss = 0.16974132\n",
      "Iteration 278, loss = 0.16949961\n",
      "Iteration 279, loss = 0.16930550\n",
      "Iteration 280, loss = 0.16919952\n",
      "Iteration 281, loss = 0.16915605\n",
      "Iteration 282, loss = 0.16889741\n",
      "Iteration 283, loss = 0.16874532\n",
      "Iteration 284, loss = 0.16851245\n",
      "Iteration 285, loss = 0.16832745\n",
      "Iteration 286, loss = 0.16824121\n",
      "Iteration 287, loss = 0.16810292\n",
      "Iteration 288, loss = 0.16775526\n",
      "Iteration 289, loss = 0.16742220\n",
      "Iteration 290, loss = 0.16742340\n",
      "Iteration 291, loss = 0.16742157\n",
      "Iteration 292, loss = 0.16716107\n",
      "Iteration 293, loss = 0.16694617\n",
      "Iteration 294, loss = 0.16679434\n",
      "Iteration 295, loss = 0.16675754\n",
      "Iteration 296, loss = 0.16666775\n",
      "Iteration 297, loss = 0.16631959\n",
      "Iteration 298, loss = 0.16634222\n",
      "Iteration 299, loss = 0.16618307\n",
      "Iteration 300, loss = 0.16587963\n",
      "Iteration 301, loss = 0.16574624\n",
      "Iteration 302, loss = 0.16567308\n",
      "Iteration 303, loss = 0.16557514\n",
      "Iteration 304, loss = 0.16543178\n",
      "Iteration 305, loss = 0.16521150\n",
      "Iteration 306, loss = 0.16507163\n",
      "Iteration 307, loss = 0.16492145\n",
      "Iteration 308, loss = 0.16494408\n",
      "Iteration 309, loss = 0.16464309\n",
      "Iteration 310, loss = 0.16450766\n",
      "Iteration 311, loss = 0.16431462\n",
      "Iteration 312, loss = 0.16413360\n",
      "Iteration 313, loss = 0.16412182\n",
      "Iteration 314, loss = 0.16403041\n",
      "Iteration 315, loss = 0.16386670\n",
      "Iteration 316, loss = 0.16370134\n",
      "Iteration 317, loss = 0.16364858\n",
      "Iteration 318, loss = 0.16375653\n",
      "Iteration 319, loss = 0.16371246\n",
      "Iteration 320, loss = 0.16351446\n",
      "Iteration 321, loss = 0.16318700\n",
      "Iteration 322, loss = 0.16336627\n",
      "Iteration 323, loss = 0.16319997\n",
      "Iteration 324, loss = 0.16294068\n",
      "Iteration 325, loss = 0.16292187\n",
      "Iteration 326, loss = 0.16295990\n",
      "Iteration 327, loss = 0.16277295\n",
      "Iteration 328, loss = 0.16252265\n",
      "Iteration 329, loss = 0.16252502\n",
      "Iteration 330, loss = 0.16267718\n",
      "Iteration 331, loss = 0.16239195\n",
      "Iteration 332, loss = 0.16223694\n",
      "Iteration 333, loss = 0.16211621\n",
      "Iteration 334, loss = 0.16213428\n",
      "Iteration 335, loss = 0.16196941\n",
      "Iteration 336, loss = 0.16181165\n",
      "Iteration 337, loss = 0.16180387\n",
      "Iteration 338, loss = 0.16166858\n",
      "Iteration 339, loss = 0.16142100\n",
      "Iteration 340, loss = 0.16148653\n",
      "Iteration 341, loss = 0.16161038\n",
      "Iteration 342, loss = 0.16130338\n",
      "Iteration 343, loss = 0.16122745\n",
      "Iteration 344, loss = 0.16125577\n",
      "Iteration 345, loss = 0.16118455\n",
      "Iteration 346, loss = 0.16109147\n",
      "Iteration 347, loss = 0.16093957\n",
      "Iteration 348, loss = 0.16073295\n",
      "Iteration 349, loss = 0.16073498\n",
      "Iteration 350, loss = 0.16058260\n",
      "Iteration 351, loss = 0.16037394\n",
      "Iteration 352, loss = 0.16043855\n",
      "Iteration 353, loss = 0.16036075\n",
      "Iteration 354, loss = 0.16036446\n",
      "Iteration 355, loss = 0.16012313\n",
      "Iteration 356, loss = 0.16007403\n",
      "Iteration 357, loss = 0.16005055\n",
      "Iteration 358, loss = 0.16001127\n",
      "Iteration 359, loss = 0.15991940\n",
      "Iteration 360, loss = 0.15992985\n",
      "Iteration 361, loss = 0.15979475\n",
      "Iteration 362, loss = 0.15966270\n",
      "Iteration 363, loss = 0.15944425\n",
      "Iteration 364, loss = 0.15958156\n",
      "Iteration 365, loss = 0.15942672\n",
      "Iteration 366, loss = 0.15931462\n",
      "Iteration 367, loss = 0.15921718\n",
      "Iteration 368, loss = 0.15915407\n",
      "Iteration 369, loss = 0.15910739\n",
      "Iteration 370, loss = 0.15896755\n",
      "Iteration 371, loss = 0.15906309\n",
      "Iteration 372, loss = 0.15892802\n",
      "Iteration 373, loss = 0.15882800\n",
      "Iteration 374, loss = 0.15884709\n",
      "Iteration 375, loss = 0.15877059\n",
      "Iteration 376, loss = 0.15862205\n",
      "Iteration 377, loss = 0.15856635\n",
      "Iteration 378, loss = 0.15870516\n",
      "Iteration 379, loss = 0.15870455\n",
      "Iteration 380, loss = 0.15848839\n",
      "Iteration 381, loss = 0.15845451\n",
      "Iteration 382, loss = 0.15842492\n",
      "Iteration 383, loss = 0.15836215\n",
      "Iteration 384, loss = 0.15820257\n",
      "Iteration 385, loss = 0.15813768\n",
      "Iteration 386, loss = 0.15809109\n",
      "Iteration 387, loss = 0.15804341\n",
      "Iteration 388, loss = 0.15804058\n",
      "Iteration 389, loss = 0.15790980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 390, loss = 0.15779403\n",
      "Iteration 391, loss = 0.15756868\n",
      "Iteration 392, loss = 0.15772811\n",
      "Iteration 393, loss = 0.15774117\n",
      "Iteration 394, loss = 0.15757822\n",
      "Iteration 395, loss = 0.15758018\n",
      "Iteration 396, loss = 0.15734297\n",
      "Iteration 397, loss = 0.15738045\n",
      "Iteration 398, loss = 0.15734690\n",
      "Iteration 399, loss = 0.15725983\n",
      "Iteration 400, loss = 0.15733370\n",
      "Iteration 401, loss = 0.15717100\n",
      "Iteration 402, loss = 0.15704612\n",
      "Iteration 403, loss = 0.15697699\n",
      "Iteration 404, loss = 0.15700878\n",
      "Iteration 405, loss = 0.15694905\n",
      "Iteration 406, loss = 0.15685759\n",
      "Iteration 407, loss = 0.15675167\n",
      "Iteration 408, loss = 0.15703643\n",
      "Iteration 409, loss = 0.15687177\n",
      "Iteration 410, loss = 0.15661230\n",
      "Iteration 411, loss = 0.15681276\n",
      "Iteration 412, loss = 0.15702582\n",
      "Iteration 413, loss = 0.15690811\n",
      "Iteration 414, loss = 0.15673606\n",
      "Iteration 415, loss = 0.15663099\n",
      "Iteration 416, loss = 0.15643758\n",
      "Iteration 417, loss = 0.15607302\n",
      "Iteration 418, loss = 0.15612872\n",
      "Iteration 419, loss = 0.15610344\n",
      "Iteration 420, loss = 0.15592660\n",
      "Iteration 421, loss = 0.15591783\n",
      "Iteration 422, loss = 0.15577653\n",
      "Iteration 423, loss = 0.15577730\n",
      "Iteration 424, loss = 0.15569629\n",
      "Iteration 425, loss = 0.15565312\n",
      "Iteration 426, loss = 0.15556314\n",
      "Iteration 427, loss = 0.15551769\n",
      "Iteration 428, loss = 0.15552692\n",
      "Iteration 429, loss = 0.15564596\n",
      "Iteration 430, loss = 0.15538373\n",
      "Iteration 431, loss = 0.15515230\n",
      "Iteration 432, loss = 0.15528841\n",
      "Iteration 433, loss = 0.15516802\n",
      "Iteration 434, loss = 0.15517448\n",
      "Iteration 435, loss = 0.15510763\n",
      "Iteration 436, loss = 0.15510931\n",
      "Iteration 437, loss = 0.15503967\n",
      "Iteration 438, loss = 0.15502667\n",
      "Iteration 439, loss = 0.15496217\n",
      "Iteration 440, loss = 0.15501248\n",
      "Iteration 441, loss = 0.15496573\n",
      "Iteration 442, loss = 0.15499674\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.940000\n",
      "Training set loss: 0.154997\n",
      "training: constant with Nesterov's momentum\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.69173802\n",
      "Iteration 3, loss = 0.68906067\n",
      "Iteration 4, loss = 0.68782808\n",
      "Iteration 5, loss = 0.68736262\n",
      "Iteration 6, loss = 0.68681760\n",
      "Iteration 7, loss = 0.68599970\n",
      "Iteration 8, loss = 0.68490591\n",
      "Iteration 9, loss = 0.68380315\n",
      "Iteration 10, loss = 0.68295050\n",
      "Iteration 11, loss = 0.68229443\n",
      "Iteration 12, loss = 0.68166351\n",
      "Iteration 13, loss = 0.68097175\n",
      "Iteration 14, loss = 0.68019827\n",
      "Iteration 15, loss = 0.67935126\n",
      "Iteration 16, loss = 0.67848266\n",
      "Iteration 17, loss = 0.67756005\n",
      "Iteration 18, loss = 0.67658273\n",
      "Iteration 19, loss = 0.67558684\n",
      "Iteration 20, loss = 0.67456970\n",
      "Iteration 21, loss = 0.67353667\n",
      "Iteration 22, loss = 0.67241696\n",
      "Iteration 23, loss = 0.67115122\n",
      "Iteration 24, loss = 0.66987799\n",
      "Iteration 25, loss = 0.66859195\n",
      "Iteration 26, loss = 0.66733302\n",
      "Iteration 27, loss = 0.66602275\n",
      "Iteration 28, loss = 0.66468451\n",
      "Iteration 29, loss = 0.66331687\n",
      "Iteration 30, loss = 0.66188298\n",
      "Iteration 31, loss = 0.66037689\n",
      "Iteration 32, loss = 0.65881210\n",
      "Iteration 33, loss = 0.65718414\n",
      "Iteration 34, loss = 0.65549072\n",
      "Iteration 35, loss = 0.65373016\n",
      "Iteration 36, loss = 0.65189545\n",
      "Iteration 37, loss = 0.64999224\n",
      "Iteration 38, loss = 0.64801478\n",
      "Iteration 39, loss = 0.64595046\n",
      "Iteration 40, loss = 0.64381536\n",
      "Iteration 41, loss = 0.64160699\n",
      "Iteration 42, loss = 0.63931247\n",
      "Iteration 43, loss = 0.63694298\n",
      "Iteration 44, loss = 0.63450770\n",
      "Iteration 45, loss = 0.63198530\n",
      "Iteration 46, loss = 0.62937217\n",
      "Iteration 47, loss = 0.62666961\n",
      "Iteration 48, loss = 0.62387639\n",
      "Iteration 49, loss = 0.62099246\n",
      "Iteration 50, loss = 0.61801954\n",
      "Iteration 51, loss = 0.61494151\n",
      "Iteration 52, loss = 0.61177170\n",
      "Iteration 53, loss = 0.60854927\n",
      "Iteration 54, loss = 0.60524125\n",
      "Iteration 55, loss = 0.60183721\n",
      "Iteration 56, loss = 0.59834539\n",
      "Iteration 57, loss = 0.59475664\n",
      "Iteration 58, loss = 0.59109045\n",
      "Iteration 59, loss = 0.58731661\n",
      "Iteration 60, loss = 0.58343229\n",
      "Iteration 61, loss = 0.57941250\n",
      "Iteration 62, loss = 0.57525996\n",
      "Iteration 63, loss = 0.57097848\n",
      "Iteration 64, loss = 0.56662284\n",
      "Iteration 65, loss = 0.56216462\n",
      "Iteration 66, loss = 0.55770116\n",
      "Iteration 67, loss = 0.55323621\n",
      "Iteration 68, loss = 0.54871800\n",
      "Iteration 69, loss = 0.54414745\n",
      "Iteration 70, loss = 0.53952174\n",
      "Iteration 71, loss = 0.53480248\n",
      "Iteration 72, loss = 0.53001095\n",
      "Iteration 73, loss = 0.52515524\n",
      "Iteration 74, loss = 0.52019710\n",
      "Iteration 75, loss = 0.51518168\n",
      "Iteration 76, loss = 0.51011718\n",
      "Iteration 77, loss = 0.50502782\n",
      "Iteration 78, loss = 0.49994509\n",
      "Iteration 79, loss = 0.49485587\n",
      "Iteration 80, loss = 0.48973417\n",
      "Iteration 81, loss = 0.48459318\n",
      "Iteration 82, loss = 0.47945032\n",
      "Iteration 83, loss = 0.47430526\n",
      "Iteration 84, loss = 0.46916010\n",
      "Iteration 85, loss = 0.46401231\n",
      "Iteration 86, loss = 0.45887544\n",
      "Iteration 87, loss = 0.45370061\n",
      "Iteration 88, loss = 0.44847884\n",
      "Iteration 89, loss = 0.44323363\n",
      "Iteration 90, loss = 0.43805752\n",
      "Iteration 91, loss = 0.43288857\n",
      "Iteration 92, loss = 0.42777136\n",
      "Iteration 93, loss = 0.42260491\n",
      "Iteration 94, loss = 0.41752539\n",
      "Iteration 95, loss = 0.41251858\n",
      "Iteration 96, loss = 0.40756751\n",
      "Iteration 97, loss = 0.40266453\n",
      "Iteration 98, loss = 0.39776936\n",
      "Iteration 99, loss = 0.39295812\n",
      "Iteration 100, loss = 0.38823416\n",
      "Iteration 101, loss = 0.38350995\n",
      "Iteration 102, loss = 0.37890033\n",
      "Iteration 103, loss = 0.37441033\n",
      "Iteration 104, loss = 0.36997560\n",
      "Iteration 105, loss = 0.36563508\n",
      "Iteration 106, loss = 0.36134890\n",
      "Iteration 107, loss = 0.35718773\n",
      "Iteration 108, loss = 0.35310259\n",
      "Iteration 109, loss = 0.34908349\n",
      "Iteration 110, loss = 0.34517294\n",
      "Iteration 111, loss = 0.34131316\n",
      "Iteration 112, loss = 0.33755062\n",
      "Iteration 113, loss = 0.33389104\n",
      "Iteration 114, loss = 0.33027592\n",
      "Iteration 115, loss = 0.32676751\n",
      "Iteration 116, loss = 0.32332128\n",
      "Iteration 117, loss = 0.32000193\n",
      "Iteration 118, loss = 0.31670188\n",
      "Iteration 119, loss = 0.31347626\n",
      "Iteration 120, loss = 0.31035311\n",
      "Iteration 121, loss = 0.30727461\n",
      "Iteration 122, loss = 0.30432304\n",
      "Iteration 123, loss = 0.30140117\n",
      "Iteration 124, loss = 0.29855491\n",
      "Iteration 125, loss = 0.29579765\n",
      "Iteration 126, loss = 0.29315624\n",
      "Iteration 127, loss = 0.29051986\n",
      "Iteration 128, loss = 0.28796467\n",
      "Iteration 129, loss = 0.28549494\n",
      "Iteration 130, loss = 0.28305040\n",
      "Iteration 131, loss = 0.28069839\n",
      "Iteration 132, loss = 0.27838564\n",
      "Iteration 133, loss = 0.27612516\n",
      "Iteration 134, loss = 0.27391360\n",
      "Iteration 135, loss = 0.27178094\n",
      "Iteration 136, loss = 0.26971548\n",
      "Iteration 137, loss = 0.26769828\n",
      "Iteration 138, loss = 0.26570435\n",
      "Iteration 139, loss = 0.26379608\n",
      "Iteration 140, loss = 0.26191706\n",
      "Iteration 141, loss = 0.26009916\n",
      "Iteration 142, loss = 0.25835286\n",
      "Iteration 143, loss = 0.25654418\n",
      "Iteration 144, loss = 0.25486990\n",
      "Iteration 145, loss = 0.25317768\n",
      "Iteration 146, loss = 0.25159170\n",
      "Iteration 147, loss = 0.24995495\n",
      "Iteration 148, loss = 0.24838164\n",
      "Iteration 149, loss = 0.24689022\n",
      "Iteration 150, loss = 0.24546711\n",
      "Iteration 151, loss = 0.24400475\n",
      "Iteration 152, loss = 0.24259025\n",
      "Iteration 153, loss = 0.24110862\n",
      "Iteration 154, loss = 0.23971364\n",
      "Iteration 155, loss = 0.23826495\n",
      "Iteration 156, loss = 0.23678748\n",
      "Iteration 157, loss = 0.23536536\n",
      "Iteration 158, loss = 0.23400577\n",
      "Iteration 159, loss = 0.23269808\n",
      "Iteration 160, loss = 0.23130988\n",
      "Iteration 161, loss = 0.23006836\n",
      "Iteration 162, loss = 0.22879904\n",
      "Iteration 163, loss = 0.22751084\n",
      "Iteration 164, loss = 0.22628483\n",
      "Iteration 165, loss = 0.22504035\n",
      "Iteration 166, loss = 0.22383057\n",
      "Iteration 167, loss = 0.22262491\n",
      "Iteration 168, loss = 0.22149707\n",
      "Iteration 169, loss = 0.22030758\n",
      "Iteration 170, loss = 0.21924145\n",
      "Iteration 171, loss = 0.21812587\n",
      "Iteration 172, loss = 0.21713841\n",
      "Iteration 173, loss = 0.21606707\n",
      "Iteration 174, loss = 0.21520247\n",
      "Iteration 175, loss = 0.21431701\n",
      "Iteration 176, loss = 0.21349515\n",
      "Iteration 177, loss = 0.21252514\n",
      "Iteration 178, loss = 0.21160283\n",
      "Iteration 179, loss = 0.21067327\n",
      "Iteration 180, loss = 0.20979881\n",
      "Iteration 181, loss = 0.20895989\n",
      "Iteration 182, loss = 0.20809523\n",
      "Iteration 183, loss = 0.20724136\n",
      "Iteration 184, loss = 0.20646827\n",
      "Iteration 185, loss = 0.20570401\n",
      "Iteration 186, loss = 0.20499483\n",
      "Iteration 187, loss = 0.20415998\n",
      "Iteration 188, loss = 0.20340569\n",
      "Iteration 189, loss = 0.20276711\n",
      "Iteration 190, loss = 0.20213960\n",
      "Iteration 191, loss = 0.20136617\n",
      "Iteration 192, loss = 0.20089424\n",
      "Iteration 193, loss = 0.20011760\n",
      "Iteration 194, loss = 0.19964964\n",
      "Iteration 195, loss = 0.19909254\n",
      "Iteration 196, loss = 0.19827786\n",
      "Iteration 197, loss = 0.19760754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 198, loss = 0.19707071\n",
      "Iteration 199, loss = 0.19635291\n",
      "Iteration 200, loss = 0.19587457\n",
      "Iteration 201, loss = 0.19528503\n",
      "Iteration 202, loss = 0.19493650\n",
      "Iteration 203, loss = 0.19429689\n",
      "Iteration 204, loss = 0.19377511\n",
      "Iteration 205, loss = 0.19312523\n",
      "Iteration 206, loss = 0.19268530\n",
      "Iteration 207, loss = 0.19213861\n",
      "Iteration 208, loss = 0.19170370\n",
      "Iteration 209, loss = 0.19121187\n",
      "Iteration 210, loss = 0.19079601\n",
      "Iteration 211, loss = 0.19028556\n",
      "Iteration 212, loss = 0.18988270\n",
      "Iteration 213, loss = 0.18942631\n",
      "Iteration 214, loss = 0.18931289\n",
      "Iteration 215, loss = 0.18877305\n",
      "Iteration 216, loss = 0.18832419\n",
      "Iteration 217, loss = 0.18771364\n",
      "Iteration 218, loss = 0.18721596\n",
      "Iteration 219, loss = 0.18683538\n",
      "Iteration 220, loss = 0.18636480\n",
      "Iteration 221, loss = 0.18607229\n",
      "Iteration 222, loss = 0.18571599\n",
      "Iteration 223, loss = 0.18527772\n",
      "Iteration 224, loss = 0.18491353\n",
      "Iteration 225, loss = 0.18456052\n",
      "Iteration 226, loss = 0.18428256\n",
      "Iteration 227, loss = 0.18414733\n",
      "Iteration 228, loss = 0.18397518\n",
      "Iteration 229, loss = 0.18336261\n",
      "Iteration 230, loss = 0.18292011\n",
      "Iteration 231, loss = 0.18252757\n",
      "Iteration 232, loss = 0.18215525\n",
      "Iteration 233, loss = 0.18183785\n",
      "Iteration 234, loss = 0.18162209\n",
      "Iteration 235, loss = 0.18124520\n",
      "Iteration 236, loss = 0.18103344\n",
      "Iteration 237, loss = 0.18077650\n",
      "Iteration 238, loss = 0.18060855\n",
      "Iteration 239, loss = 0.18012441\n",
      "Iteration 240, loss = 0.17964988\n",
      "Iteration 241, loss = 0.17949288\n",
      "Iteration 242, loss = 0.17912725\n",
      "Iteration 243, loss = 0.17867515\n",
      "Iteration 244, loss = 0.17835869\n",
      "Iteration 245, loss = 0.17794511\n",
      "Iteration 246, loss = 0.17754737\n",
      "Iteration 247, loss = 0.17727521\n",
      "Iteration 248, loss = 0.17698122\n",
      "Iteration 249, loss = 0.17659984\n",
      "Iteration 250, loss = 0.17645962\n",
      "Iteration 251, loss = 0.17599206\n",
      "Iteration 252, loss = 0.17587993\n",
      "Iteration 253, loss = 0.17545404\n",
      "Iteration 254, loss = 0.17525530\n",
      "Iteration 255, loss = 0.17494266\n",
      "Iteration 256, loss = 0.17470993\n",
      "Iteration 257, loss = 0.17434751\n",
      "Iteration 258, loss = 0.17419585\n",
      "Iteration 259, loss = 0.17387097\n",
      "Iteration 260, loss = 0.17368451\n",
      "Iteration 261, loss = 0.17342704\n",
      "Iteration 262, loss = 0.17322669\n",
      "Iteration 263, loss = 0.17299067\n",
      "Iteration 264, loss = 0.17264461\n",
      "Iteration 265, loss = 0.17236001\n",
      "Iteration 266, loss = 0.17225349\n",
      "Iteration 267, loss = 0.17203361\n",
      "Iteration 268, loss = 0.17189219\n",
      "Iteration 269, loss = 0.17150200\n",
      "Iteration 270, loss = 0.17131737\n",
      "Iteration 271, loss = 0.17101383\n",
      "Iteration 272, loss = 0.17086317\n",
      "Iteration 273, loss = 0.17036227\n",
      "Iteration 274, loss = 0.17015743\n",
      "Iteration 275, loss = 0.16981254\n",
      "Iteration 276, loss = 0.16960982\n",
      "Iteration 277, loss = 0.16932377\n",
      "Iteration 278, loss = 0.16918931\n",
      "Iteration 279, loss = 0.16906647\n",
      "Iteration 280, loss = 0.16883940\n",
      "Iteration 281, loss = 0.16858475\n",
      "Iteration 282, loss = 0.16848918\n",
      "Iteration 283, loss = 0.16817358\n",
      "Iteration 284, loss = 0.16780860\n",
      "Iteration 285, loss = 0.16763283\n",
      "Iteration 286, loss = 0.16751332\n",
      "Iteration 287, loss = 0.16730346\n",
      "Iteration 288, loss = 0.16707045\n",
      "Iteration 289, loss = 0.16688564\n",
      "Iteration 290, loss = 0.16657101\n",
      "Iteration 291, loss = 0.16635597\n",
      "Iteration 292, loss = 0.16604319\n",
      "Iteration 293, loss = 0.16589797\n",
      "Iteration 294, loss = 0.16565564\n",
      "Iteration 295, loss = 0.16539219\n",
      "Iteration 296, loss = 0.16530551\n",
      "Iteration 297, loss = 0.16501025\n",
      "Iteration 298, loss = 0.16474626\n",
      "Iteration 299, loss = 0.16466885\n",
      "Iteration 300, loss = 0.16443101\n",
      "Iteration 301, loss = 0.16423010\n",
      "Iteration 302, loss = 0.16412491\n",
      "Iteration 303, loss = 0.16395920\n",
      "Iteration 304, loss = 0.16388488\n",
      "Iteration 305, loss = 0.16369722\n",
      "Iteration 306, loss = 0.16335329\n",
      "Iteration 307, loss = 0.16327809\n",
      "Iteration 308, loss = 0.16304602\n",
      "Iteration 309, loss = 0.16299264\n",
      "Iteration 310, loss = 0.16314702\n",
      "Iteration 311, loss = 0.16301242\n",
      "Iteration 312, loss = 0.16275554\n",
      "Iteration 313, loss = 0.16254634\n",
      "Iteration 314, loss = 0.16224110\n",
      "Iteration 315, loss = 0.16226044\n",
      "Iteration 316, loss = 0.16253783\n",
      "Iteration 317, loss = 0.16245245\n",
      "Iteration 318, loss = 0.16210045\n",
      "Iteration 319, loss = 0.16174295\n",
      "Iteration 320, loss = 0.16172888\n",
      "Iteration 321, loss = 0.16145982\n",
      "Iteration 322, loss = 0.16161695\n",
      "Iteration 323, loss = 0.16145593\n",
      "Iteration 324, loss = 0.16144188\n",
      "Iteration 325, loss = 0.16127403\n",
      "Iteration 326, loss = 0.16108659\n",
      "Iteration 327, loss = 0.16102223\n",
      "Iteration 328, loss = 0.16098560\n",
      "Iteration 329, loss = 0.16091065\n",
      "Iteration 330, loss = 0.16078031\n",
      "Iteration 331, loss = 0.16073266\n",
      "Iteration 332, loss = 0.16047712\n",
      "Iteration 333, loss = 0.16017375\n",
      "Iteration 334, loss = 0.16015466\n",
      "Iteration 335, loss = 0.16006269\n",
      "Iteration 336, loss = 0.15978348\n",
      "Iteration 337, loss = 0.15981617\n",
      "Iteration 338, loss = 0.15973677\n",
      "Iteration 339, loss = 0.15950787\n",
      "Iteration 340, loss = 0.15950495\n",
      "Iteration 341, loss = 0.15920358\n",
      "Iteration 342, loss = 0.15916684\n",
      "Iteration 343, loss = 0.15929049\n",
      "Iteration 344, loss = 0.15903509\n",
      "Iteration 345, loss = 0.15891386\n",
      "Iteration 346, loss = 0.15883774\n",
      "Iteration 347, loss = 0.15881492\n",
      "Iteration 348, loss = 0.15864655\n",
      "Iteration 349, loss = 0.15879495\n",
      "Iteration 350, loss = 0.15890342\n",
      "Iteration 351, loss = 0.15843890\n",
      "Iteration 352, loss = 0.15855653\n",
      "Iteration 353, loss = 0.15830247\n",
      "Iteration 354, loss = 0.15819351\n",
      "Iteration 355, loss = 0.15810248\n",
      "Iteration 356, loss = 0.15789648\n",
      "Iteration 357, loss = 0.15770931\n",
      "Iteration 358, loss = 0.15755008\n",
      "Iteration 359, loss = 0.15737272\n",
      "Iteration 360, loss = 0.15733105\n",
      "Iteration 361, loss = 0.15724268\n",
      "Iteration 362, loss = 0.15715757\n",
      "Iteration 363, loss = 0.15722102\n",
      "Iteration 364, loss = 0.15708946\n",
      "Iteration 365, loss = 0.15705175\n",
      "Iteration 366, loss = 0.15688825\n",
      "Iteration 367, loss = 0.15718391\n",
      "Iteration 368, loss = 0.15723099\n",
      "Iteration 369, loss = 0.15711337\n",
      "Iteration 370, loss = 0.15687288\n",
      "Iteration 371, loss = 0.15637070\n",
      "Iteration 372, loss = 0.15627848\n",
      "Iteration 373, loss = 0.15640037\n",
      "Iteration 374, loss = 0.15623785\n",
      "Iteration 375, loss = 0.15649189\n",
      "Iteration 376, loss = 0.15638476\n",
      "Iteration 377, loss = 0.15587490\n",
      "Iteration 378, loss = 0.15581593\n",
      "Iteration 379, loss = 0.15579484\n",
      "Iteration 380, loss = 0.15570650\n",
      "Iteration 381, loss = 0.15555498\n",
      "Iteration 382, loss = 0.15556966\n",
      "Iteration 383, loss = 0.15541657\n",
      "Iteration 384, loss = 0.15536625\n",
      "Iteration 385, loss = 0.15528000\n",
      "Iteration 386, loss = 0.15521452\n",
      "Iteration 387, loss = 0.15517159\n",
      "Iteration 388, loss = 0.15508000\n",
      "Iteration 389, loss = 0.15497784\n",
      "Iteration 390, loss = 0.15506279\n",
      "Iteration 391, loss = 0.15485289\n",
      "Iteration 392, loss = 0.15475680\n",
      "Iteration 393, loss = 0.15472825\n",
      "Iteration 394, loss = 0.15468563\n",
      "Iteration 395, loss = 0.15479197\n",
      "Iteration 396, loss = 0.15456967\n",
      "Iteration 397, loss = 0.15460937\n",
      "Iteration 398, loss = 0.15449091\n",
      "Iteration 399, loss = 0.15447440\n",
      "Iteration 400, loss = 0.15445298\n",
      "Iteration 401, loss = 0.15457471\n",
      "Iteration 402, loss = 0.15479725\n",
      "Iteration 403, loss = 0.15472011\n",
      "Iteration 404, loss = 0.15455205\n",
      "Iteration 405, loss = 0.15416715\n",
      "Iteration 406, loss = 0.15412853\n",
      "Iteration 407, loss = 0.15417143\n",
      "Iteration 408, loss = 0.15416945\n",
      "Iteration 409, loss = 0.15398501\n",
      "Iteration 410, loss = 0.15430932\n",
      "Iteration 411, loss = 0.15403809\n",
      "Iteration 412, loss = 0.15406628\n",
      "Iteration 413, loss = 0.15388748\n",
      "Iteration 414, loss = 0.15375729\n",
      "Iteration 415, loss = 0.15378853\n",
      "Iteration 416, loss = 0.15372453\n",
      "Iteration 417, loss = 0.15346981\n",
      "Iteration 418, loss = 0.15370411\n",
      "Iteration 419, loss = 0.15346293\n",
      "Iteration 420, loss = 0.15349659\n",
      "Iteration 421, loss = 0.15345920\n",
      "Iteration 422, loss = 0.15324202\n",
      "Iteration 423, loss = 0.15313755\n",
      "Iteration 424, loss = 0.15327467\n",
      "Iteration 425, loss = 0.15307070\n",
      "Iteration 426, loss = 0.15323286\n",
      "Iteration 427, loss = 0.15338072\n",
      "Iteration 428, loss = 0.15318118\n",
      "Iteration 429, loss = 0.15304240\n",
      "Iteration 430, loss = 0.15315122\n",
      "Iteration 431, loss = 0.15290180\n",
      "Iteration 432, loss = 0.15292630\n",
      "Iteration 433, loss = 0.15275007\n",
      "Iteration 434, loss = 0.15266756\n",
      "Iteration 435, loss = 0.15255054\n",
      "Iteration 436, loss = 0.15261472\n",
      "Iteration 437, loss = 0.15282330\n",
      "Iteration 438, loss = 0.15259034\n",
      "Iteration 439, loss = 0.15272401\n",
      "Iteration 440, loss = 0.15266677\n",
      "Iteration 441, loss = 0.15247047\n",
      "Iteration 442, loss = 0.15253325\n",
      "Iteration 443, loss = 0.15249992\n",
      "Iteration 444, loss = 0.15218305\n",
      "Iteration 445, loss = 0.15212663\n",
      "Iteration 446, loss = 0.15220442\n",
      "Iteration 447, loss = 0.15231078\n",
      "Iteration 448, loss = 0.15211688\n",
      "Iteration 449, loss = 0.15234023\n",
      "Iteration 450, loss = 0.15204521\n",
      "Iteration 451, loss = 0.15204252\n",
      "Iteration 452, loss = 0.15214002\n",
      "Iteration 453, loss = 0.15187156\n",
      "Iteration 454, loss = 0.15199468\n",
      "Iteration 455, loss = 0.15167028\n",
      "Iteration 456, loss = 0.15164226\n",
      "Iteration 457, loss = 0.15179472\n",
      "Iteration 458, loss = 0.15177925\n",
      "Iteration 459, loss = 0.15193631\n",
      "Iteration 460, loss = 0.15173451\n",
      "Iteration 461, loss = 0.15150839\n",
      "Iteration 462, loss = 0.15150750\n",
      "Iteration 463, loss = 0.15142518\n",
      "Iteration 464, loss = 0.15139679\n",
      "Iteration 465, loss = 0.15163926\n",
      "Iteration 466, loss = 0.15143777\n",
      "Iteration 467, loss = 0.15126184\n",
      "Iteration 468, loss = 0.15126558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 469, loss = 0.15147071\n",
      "Iteration 470, loss = 0.15117556\n",
      "Iteration 471, loss = 0.15120016\n",
      "Iteration 472, loss = 0.15126179\n",
      "Iteration 473, loss = 0.15107650\n",
      "Iteration 474, loss = 0.15112809\n",
      "Iteration 475, loss = 0.15108677\n",
      "Iteration 476, loss = 0.15106361\n",
      "Iteration 477, loss = 0.15104602\n",
      "Iteration 478, loss = 0.15094185\n",
      "Iteration 479, loss = 0.15083350\n",
      "Iteration 480, loss = 0.15090645\n",
      "Iteration 481, loss = 0.15074937\n",
      "Iteration 482, loss = 0.15096710\n",
      "Iteration 483, loss = 0.15072933\n",
      "Iteration 484, loss = 0.15084782\n",
      "Iteration 485, loss = 0.15064565\n",
      "Iteration 486, loss = 0.15072982\n",
      "Iteration 487, loss = 0.15070215\n",
      "Iteration 488, loss = 0.15067579\n",
      "Iteration 489, loss = 0.15086284\n",
      "Iteration 490, loss = 0.15063008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.940000\n",
      "Training set loss: 0.150630\n",
      "training: inv-scaling learning-rate\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.69336674\n",
      "Iteration 3, loss = 0.69320289\n",
      "Iteration 4, loss = 0.69309010\n",
      "Iteration 5, loss = 0.69299958\n",
      "Iteration 6, loss = 0.69292251\n",
      "Iteration 7, loss = 0.69285482\n",
      "Iteration 8, loss = 0.69279383\n",
      "Iteration 9, loss = 0.69273791\n",
      "Iteration 10, loss = 0.69268593\n",
      "Iteration 11, loss = 0.69263727\n",
      "Iteration 12, loss = 0.69259187\n",
      "Iteration 13, loss = 0.69254902\n",
      "Iteration 14, loss = 0.69250834\n",
      "Iteration 15, loss = 0.69246955\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.500000\n",
      "Training set loss: 0.692470\n",
      "training: inv-scaling with momentum\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.69173802\n",
      "Iteration 3, loss = 0.69037556\n",
      "Iteration 4, loss = 0.68942003\n",
      "Iteration 5, loss = 0.68880596\n",
      "Iteration 6, loss = 0.68843261\n",
      "Iteration 7, loss = 0.68825649\n",
      "Iteration 8, loss = 0.68821788\n",
      "Iteration 9, loss = 0.68827008\n",
      "Iteration 10, loss = 0.68837637\n",
      "Iteration 11, loss = 0.68850624\n",
      "Iteration 12, loss = 0.68864765\n",
      "Iteration 13, loss = 0.68878599\n",
      "Iteration 14, loss = 0.68890520\n",
      "Iteration 15, loss = 0.68900227\n",
      "Iteration 16, loss = 0.68907489\n",
      "Iteration 17, loss = 0.68912003\n",
      "Iteration 18, loss = 0.68914266\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.500000\n",
      "Training set loss: 0.689143\n",
      "training: inv-scaling with Nesterov's momentum\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.69336674\n",
      "Iteration 3, loss = 0.69160252\n",
      "Iteration 4, loss = 0.69028294\n",
      "Iteration 5, loss = 0.68934700\n",
      "Iteration 6, loss = 0.68875607\n",
      "Iteration 7, loss = 0.68842048\n",
      "Iteration 8, loss = 0.68829201\n",
      "Iteration 9, loss = 0.68830931\n",
      "Iteration 10, loss = 0.68842455\n",
      "Iteration 11, loss = 0.68859263\n",
      "Iteration 12, loss = 0.68878581\n",
      "Iteration 13, loss = 0.68899178\n",
      "Iteration 14, loss = 0.68918736\n",
      "Iteration 15, loss = 0.68936029\n",
      "Iteration 16, loss = 0.68950674\n",
      "Iteration 17, loss = 0.68961951\n",
      "Iteration 18, loss = 0.68970133\n",
      "Iteration 19, loss = 0.68975064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.500000\n",
      "Training set loss: 0.689751\n",
      "training: adam\n",
      "Iteration 1, loss = 0.69555446\n",
      "Iteration 2, loss = 0.68795907\n",
      "Iteration 3, loss = 0.68701339\n",
      "Iteration 4, loss = 0.68539628\n",
      "Iteration 5, loss = 0.68238095\n",
      "Iteration 6, loss = 0.68001512\n",
      "Iteration 7, loss = 0.67895101\n",
      "Iteration 8, loss = 0.67820336\n",
      "Iteration 9, loss = 0.67669839\n",
      "Iteration 10, loss = 0.67467854\n",
      "Iteration 11, loss = 0.67264650\n",
      "Iteration 12, loss = 0.67092944\n",
      "Iteration 13, loss = 0.66940057\n",
      "Iteration 14, loss = 0.66765192\n",
      "Iteration 15, loss = 0.66549001\n",
      "Iteration 16, loss = 0.66313907\n",
      "Iteration 17, loss = 0.66087438\n",
      "Iteration 18, loss = 0.65867068\n",
      "Iteration 19, loss = 0.65631409\n",
      "Iteration 20, loss = 0.65368460\n",
      "Iteration 21, loss = 0.65087252\n",
      "Iteration 22, loss = 0.64803663\n",
      "Iteration 23, loss = 0.64519251\n",
      "Iteration 24, loss = 0.64222768\n",
      "Iteration 25, loss = 0.63902649\n",
      "Iteration 26, loss = 0.63558805\n",
      "Iteration 27, loss = 0.63211882\n",
      "Iteration 28, loss = 0.62856781\n",
      "Iteration 29, loss = 0.62487401\n",
      "Iteration 30, loss = 0.62101237\n",
      "Iteration 31, loss = 0.61696918\n",
      "Iteration 32, loss = 0.61281824\n",
      "Iteration 33, loss = 0.60848754\n",
      "Iteration 34, loss = 0.60393867\n",
      "Iteration 35, loss = 0.59918875\n",
      "Iteration 36, loss = 0.59435480\n",
      "Iteration 37, loss = 0.58957146\n",
      "Iteration 38, loss = 0.58471825\n",
      "Iteration 39, loss = 0.57974638\n",
      "Iteration 40, loss = 0.57465448\n",
      "Iteration 41, loss = 0.56942429\n",
      "Iteration 42, loss = 0.56408204\n",
      "Iteration 43, loss = 0.55860873\n",
      "Iteration 44, loss = 0.55300221\n",
      "Iteration 45, loss = 0.54729984\n",
      "Iteration 46, loss = 0.54151386\n",
      "Iteration 47, loss = 0.53560947\n",
      "Iteration 48, loss = 0.52959541\n",
      "Iteration 49, loss = 0.52351080\n",
      "Iteration 50, loss = 0.51738085\n",
      "Iteration 51, loss = 0.51124102\n",
      "Iteration 52, loss = 0.50507499\n",
      "Iteration 53, loss = 0.49888673\n",
      "Iteration 54, loss = 0.49269272\n",
      "Iteration 55, loss = 0.48645309\n",
      "Iteration 56, loss = 0.48020065\n",
      "Iteration 57, loss = 0.47393289\n",
      "Iteration 58, loss = 0.46766942\n",
      "Iteration 59, loss = 0.46139991\n",
      "Iteration 60, loss = 0.45512125\n",
      "Iteration 61, loss = 0.44887953\n",
      "Iteration 62, loss = 0.44268392\n",
      "Iteration 63, loss = 0.43656572\n",
      "Iteration 64, loss = 0.43048247\n",
      "Iteration 65, loss = 0.42443623\n",
      "Iteration 66, loss = 0.41842673\n",
      "Iteration 67, loss = 0.41253272\n",
      "Iteration 68, loss = 0.40667597\n",
      "Iteration 69, loss = 0.40089791\n",
      "Iteration 70, loss = 0.39519206\n",
      "Iteration 71, loss = 0.38959107\n",
      "Iteration 72, loss = 0.38405050\n",
      "Iteration 73, loss = 0.37859593\n",
      "Iteration 74, loss = 0.37321082\n",
      "Iteration 75, loss = 0.36790373\n",
      "Iteration 76, loss = 0.36267413\n",
      "Iteration 77, loss = 0.35749738\n",
      "Iteration 78, loss = 0.35246536\n",
      "Iteration 79, loss = 0.34757870\n",
      "Iteration 80, loss = 0.34282694\n",
      "Iteration 81, loss = 0.33812181\n",
      "Iteration 82, loss = 0.33352066\n",
      "Iteration 83, loss = 0.32901617\n",
      "Iteration 84, loss = 0.32459877\n",
      "Iteration 85, loss = 0.32031104\n",
      "Iteration 86, loss = 0.31603872\n",
      "Iteration 87, loss = 0.31180232\n",
      "Iteration 88, loss = 0.30776305\n",
      "Iteration 89, loss = 0.30395738\n",
      "Iteration 90, loss = 0.30030775\n",
      "Iteration 91, loss = 0.29677718\n",
      "Iteration 92, loss = 0.29323544\n",
      "Iteration 93, loss = 0.28979111\n",
      "Iteration 94, loss = 0.28646098\n",
      "Iteration 95, loss = 0.28317303\n",
      "Iteration 96, loss = 0.28003165\n",
      "Iteration 97, loss = 0.27703033\n",
      "Iteration 98, loss = 0.27412458\n",
      "Iteration 99, loss = 0.27133273\n",
      "Iteration 100, loss = 0.26853852\n",
      "Iteration 101, loss = 0.26582938\n",
      "Iteration 102, loss = 0.26325000\n",
      "Iteration 103, loss = 0.26078144\n",
      "Iteration 104, loss = 0.25834108\n",
      "Iteration 105, loss = 0.25594374\n",
      "Iteration 106, loss = 0.25366556\n",
      "Iteration 107, loss = 0.25145191\n",
      "Iteration 108, loss = 0.24933660\n",
      "Iteration 109, loss = 0.24728527\n",
      "Iteration 110, loss = 0.24529232\n",
      "Iteration 111, loss = 0.24334672\n",
      "Iteration 112, loss = 0.24148423\n",
      "Iteration 113, loss = 0.23960658\n",
      "Iteration 114, loss = 0.23779512\n",
      "Iteration 115, loss = 0.23608345\n",
      "Iteration 116, loss = 0.23440067\n",
      "Iteration 117, loss = 0.23271517\n",
      "Iteration 118, loss = 0.23110520\n",
      "Iteration 119, loss = 0.22951459\n",
      "Iteration 120, loss = 0.22805924\n",
      "Iteration 121, loss = 0.22661271\n",
      "Iteration 122, loss = 0.22511679\n",
      "Iteration 123, loss = 0.22370580\n",
      "Iteration 124, loss = 0.22242208\n",
      "Iteration 125, loss = 0.22111086\n",
      "Iteration 126, loss = 0.21981432\n",
      "Iteration 127, loss = 0.21853024\n",
      "Iteration 128, loss = 0.21734259\n",
      "Iteration 129, loss = 0.21617127\n",
      "Iteration 130, loss = 0.21499692\n",
      "Iteration 131, loss = 0.21386934\n",
      "Iteration 132, loss = 0.21280033\n",
      "Iteration 133, loss = 0.21171681\n",
      "Iteration 134, loss = 0.21065176\n",
      "Iteration 135, loss = 0.20961314\n",
      "Iteration 136, loss = 0.20864016\n",
      "Iteration 137, loss = 0.20768427\n",
      "Iteration 138, loss = 0.20684350\n",
      "Iteration 139, loss = 0.20591488\n",
      "Iteration 140, loss = 0.20492691\n",
      "Iteration 141, loss = 0.20412142\n",
      "Iteration 142, loss = 0.20334697\n",
      "Iteration 143, loss = 0.20248233\n",
      "Iteration 144, loss = 0.20164952\n",
      "Iteration 145, loss = 0.20083173\n",
      "Iteration 146, loss = 0.19999978\n",
      "Iteration 147, loss = 0.19922824\n",
      "Iteration 148, loss = 0.19850373\n",
      "Iteration 149, loss = 0.19779634\n",
      "Iteration 150, loss = 0.19707061\n",
      "Iteration 151, loss = 0.19637963\n",
      "Iteration 152, loss = 0.19564752\n",
      "Iteration 153, loss = 0.19500526\n",
      "Iteration 154, loss = 0.19434004\n",
      "Iteration 155, loss = 0.19371348\n",
      "Iteration 156, loss = 0.19307446\n",
      "Iteration 157, loss = 0.19230886\n",
      "Iteration 158, loss = 0.19172809\n",
      "Iteration 159, loss = 0.19115257\n",
      "Iteration 160, loss = 0.19051935\n",
      "Iteration 161, loss = 0.18986040\n",
      "Iteration 162, loss = 0.18920501\n",
      "Iteration 163, loss = 0.18863513\n",
      "Iteration 164, loss = 0.18814632\n",
      "Iteration 165, loss = 0.18750258\n",
      "Iteration 166, loss = 0.18688048\n",
      "Iteration 167, loss = 0.18634485\n",
      "Iteration 168, loss = 0.18583358\n",
      "Iteration 169, loss = 0.18527069\n",
      "Iteration 170, loss = 0.18470148\n",
      "Iteration 171, loss = 0.18412890\n",
      "Iteration 172, loss = 0.18358223\n",
      "Iteration 173, loss = 0.18307446\n",
      "Iteration 174, loss = 0.18253038\n",
      "Iteration 175, loss = 0.18199625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 176, loss = 0.18152342\n",
      "Iteration 177, loss = 0.18111579\n",
      "Iteration 178, loss = 0.18068406\n",
      "Iteration 179, loss = 0.18026546\n",
      "Iteration 180, loss = 0.17988721\n",
      "Iteration 181, loss = 0.17951998\n",
      "Iteration 182, loss = 0.17905216\n",
      "Iteration 183, loss = 0.17867668\n",
      "Iteration 184, loss = 0.17833787\n",
      "Iteration 185, loss = 0.17787040\n",
      "Iteration 186, loss = 0.17749493\n",
      "Iteration 187, loss = 0.17720523\n",
      "Iteration 188, loss = 0.17680050\n",
      "Iteration 189, loss = 0.17649266\n",
      "Iteration 190, loss = 0.17616648\n",
      "Iteration 191, loss = 0.17593006\n",
      "Iteration 192, loss = 0.17562409\n",
      "Iteration 193, loss = 0.17520764\n",
      "Iteration 194, loss = 0.17480198\n",
      "Iteration 195, loss = 0.17452555\n",
      "Iteration 196, loss = 0.17422082\n",
      "Iteration 197, loss = 0.17390409\n",
      "Iteration 198, loss = 0.17361248\n",
      "Iteration 199, loss = 0.17328120\n",
      "Iteration 200, loss = 0.17308927\n",
      "Iteration 201, loss = 0.17278302\n",
      "Iteration 202, loss = 0.17256550\n",
      "Iteration 203, loss = 0.17227969\n",
      "Iteration 204, loss = 0.17189591\n",
      "Iteration 205, loss = 0.17164268\n",
      "Iteration 206, loss = 0.17143812\n",
      "Iteration 207, loss = 0.17118847\n",
      "Iteration 208, loss = 0.17089973\n",
      "Iteration 209, loss = 0.17057277\n",
      "Iteration 210, loss = 0.17038810\n",
      "Iteration 211, loss = 0.17020389\n",
      "Iteration 212, loss = 0.16991964\n",
      "Iteration 213, loss = 0.16962679\n",
      "Iteration 214, loss = 0.16954995\n",
      "Iteration 215, loss = 0.16927867\n",
      "Iteration 216, loss = 0.16890545\n",
      "Iteration 217, loss = 0.16872817\n",
      "Iteration 218, loss = 0.16858220\n",
      "Iteration 219, loss = 0.16838264\n",
      "Iteration 220, loss = 0.16802646\n",
      "Iteration 221, loss = 0.16781166\n",
      "Iteration 222, loss = 0.16766641\n",
      "Iteration 223, loss = 0.16743812\n",
      "Iteration 224, loss = 0.16730703\n",
      "Iteration 225, loss = 0.16712906\n",
      "Iteration 226, loss = 0.16700858\n",
      "Iteration 227, loss = 0.16686994\n",
      "Iteration 228, loss = 0.16664727\n",
      "Iteration 229, loss = 0.16646699\n",
      "Iteration 230, loss = 0.16624586\n",
      "Iteration 231, loss = 0.16606867\n",
      "Iteration 232, loss = 0.16591877\n",
      "Iteration 233, loss = 0.16567776\n",
      "Iteration 234, loss = 0.16551524\n",
      "Iteration 235, loss = 0.16538286\n",
      "Iteration 236, loss = 0.16522373\n",
      "Iteration 237, loss = 0.16499428\n",
      "Iteration 238, loss = 0.16477423\n",
      "Iteration 239, loss = 0.16472041\n",
      "Iteration 240, loss = 0.16450869\n",
      "Iteration 241, loss = 0.16434811\n",
      "Iteration 242, loss = 0.16424797\n",
      "Iteration 243, loss = 0.16407090\n",
      "Iteration 244, loss = 0.16393680\n",
      "Iteration 245, loss = 0.16370543\n",
      "Iteration 246, loss = 0.16364706\n",
      "Iteration 247, loss = 0.16351188\n",
      "Iteration 248, loss = 0.16334136\n",
      "Iteration 249, loss = 0.16312856\n",
      "Iteration 250, loss = 0.16315045\n",
      "Iteration 251, loss = 0.16307818\n",
      "Iteration 252, loss = 0.16285873\n",
      "Iteration 253, loss = 0.16273101\n",
      "Iteration 254, loss = 0.16270689\n",
      "Iteration 255, loss = 0.16268046\n",
      "Iteration 256, loss = 0.16249139\n",
      "Iteration 257, loss = 0.16229965\n",
      "Iteration 258, loss = 0.16213803\n",
      "Iteration 259, loss = 0.16209562\n",
      "Iteration 260, loss = 0.16204044\n",
      "Iteration 261, loss = 0.16185007\n",
      "Iteration 262, loss = 0.16170159\n",
      "Iteration 263, loss = 0.16174021\n",
      "Iteration 264, loss = 0.16148711\n",
      "Iteration 265, loss = 0.16132589\n",
      "Iteration 266, loss = 0.16121277\n",
      "Iteration 267, loss = 0.16105084\n",
      "Iteration 268, loss = 0.16091855\n",
      "Iteration 269, loss = 0.16092187\n",
      "Iteration 270, loss = 0.16070058\n",
      "Iteration 271, loss = 0.16052372\n",
      "Iteration 272, loss = 0.16047888\n",
      "Iteration 273, loss = 0.16039852\n",
      "Iteration 274, loss = 0.16021266\n",
      "Iteration 275, loss = 0.16014344\n",
      "Iteration 276, loss = 0.16008025\n",
      "Iteration 277, loss = 0.15998833\n",
      "Iteration 278, loss = 0.15979132\n",
      "Iteration 279, loss = 0.15968349\n",
      "Iteration 280, loss = 0.15933471\n",
      "Iteration 281, loss = 0.15928555\n",
      "Iteration 282, loss = 0.15918738\n",
      "Iteration 283, loss = 0.15891605\n",
      "Iteration 284, loss = 0.15877286\n",
      "Iteration 285, loss = 0.15870763\n",
      "Iteration 286, loss = 0.15862100\n",
      "Iteration 287, loss = 0.15845317\n",
      "Iteration 288, loss = 0.15827007\n",
      "Iteration 289, loss = 0.15814195\n",
      "Iteration 290, loss = 0.15801809\n",
      "Iteration 291, loss = 0.15788419\n",
      "Iteration 292, loss = 0.15770102\n",
      "Iteration 293, loss = 0.15747740\n",
      "Iteration 294, loss = 0.15758132\n",
      "Iteration 295, loss = 0.15749557\n",
      "Iteration 296, loss = 0.15728609\n",
      "Iteration 297, loss = 0.15734457\n",
      "Iteration 298, loss = 0.15726197\n",
      "Iteration 299, loss = 0.15701341\n",
      "Iteration 300, loss = 0.15686822\n",
      "Iteration 301, loss = 0.15693475\n",
      "Iteration 302, loss = 0.15689024\n",
      "Iteration 303, loss = 0.15667834\n",
      "Iteration 304, loss = 0.15660259\n",
      "Iteration 305, loss = 0.15647269\n",
      "Iteration 306, loss = 0.15648193\n",
      "Iteration 307, loss = 0.15646196\n",
      "Iteration 308, loss = 0.15626355\n",
      "Iteration 309, loss = 0.15613192\n",
      "Iteration 310, loss = 0.15617874\n",
      "Iteration 311, loss = 0.15613319\n",
      "Iteration 312, loss = 0.15593203\n",
      "Iteration 313, loss = 0.15579408\n",
      "Iteration 314, loss = 0.15577531\n",
      "Iteration 315, loss = 0.15568427\n",
      "Iteration 316, loss = 0.15561457\n",
      "Iteration 317, loss = 0.15541288\n",
      "Iteration 318, loss = 0.15531348\n",
      "Iteration 319, loss = 0.15520998\n",
      "Iteration 320, loss = 0.15513885\n",
      "Iteration 321, loss = 0.15501079\n",
      "Iteration 322, loss = 0.15487656\n",
      "Iteration 323, loss = 0.15488549\n",
      "Iteration 324, loss = 0.15465055\n",
      "Iteration 325, loss = 0.15452021\n",
      "Iteration 326, loss = 0.15453807\n",
      "Iteration 327, loss = 0.15445496\n",
      "Iteration 328, loss = 0.15416414\n",
      "Iteration 329, loss = 0.15410189\n",
      "Iteration 330, loss = 0.15404753\n",
      "Iteration 331, loss = 0.15401560\n",
      "Iteration 332, loss = 0.15385070\n",
      "Iteration 333, loss = 0.15368871\n",
      "Iteration 334, loss = 0.15362820\n",
      "Iteration 335, loss = 0.15368866\n",
      "Iteration 336, loss = 0.15369400\n",
      "Iteration 337, loss = 0.15347436\n",
      "Iteration 338, loss = 0.15333059\n",
      "Iteration 339, loss = 0.15337635\n",
      "Iteration 340, loss = 0.15326722\n",
      "Iteration 341, loss = 0.15317888\n",
      "Iteration 342, loss = 0.15320708\n",
      "Iteration 343, loss = 0.15309832\n",
      "Iteration 344, loss = 0.15307198\n",
      "Iteration 345, loss = 0.15291617\n",
      "Iteration 346, loss = 0.15299325\n",
      "Iteration 347, loss = 0.15279094\n",
      "Iteration 348, loss = 0.15266029\n",
      "Iteration 349, loss = 0.15265271\n",
      "Iteration 350, loss = 0.15258555\n",
      "Iteration 351, loss = 0.15257561\n",
      "Iteration 352, loss = 0.15254177\n",
      "Iteration 353, loss = 0.15244568\n",
      "Iteration 354, loss = 0.15244917\n",
      "Iteration 355, loss = 0.15243140\n",
      "Iteration 356, loss = 0.15227026\n",
      "Iteration 357, loss = 0.15218517\n",
      "Iteration 358, loss = 0.15212608\n",
      "Iteration 359, loss = 0.15215261\n",
      "Iteration 360, loss = 0.15208624\n",
      "Iteration 361, loss = 0.15195602\n",
      "Iteration 362, loss = 0.15197737\n",
      "Iteration 363, loss = 0.15192561\n",
      "Iteration 364, loss = 0.15174373\n",
      "Iteration 365, loss = 0.15169830\n",
      "Iteration 366, loss = 0.15172702\n",
      "Iteration 367, loss = 0.15159842\n",
      "Iteration 368, loss = 0.15164086\n",
      "Iteration 369, loss = 0.15179075\n",
      "Iteration 370, loss = 0.15172002\n",
      "Iteration 371, loss = 0.15143885\n",
      "Iteration 372, loss = 0.15147801\n",
      "Iteration 373, loss = 0.15154075\n",
      "Iteration 374, loss = 0.15141120\n",
      "Iteration 375, loss = 0.15117691\n",
      "Iteration 376, loss = 0.15119258\n",
      "Iteration 377, loss = 0.15123721\n",
      "Iteration 378, loss = 0.15107560\n",
      "Iteration 379, loss = 0.15095991\n",
      "Iteration 380, loss = 0.15105004\n",
      "Iteration 381, loss = 0.15102192\n",
      "Iteration 382, loss = 0.15081956\n",
      "Iteration 383, loss = 0.15077400\n",
      "Iteration 384, loss = 0.15074626\n",
      "Iteration 385, loss = 0.15074449\n",
      "Iteration 386, loss = 0.15074832\n",
      "Iteration 387, loss = 0.15066307\n",
      "Iteration 388, loss = 0.15059328\n",
      "Iteration 389, loss = 0.15060471\n",
      "Iteration 390, loss = 0.15061652\n",
      "Iteration 391, loss = 0.15049954\n",
      "Iteration 392, loss = 0.15053544\n",
      "Iteration 393, loss = 0.15052653\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.940000\n",
      "Training set loss: 0.150527\n",
      "\n",
      "learning on dataset moons\n",
      "training: constant learning-rate\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.69564741\n",
      "Iteration 3, loss = 0.68980404\n",
      "Iteration 4, loss = 0.68433863\n",
      "Iteration 5, loss = 0.67912932\n",
      "Iteration 6, loss = 0.67414633\n",
      "Iteration 7, loss = 0.66942320\n",
      "Iteration 8, loss = 0.66485589\n",
      "Iteration 9, loss = 0.66050975\n",
      "Iteration 10, loss = 0.65622883\n",
      "Iteration 11, loss = 0.65196026\n",
      "Iteration 12, loss = 0.64772294\n",
      "Iteration 13, loss = 0.64356352\n",
      "Iteration 14, loss = 0.63946969\n",
      "Iteration 15, loss = 0.63537651\n",
      "Iteration 16, loss = 0.63129319\n",
      "Iteration 17, loss = 0.62726989\n",
      "Iteration 18, loss = 0.62324445\n",
      "Iteration 19, loss = 0.61924561\n",
      "Iteration 20, loss = 0.61525819\n",
      "Iteration 21, loss = 0.61134653\n",
      "Iteration 22, loss = 0.60748732\n",
      "Iteration 23, loss = 0.60366053\n",
      "Iteration 24, loss = 0.59984553\n",
      "Iteration 25, loss = 0.59603630\n",
      "Iteration 26, loss = 0.59223845\n",
      "Iteration 27, loss = 0.58847447\n",
      "Iteration 28, loss = 0.58473976\n",
      "Iteration 29, loss = 0.58100630\n",
      "Iteration 30, loss = 0.57727642\n",
      "Iteration 31, loss = 0.57354392\n",
      "Iteration 32, loss = 0.56982589\n",
      "Iteration 33, loss = 0.56610806\n",
      "Iteration 34, loss = 0.56238563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 0.55866608\n",
      "Iteration 36, loss = 0.55495811\n",
      "Iteration 37, loss = 0.55126928\n",
      "Iteration 38, loss = 0.54758376\n",
      "Iteration 39, loss = 0.54389468\n",
      "Iteration 40, loss = 0.54022557\n",
      "Iteration 41, loss = 0.53657988\n",
      "Iteration 42, loss = 0.53296072\n",
      "Iteration 43, loss = 0.52934749\n",
      "Iteration 44, loss = 0.52576157\n",
      "Iteration 45, loss = 0.52220770\n",
      "Iteration 46, loss = 0.51868031\n",
      "Iteration 47, loss = 0.51516223\n",
      "Iteration 48, loss = 0.51167311\n",
      "Iteration 49, loss = 0.50822951\n",
      "Iteration 50, loss = 0.50481848\n",
      "Iteration 51, loss = 0.50142213\n",
      "Iteration 52, loss = 0.49806778\n",
      "Iteration 53, loss = 0.49474885\n",
      "Iteration 54, loss = 0.49147869\n",
      "Iteration 55, loss = 0.48825265\n",
      "Iteration 56, loss = 0.48507827\n",
      "Iteration 57, loss = 0.48195294\n",
      "Iteration 58, loss = 0.47888562\n",
      "Iteration 59, loss = 0.47587722\n",
      "Iteration 60, loss = 0.47291800\n",
      "Iteration 61, loss = 0.47000215\n",
      "Iteration 62, loss = 0.46713241\n",
      "Iteration 63, loss = 0.46431064\n",
      "Iteration 64, loss = 0.46154153\n",
      "Iteration 65, loss = 0.45882134\n",
      "Iteration 66, loss = 0.45614960\n",
      "Iteration 67, loss = 0.45352633\n",
      "Iteration 68, loss = 0.45095165\n",
      "Iteration 69, loss = 0.44842560\n",
      "Iteration 70, loss = 0.44594758\n",
      "Iteration 71, loss = 0.44351868\n",
      "Iteration 72, loss = 0.44113800\n",
      "Iteration 73, loss = 0.43880463\n",
      "Iteration 74, loss = 0.43651843\n",
      "Iteration 75, loss = 0.43427971\n",
      "Iteration 76, loss = 0.43208979\n",
      "Iteration 77, loss = 0.42994564\n",
      "Iteration 78, loss = 0.42784673\n",
      "Iteration 79, loss = 0.42579230\n",
      "Iteration 80, loss = 0.42378206\n",
      "Iteration 81, loss = 0.42181628\n",
      "Iteration 82, loss = 0.41989404\n",
      "Iteration 83, loss = 0.41801443\n",
      "Iteration 84, loss = 0.41617978\n",
      "Iteration 85, loss = 0.41438736\n",
      "Iteration 86, loss = 0.41263632\n",
      "Iteration 87, loss = 0.41092683\n",
      "Iteration 88, loss = 0.40925808\n",
      "Iteration 89, loss = 0.40762829\n",
      "Iteration 90, loss = 0.40603727\n",
      "Iteration 91, loss = 0.40448387\n",
      "Iteration 92, loss = 0.40296728\n",
      "Iteration 93, loss = 0.40148697\n",
      "Iteration 94, loss = 0.40004303\n",
      "Iteration 95, loss = 0.39863419\n",
      "Iteration 96, loss = 0.39725946\n",
      "Iteration 97, loss = 0.39591813\n",
      "Iteration 98, loss = 0.39460936\n",
      "Iteration 99, loss = 0.39333313\n",
      "Iteration 100, loss = 0.39208820\n",
      "Iteration 101, loss = 0.39087428\n",
      "Iteration 102, loss = 0.38969076\n",
      "Iteration 103, loss = 0.38853614\n",
      "Iteration 104, loss = 0.38740981\n",
      "Iteration 105, loss = 0.38631116\n",
      "Iteration 106, loss = 0.38523954\n",
      "Iteration 107, loss = 0.38419424\n",
      "Iteration 108, loss = 0.38317468\n",
      "Iteration 109, loss = 0.38218029\n",
      "Iteration 110, loss = 0.38121035\n",
      "Iteration 111, loss = 0.38026450\n",
      "Iteration 112, loss = 0.37934204\n",
      "Iteration 113, loss = 0.37844216\n",
      "Iteration 114, loss = 0.37756427\n",
      "Iteration 115, loss = 0.37670786\n",
      "Iteration 116, loss = 0.37587244\n",
      "Iteration 117, loss = 0.37505722\n",
      "Iteration 118, loss = 0.37426154\n",
      "Iteration 119, loss = 0.37348498\n",
      "Iteration 120, loss = 0.37272720\n",
      "Iteration 121, loss = 0.37198768\n",
      "Iteration 122, loss = 0.37126594\n",
      "Iteration 123, loss = 0.37056150\n",
      "Iteration 124, loss = 0.36987377\n",
      "Iteration 125, loss = 0.36920223\n",
      "Iteration 126, loss = 0.36854680\n",
      "Iteration 127, loss = 0.36790689\n",
      "Iteration 128, loss = 0.36728199\n",
      "Iteration 129, loss = 0.36667183\n",
      "Iteration 130, loss = 0.36607587\n",
      "Iteration 131, loss = 0.36549381\n",
      "Iteration 132, loss = 0.36492539\n",
      "Iteration 133, loss = 0.36437006\n",
      "Iteration 134, loss = 0.36382759\n",
      "Iteration 135, loss = 0.36329744\n",
      "Iteration 136, loss = 0.36277946\n",
      "Iteration 137, loss = 0.36227340\n",
      "Iteration 138, loss = 0.36177886\n",
      "Iteration 139, loss = 0.36129553\n",
      "Iteration 140, loss = 0.36082290\n",
      "Iteration 141, loss = 0.36036072\n",
      "Iteration 142, loss = 0.35990860\n",
      "Iteration 143, loss = 0.35946641\n",
      "Iteration 144, loss = 0.35903400\n",
      "Iteration 145, loss = 0.35861215\n",
      "Iteration 146, loss = 0.35819954\n",
      "Iteration 147, loss = 0.35779598\n",
      "Iteration 148, loss = 0.35740107\n",
      "Iteration 149, loss = 0.35701472\n",
      "Iteration 150, loss = 0.35663675\n",
      "Iteration 151, loss = 0.35626687\n",
      "Iteration 152, loss = 0.35590487\n",
      "Iteration 153, loss = 0.35555063\n",
      "Iteration 154, loss = 0.35520425\n",
      "Iteration 155, loss = 0.35486504\n",
      "Iteration 156, loss = 0.35453313\n",
      "Iteration 157, loss = 0.35420812\n",
      "Iteration 158, loss = 0.35388978\n",
      "Iteration 159, loss = 0.35357837\n",
      "Iteration 160, loss = 0.35327330\n",
      "Iteration 161, loss = 0.35297454\n",
      "Iteration 162, loss = 0.35268191\n",
      "Iteration 163, loss = 0.35239522\n",
      "Iteration 164, loss = 0.35211443\n",
      "Iteration 165, loss = 0.35183924\n",
      "Iteration 166, loss = 0.35156968\n",
      "Iteration 167, loss = 0.35130560\n",
      "Iteration 168, loss = 0.35104663\n",
      "Iteration 169, loss = 0.35079291\n",
      "Iteration 170, loss = 0.35054414\n",
      "Iteration 171, loss = 0.35030014\n",
      "Iteration 172, loss = 0.35006110\n",
      "Iteration 173, loss = 0.34982644\n",
      "Iteration 174, loss = 0.34959633\n",
      "Iteration 175, loss = 0.34937081\n",
      "Iteration 176, loss = 0.34914932\n",
      "Iteration 177, loss = 0.34893225\n",
      "Iteration 178, loss = 0.34871941\n",
      "Iteration 179, loss = 0.34851080\n",
      "Iteration 180, loss = 0.34830607\n",
      "Iteration 181, loss = 0.34810531\n",
      "Iteration 182, loss = 0.34790872\n",
      "Iteration 183, loss = 0.34771552\n",
      "Iteration 184, loss = 0.34752621\n",
      "Iteration 185, loss = 0.34734026\n",
      "Iteration 186, loss = 0.34715777\n",
      "Iteration 187, loss = 0.34697871\n",
      "Iteration 188, loss = 0.34680276\n",
      "Iteration 189, loss = 0.34663016\n",
      "Iteration 190, loss = 0.34646053\n",
      "Iteration 191, loss = 0.34629414\n",
      "Iteration 192, loss = 0.34613061\n",
      "Iteration 193, loss = 0.34596991\n",
      "Iteration 194, loss = 0.34581223\n",
      "Iteration 195, loss = 0.34565714\n",
      "Iteration 196, loss = 0.34550502\n",
      "Iteration 197, loss = 0.34535543\n",
      "Iteration 198, loss = 0.34520845\n",
      "Iteration 199, loss = 0.34506400\n",
      "Iteration 200, loss = 0.34492208\n",
      "Iteration 201, loss = 0.34478275\n",
      "Iteration 202, loss = 0.34464567\n",
      "Iteration 203, loss = 0.34451116\n",
      "Iteration 204, loss = 0.34437889\n",
      "Iteration 205, loss = 0.34424876\n",
      "Iteration 206, loss = 0.34412088\n",
      "Iteration 207, loss = 0.34399506\n",
      "Iteration 208, loss = 0.34387175\n",
      "Iteration 209, loss = 0.34375029\n",
      "Iteration 210, loss = 0.34363093\n",
      "Iteration 211, loss = 0.34351352\n",
      "Iteration 212, loss = 0.34339802\n",
      "Iteration 213, loss = 0.34328450\n",
      "Iteration 214, loss = 0.34317280\n",
      "Iteration 215, loss = 0.34306301\n",
      "Iteration 216, loss = 0.34295490\n",
      "Iteration 217, loss = 0.34284862\n",
      "Iteration 218, loss = 0.34274410\n",
      "Iteration 219, loss = 0.34264124\n",
      "Iteration 220, loss = 0.34253989\n",
      "Iteration 221, loss = 0.34244004\n",
      "Iteration 222, loss = 0.34234180\n",
      "Iteration 223, loss = 0.34224511\n",
      "Iteration 224, loss = 0.34214991\n",
      "Iteration 225, loss = 0.34205621\n",
      "Iteration 226, loss = 0.34196404\n",
      "Iteration 227, loss = 0.34187322\n",
      "Iteration 228, loss = 0.34178354\n",
      "Iteration 229, loss = 0.34169507\n",
      "Iteration 230, loss = 0.34160817\n",
      "Iteration 231, loss = 0.34152258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.850000\n",
      "Training set loss: 0.341523\n",
      "training: constant with momentum\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.69564741\n",
      "Iteration 3, loss = 0.68459115\n",
      "Iteration 4, loss = 0.67087272\n",
      "Iteration 5, loss = 0.65602620\n",
      "Iteration 6, loss = 0.64019138\n",
      "Iteration 7, loss = 0.62316597\n",
      "Iteration 8, loss = 0.60471564\n",
      "Iteration 9, loss = 0.58511995\n",
      "Iteration 10, loss = 0.56475645\n",
      "Iteration 11, loss = 0.54386316\n",
      "Iteration 12, loss = 0.52252261\n",
      "Iteration 13, loss = 0.50070217\n",
      "Iteration 14, loss = 0.47869801\n",
      "Iteration 15, loss = 0.45697959\n",
      "Iteration 16, loss = 0.43638625\n",
      "Iteration 17, loss = 0.41773446\n",
      "Iteration 18, loss = 0.40124814\n",
      "Iteration 19, loss = 0.38699357\n",
      "Iteration 20, loss = 0.37507394\n",
      "Iteration 21, loss = 0.36565181\n",
      "Iteration 22, loss = 0.35862838\n",
      "Iteration 23, loss = 0.35356951\n",
      "Iteration 24, loss = 0.34996638\n",
      "Iteration 25, loss = 0.34755402\n",
      "Iteration 26, loss = 0.34618278\n",
      "Iteration 27, loss = 0.34559130\n",
      "Iteration 28, loss = 0.34541583\n",
      "Iteration 29, loss = 0.34539644\n",
      "Iteration 30, loss = 0.34545687\n",
      "Iteration 31, loss = 0.34557996\n",
      "Iteration 32, loss = 0.34567600\n",
      "Iteration 33, loss = 0.34562208\n",
      "Iteration 34, loss = 0.34538192\n",
      "Iteration 35, loss = 0.34501493\n",
      "Iteration 36, loss = 0.34457057\n",
      "Iteration 37, loss = 0.34403134\n",
      "Iteration 38, loss = 0.34337812\n",
      "Iteration 39, loss = 0.34264978\n",
      "Iteration 40, loss = 0.34191643\n",
      "Iteration 41, loss = 0.34121013\n",
      "Iteration 42, loss = 0.34052329\n",
      "Iteration 43, loss = 0.33985266\n",
      "Iteration 44, loss = 0.33923238\n",
      "Iteration 45, loss = 0.33869415\n",
      "Iteration 46, loss = 0.33823561\n",
      "Iteration 47, loss = 0.33784077\n",
      "Iteration 48, loss = 0.33750652\n",
      "Iteration 49, loss = 0.33724353\n",
      "Iteration 50, loss = 0.33704909\n",
      "Iteration 51, loss = 0.33690255\n",
      "Iteration 52, loss = 0.33678730\n",
      "Iteration 53, loss = 0.33670131\n",
      "Iteration 54, loss = 0.33664461\n",
      "Iteration 55, loss = 0.33660551\n",
      "Iteration 56, loss = 0.33656895\n",
      "Iteration 57, loss = 0.33652967\n",
      "Iteration 58, loss = 0.33649238\n",
      "Iteration 59, loss = 0.33645252\n",
      "Iteration 60, loss = 0.33640241\n",
      "Iteration 61, loss = 0.33633909\n",
      "Iteration 62, loss = 0.33626633\n",
      "Iteration 63, loss = 0.33618803\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.850000\n",
      "Training set loss: 0.336188\n",
      "training: constant with Nesterov's momentum\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.69018918\n",
      "Iteration 3, loss = 0.67569838\n",
      "Iteration 4, loss = 0.66004217\n",
      "Iteration 5, loss = 0.64350275\n",
      "Iteration 6, loss = 0.62573147\n",
      "Iteration 7, loss = 0.60671354\n",
      "Iteration 8, loss = 0.58671284\n",
      "Iteration 9, loss = 0.56591368\n",
      "Iteration 10, loss = 0.54439540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.52210639\n",
      "Iteration 12, loss = 0.49941114\n",
      "Iteration 13, loss = 0.47689248\n",
      "Iteration 14, loss = 0.45513069\n",
      "Iteration 15, loss = 0.43472066\n",
      "Iteration 16, loss = 0.41619927\n",
      "Iteration 17, loss = 0.39997015\n",
      "Iteration 18, loss = 0.38620807\n",
      "Iteration 19, loss = 0.37490509\n",
      "Iteration 20, loss = 0.36589751\n",
      "Iteration 21, loss = 0.35896386\n",
      "Iteration 22, loss = 0.35380095\n",
      "Iteration 23, loss = 0.35009348\n",
      "Iteration 24, loss = 0.34753605\n",
      "Iteration 25, loss = 0.34584704\n",
      "Iteration 26, loss = 0.34479220\n",
      "Iteration 27, loss = 0.34416466\n",
      "Iteration 28, loss = 0.34380082\n",
      "Iteration 29, loss = 0.34358053\n",
      "Iteration 30, loss = 0.34341337\n",
      "Iteration 31, loss = 0.34324059\n",
      "Iteration 32, loss = 0.34302655\n",
      "Iteration 33, loss = 0.34275101\n",
      "Iteration 34, loss = 0.34240900\n",
      "Iteration 35, loss = 0.34200392\n",
      "Iteration 36, loss = 0.34154741\n",
      "Iteration 37, loss = 0.34105399\n",
      "Iteration 38, loss = 0.34054132\n",
      "Iteration 39, loss = 0.34002318\n",
      "Iteration 40, loss = 0.33951499\n",
      "Iteration 41, loss = 0.33902781\n",
      "Iteration 42, loss = 0.33857234\n",
      "Iteration 43, loss = 0.33815629\n",
      "Iteration 44, loss = 0.33778425\n",
      "Iteration 45, loss = 0.33746270\n",
      "Iteration 46, loss = 0.33718850\n",
      "Iteration 47, loss = 0.33696180\n",
      "Iteration 48, loss = 0.33677621\n",
      "Iteration 49, loss = 0.33662665\n",
      "Iteration 50, loss = 0.33650765\n",
      "Iteration 51, loss = 0.33641390\n",
      "Iteration 52, loss = 0.33634142\n",
      "Iteration 53, loss = 0.33628367\n",
      "Iteration 54, loss = 0.33623536\n",
      "Iteration 55, loss = 0.33619271\n",
      "Iteration 56, loss = 0.33615290\n",
      "Iteration 57, loss = 0.33611230\n",
      "Iteration 58, loss = 0.33606908\n",
      "Iteration 59, loss = 0.33602216\n",
      "Iteration 60, loss = 0.33597249\n",
      "Iteration 61, loss = 0.33591920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.850000\n",
      "Training set loss: 0.335919\n",
      "training: inv-scaling learning-rate\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.69564741\n",
      "Iteration 3, loss = 0.69505668\n",
      "Iteration 4, loss = 0.69464229\n",
      "Iteration 5, loss = 0.69430424\n",
      "Iteration 6, loss = 0.69401243\n",
      "Iteration 7, loss = 0.69375178\n",
      "Iteration 8, loss = 0.69351412\n",
      "Iteration 9, loss = 0.69329505\n",
      "Iteration 10, loss = 0.69308944\n",
      "Iteration 11, loss = 0.69289538\n",
      "Iteration 12, loss = 0.69271158\n",
      "Iteration 13, loss = 0.69253729\n",
      "Iteration 14, loss = 0.69237142\n",
      "Iteration 15, loss = 0.69221255\n",
      "Iteration 16, loss = 0.69205994\n",
      "Iteration 17, loss = 0.69191286\n",
      "Iteration 18, loss = 0.69177098\n",
      "Iteration 19, loss = 0.69163326\n",
      "Iteration 20, loss = 0.69149904\n",
      "Iteration 21, loss = 0.69136818\n",
      "Iteration 22, loss = 0.69124078\n",
      "Iteration 23, loss = 0.69111682\n",
      "Iteration 24, loss = 0.69099583\n",
      "Iteration 25, loss = 0.69087762\n",
      "Iteration 26, loss = 0.69076245\n",
      "Iteration 27, loss = 0.69064977\n",
      "Iteration 28, loss = 0.69053932\n",
      "Iteration 29, loss = 0.69043105\n",
      "Iteration 30, loss = 0.69032497\n",
      "Iteration 31, loss = 0.69022108\n",
      "Iteration 32, loss = 0.69011901\n",
      "Iteration 33, loss = 0.69001872\n",
      "Iteration 34, loss = 0.68992043\n",
      "Iteration 35, loss = 0.68982373\n",
      "Iteration 36, loss = 0.68972853\n",
      "Iteration 37, loss = 0.68963477\n",
      "Iteration 38, loss = 0.68954239\n",
      "Iteration 39, loss = 0.68945138\n",
      "Iteration 40, loss = 0.68936172\n",
      "Iteration 41, loss = 0.68927328\n",
      "Iteration 42, loss = 0.68918600\n",
      "Iteration 43, loss = 0.68909984\n",
      "Iteration 44, loss = 0.68901475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.500000\n",
      "Training set loss: 0.689015\n",
      "training: inv-scaling with momentum\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.69018918\n",
      "Iteration 3, loss = 0.68449881\n",
      "Iteration 4, loss = 0.67946035\n",
      "Iteration 5, loss = 0.67497597\n",
      "Iteration 6, loss = 0.67095941\n",
      "Iteration 7, loss = 0.66734698\n",
      "Iteration 8, loss = 0.66406503\n",
      "Iteration 9, loss = 0.66110121\n",
      "Iteration 10, loss = 0.65836499\n",
      "Iteration 11, loss = 0.65581703\n",
      "Iteration 12, loss = 0.65343656\n",
      "Iteration 13, loss = 0.65122213\n",
      "Iteration 14, loss = 0.64915191\n",
      "Iteration 15, loss = 0.64720476\n",
      "Iteration 16, loss = 0.64536180\n",
      "Iteration 17, loss = 0.64361420\n",
      "Iteration 18, loss = 0.64194657\n",
      "Iteration 19, loss = 0.64034956\n",
      "Iteration 20, loss = 0.63881861\n",
      "Iteration 21, loss = 0.63735387\n",
      "Iteration 22, loss = 0.63595077\n",
      "Iteration 23, loss = 0.63460296\n",
      "Iteration 24, loss = 0.63330252\n",
      "Iteration 25, loss = 0.63205258\n",
      "Iteration 26, loss = 0.63084934\n",
      "Iteration 27, loss = 0.62969016\n",
      "Iteration 28, loss = 0.62857102\n",
      "Iteration 29, loss = 0.62749161\n",
      "Iteration 30, loss = 0.62644515\n",
      "Iteration 31, loss = 0.62543284\n",
      "Iteration 32, loss = 0.62445348\n",
      "Iteration 33, loss = 0.62350210\n",
      "Iteration 34, loss = 0.62257528\n",
      "Iteration 35, loss = 0.62167397\n",
      "Iteration 36, loss = 0.62079887\n",
      "Iteration 37, loss = 0.61994781\n",
      "Iteration 38, loss = 0.61911877\n",
      "Iteration 39, loss = 0.61830962\n",
      "Iteration 40, loss = 0.61752209\n",
      "Iteration 41, loss = 0.61675369\n",
      "Iteration 42, loss = 0.61600273\n",
      "Iteration 43, loss = 0.61526775\n",
      "Iteration 44, loss = 0.61454779\n",
      "Iteration 45, loss = 0.61384241\n",
      "Iteration 46, loss = 0.61315206\n",
      "Iteration 47, loss = 0.61247528\n",
      "Iteration 48, loss = 0.61181200\n",
      "Iteration 49, loss = 0.61116209\n",
      "Iteration 50, loss = 0.61052408\n",
      "Iteration 51, loss = 0.60989655\n",
      "Iteration 52, loss = 0.60927887\n",
      "Iteration 53, loss = 0.60867019\n",
      "Iteration 54, loss = 0.60807114\n",
      "Iteration 55, loss = 0.60748070\n",
      "Iteration 56, loss = 0.60689821\n",
      "Iteration 57, loss = 0.60632440\n",
      "Iteration 58, loss = 0.60575826\n",
      "Iteration 59, loss = 0.60519948\n",
      "Iteration 60, loss = 0.60464775\n",
      "Iteration 61, loss = 0.60410191\n",
      "Iteration 62, loss = 0.60356261\n",
      "Iteration 63, loss = 0.60302945\n",
      "Iteration 64, loss = 0.60250236\n",
      "Iteration 65, loss = 0.60198140\n",
      "Iteration 66, loss = 0.60146693\n",
      "Iteration 67, loss = 0.60095800\n",
      "Iteration 68, loss = 0.60045501\n",
      "Iteration 69, loss = 0.59995681\n",
      "Iteration 70, loss = 0.59946332\n",
      "Iteration 71, loss = 0.59897541\n",
      "Iteration 72, loss = 0.59849210\n",
      "Iteration 73, loss = 0.59801338\n",
      "Iteration 74, loss = 0.59753865\n",
      "Iteration 75, loss = 0.59706775\n",
      "Iteration 76, loss = 0.59660234\n",
      "Iteration 77, loss = 0.59614134\n",
      "Iteration 78, loss = 0.59568445\n",
      "Iteration 79, loss = 0.59523185\n",
      "Iteration 80, loss = 0.59478362\n",
      "Iteration 81, loss = 0.59433899\n",
      "Iteration 82, loss = 0.59389794\n",
      "Iteration 83, loss = 0.59346020\n",
      "Iteration 84, loss = 0.59302580\n",
      "Iteration 85, loss = 0.59259470\n",
      "Iteration 86, loss = 0.59216726\n",
      "Iteration 87, loss = 0.59174272\n",
      "Iteration 88, loss = 0.59132100\n",
      "Iteration 89, loss = 0.59090234\n",
      "Iteration 90, loss = 0.59048667\n",
      "Iteration 91, loss = 0.59007351\n",
      "Iteration 92, loss = 0.58966318\n",
      "Iteration 93, loss = 0.58925581\n",
      "Iteration 94, loss = 0.58885101\n",
      "Iteration 95, loss = 0.58844873\n",
      "Iteration 96, loss = 0.58804857\n",
      "Iteration 97, loss = 0.58765048\n",
      "Iteration 98, loss = 0.58725456\n",
      "Iteration 99, loss = 0.58686087\n",
      "Iteration 100, loss = 0.58646962\n",
      "Iteration 101, loss = 0.58608096\n",
      "Iteration 102, loss = 0.58569466\n",
      "Iteration 103, loss = 0.58531049\n",
      "Iteration 104, loss = 0.58492848\n",
      "Iteration 105, loss = 0.58454871\n",
      "Iteration 106, loss = 0.58417107\n",
      "Iteration 107, loss = 0.58379549\n",
      "Iteration 108, loss = 0.58342139\n",
      "Iteration 109, loss = 0.58304931\n",
      "Iteration 110, loss = 0.58267888\n",
      "Iteration 111, loss = 0.58231041\n",
      "Iteration 112, loss = 0.58194375\n",
      "Iteration 113, loss = 0.58157922\n",
      "Iteration 114, loss = 0.58121651\n",
      "Iteration 115, loss = 0.58085558\n",
      "Iteration 116, loss = 0.58049647\n",
      "Iteration 117, loss = 0.58013914\n",
      "Iteration 118, loss = 0.57978349\n",
      "Iteration 119, loss = 0.57942949\n",
      "Iteration 120, loss = 0.57907701\n",
      "Iteration 121, loss = 0.57872616\n",
      "Iteration 122, loss = 0.57837681\n",
      "Iteration 123, loss = 0.57802904\n",
      "Iteration 124, loss = 0.57768262\n",
      "Iteration 125, loss = 0.57733739\n",
      "Iteration 126, loss = 0.57699371\n",
      "Iteration 127, loss = 0.57665165\n",
      "Iteration 128, loss = 0.57631108\n",
      "Iteration 129, loss = 0.57597197\n",
      "Iteration 130, loss = 0.57563450\n",
      "Iteration 131, loss = 0.57529859\n",
      "Iteration 132, loss = 0.57496415\n",
      "Iteration 133, loss = 0.57463131\n",
      "Iteration 134, loss = 0.57429972\n",
      "Iteration 135, loss = 0.57396952\n",
      "Iteration 136, loss = 0.57364064\n",
      "Iteration 137, loss = 0.57331309\n",
      "Iteration 138, loss = 0.57298690\n",
      "Iteration 139, loss = 0.57266200\n",
      "Iteration 140, loss = 0.57233862\n",
      "Iteration 141, loss = 0.57201711\n",
      "Iteration 142, loss = 0.57169727\n",
      "Iteration 143, loss = 0.57137875\n",
      "Iteration 144, loss = 0.57106154\n",
      "Iteration 145, loss = 0.57074553\n",
      "Iteration 146, loss = 0.57043072\n",
      "Iteration 147, loss = 0.57011672\n",
      "Iteration 148, loss = 0.56980383\n",
      "Iteration 149, loss = 0.56949227\n",
      "Iteration 150, loss = 0.56918205\n",
      "Iteration 151, loss = 0.56887290\n",
      "Iteration 152, loss = 0.56856483\n",
      "Iteration 153, loss = 0.56825797\n",
      "Iteration 154, loss = 0.56795245\n",
      "Iteration 155, loss = 0.56764811\n",
      "Iteration 156, loss = 0.56734487\n",
      "Iteration 157, loss = 0.56704270\n",
      "Iteration 158, loss = 0.56674156\n",
      "Iteration 159, loss = 0.56644148\n",
      "Iteration 160, loss = 0.56614244\n",
      "Iteration 161, loss = 0.56584441\n",
      "Iteration 162, loss = 0.56554759\n",
      "Iteration 163, loss = 0.56525180\n",
      "Iteration 164, loss = 0.56495699\n",
      "Iteration 165, loss = 0.56466315\n",
      "Iteration 166, loss = 0.56437046\n",
      "Iteration 167, loss = 0.56407876\n",
      "Iteration 168, loss = 0.56378804\n",
      "Iteration 169, loss = 0.56349866\n",
      "Iteration 170, loss = 0.56321040\n",
      "Iteration 171, loss = 0.56292298\n",
      "Iteration 172, loss = 0.56263642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 173, loss = 0.56235091\n",
      "Iteration 174, loss = 0.56206635\n",
      "Iteration 175, loss = 0.56178265\n",
      "Iteration 176, loss = 0.56149968\n",
      "Iteration 177, loss = 0.56121761\n",
      "Iteration 178, loss = 0.56093640\n",
      "Iteration 179, loss = 0.56065602\n",
      "Iteration 180, loss = 0.56037647\n",
      "Iteration 181, loss = 0.56009784\n",
      "Iteration 182, loss = 0.55982015\n",
      "Iteration 183, loss = 0.55954321\n",
      "Iteration 184, loss = 0.55926704\n",
      "Iteration 185, loss = 0.55899185\n",
      "Iteration 186, loss = 0.55871738\n",
      "Iteration 187, loss = 0.55844377\n",
      "Iteration 188, loss = 0.55817078\n",
      "Iteration 189, loss = 0.55789882\n",
      "Iteration 190, loss = 0.55762751\n",
      "Iteration 191, loss = 0.55735682\n",
      "Iteration 192, loss = 0.55708686\n",
      "Iteration 193, loss = 0.55681765\n",
      "Iteration 194, loss = 0.55654917\n",
      "Iteration 195, loss = 0.55628147\n",
      "Iteration 196, loss = 0.55601453\n",
      "Iteration 197, loss = 0.55574832\n",
      "Iteration 198, loss = 0.55548283\n",
      "Iteration 199, loss = 0.55521805\n",
      "Iteration 200, loss = 0.55495402\n",
      "Iteration 201, loss = 0.55469074\n",
      "Iteration 202, loss = 0.55442817\n",
      "Iteration 203, loss = 0.55416592\n",
      "Iteration 204, loss = 0.55390436\n",
      "Iteration 205, loss = 0.55364359\n",
      "Iteration 206, loss = 0.55338351\n",
      "Iteration 207, loss = 0.55312413\n",
      "Iteration 208, loss = 0.55286543\n",
      "Iteration 209, loss = 0.55260743\n",
      "Iteration 210, loss = 0.55235019\n",
      "Iteration 211, loss = 0.55209365\n",
      "Iteration 212, loss = 0.55183787\n",
      "Iteration 213, loss = 0.55158286\n",
      "Iteration 214, loss = 0.55132863\n",
      "Iteration 215, loss = 0.55107501\n",
      "Iteration 216, loss = 0.55082212\n",
      "Iteration 217, loss = 0.55056998\n",
      "Iteration 218, loss = 0.55031850\n",
      "Iteration 219, loss = 0.55006797\n",
      "Iteration 220, loss = 0.54981824\n",
      "Iteration 221, loss = 0.54956912\n",
      "Iteration 222, loss = 0.54932049\n",
      "Iteration 223, loss = 0.54907250\n",
      "Iteration 224, loss = 0.54882514\n",
      "Iteration 225, loss = 0.54857839\n",
      "Iteration 226, loss = 0.54833228\n",
      "Iteration 227, loss = 0.54808673\n",
      "Iteration 228, loss = 0.54784164\n",
      "Iteration 229, loss = 0.54759711\n",
      "Iteration 230, loss = 0.54735329\n",
      "Iteration 231, loss = 0.54711004\n",
      "Iteration 232, loss = 0.54686745\n",
      "Iteration 233, loss = 0.54662547\n",
      "Iteration 234, loss = 0.54638406\n",
      "Iteration 235, loss = 0.54614323\n",
      "Iteration 236, loss = 0.54590303\n",
      "Iteration 237, loss = 0.54566343\n",
      "Iteration 238, loss = 0.54542439\n",
      "Iteration 239, loss = 0.54518598\n",
      "Iteration 240, loss = 0.54494817\n",
      "Iteration 241, loss = 0.54471091\n",
      "Iteration 242, loss = 0.54447405\n",
      "Iteration 243, loss = 0.54423756\n",
      "Iteration 244, loss = 0.54400160\n",
      "Iteration 245, loss = 0.54376622\n",
      "Iteration 246, loss = 0.54353132\n",
      "Iteration 247, loss = 0.54329686\n",
      "Iteration 248, loss = 0.54306291\n",
      "Iteration 249, loss = 0.54282946\n",
      "Iteration 250, loss = 0.54259645\n",
      "Iteration 251, loss = 0.54236395\n",
      "Iteration 252, loss = 0.54213194\n",
      "Iteration 253, loss = 0.54190044\n",
      "Iteration 254, loss = 0.54166943\n",
      "Iteration 255, loss = 0.54143890\n",
      "Iteration 256, loss = 0.54120887\n",
      "Iteration 257, loss = 0.54097934\n",
      "Iteration 258, loss = 0.54075031\n",
      "Iteration 259, loss = 0.54052178\n",
      "Iteration 260, loss = 0.54029387\n",
      "Iteration 261, loss = 0.54006668\n",
      "Iteration 262, loss = 0.53984013\n",
      "Iteration 263, loss = 0.53961410\n",
      "Iteration 264, loss = 0.53938866\n",
      "Iteration 265, loss = 0.53916376\n",
      "Iteration 266, loss = 0.53893938\n",
      "Iteration 267, loss = 0.53871549\n",
      "Iteration 268, loss = 0.53849211\n",
      "Iteration 269, loss = 0.53826922\n",
      "Iteration 270, loss = 0.53804682\n",
      "Iteration 271, loss = 0.53782479\n",
      "Iteration 272, loss = 0.53760319\n",
      "Iteration 273, loss = 0.53738216\n",
      "Iteration 274, loss = 0.53716169\n",
      "Iteration 275, loss = 0.53694174\n",
      "Iteration 276, loss = 0.53672235\n",
      "Iteration 277, loss = 0.53650333\n",
      "Iteration 278, loss = 0.53628473\n",
      "Iteration 279, loss = 0.53606661\n",
      "Iteration 280, loss = 0.53584895\n",
      "Iteration 281, loss = 0.53563192\n",
      "Iteration 282, loss = 0.53541540\n",
      "Iteration 283, loss = 0.53519934\n",
      "Iteration 284, loss = 0.53498370\n",
      "Iteration 285, loss = 0.53476851\n",
      "Iteration 286, loss = 0.53455366\n",
      "Iteration 287, loss = 0.53433918\n",
      "Iteration 288, loss = 0.53412515\n",
      "Iteration 289, loss = 0.53391164\n",
      "Iteration 290, loss = 0.53369858\n",
      "Iteration 291, loss = 0.53348610\n",
      "Iteration 292, loss = 0.53327417\n",
      "Iteration 293, loss = 0.53306268\n",
      "Iteration 294, loss = 0.53285149\n",
      "Iteration 295, loss = 0.53264069\n",
      "Iteration 296, loss = 0.53243029\n",
      "Iteration 297, loss = 0.53222031\n",
      "Iteration 298, loss = 0.53201078\n",
      "Iteration 299, loss = 0.53180178\n",
      "Iteration 300, loss = 0.53159321\n",
      "Iteration 301, loss = 0.53138507\n",
      "Iteration 302, loss = 0.53117733\n",
      "Iteration 303, loss = 0.53097001\n",
      "Iteration 304, loss = 0.53076309\n",
      "Iteration 305, loss = 0.53055657\n",
      "Iteration 306, loss = 0.53035048\n",
      "Iteration 307, loss = 0.53014487\n",
      "Iteration 308, loss = 0.52993967\n",
      "Iteration 309, loss = 0.52973488\n",
      "Iteration 310, loss = 0.52953049\n",
      "Iteration 311, loss = 0.52932636\n",
      "Iteration 312, loss = 0.52912256\n",
      "Iteration 313, loss = 0.52891890\n",
      "Iteration 314, loss = 0.52871573\n",
      "Iteration 315, loss = 0.52851294\n",
      "Iteration 316, loss = 0.52831051\n",
      "Iteration 317, loss = 0.52810846\n",
      "Iteration 318, loss = 0.52790677\n",
      "Iteration 319, loss = 0.52770545\n",
      "Iteration 320, loss = 0.52750450\n",
      "Iteration 321, loss = 0.52730393\n",
      "Iteration 322, loss = 0.52710374\n",
      "Iteration 323, loss = 0.52690392\n",
      "Iteration 324, loss = 0.52670447\n",
      "Iteration 325, loss = 0.52650535\n",
      "Iteration 326, loss = 0.52630651\n",
      "Iteration 327, loss = 0.52610804\n",
      "Iteration 328, loss = 0.52590993\n",
      "Iteration 329, loss = 0.52571221\n",
      "Iteration 330, loss = 0.52551500\n",
      "Iteration 331, loss = 0.52531816\n",
      "Iteration 332, loss = 0.52512169\n",
      "Iteration 333, loss = 0.52492558\n",
      "Iteration 334, loss = 0.52472994\n",
      "Iteration 335, loss = 0.52453476\n",
      "Iteration 336, loss = 0.52433991\n",
      "Iteration 337, loss = 0.52414531\n",
      "Iteration 338, loss = 0.52395108\n",
      "Iteration 339, loss = 0.52375722\n",
      "Iteration 340, loss = 0.52356371\n",
      "Iteration 341, loss = 0.52337061\n",
      "Iteration 342, loss = 0.52317804\n",
      "Iteration 343, loss = 0.52298584\n",
      "Iteration 344, loss = 0.52279401\n",
      "Iteration 345, loss = 0.52260263\n",
      "Iteration 346, loss = 0.52241165\n",
      "Iteration 347, loss = 0.52222105\n",
      "Iteration 348, loss = 0.52203082\n",
      "Iteration 349, loss = 0.52184094\n",
      "Iteration 350, loss = 0.52165149\n",
      "Iteration 351, loss = 0.52146243\n",
      "Iteration 352, loss = 0.52127372\n",
      "Iteration 353, loss = 0.52108538\n",
      "Iteration 354, loss = 0.52089739\n",
      "Iteration 355, loss = 0.52070975\n",
      "Iteration 356, loss = 0.52052251\n",
      "Iteration 357, loss = 0.52033565\n",
      "Iteration 358, loss = 0.52014915\n",
      "Iteration 359, loss = 0.51996300\n",
      "Iteration 360, loss = 0.51977720\n",
      "Iteration 361, loss = 0.51959172\n",
      "Iteration 362, loss = 0.51940658\n",
      "Iteration 363, loss = 0.51922178\n",
      "Iteration 364, loss = 0.51903730\n",
      "Iteration 365, loss = 0.51885319\n",
      "Iteration 366, loss = 0.51866944\n",
      "Iteration 367, loss = 0.51848601\n",
      "Iteration 368, loss = 0.51830303\n",
      "Iteration 369, loss = 0.51812035\n",
      "Iteration 370, loss = 0.51793790\n",
      "Iteration 371, loss = 0.51775576\n",
      "Iteration 372, loss = 0.51757395\n",
      "Iteration 373, loss = 0.51739246\n",
      "Iteration 374, loss = 0.51721131\n",
      "Iteration 375, loss = 0.51703047\n",
      "Iteration 376, loss = 0.51684992\n",
      "Iteration 377, loss = 0.51666951\n",
      "Iteration 378, loss = 0.51648943\n",
      "Iteration 379, loss = 0.51630965\n",
      "Iteration 380, loss = 0.51613019\n",
      "Iteration 381, loss = 0.51595103\n",
      "Iteration 382, loss = 0.51577217\n",
      "Iteration 383, loss = 0.51559349\n",
      "Iteration 384, loss = 0.51541502\n",
      "Iteration 385, loss = 0.51523683\n",
      "Iteration 386, loss = 0.51505893\n",
      "Iteration 387, loss = 0.51488132\n",
      "Iteration 388, loss = 0.51470399\n",
      "Iteration 389, loss = 0.51452695\n",
      "Iteration 390, loss = 0.51435019\n",
      "Iteration 391, loss = 0.51417359\n",
      "Iteration 392, loss = 0.51399721\n",
      "Iteration 393, loss = 0.51382110\n",
      "Iteration 394, loss = 0.51364525\n",
      "Iteration 395, loss = 0.51346969\n",
      "Iteration 396, loss = 0.51329429\n",
      "Iteration 397, loss = 0.51311909\n",
      "Iteration 398, loss = 0.51294402\n",
      "Iteration 399, loss = 0.51276921\n",
      "Iteration 400, loss = 0.51259465\n",
      "Iteration 401, loss = 0.51242035\n",
      "Iteration 402, loss = 0.51224632\n",
      "Iteration 403, loss = 0.51207255\n",
      "Iteration 404, loss = 0.51189905\n",
      "Iteration 405, loss = 0.51172582\n",
      "Iteration 406, loss = 0.51155286\n",
      "Iteration 407, loss = 0.51138017\n",
      "Iteration 408, loss = 0.51120782\n",
      "Iteration 409, loss = 0.51103578\n",
      "Iteration 410, loss = 0.51086403\n",
      "Iteration 411, loss = 0.51069260\n",
      "Iteration 412, loss = 0.51052145\n",
      "Iteration 413, loss = 0.51035059\n",
      "Iteration 414, loss = 0.51018000\n",
      "Iteration 415, loss = 0.51000970\n",
      "Iteration 416, loss = 0.50983968\n",
      "Iteration 417, loss = 0.50966994\n",
      "Iteration 418, loss = 0.50950031\n",
      "Iteration 419, loss = 0.50933086\n",
      "Iteration 420, loss = 0.50916172\n",
      "Iteration 421, loss = 0.50899287\n",
      "Iteration 422, loss = 0.50882429\n",
      "Iteration 423, loss = 0.50865610\n",
      "Iteration 424, loss = 0.50848831\n",
      "Iteration 425, loss = 0.50832080\n",
      "Iteration 426, loss = 0.50815357\n",
      "Iteration 427, loss = 0.50798682\n",
      "Iteration 428, loss = 0.50782038\n",
      "Iteration 429, loss = 0.50765441\n",
      "Iteration 430, loss = 0.50748874\n",
      "Iteration 431, loss = 0.50732336\n",
      "Iteration 432, loss = 0.50715830\n",
      "Iteration 433, loss = 0.50699353\n",
      "Iteration 434, loss = 0.50682904\n",
      "Iteration 435, loss = 0.50666484\n",
      "Iteration 436, loss = 0.50650091\n",
      "Iteration 437, loss = 0.50633726\n",
      "Iteration 438, loss = 0.50617389\n",
      "Iteration 439, loss = 0.50601079\n",
      "Iteration 440, loss = 0.50584802\n",
      "Iteration 441, loss = 0.50568562\n",
      "Iteration 442, loss = 0.50552350\n",
      "Iteration 443, loss = 0.50536165\n",
      "Iteration 444, loss = 0.50520008\n",
      "Iteration 445, loss = 0.50503877\n",
      "Iteration 446, loss = 0.50487773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 447, loss = 0.50471701\n",
      "Iteration 448, loss = 0.50455652\n",
      "Iteration 449, loss = 0.50439618\n",
      "Iteration 450, loss = 0.50423608\n",
      "Iteration 451, loss = 0.50407620\n",
      "Iteration 452, loss = 0.50391659\n",
      "Iteration 453, loss = 0.50375729\n",
      "Iteration 454, loss = 0.50359831\n",
      "Iteration 455, loss = 0.50343958\n",
      "Iteration 456, loss = 0.50328111\n",
      "Iteration 457, loss = 0.50312296\n",
      "Iteration 458, loss = 0.50296515\n",
      "Iteration 459, loss = 0.50280767\n",
      "Iteration 460, loss = 0.50265046\n",
      "Iteration 461, loss = 0.50249351\n",
      "Iteration 462, loss = 0.50233682\n",
      "Iteration 463, loss = 0.50218038\n",
      "Iteration 464, loss = 0.50202420\n",
      "Iteration 465, loss = 0.50186826\n",
      "Iteration 466, loss = 0.50171262\n",
      "Iteration 467, loss = 0.50155734\n",
      "Iteration 468, loss = 0.50140233\n",
      "Iteration 469, loss = 0.50124757\n",
      "Iteration 470, loss = 0.50109297\n",
      "Iteration 471, loss = 0.50093847\n",
      "Iteration 472, loss = 0.50078421\n",
      "Iteration 473, loss = 0.50063010\n",
      "Iteration 474, loss = 0.50047606\n",
      "Iteration 475, loss = 0.50032224\n",
      "Iteration 476, loss = 0.50016855\n",
      "Iteration 477, loss = 0.50001493\n",
      "Iteration 478, loss = 0.49986150\n",
      "Iteration 479, loss = 0.49970828\n",
      "Iteration 480, loss = 0.49955526\n",
      "Iteration 481, loss = 0.49940245\n",
      "Iteration 482, loss = 0.49924985\n",
      "Iteration 483, loss = 0.49909746\n",
      "Iteration 484, loss = 0.49894528\n",
      "Iteration 485, loss = 0.49879332\n",
      "Iteration 486, loss = 0.49864157\n",
      "Iteration 487, loss = 0.49849007\n",
      "Iteration 488, loss = 0.49833882\n",
      "Iteration 489, loss = 0.49818779\n",
      "Iteration 490, loss = 0.49803698\n",
      "Iteration 491, loss = 0.49788639\n",
      "Iteration 492, loss = 0.49773602\n",
      "Iteration 493, loss = 0.49758587\n",
      "Iteration 494, loss = 0.49743594\n",
      "Iteration 495, loss = 0.49728624\n",
      "Iteration 496, loss = 0.49713675\n",
      "Iteration 497, loss = 0.49698758\n",
      "Iteration 498, loss = 0.49683867\n",
      "Iteration 499, loss = 0.49669000\n",
      "Iteration 500, loss = 0.49654155\n",
      "Iteration 501, loss = 0.49639333\n",
      "Iteration 502, loss = 0.49624534\n",
      "Iteration 503, loss = 0.49609757\n",
      "Iteration 504, loss = 0.49595004\n",
      "Iteration 505, loss = 0.49580273\n",
      "Iteration 506, loss = 0.49565567\n",
      "Iteration 507, loss = 0.49550885\n",
      "Iteration 508, loss = 0.49536225\n",
      "Iteration 509, loss = 0.49521586\n",
      "Iteration 510, loss = 0.49506972\n",
      "Iteration 511, loss = 0.49492381\n",
      "Iteration 512, loss = 0.49477812\n",
      "Iteration 513, loss = 0.49463265\n",
      "Iteration 514, loss = 0.49448738\n",
      "Iteration 515, loss = 0.49434234\n",
      "Iteration 516, loss = 0.49419751\n",
      "Iteration 517, loss = 0.49405289\n",
      "Iteration 518, loss = 0.49390847\n",
      "Iteration 519, loss = 0.49376420\n",
      "Iteration 520, loss = 0.49362013\n",
      "Iteration 521, loss = 0.49347627\n",
      "Iteration 522, loss = 0.49333262\n",
      "Iteration 523, loss = 0.49318918\n",
      "Iteration 524, loss = 0.49304594\n",
      "Iteration 525, loss = 0.49290292\n",
      "Iteration 526, loss = 0.49276010\n",
      "Iteration 527, loss = 0.49261748\n",
      "Iteration 528, loss = 0.49247510\n",
      "Iteration 529, loss = 0.49233305\n",
      "Iteration 530, loss = 0.49219121\n",
      "Iteration 531, loss = 0.49204958\n",
      "Iteration 532, loss = 0.49190817\n",
      "Iteration 533, loss = 0.49176699\n",
      "Iteration 534, loss = 0.49162604\n",
      "Iteration 535, loss = 0.49148531\n",
      "Iteration 536, loss = 0.49134479\n",
      "Iteration 537, loss = 0.49120447\n",
      "Iteration 538, loss = 0.49106438\n",
      "Iteration 539, loss = 0.49092450\n",
      "Iteration 540, loss = 0.49078482\n",
      "Iteration 541, loss = 0.49064534\n",
      "Iteration 542, loss = 0.49050607\n",
      "Iteration 543, loss = 0.49036701\n",
      "Iteration 544, loss = 0.49022814\n",
      "Iteration 545, loss = 0.49008948\n",
      "Iteration 546, loss = 0.48995103\n",
      "Iteration 547, loss = 0.48981277\n",
      "Iteration 548, loss = 0.48967472\n",
      "Iteration 549, loss = 0.48953686\n",
      "Iteration 550, loss = 0.48939928\n",
      "Iteration 551, loss = 0.48926196\n",
      "Iteration 552, loss = 0.48912485\n",
      "Iteration 553, loss = 0.48898794\n",
      "Iteration 554, loss = 0.48885125\n",
      "Iteration 555, loss = 0.48871475\n",
      "Iteration 556, loss = 0.48857845\n",
      "Iteration 557, loss = 0.48844236\n",
      "Iteration 558, loss = 0.48830646\n",
      "Iteration 559, loss = 0.48817076\n",
      "Iteration 560, loss = 0.48803526\n",
      "Iteration 561, loss = 0.48789996\n",
      "Iteration 562, loss = 0.48776485\n",
      "Iteration 563, loss = 0.48762993\n",
      "Iteration 564, loss = 0.48749521\n",
      "Iteration 565, loss = 0.48736068\n",
      "Iteration 566, loss = 0.48722634\n",
      "Iteration 567, loss = 0.48709220\n",
      "Iteration 568, loss = 0.48695824\n",
      "Iteration 569, loss = 0.48682448\n",
      "Iteration 570, loss = 0.48669090\n",
      "Iteration 571, loss = 0.48655752\n",
      "Iteration 572, loss = 0.48642433\n",
      "Iteration 573, loss = 0.48629132\n",
      "Iteration 574, loss = 0.48615850\n",
      "Iteration 575, loss = 0.48602587\n",
      "Iteration 576, loss = 0.48589344\n",
      "Iteration 577, loss = 0.48576119\n",
      "Iteration 578, loss = 0.48562912\n",
      "Iteration 579, loss = 0.48549724\n",
      "Iteration 580, loss = 0.48536555\n",
      "Iteration 581, loss = 0.48523404\n",
      "Iteration 582, loss = 0.48510271\n",
      "Iteration 583, loss = 0.48497157\n",
      "Iteration 584, loss = 0.48484061\n",
      "Iteration 585, loss = 0.48470985\n",
      "Iteration 586, loss = 0.48457927\n",
      "Iteration 587, loss = 0.48444888\n",
      "Iteration 588, loss = 0.48431867\n",
      "Iteration 589, loss = 0.48418876\n",
      "Iteration 590, loss = 0.48405910\n",
      "Iteration 591, loss = 0.48392964\n",
      "Iteration 592, loss = 0.48380037\n",
      "Iteration 593, loss = 0.48367129\n",
      "Iteration 594, loss = 0.48354239\n",
      "Iteration 595, loss = 0.48341369\n",
      "Iteration 596, loss = 0.48328519\n",
      "Iteration 597, loss = 0.48315689\n",
      "Iteration 598, loss = 0.48302879\n",
      "Iteration 599, loss = 0.48290087\n",
      "Iteration 600, loss = 0.48277314\n",
      "Iteration 601, loss = 0.48264560\n",
      "Iteration 602, loss = 0.48251831\n",
      "Iteration 603, loss = 0.48239122\n",
      "Iteration 604, loss = 0.48226430\n",
      "Iteration 605, loss = 0.48213757\n",
      "Iteration 606, loss = 0.48201102\n",
      "Iteration 607, loss = 0.48188465\n",
      "Iteration 608, loss = 0.48175854\n",
      "Iteration 609, loss = 0.48163264\n",
      "Iteration 610, loss = 0.48150691\n",
      "Iteration 611, loss = 0.48138145\n",
      "Iteration 612, loss = 0.48125618\n",
      "Iteration 613, loss = 0.48113111\n",
      "Iteration 614, loss = 0.48100622\n",
      "Iteration 615, loss = 0.48088152\n",
      "Iteration 616, loss = 0.48075699\n",
      "Iteration 617, loss = 0.48063265\n",
      "Iteration 618, loss = 0.48050852\n",
      "Iteration 619, loss = 0.48038463\n",
      "Iteration 620, loss = 0.48026093\n",
      "Iteration 621, loss = 0.48013741\n",
      "Iteration 622, loss = 0.48001406\n",
      "Iteration 623, loss = 0.47989090\n",
      "Iteration 624, loss = 0.47976800\n",
      "Iteration 625, loss = 0.47964532\n",
      "Iteration 626, loss = 0.47952282\n",
      "Iteration 627, loss = 0.47940050\n",
      "Iteration 628, loss = 0.47927847\n",
      "Iteration 629, loss = 0.47915665\n",
      "Iteration 630, loss = 0.47903503\n",
      "Iteration 631, loss = 0.47891360\n",
      "Iteration 632, loss = 0.47879236\n",
      "Iteration 633, loss = 0.47867130\n",
      "Iteration 634, loss = 0.47855042\n",
      "Iteration 635, loss = 0.47842971\n",
      "Iteration 636, loss = 0.47830917\n",
      "Iteration 637, loss = 0.47818880\n",
      "Iteration 638, loss = 0.47806861\n",
      "Iteration 639, loss = 0.47794858\n",
      "Iteration 640, loss = 0.47782872\n",
      "Iteration 641, loss = 0.47770903\n",
      "Iteration 642, loss = 0.47758951\n",
      "Iteration 643, loss = 0.47747015\n",
      "Iteration 644, loss = 0.47735095\n",
      "Iteration 645, loss = 0.47723192\n",
      "Iteration 646, loss = 0.47711304\n",
      "Iteration 647, loss = 0.47699438\n",
      "Iteration 648, loss = 0.47687595\n",
      "Iteration 649, loss = 0.47675769\n",
      "Iteration 650, loss = 0.47663958\n",
      "Iteration 651, loss = 0.47652164\n",
      "Iteration 652, loss = 0.47640392\n",
      "Iteration 653, loss = 0.47628640\n",
      "Iteration 654, loss = 0.47616905\n",
      "Iteration 655, loss = 0.47605186\n",
      "Iteration 656, loss = 0.47593486\n",
      "Iteration 657, loss = 0.47581807\n",
      "Iteration 658, loss = 0.47570152\n",
      "Iteration 659, loss = 0.47558514\n",
      "Iteration 660, loss = 0.47546894\n",
      "Iteration 661, loss = 0.47535293\n",
      "Iteration 662, loss = 0.47523708\n",
      "Iteration 663, loss = 0.47512139\n",
      "Iteration 664, loss = 0.47500586\n",
      "Iteration 665, loss = 0.47489050\n",
      "Iteration 666, loss = 0.47477529\n",
      "Iteration 667, loss = 0.47466025\n",
      "Iteration 668, loss = 0.47454536\n",
      "Iteration 669, loss = 0.47443063\n",
      "Iteration 670, loss = 0.47431605\n",
      "Iteration 671, loss = 0.47420165\n",
      "Iteration 672, loss = 0.47408742\n",
      "Iteration 673, loss = 0.47397335\n",
      "Iteration 674, loss = 0.47385944\n",
      "Iteration 675, loss = 0.47374570\n",
      "Iteration 676, loss = 0.47363211\n",
      "Iteration 677, loss = 0.47351867\n",
      "Iteration 678, loss = 0.47340539\n",
      "Iteration 679, loss = 0.47329226\n",
      "Iteration 680, loss = 0.47317928\n",
      "Iteration 681, loss = 0.47306645\n",
      "Iteration 682, loss = 0.47295381\n",
      "Iteration 683, loss = 0.47284135\n",
      "Iteration 684, loss = 0.47272904\n",
      "Iteration 685, loss = 0.47261688\n",
      "Iteration 686, loss = 0.47250487\n",
      "Iteration 687, loss = 0.47239301\n",
      "Iteration 688, loss = 0.47228130\n",
      "Iteration 689, loss = 0.47216974\n",
      "Iteration 690, loss = 0.47205832\n",
      "Iteration 691, loss = 0.47194706\n",
      "Iteration 692, loss = 0.47183594\n",
      "Iteration 693, loss = 0.47172496\n",
      "Iteration 694, loss = 0.47161413\n",
      "Iteration 695, loss = 0.47150345\n",
      "Iteration 696, loss = 0.47139291\n",
      "Iteration 697, loss = 0.47128255\n",
      "Iteration 698, loss = 0.47117239\n",
      "Iteration 699, loss = 0.47106238\n",
      "Iteration 700, loss = 0.47095251\n",
      "Iteration 701, loss = 0.47084279\n",
      "Iteration 702, loss = 0.47073322\n",
      "Iteration 703, loss = 0.47062380\n",
      "Iteration 704, loss = 0.47051452\n",
      "Iteration 705, loss = 0.47040538\n",
      "Iteration 706, loss = 0.47029639\n",
      "Iteration 707, loss = 0.47018754\n",
      "Iteration 708, loss = 0.47007883\n",
      "Iteration 709, loss = 0.46997028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 710, loss = 0.46986186\n",
      "Iteration 711, loss = 0.46975359\n",
      "Iteration 712, loss = 0.46964546\n",
      "Iteration 713, loss = 0.46953746\n",
      "Iteration 714, loss = 0.46942961\n",
      "Iteration 715, loss = 0.46932190\n",
      "Iteration 716, loss = 0.46921432\n",
      "Iteration 717, loss = 0.46910689\n",
      "Iteration 718, loss = 0.46899959\n",
      "Iteration 719, loss = 0.46889243\n",
      "Iteration 720, loss = 0.46878541\n",
      "Iteration 721, loss = 0.46867853\n",
      "Iteration 722, loss = 0.46857178\n",
      "Iteration 723, loss = 0.46846517\n",
      "Iteration 724, loss = 0.46835870\n",
      "Iteration 725, loss = 0.46825236\n",
      "Iteration 726, loss = 0.46814616\n",
      "Iteration 727, loss = 0.46804009\n",
      "Iteration 728, loss = 0.46793416\n",
      "Iteration 729, loss = 0.46782837\n",
      "Iteration 730, loss = 0.46772274\n",
      "Iteration 731, loss = 0.46761725\n",
      "Iteration 732, loss = 0.46751186\n",
      "Iteration 733, loss = 0.46740651\n",
      "Iteration 734, loss = 0.46730128\n",
      "Iteration 735, loss = 0.46719618\n",
      "Iteration 736, loss = 0.46709120\n",
      "Iteration 737, loss = 0.46698635\n",
      "Iteration 738, loss = 0.46688162\n",
      "Iteration 739, loss = 0.46677703\n",
      "Iteration 740, loss = 0.46667255\n",
      "Iteration 741, loss = 0.46656821\n",
      "Iteration 742, loss = 0.46646399\n",
      "Iteration 743, loss = 0.46635990\n",
      "Iteration 744, loss = 0.46625595\n",
      "Iteration 745, loss = 0.46615219\n",
      "Iteration 746, loss = 0.46604863\n",
      "Iteration 747, loss = 0.46594521\n",
      "Iteration 748, loss = 0.46584192\n",
      "Iteration 749, loss = 0.46573876\n",
      "Iteration 750, loss = 0.46563574\n",
      "Iteration 751, loss = 0.46553285\n",
      "Iteration 752, loss = 0.46543010\n",
      "Iteration 753, loss = 0.46532748\n",
      "Iteration 754, loss = 0.46522498\n",
      "Iteration 755, loss = 0.46512262\n",
      "Iteration 756, loss = 0.46502039\n",
      "Iteration 757, loss = 0.46491830\n",
      "Iteration 758, loss = 0.46481634\n",
      "Iteration 759, loss = 0.46471451\n",
      "Iteration 760, loss = 0.46461282\n",
      "Iteration 761, loss = 0.46451125\n",
      "Iteration 762, loss = 0.46440981\n",
      "Iteration 763, loss = 0.46430850\n",
      "Iteration 764, loss = 0.46420732\n",
      "Iteration 765, loss = 0.46410629\n",
      "Iteration 766, loss = 0.46400539\n",
      "Iteration 767, loss = 0.46390462\n",
      "Iteration 768, loss = 0.46380397\n",
      "Iteration 769, loss = 0.46370346\n",
      "Iteration 770, loss = 0.46360312\n",
      "Iteration 771, loss = 0.46350290\n",
      "Iteration 772, loss = 0.46340280\n",
      "Iteration 773, loss = 0.46330284\n",
      "Iteration 774, loss = 0.46320300\n",
      "Iteration 775, loss = 0.46310329\n",
      "Iteration 776, loss = 0.46300371\n",
      "Iteration 777, loss = 0.46290425\n",
      "Iteration 778, loss = 0.46280491\n",
      "Iteration 779, loss = 0.46270570\n",
      "Iteration 780, loss = 0.46260662\n",
      "Iteration 781, loss = 0.46250766\n",
      "Iteration 782, loss = 0.46240882\n",
      "Iteration 783, loss = 0.46231010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.830000\n",
      "Training set loss: 0.462310\n",
      "training: inv-scaling with Nesterov's momentum\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.69564741\n",
      "Iteration 3, loss = 0.68962121\n",
      "Iteration 4, loss = 0.68410154\n",
      "Iteration 5, loss = 0.67914062\n",
      "Iteration 6, loss = 0.67470961\n",
      "Iteration 7, loss = 0.67074331\n",
      "Iteration 8, loss = 0.66718107\n",
      "Iteration 9, loss = 0.66394707\n",
      "Iteration 10, loss = 0.66103039\n",
      "Iteration 11, loss = 0.65833859\n",
      "Iteration 12, loss = 0.65583627\n",
      "Iteration 13, loss = 0.65349783\n",
      "Iteration 14, loss = 0.65132454\n",
      "Iteration 15, loss = 0.64929157\n",
      "Iteration 16, loss = 0.64737641\n",
      "Iteration 17, loss = 0.64556167\n",
      "Iteration 18, loss = 0.64383799\n",
      "Iteration 19, loss = 0.64219133\n",
      "Iteration 20, loss = 0.64060967\n",
      "Iteration 21, loss = 0.63909344\n",
      "Iteration 22, loss = 0.63763960\n",
      "Iteration 23, loss = 0.63624472\n",
      "Iteration 24, loss = 0.63490238\n",
      "Iteration 25, loss = 0.63360756\n",
      "Iteration 26, loss = 0.63235873\n",
      "Iteration 27, loss = 0.63115426\n",
      "Iteration 28, loss = 0.62999024\n",
      "Iteration 29, loss = 0.62886723\n",
      "Iteration 30, loss = 0.62777872\n",
      "Iteration 31, loss = 0.62672591\n",
      "Iteration 32, loss = 0.62570652\n",
      "Iteration 33, loss = 0.62471992\n",
      "Iteration 34, loss = 0.62376396\n",
      "Iteration 35, loss = 0.62283415\n",
      "Iteration 36, loss = 0.62192842\n",
      "Iteration 37, loss = 0.62104678\n",
      "Iteration 38, loss = 0.62018801\n",
      "Iteration 39, loss = 0.61935148\n",
      "Iteration 40, loss = 0.61853679\n",
      "Iteration 41, loss = 0.61774264\n",
      "Iteration 42, loss = 0.61696820\n",
      "Iteration 43, loss = 0.61621231\n",
      "Iteration 44, loss = 0.61547407\n",
      "Iteration 45, loss = 0.61475166\n",
      "Iteration 46, loss = 0.61404392\n",
      "Iteration 47, loss = 0.61335080\n",
      "Iteration 48, loss = 0.61267307\n",
      "Iteration 49, loss = 0.61200746\n",
      "Iteration 50, loss = 0.61135433\n",
      "Iteration 51, loss = 0.61071320\n",
      "Iteration 52, loss = 0.61008470\n",
      "Iteration 53, loss = 0.60946678\n",
      "Iteration 54, loss = 0.60885880\n",
      "Iteration 55, loss = 0.60826000\n",
      "Iteration 56, loss = 0.60766943\n",
      "Iteration 57, loss = 0.60708775\n",
      "Iteration 58, loss = 0.60651459\n",
      "Iteration 59, loss = 0.60594875\n",
      "Iteration 60, loss = 0.60539047\n",
      "Iteration 61, loss = 0.60483951\n",
      "Iteration 62, loss = 0.60429543\n",
      "Iteration 63, loss = 0.60375785\n",
      "Iteration 64, loss = 0.60322648\n",
      "Iteration 65, loss = 0.60270113\n",
      "Iteration 66, loss = 0.60218125\n",
      "Iteration 67, loss = 0.60166744\n",
      "Iteration 68, loss = 0.60115909\n",
      "Iteration 69, loss = 0.60065618\n",
      "Iteration 70, loss = 0.60015880\n",
      "Iteration 71, loss = 0.59966692\n",
      "Iteration 72, loss = 0.59917995\n",
      "Iteration 73, loss = 0.59869703\n",
      "Iteration 74, loss = 0.59821855\n",
      "Iteration 75, loss = 0.59774507\n",
      "Iteration 76, loss = 0.59727581\n",
      "Iteration 77, loss = 0.59681059\n",
      "Iteration 78, loss = 0.59634890\n",
      "Iteration 79, loss = 0.59589096\n",
      "Iteration 80, loss = 0.59543844\n",
      "Iteration 81, loss = 0.59498973\n",
      "Iteration 82, loss = 0.59454499\n",
      "Iteration 83, loss = 0.59410461\n",
      "Iteration 84, loss = 0.59366771\n",
      "Iteration 85, loss = 0.59323420\n",
      "Iteration 86, loss = 0.59280416\n",
      "Iteration 87, loss = 0.59237722\n",
      "Iteration 88, loss = 0.59195321\n",
      "Iteration 89, loss = 0.59153232\n",
      "Iteration 90, loss = 0.59111463\n",
      "Iteration 91, loss = 0.59069956\n",
      "Iteration 92, loss = 0.59028735\n",
      "Iteration 93, loss = 0.58987779\n",
      "Iteration 94, loss = 0.58947123\n",
      "Iteration 95, loss = 0.58906745\n",
      "Iteration 96, loss = 0.58866629\n",
      "Iteration 97, loss = 0.58826763\n",
      "Iteration 98, loss = 0.58787122\n",
      "Iteration 99, loss = 0.58747711\n",
      "Iteration 100, loss = 0.58708521\n",
      "Iteration 101, loss = 0.58669517\n",
      "Iteration 102, loss = 0.58630726\n",
      "Iteration 103, loss = 0.58592133\n",
      "Iteration 104, loss = 0.58553804\n",
      "Iteration 105, loss = 0.58515688\n",
      "Iteration 106, loss = 0.58477782\n",
      "Iteration 107, loss = 0.58440051\n",
      "Iteration 108, loss = 0.58402542\n",
      "Iteration 109, loss = 0.58365244\n",
      "Iteration 110, loss = 0.58328169\n",
      "Iteration 111, loss = 0.58291303\n",
      "Iteration 112, loss = 0.58254604\n",
      "Iteration 113, loss = 0.58218044\n",
      "Iteration 114, loss = 0.58181661\n",
      "Iteration 115, loss = 0.58145433\n",
      "Iteration 116, loss = 0.58109382\n",
      "Iteration 117, loss = 0.58073501\n",
      "Iteration 118, loss = 0.58037796\n",
      "Iteration 119, loss = 0.58002278\n",
      "Iteration 120, loss = 0.57966942\n",
      "Iteration 121, loss = 0.57931771\n",
      "Iteration 122, loss = 0.57896748\n",
      "Iteration 123, loss = 0.57861873\n",
      "Iteration 124, loss = 0.57827166\n",
      "Iteration 125, loss = 0.57792588\n",
      "Iteration 126, loss = 0.57758124\n",
      "Iteration 127, loss = 0.57723831\n",
      "Iteration 128, loss = 0.57689691\n",
      "Iteration 129, loss = 0.57655694\n",
      "Iteration 130, loss = 0.57621813\n",
      "Iteration 131, loss = 0.57588064\n",
      "Iteration 132, loss = 0.57554484\n",
      "Iteration 133, loss = 0.57521054\n",
      "Iteration 134, loss = 0.57487778\n",
      "Iteration 135, loss = 0.57454675\n",
      "Iteration 136, loss = 0.57421706\n",
      "Iteration 137, loss = 0.57388851\n",
      "Iteration 138, loss = 0.57356146\n",
      "Iteration 139, loss = 0.57323571\n",
      "Iteration 140, loss = 0.57291120\n",
      "Iteration 141, loss = 0.57258793\n",
      "Iteration 142, loss = 0.57226584\n",
      "Iteration 143, loss = 0.57194498\n",
      "Iteration 144, loss = 0.57162549\n",
      "Iteration 145, loss = 0.57130744\n",
      "Iteration 146, loss = 0.57099117\n",
      "Iteration 147, loss = 0.57067654\n",
      "Iteration 148, loss = 0.57036308\n",
      "Iteration 149, loss = 0.57005102\n",
      "Iteration 150, loss = 0.56973996\n",
      "Iteration 151, loss = 0.56942998\n",
      "Iteration 152, loss = 0.56912116\n",
      "Iteration 153, loss = 0.56881358\n",
      "Iteration 154, loss = 0.56850706\n",
      "Iteration 155, loss = 0.56820163\n",
      "Iteration 156, loss = 0.56789727\n",
      "Iteration 157, loss = 0.56759392\n",
      "Iteration 158, loss = 0.56729152\n",
      "Iteration 159, loss = 0.56699013\n",
      "Iteration 160, loss = 0.56668985\n",
      "Iteration 161, loss = 0.56639077\n",
      "Iteration 162, loss = 0.56609276\n",
      "Iteration 163, loss = 0.56579577\n",
      "Iteration 164, loss = 0.56549976\n",
      "Iteration 165, loss = 0.56520479\n",
      "Iteration 166, loss = 0.56491089\n",
      "Iteration 167, loss = 0.56461790\n",
      "Iteration 168, loss = 0.56432580\n",
      "Iteration 169, loss = 0.56403466\n",
      "Iteration 170, loss = 0.56374453\n",
      "Iteration 171, loss = 0.56345532\n",
      "Iteration 172, loss = 0.56316717\n",
      "Iteration 173, loss = 0.56288003\n",
      "Iteration 174, loss = 0.56259390\n",
      "Iteration 175, loss = 0.56230860\n",
      "Iteration 176, loss = 0.56202438\n",
      "Iteration 177, loss = 0.56174125\n",
      "Iteration 178, loss = 0.56145910\n",
      "Iteration 179, loss = 0.56117772\n",
      "Iteration 180, loss = 0.56089715\n",
      "Iteration 181, loss = 0.56061750\n",
      "Iteration 182, loss = 0.56033887\n",
      "Iteration 183, loss = 0.56006118\n",
      "Iteration 184, loss = 0.55978436\n",
      "Iteration 185, loss = 0.55950836\n",
      "Iteration 186, loss = 0.55923312\n",
      "Iteration 187, loss = 0.55895868\n",
      "Iteration 188, loss = 0.55868518\n",
      "Iteration 189, loss = 0.55841275\n",
      "Iteration 190, loss = 0.55814111\n",
      "Iteration 191, loss = 0.55787033\n",
      "Iteration 192, loss = 0.55760046\n",
      "Iteration 193, loss = 0.55733164\n",
      "Iteration 194, loss = 0.55706329\n",
      "Iteration 195, loss = 0.55679552\n",
      "Iteration 196, loss = 0.55652836\n",
      "Iteration 197, loss = 0.55626194\n",
      "Iteration 198, loss = 0.55599624\n",
      "Iteration 199, loss = 0.55573126\n",
      "Iteration 200, loss = 0.55546698\n",
      "Iteration 201, loss = 0.55520340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 202, loss = 0.55494052\n",
      "Iteration 203, loss = 0.55467832\n",
      "Iteration 204, loss = 0.55441682\n",
      "Iteration 205, loss = 0.55415601\n",
      "Iteration 206, loss = 0.55389598\n",
      "Iteration 207, loss = 0.55363649\n",
      "Iteration 208, loss = 0.55337780\n",
      "Iteration 209, loss = 0.55311986\n",
      "Iteration 210, loss = 0.55286257\n",
      "Iteration 211, loss = 0.55260577\n",
      "Iteration 212, loss = 0.55234965\n",
      "Iteration 213, loss = 0.55209418\n",
      "Iteration 214, loss = 0.55183935\n",
      "Iteration 215, loss = 0.55158517\n",
      "Iteration 216, loss = 0.55133163\n",
      "Iteration 217, loss = 0.55107880\n",
      "Iteration 218, loss = 0.55082660\n",
      "Iteration 219, loss = 0.55057496\n",
      "Iteration 220, loss = 0.55032404\n",
      "Iteration 221, loss = 0.55007389\n",
      "Iteration 222, loss = 0.54982450\n",
      "Iteration 223, loss = 0.54957601\n",
      "Iteration 224, loss = 0.54932825\n",
      "Iteration 225, loss = 0.54908107\n",
      "Iteration 226, loss = 0.54883441\n",
      "Iteration 227, loss = 0.54858836\n",
      "Iteration 228, loss = 0.54834292\n",
      "Iteration 229, loss = 0.54809810\n",
      "Iteration 230, loss = 0.54785390\n",
      "Iteration 231, loss = 0.54761022\n",
      "Iteration 232, loss = 0.54736708\n",
      "Iteration 233, loss = 0.54712440\n",
      "Iteration 234, loss = 0.54688233\n",
      "Iteration 235, loss = 0.54664086\n",
      "Iteration 236, loss = 0.54639999\n",
      "Iteration 237, loss = 0.54615968\n",
      "Iteration 238, loss = 0.54591993\n",
      "Iteration 239, loss = 0.54568073\n",
      "Iteration 240, loss = 0.54544213\n",
      "Iteration 241, loss = 0.54520414\n",
      "Iteration 242, loss = 0.54496670\n",
      "Iteration 243, loss = 0.54472986\n",
      "Iteration 244, loss = 0.54449356\n",
      "Iteration 245, loss = 0.54425782\n",
      "Iteration 246, loss = 0.54402255\n",
      "Iteration 247, loss = 0.54378754\n",
      "Iteration 248, loss = 0.54355303\n",
      "Iteration 249, loss = 0.54331903\n",
      "Iteration 250, loss = 0.54308540\n",
      "Iteration 251, loss = 0.54285213\n",
      "Iteration 252, loss = 0.54261934\n",
      "Iteration 253, loss = 0.54238709\n",
      "Iteration 254, loss = 0.54215537\n",
      "Iteration 255, loss = 0.54192425\n",
      "Iteration 256, loss = 0.54169362\n",
      "Iteration 257, loss = 0.54146350\n",
      "Iteration 258, loss = 0.54123395\n",
      "Iteration 259, loss = 0.54100493\n",
      "Iteration 260, loss = 0.54077642\n",
      "Iteration 261, loss = 0.54054840\n",
      "Iteration 262, loss = 0.54032088\n",
      "Iteration 263, loss = 0.54009384\n",
      "Iteration 264, loss = 0.53986736\n",
      "Iteration 265, loss = 0.53964155\n",
      "Iteration 266, loss = 0.53941646\n",
      "Iteration 267, loss = 0.53919189\n",
      "Iteration 268, loss = 0.53896783\n",
      "Iteration 269, loss = 0.53874437\n",
      "Iteration 270, loss = 0.53852147\n",
      "Iteration 271, loss = 0.53829914\n",
      "Iteration 272, loss = 0.53807734\n",
      "Iteration 273, loss = 0.53785605\n",
      "Iteration 274, loss = 0.53763527\n",
      "Iteration 275, loss = 0.53741495\n",
      "Iteration 276, loss = 0.53719502\n",
      "Iteration 277, loss = 0.53697558\n",
      "Iteration 278, loss = 0.53675661\n",
      "Iteration 279, loss = 0.53653812\n",
      "Iteration 280, loss = 0.53632012\n",
      "Iteration 281, loss = 0.53610265\n",
      "Iteration 282, loss = 0.53588552\n",
      "Iteration 283, loss = 0.53566885\n",
      "Iteration 284, loss = 0.53545266\n",
      "Iteration 285, loss = 0.53523692\n",
      "Iteration 286, loss = 0.53502184\n",
      "Iteration 287, loss = 0.53480722\n",
      "Iteration 288, loss = 0.53459313\n",
      "Iteration 289, loss = 0.53437951\n",
      "Iteration 290, loss = 0.53416638\n",
      "Iteration 291, loss = 0.53395375\n",
      "Iteration 292, loss = 0.53374158\n",
      "Iteration 293, loss = 0.53352983\n",
      "Iteration 294, loss = 0.53331852\n",
      "Iteration 295, loss = 0.53310765\n",
      "Iteration 296, loss = 0.53289714\n",
      "Iteration 297, loss = 0.53268712\n",
      "Iteration 298, loss = 0.53247756\n",
      "Iteration 299, loss = 0.53226831\n",
      "Iteration 300, loss = 0.53205944\n",
      "Iteration 301, loss = 0.53185103\n",
      "Iteration 302, loss = 0.53164304\n",
      "Iteration 303, loss = 0.53143543\n",
      "Iteration 304, loss = 0.53122822\n",
      "Iteration 305, loss = 0.53102145\n",
      "Iteration 306, loss = 0.53081509\n",
      "Iteration 307, loss = 0.53060912\n",
      "Iteration 308, loss = 0.53040354\n",
      "Iteration 309, loss = 0.53019834\n",
      "Iteration 310, loss = 0.52999357\n",
      "Iteration 311, loss = 0.52978922\n",
      "Iteration 312, loss = 0.52958537\n",
      "Iteration 313, loss = 0.52938192\n",
      "Iteration 314, loss = 0.52917887\n",
      "Iteration 315, loss = 0.52897641\n",
      "Iteration 316, loss = 0.52877422\n",
      "Iteration 317, loss = 0.52857230\n",
      "Iteration 318, loss = 0.52837056\n",
      "Iteration 319, loss = 0.52816917\n",
      "Iteration 320, loss = 0.52796814\n",
      "Iteration 321, loss = 0.52776747\n",
      "Iteration 322, loss = 0.52756716\n",
      "Iteration 323, loss = 0.52736720\n",
      "Iteration 324, loss = 0.52716762\n",
      "Iteration 325, loss = 0.52696839\n",
      "Iteration 326, loss = 0.52676953\n",
      "Iteration 327, loss = 0.52657103\n",
      "Iteration 328, loss = 0.52637288\n",
      "Iteration 329, loss = 0.52617515\n",
      "Iteration 330, loss = 0.52597773\n",
      "Iteration 331, loss = 0.52578056\n",
      "Iteration 332, loss = 0.52558375\n",
      "Iteration 333, loss = 0.52538731\n",
      "Iteration 334, loss = 0.52519122\n",
      "Iteration 335, loss = 0.52499548\n",
      "Iteration 336, loss = 0.52480009\n",
      "Iteration 337, loss = 0.52460505\n",
      "Iteration 338, loss = 0.52441036\n",
      "Iteration 339, loss = 0.52421611\n",
      "Iteration 340, loss = 0.52402239\n",
      "Iteration 341, loss = 0.52382908\n",
      "Iteration 342, loss = 0.52363596\n",
      "Iteration 343, loss = 0.52344320\n",
      "Iteration 344, loss = 0.52325078\n",
      "Iteration 345, loss = 0.52305873\n",
      "Iteration 346, loss = 0.52286704\n",
      "Iteration 347, loss = 0.52267587\n",
      "Iteration 348, loss = 0.52248506\n",
      "Iteration 349, loss = 0.52229468\n",
      "Iteration 350, loss = 0.52210468\n",
      "Iteration 351, loss = 0.52191504\n",
      "Iteration 352, loss = 0.52172576\n",
      "Iteration 353, loss = 0.52153684\n",
      "Iteration 354, loss = 0.52134827\n",
      "Iteration 355, loss = 0.52116008\n",
      "Iteration 356, loss = 0.52097233\n",
      "Iteration 357, loss = 0.52078491\n",
      "Iteration 358, loss = 0.52059783\n",
      "Iteration 359, loss = 0.52041110\n",
      "Iteration 360, loss = 0.52022477\n",
      "Iteration 361, loss = 0.52003887\n",
      "Iteration 362, loss = 0.51985333\n",
      "Iteration 363, loss = 0.51966812\n",
      "Iteration 364, loss = 0.51948325\n",
      "Iteration 365, loss = 0.51929874\n",
      "Iteration 366, loss = 0.51911457\n",
      "Iteration 367, loss = 0.51893074\n",
      "Iteration 368, loss = 0.51874723\n",
      "Iteration 369, loss = 0.51856406\n",
      "Iteration 370, loss = 0.51838121\n",
      "Iteration 371, loss = 0.51819869\n",
      "Iteration 372, loss = 0.51801649\n",
      "Iteration 373, loss = 0.51783468\n",
      "Iteration 374, loss = 0.51765325\n",
      "Iteration 375, loss = 0.51747198\n",
      "Iteration 376, loss = 0.51729103\n",
      "Iteration 377, loss = 0.51711044\n",
      "Iteration 378, loss = 0.51693017\n",
      "Iteration 379, loss = 0.51675021\n",
      "Iteration 380, loss = 0.51657056\n",
      "Iteration 381, loss = 0.51639122\n",
      "Iteration 382, loss = 0.51621207\n",
      "Iteration 383, loss = 0.51603316\n",
      "Iteration 384, loss = 0.51585455\n",
      "Iteration 385, loss = 0.51567627\n",
      "Iteration 386, loss = 0.51549828\n",
      "Iteration 387, loss = 0.51532058\n",
      "Iteration 388, loss = 0.51514316\n",
      "Iteration 389, loss = 0.51496586\n",
      "Iteration 390, loss = 0.51478885\n",
      "Iteration 391, loss = 0.51461214\n",
      "Iteration 392, loss = 0.51443570\n",
      "Iteration 393, loss = 0.51425955\n",
      "Iteration 394, loss = 0.51408368\n",
      "Iteration 395, loss = 0.51390811\n",
      "Iteration 396, loss = 0.51373279\n",
      "Iteration 397, loss = 0.51355758\n",
      "Iteration 398, loss = 0.51338263\n",
      "Iteration 399, loss = 0.51320796\n",
      "Iteration 400, loss = 0.51303355\n",
      "Iteration 401, loss = 0.51285942\n",
      "Iteration 402, loss = 0.51268552\n",
      "Iteration 403, loss = 0.51251168\n",
      "Iteration 404, loss = 0.51233805\n",
      "Iteration 405, loss = 0.51216468\n",
      "Iteration 406, loss = 0.51199156\n",
      "Iteration 407, loss = 0.51181869\n",
      "Iteration 408, loss = 0.51164609\n",
      "Iteration 409, loss = 0.51147375\n",
      "Iteration 410, loss = 0.51130167\n",
      "Iteration 411, loss = 0.51112990\n",
      "Iteration 412, loss = 0.51095842\n",
      "Iteration 413, loss = 0.51078719\n",
      "Iteration 414, loss = 0.51061627\n",
      "Iteration 415, loss = 0.51044567\n",
      "Iteration 416, loss = 0.51027536\n",
      "Iteration 417, loss = 0.51010532\n",
      "Iteration 418, loss = 0.50993556\n",
      "Iteration 419, loss = 0.50976607\n",
      "Iteration 420, loss = 0.50959689\n",
      "Iteration 421, loss = 0.50942799\n",
      "Iteration 422, loss = 0.50925939\n",
      "Iteration 423, loss = 0.50909109\n",
      "Iteration 424, loss = 0.50892290\n",
      "Iteration 425, loss = 0.50875494\n",
      "Iteration 426, loss = 0.50858725\n",
      "Iteration 427, loss = 0.50841989\n",
      "Iteration 428, loss = 0.50825280\n",
      "Iteration 429, loss = 0.50808607\n",
      "Iteration 430, loss = 0.50791969\n",
      "Iteration 431, loss = 0.50775359\n",
      "Iteration 432, loss = 0.50758777\n",
      "Iteration 433, loss = 0.50742239\n",
      "Iteration 434, loss = 0.50725732\n",
      "Iteration 435, loss = 0.50709272\n",
      "Iteration 436, loss = 0.50692841\n",
      "Iteration 437, loss = 0.50676440\n",
      "Iteration 438, loss = 0.50660068\n",
      "Iteration 439, loss = 0.50643724\n",
      "Iteration 440, loss = 0.50627408\n",
      "Iteration 441, loss = 0.50611120\n",
      "Iteration 442, loss = 0.50594859\n",
      "Iteration 443, loss = 0.50578626\n",
      "Iteration 444, loss = 0.50562419\n",
      "Iteration 445, loss = 0.50546240\n",
      "Iteration 446, loss = 0.50530092\n",
      "Iteration 447, loss = 0.50513981\n",
      "Iteration 448, loss = 0.50497897\n",
      "Iteration 449, loss = 0.50481840\n",
      "Iteration 450, loss = 0.50465808\n",
      "Iteration 451, loss = 0.50449801\n",
      "Iteration 452, loss = 0.50433820\n",
      "Iteration 453, loss = 0.50417870\n",
      "Iteration 454, loss = 0.50401944\n",
      "Iteration 455, loss = 0.50386033\n",
      "Iteration 456, loss = 0.50370150\n",
      "Iteration 457, loss = 0.50354294\n",
      "Iteration 458, loss = 0.50338463\n",
      "Iteration 459, loss = 0.50322657\n",
      "Iteration 460, loss = 0.50306879\n",
      "Iteration 461, loss = 0.50291133\n",
      "Iteration 462, loss = 0.50275413\n",
      "Iteration 463, loss = 0.50259720\n",
      "Iteration 464, loss = 0.50244058\n",
      "Iteration 465, loss = 0.50228425\n",
      "Iteration 466, loss = 0.50212824\n",
      "Iteration 467, loss = 0.50197249\n",
      "Iteration 468, loss = 0.50181703\n",
      "Iteration 469, loss = 0.50166193\n",
      "Iteration 470, loss = 0.50150709\n",
      "Iteration 471, loss = 0.50135252\n",
      "Iteration 472, loss = 0.50119822\n",
      "Iteration 473, loss = 0.50104418\n",
      "Iteration 474, loss = 0.50089040\n",
      "Iteration 475, loss = 0.50073686\n",
      "Iteration 476, loss = 0.50058354\n",
      "Iteration 477, loss = 0.50043028\n",
      "Iteration 478, loss = 0.50027725\n",
      "Iteration 479, loss = 0.50012443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 480, loss = 0.49997163\n",
      "Iteration 481, loss = 0.49981904\n",
      "Iteration 482, loss = 0.49966666\n",
      "Iteration 483, loss = 0.49951428\n",
      "Iteration 484, loss = 0.49936208\n",
      "Iteration 485, loss = 0.49921007\n",
      "Iteration 486, loss = 0.49905826\n",
      "Iteration 487, loss = 0.49890665\n",
      "Iteration 488, loss = 0.49875523\n",
      "Iteration 489, loss = 0.49860402\n",
      "Iteration 490, loss = 0.49845301\n",
      "Iteration 491, loss = 0.49830221\n",
      "Iteration 492, loss = 0.49815162\n",
      "Iteration 493, loss = 0.49800124\n",
      "Iteration 494, loss = 0.49785111\n",
      "Iteration 495, loss = 0.49770122\n",
      "Iteration 496, loss = 0.49755155\n",
      "Iteration 497, loss = 0.49740209\n",
      "Iteration 498, loss = 0.49725285\n",
      "Iteration 499, loss = 0.49710383\n",
      "Iteration 500, loss = 0.49695502\n",
      "Iteration 501, loss = 0.49680643\n",
      "Iteration 502, loss = 0.49665806\n",
      "Iteration 503, loss = 0.49650992\n",
      "Iteration 504, loss = 0.49636211\n",
      "Iteration 505, loss = 0.49621452\n",
      "Iteration 506, loss = 0.49606716\n",
      "Iteration 507, loss = 0.49592003\n",
      "Iteration 508, loss = 0.49577312\n",
      "Iteration 509, loss = 0.49562645\n",
      "Iteration 510, loss = 0.49548002\n",
      "Iteration 511, loss = 0.49533382\n",
      "Iteration 512, loss = 0.49518783\n",
      "Iteration 513, loss = 0.49504207\n",
      "Iteration 514, loss = 0.49489652\n",
      "Iteration 515, loss = 0.49475122\n",
      "Iteration 516, loss = 0.49460614\n",
      "Iteration 517, loss = 0.49446129\n",
      "Iteration 518, loss = 0.49431665\n",
      "Iteration 519, loss = 0.49417223\n",
      "Iteration 520, loss = 0.49402803\n",
      "Iteration 521, loss = 0.49388405\n",
      "Iteration 522, loss = 0.49374027\n",
      "Iteration 523, loss = 0.49359671\n",
      "Iteration 524, loss = 0.49345336\n",
      "Iteration 525, loss = 0.49331015\n",
      "Iteration 526, loss = 0.49316711\n",
      "Iteration 527, loss = 0.49302428\n",
      "Iteration 528, loss = 0.49288163\n",
      "Iteration 529, loss = 0.49273919\n",
      "Iteration 530, loss = 0.49259695\n",
      "Iteration 531, loss = 0.49245491\n",
      "Iteration 532, loss = 0.49231311\n",
      "Iteration 533, loss = 0.49217151\n",
      "Iteration 534, loss = 0.49203011\n",
      "Iteration 535, loss = 0.49188897\n",
      "Iteration 536, loss = 0.49174811\n",
      "Iteration 537, loss = 0.49160747\n",
      "Iteration 538, loss = 0.49146705\n",
      "Iteration 539, loss = 0.49132683\n",
      "Iteration 540, loss = 0.49118683\n",
      "Iteration 541, loss = 0.49104703\n",
      "Iteration 542, loss = 0.49090743\n",
      "Iteration 543, loss = 0.49076805\n",
      "Iteration 544, loss = 0.49062886\n",
      "Iteration 545, loss = 0.49048988\n",
      "Iteration 546, loss = 0.49035110\n",
      "Iteration 547, loss = 0.49021256\n",
      "Iteration 548, loss = 0.49007422\n",
      "Iteration 549, loss = 0.48993609\n",
      "Iteration 550, loss = 0.48979816\n",
      "Iteration 551, loss = 0.48966044\n",
      "Iteration 552, loss = 0.48952293\n",
      "Iteration 553, loss = 0.48938561\n",
      "Iteration 554, loss = 0.48924850\n",
      "Iteration 555, loss = 0.48911158\n",
      "Iteration 556, loss = 0.48897486\n",
      "Iteration 557, loss = 0.48883840\n",
      "Iteration 558, loss = 0.48870219\n",
      "Iteration 559, loss = 0.48856620\n",
      "Iteration 560, loss = 0.48843040\n",
      "Iteration 561, loss = 0.48829481\n",
      "Iteration 562, loss = 0.48815942\n",
      "Iteration 563, loss = 0.48802422\n",
      "Iteration 564, loss = 0.48788923\n",
      "Iteration 565, loss = 0.48775443\n",
      "Iteration 566, loss = 0.48761982\n",
      "Iteration 567, loss = 0.48748541\n",
      "Iteration 568, loss = 0.48735119\n",
      "Iteration 569, loss = 0.48721716\n",
      "Iteration 570, loss = 0.48708333\n",
      "Iteration 571, loss = 0.48694969\n",
      "Iteration 572, loss = 0.48681624\n",
      "Iteration 573, loss = 0.48668298\n",
      "Iteration 574, loss = 0.48654990\n",
      "Iteration 575, loss = 0.48641702\n",
      "Iteration 576, loss = 0.48628432\n",
      "Iteration 577, loss = 0.48615181\n",
      "Iteration 578, loss = 0.48601948\n",
      "Iteration 579, loss = 0.48588734\n",
      "Iteration 580, loss = 0.48575539\n",
      "Iteration 581, loss = 0.48562362\n",
      "Iteration 582, loss = 0.48549205\n",
      "Iteration 583, loss = 0.48536066\n",
      "Iteration 584, loss = 0.48522945\n",
      "Iteration 585, loss = 0.48509843\n",
      "Iteration 586, loss = 0.48496758\n",
      "Iteration 587, loss = 0.48483692\n",
      "Iteration 588, loss = 0.48470645\n",
      "Iteration 589, loss = 0.48457615\n",
      "Iteration 590, loss = 0.48444604\n",
      "Iteration 591, loss = 0.48431612\n",
      "Iteration 592, loss = 0.48418638\n",
      "Iteration 593, loss = 0.48405682\n",
      "Iteration 594, loss = 0.48392745\n",
      "Iteration 595, loss = 0.48379825\n",
      "Iteration 596, loss = 0.48366928\n",
      "Iteration 597, loss = 0.48354063\n",
      "Iteration 598, loss = 0.48341216\n",
      "Iteration 599, loss = 0.48328389\n",
      "Iteration 600, loss = 0.48315581\n",
      "Iteration 601, loss = 0.48302791\n",
      "Iteration 602, loss = 0.48290020\n",
      "Iteration 603, loss = 0.48277268\n",
      "Iteration 604, loss = 0.48264537\n",
      "Iteration 605, loss = 0.48251827\n",
      "Iteration 606, loss = 0.48239134\n",
      "Iteration 607, loss = 0.48226461\n",
      "Iteration 608, loss = 0.48213805\n",
      "Iteration 609, loss = 0.48201168\n",
      "Iteration 610, loss = 0.48188561\n",
      "Iteration 611, loss = 0.48175977\n",
      "Iteration 612, loss = 0.48163412\n",
      "Iteration 613, loss = 0.48150865\n",
      "Iteration 614, loss = 0.48138336\n",
      "Iteration 615, loss = 0.48125826\n",
      "Iteration 616, loss = 0.48113334\n",
      "Iteration 617, loss = 0.48100859\n",
      "Iteration 618, loss = 0.48088405\n",
      "Iteration 619, loss = 0.48075976\n",
      "Iteration 620, loss = 0.48063564\n",
      "Iteration 621, loss = 0.48051171\n",
      "Iteration 622, loss = 0.48038795\n",
      "Iteration 623, loss = 0.48026438\n",
      "Iteration 624, loss = 0.48014097\n",
      "Iteration 625, loss = 0.48001777\n",
      "Iteration 626, loss = 0.47989486\n",
      "Iteration 627, loss = 0.47977216\n",
      "Iteration 628, loss = 0.47964965\n",
      "Iteration 629, loss = 0.47952731\n",
      "Iteration 630, loss = 0.47940516\n",
      "Iteration 631, loss = 0.47928318\n",
      "Iteration 632, loss = 0.47916138\n",
      "Iteration 633, loss = 0.47903979\n",
      "Iteration 634, loss = 0.47891838\n",
      "Iteration 635, loss = 0.47879717\n",
      "Iteration 636, loss = 0.47867624\n",
      "Iteration 637, loss = 0.47855550\n",
      "Iteration 638, loss = 0.47843493\n",
      "Iteration 639, loss = 0.47831454\n",
      "Iteration 640, loss = 0.47819433\n",
      "Iteration 641, loss = 0.47807429\n",
      "Iteration 642, loss = 0.47795442\n",
      "Iteration 643, loss = 0.47783471\n",
      "Iteration 644, loss = 0.47771519\n",
      "Iteration 645, loss = 0.47759585\n",
      "Iteration 646, loss = 0.47747667\n",
      "Iteration 647, loss = 0.47735766\n",
      "Iteration 648, loss = 0.47723881\n",
      "Iteration 649, loss = 0.47712013\n",
      "Iteration 650, loss = 0.47700161\n",
      "Iteration 651, loss = 0.47688325\n",
      "Iteration 652, loss = 0.47676505\n",
      "Iteration 653, loss = 0.47664700\n",
      "Iteration 654, loss = 0.47652911\n",
      "Iteration 655, loss = 0.47641145\n",
      "Iteration 656, loss = 0.47629398\n",
      "Iteration 657, loss = 0.47617669\n",
      "Iteration 658, loss = 0.47605956\n",
      "Iteration 659, loss = 0.47594261\n",
      "Iteration 660, loss = 0.47582590\n",
      "Iteration 661, loss = 0.47570937\n",
      "Iteration 662, loss = 0.47559301\n",
      "Iteration 663, loss = 0.47547681\n",
      "Iteration 664, loss = 0.47536082\n",
      "Iteration 665, loss = 0.47524503\n",
      "Iteration 666, loss = 0.47512944\n",
      "Iteration 667, loss = 0.47501403\n",
      "Iteration 668, loss = 0.47489878\n",
      "Iteration 669, loss = 0.47478372\n",
      "Iteration 670, loss = 0.47466883\n",
      "Iteration 671, loss = 0.47455411\n",
      "Iteration 672, loss = 0.47443956\n",
      "Iteration 673, loss = 0.47432516\n",
      "Iteration 674, loss = 0.47421093\n",
      "Iteration 675, loss = 0.47409685\n",
      "Iteration 676, loss = 0.47398293\n",
      "Iteration 677, loss = 0.47386917\n",
      "Iteration 678, loss = 0.47375557\n",
      "Iteration 679, loss = 0.47364212\n",
      "Iteration 680, loss = 0.47352882\n",
      "Iteration 681, loss = 0.47341568\n",
      "Iteration 682, loss = 0.47330272\n",
      "Iteration 683, loss = 0.47318991\n",
      "Iteration 684, loss = 0.47307725\n",
      "Iteration 685, loss = 0.47296474\n",
      "Iteration 686, loss = 0.47285238\n",
      "Iteration 687, loss = 0.47274017\n",
      "Iteration 688, loss = 0.47262811\n",
      "Iteration 689, loss = 0.47251620\n",
      "Iteration 690, loss = 0.47240448\n",
      "Iteration 691, loss = 0.47229293\n",
      "Iteration 692, loss = 0.47218152\n",
      "Iteration 693, loss = 0.47207026\n",
      "Iteration 694, loss = 0.47195915\n",
      "Iteration 695, loss = 0.47184819\n",
      "Iteration 696, loss = 0.47173738\n",
      "Iteration 697, loss = 0.47162671\n",
      "Iteration 698, loss = 0.47151619\n",
      "Iteration 699, loss = 0.47140581\n",
      "Iteration 700, loss = 0.47129558\n",
      "Iteration 701, loss = 0.47118549\n",
      "Iteration 702, loss = 0.47107554\n",
      "Iteration 703, loss = 0.47096574\n",
      "Iteration 704, loss = 0.47085608\n",
      "Iteration 705, loss = 0.47074660\n",
      "Iteration 706, loss = 0.47063732\n",
      "Iteration 707, loss = 0.47052819\n",
      "Iteration 708, loss = 0.47041921\n",
      "Iteration 709, loss = 0.47031037\n",
      "Iteration 710, loss = 0.47020168\n",
      "Iteration 711, loss = 0.47009313\n",
      "Iteration 712, loss = 0.46998473\n",
      "Iteration 713, loss = 0.46987647\n",
      "Iteration 714, loss = 0.46976835\n",
      "Iteration 715, loss = 0.46966038\n",
      "Iteration 716, loss = 0.46955256\n",
      "Iteration 717, loss = 0.46944487\n",
      "Iteration 718, loss = 0.46933732\n",
      "Iteration 719, loss = 0.46922992\n",
      "Iteration 720, loss = 0.46912265\n",
      "Iteration 721, loss = 0.46901552\n",
      "Iteration 722, loss = 0.46890854\n",
      "Iteration 723, loss = 0.46880169\n",
      "Iteration 724, loss = 0.46869497\n",
      "Iteration 725, loss = 0.46858840\n",
      "Iteration 726, loss = 0.46848196\n",
      "Iteration 727, loss = 0.46837565\n",
      "Iteration 728, loss = 0.46826949\n",
      "Iteration 729, loss = 0.46816345\n",
      "Iteration 730, loss = 0.46805756\n",
      "Iteration 731, loss = 0.46795180\n",
      "Iteration 732, loss = 0.46784617\n",
      "Iteration 733, loss = 0.46774068\n",
      "Iteration 734, loss = 0.46763532\n",
      "Iteration 735, loss = 0.46753009\n",
      "Iteration 736, loss = 0.46742501\n",
      "Iteration 737, loss = 0.46732007\n",
      "Iteration 738, loss = 0.46721528\n",
      "Iteration 739, loss = 0.46711062\n",
      "Iteration 740, loss = 0.46700607\n",
      "Iteration 741, loss = 0.46690156\n",
      "Iteration 742, loss = 0.46679718\n",
      "Iteration 743, loss = 0.46669291\n",
      "Iteration 744, loss = 0.46658877\n",
      "Iteration 745, loss = 0.46648475\n",
      "Iteration 746, loss = 0.46638088\n",
      "Iteration 747, loss = 0.46627718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 748, loss = 0.46617361\n",
      "Iteration 749, loss = 0.46607018\n",
      "Iteration 750, loss = 0.46596687\n",
      "Iteration 751, loss = 0.46586369\n",
      "Iteration 752, loss = 0.46576064\n",
      "Iteration 753, loss = 0.46565778\n",
      "Iteration 754, loss = 0.46555506\n",
      "Iteration 755, loss = 0.46545247\n",
      "Iteration 756, loss = 0.46535002\n",
      "Iteration 757, loss = 0.46524769\n",
      "Iteration 758, loss = 0.46514550\n",
      "Iteration 759, loss = 0.46504344\n",
      "Iteration 760, loss = 0.46494151\n",
      "Iteration 761, loss = 0.46483971\n",
      "Iteration 762, loss = 0.46473804\n",
      "Iteration 763, loss = 0.46463649\n",
      "Iteration 764, loss = 0.46453508\n",
      "Iteration 765, loss = 0.46443379\n",
      "Iteration 766, loss = 0.46433263\n",
      "Iteration 767, loss = 0.46423160\n",
      "Iteration 768, loss = 0.46413069\n",
      "Iteration 769, loss = 0.46402991\n",
      "Iteration 770, loss = 0.46392926\n",
      "Iteration 771, loss = 0.46382874\n",
      "Iteration 772, loss = 0.46372834\n",
      "Iteration 773, loss = 0.46362807\n",
      "Iteration 774, loss = 0.46352795\n",
      "Iteration 775, loss = 0.46342795\n",
      "Iteration 776, loss = 0.46332808\n",
      "Iteration 777, loss = 0.46322834\n",
      "Iteration 778, loss = 0.46312876\n",
      "Iteration 779, loss = 0.46302930\n",
      "Iteration 780, loss = 0.46292997\n",
      "Iteration 781, loss = 0.46283076\n",
      "Iteration 782, loss = 0.46273168\n",
      "Iteration 783, loss = 0.46263273\n",
      "Iteration 784, loss = 0.46253389\n",
      "Iteration 785, loss = 0.46243519\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.830000\n",
      "Training set loss: 0.462435\n",
      "training: adam\n",
      "Iteration 1, loss = 0.70205089\n",
      "Iteration 2, loss = 0.68106102\n",
      "Iteration 3, loss = 0.66182315\n",
      "Iteration 4, loss = 0.64448290\n",
      "Iteration 5, loss = 0.62787734\n",
      "Iteration 6, loss = 0.61134223\n",
      "Iteration 7, loss = 0.59451705\n",
      "Iteration 8, loss = 0.57753243\n",
      "Iteration 9, loss = 0.56077073\n",
      "Iteration 10, loss = 0.54456220\n",
      "Iteration 11, loss = 0.52877007\n",
      "Iteration 12, loss = 0.51324851\n",
      "Iteration 13, loss = 0.49790490\n",
      "Iteration 14, loss = 0.48289293\n",
      "Iteration 15, loss = 0.46840178\n",
      "Iteration 16, loss = 0.45456744\n",
      "Iteration 17, loss = 0.44149629\n",
      "Iteration 18, loss = 0.42917531\n",
      "Iteration 19, loss = 0.41766200\n",
      "Iteration 20, loss = 0.40713847\n",
      "Iteration 21, loss = 0.39766541\n",
      "Iteration 22, loss = 0.38921958\n",
      "Iteration 23, loss = 0.38174028\n",
      "Iteration 24, loss = 0.37518050\n",
      "Iteration 25, loss = 0.36950418\n",
      "Iteration 26, loss = 0.36466924\n",
      "Iteration 27, loss = 0.36059261\n",
      "Iteration 28, loss = 0.35718415\n",
      "Iteration 29, loss = 0.35435243\n",
      "Iteration 30, loss = 0.35201517\n",
      "Iteration 31, loss = 0.35009869\n",
      "Iteration 32, loss = 0.34848705\n",
      "Iteration 33, loss = 0.34711121\n",
      "Iteration 34, loss = 0.34594989\n",
      "Iteration 35, loss = 0.34494737\n",
      "Iteration 36, loss = 0.34404608\n",
      "Iteration 37, loss = 0.34320319\n",
      "Iteration 38, loss = 0.34241979\n",
      "Iteration 39, loss = 0.34168580\n",
      "Iteration 40, loss = 0.34096642\n",
      "Iteration 41, loss = 0.34027263\n",
      "Iteration 42, loss = 0.33961287\n",
      "Iteration 43, loss = 0.33898148\n",
      "Iteration 44, loss = 0.33838072\n",
      "Iteration 45, loss = 0.33781954\n",
      "Iteration 46, loss = 0.33728108\n",
      "Iteration 47, loss = 0.33680752\n",
      "Iteration 48, loss = 0.33637796\n",
      "Iteration 49, loss = 0.33598823\n",
      "Iteration 50, loss = 0.33565366\n",
      "Iteration 51, loss = 0.33538737\n",
      "Iteration 52, loss = 0.33513895\n",
      "Iteration 53, loss = 0.33494621\n",
      "Iteration 54, loss = 0.33479678\n",
      "Iteration 55, loss = 0.33467545\n",
      "Iteration 56, loss = 0.33457670\n",
      "Iteration 57, loss = 0.33446850\n",
      "Iteration 58, loss = 0.33440244\n",
      "Iteration 59, loss = 0.33432882\n",
      "Iteration 60, loss = 0.33424262\n",
      "Iteration 61, loss = 0.33413614\n",
      "Iteration 62, loss = 0.33403104\n",
      "Iteration 63, loss = 0.33389994\n",
      "Iteration 64, loss = 0.33375410\n",
      "Iteration 65, loss = 0.33358515\n",
      "Iteration 66, loss = 0.33339739\n",
      "Iteration 67, loss = 0.33318531\n",
      "Iteration 68, loss = 0.33296235\n",
      "Iteration 69, loss = 0.33271561\n",
      "Iteration 70, loss = 0.33246511\n",
      "Iteration 71, loss = 0.33220658\n",
      "Iteration 72, loss = 0.33193758\n",
      "Iteration 73, loss = 0.33163835\n",
      "Iteration 74, loss = 0.33136625\n",
      "Iteration 75, loss = 0.33107465\n",
      "Iteration 76, loss = 0.33076836\n",
      "Iteration 77, loss = 0.33046313\n",
      "Iteration 78, loss = 0.33015092\n",
      "Iteration 79, loss = 0.32983391\n",
      "Iteration 80, loss = 0.32953166\n",
      "Iteration 81, loss = 0.32920634\n",
      "Iteration 82, loss = 0.32886338\n",
      "Iteration 83, loss = 0.32853861\n",
      "Iteration 84, loss = 0.32818711\n",
      "Iteration 85, loss = 0.32782762\n",
      "Iteration 86, loss = 0.32747796\n",
      "Iteration 87, loss = 0.32711024\n",
      "Iteration 88, loss = 0.32670376\n",
      "Iteration 89, loss = 0.32633913\n",
      "Iteration 90, loss = 0.32594329\n",
      "Iteration 91, loss = 0.32551346\n",
      "Iteration 92, loss = 0.32508099\n",
      "Iteration 93, loss = 0.32464489\n",
      "Iteration 94, loss = 0.32421735\n",
      "Iteration 95, loss = 0.32375698\n",
      "Iteration 96, loss = 0.32331006\n",
      "Iteration 97, loss = 0.32285890\n",
      "Iteration 98, loss = 0.32239123\n",
      "Iteration 99, loss = 0.32191958\n",
      "Iteration 100, loss = 0.32143744\n",
      "Iteration 101, loss = 0.32092783\n",
      "Iteration 102, loss = 0.32044167\n",
      "Iteration 103, loss = 0.31992657\n",
      "Iteration 104, loss = 0.31940652\n",
      "Iteration 105, loss = 0.31888343\n",
      "Iteration 106, loss = 0.31837019\n",
      "Iteration 107, loss = 0.31782398\n",
      "Iteration 108, loss = 0.31730417\n",
      "Iteration 109, loss = 0.31676623\n",
      "Iteration 110, loss = 0.31621686\n",
      "Iteration 111, loss = 0.31568013\n",
      "Iteration 112, loss = 0.31510787\n",
      "Iteration 113, loss = 0.31452934\n",
      "Iteration 114, loss = 0.31398626\n",
      "Iteration 115, loss = 0.31340610\n",
      "Iteration 116, loss = 0.31280496\n",
      "Iteration 117, loss = 0.31226205\n",
      "Iteration 118, loss = 0.31167575\n",
      "Iteration 119, loss = 0.31107725\n",
      "Iteration 120, loss = 0.31047811\n",
      "Iteration 121, loss = 0.30992243\n",
      "Iteration 122, loss = 0.30932603\n",
      "Iteration 123, loss = 0.30868689\n",
      "Iteration 124, loss = 0.30812768\n",
      "Iteration 125, loss = 0.30754240\n",
      "Iteration 126, loss = 0.30689195\n",
      "Iteration 127, loss = 0.30629549\n",
      "Iteration 128, loss = 0.30572876\n",
      "Iteration 129, loss = 0.30508611\n",
      "Iteration 130, loss = 0.30451731\n",
      "Iteration 131, loss = 0.30393184\n",
      "Iteration 132, loss = 0.30331231\n",
      "Iteration 133, loss = 0.30273305\n",
      "Iteration 134, loss = 0.30214459\n",
      "Iteration 135, loss = 0.30154584\n",
      "Iteration 136, loss = 0.30096520\n",
      "Iteration 137, loss = 0.30034795\n",
      "Iteration 138, loss = 0.29971214\n",
      "Iteration 139, loss = 0.29912353\n",
      "Iteration 140, loss = 0.29854480\n",
      "Iteration 141, loss = 0.29792588\n",
      "Iteration 142, loss = 0.29734779\n",
      "Iteration 143, loss = 0.29672453\n",
      "Iteration 144, loss = 0.29616375\n",
      "Iteration 145, loss = 0.29554531\n",
      "Iteration 146, loss = 0.29498765\n",
      "Iteration 147, loss = 0.29438345\n",
      "Iteration 148, loss = 0.29381684\n",
      "Iteration 149, loss = 0.29324904\n",
      "Iteration 150, loss = 0.29262852\n",
      "Iteration 151, loss = 0.29204227\n",
      "Iteration 152, loss = 0.29150410\n",
      "Iteration 153, loss = 0.29093672\n",
      "Iteration 154, loss = 0.29032202\n",
      "Iteration 155, loss = 0.28967352\n",
      "Iteration 156, loss = 0.28914841\n",
      "Iteration 157, loss = 0.28862377\n",
      "Iteration 158, loss = 0.28802386\n",
      "Iteration 159, loss = 0.28738939\n",
      "Iteration 160, loss = 0.28680421\n",
      "Iteration 161, loss = 0.28625078\n",
      "Iteration 162, loss = 0.28566011\n",
      "Iteration 163, loss = 0.28516522\n",
      "Iteration 164, loss = 0.28459941\n",
      "Iteration 165, loss = 0.28397617\n",
      "Iteration 166, loss = 0.28334586\n",
      "Iteration 167, loss = 0.28281704\n",
      "Iteration 168, loss = 0.28229347\n",
      "Iteration 169, loss = 0.28169962\n",
      "Iteration 170, loss = 0.28111462\n",
      "Iteration 171, loss = 0.28053396\n",
      "Iteration 172, loss = 0.27988434\n",
      "Iteration 173, loss = 0.27933167\n",
      "Iteration 174, loss = 0.27879728\n",
      "Iteration 175, loss = 0.27820285\n",
      "Iteration 176, loss = 0.27758827\n",
      "Iteration 177, loss = 0.27701117\n",
      "Iteration 178, loss = 0.27645986\n",
      "Iteration 179, loss = 0.27590075\n",
      "Iteration 180, loss = 0.27530162\n",
      "Iteration 181, loss = 0.27468103\n",
      "Iteration 182, loss = 0.27408370\n",
      "Iteration 183, loss = 0.27360496\n",
      "Iteration 184, loss = 0.27297710\n",
      "Iteration 185, loss = 0.27246666\n",
      "Iteration 186, loss = 0.27182307\n",
      "Iteration 187, loss = 0.27127458\n",
      "Iteration 188, loss = 0.27074470\n",
      "Iteration 189, loss = 0.27010247\n",
      "Iteration 190, loss = 0.26951256\n",
      "Iteration 191, loss = 0.26892960\n",
      "Iteration 192, loss = 0.26832284\n",
      "Iteration 193, loss = 0.26775259\n",
      "Iteration 194, loss = 0.26711712\n",
      "Iteration 195, loss = 0.26656496\n",
      "Iteration 196, loss = 0.26594466\n",
      "Iteration 197, loss = 0.26542074\n",
      "Iteration 198, loss = 0.26482027\n",
      "Iteration 199, loss = 0.26415016\n",
      "Iteration 200, loss = 0.26352350\n",
      "Iteration 201, loss = 0.26292460\n",
      "Iteration 202, loss = 0.26231991\n",
      "Iteration 203, loss = 0.26172159\n",
      "Iteration 204, loss = 0.26110581\n",
      "Iteration 205, loss = 0.26054166\n",
      "Iteration 206, loss = 0.25990907\n",
      "Iteration 207, loss = 0.25928567\n",
      "Iteration 208, loss = 0.25869000\n",
      "Iteration 209, loss = 0.25807948\n",
      "Iteration 210, loss = 0.25746593\n",
      "Iteration 211, loss = 0.25689963\n",
      "Iteration 212, loss = 0.25633573\n",
      "Iteration 213, loss = 0.25568074\n",
      "Iteration 214, loss = 0.25500922\n",
      "Iteration 215, loss = 0.25446965\n",
      "Iteration 216, loss = 0.25382962\n",
      "Iteration 217, loss = 0.25323343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 218, loss = 0.25260743\n",
      "Iteration 219, loss = 0.25198806\n",
      "Iteration 220, loss = 0.25136822\n",
      "Iteration 221, loss = 0.25080349\n",
      "Iteration 222, loss = 0.25022441\n",
      "Iteration 223, loss = 0.24956934\n",
      "Iteration 224, loss = 0.24891779\n",
      "Iteration 225, loss = 0.24831614\n",
      "Iteration 226, loss = 0.24773137\n",
      "Iteration 227, loss = 0.24710182\n",
      "Iteration 228, loss = 0.24650446\n",
      "Iteration 229, loss = 0.24595941\n",
      "Iteration 230, loss = 0.24539008\n",
      "Iteration 231, loss = 0.24469075\n",
      "Iteration 232, loss = 0.24403913\n",
      "Iteration 233, loss = 0.24354180\n",
      "Iteration 234, loss = 0.24298885\n",
      "Iteration 235, loss = 0.24235997\n",
      "Iteration 236, loss = 0.24167101\n",
      "Iteration 237, loss = 0.24103968\n",
      "Iteration 238, loss = 0.24048915\n",
      "Iteration 239, loss = 0.23985274\n",
      "Iteration 240, loss = 0.23919244\n",
      "Iteration 241, loss = 0.23862566\n",
      "Iteration 242, loss = 0.23811825\n",
      "Iteration 243, loss = 0.23749388\n",
      "Iteration 244, loss = 0.23692177\n",
      "Iteration 245, loss = 0.23630636\n",
      "Iteration 246, loss = 0.23565192\n",
      "Iteration 247, loss = 0.23510214\n",
      "Iteration 248, loss = 0.23457556\n",
      "Iteration 249, loss = 0.23393911\n",
      "Iteration 250, loss = 0.23329366\n",
      "Iteration 251, loss = 0.23275499\n",
      "Iteration 252, loss = 0.23221235\n",
      "Iteration 253, loss = 0.23159961\n",
      "Iteration 254, loss = 0.23093758\n",
      "Iteration 255, loss = 0.23033375\n",
      "Iteration 256, loss = 0.22983410\n",
      "Iteration 257, loss = 0.22925146\n",
      "Iteration 258, loss = 0.22864393\n",
      "Iteration 259, loss = 0.22814676\n",
      "Iteration 260, loss = 0.22756800\n",
      "Iteration 261, loss = 0.22695855\n",
      "Iteration 262, loss = 0.22647510\n",
      "Iteration 263, loss = 0.22591574\n",
      "Iteration 264, loss = 0.22534767\n",
      "Iteration 265, loss = 0.22472516\n",
      "Iteration 266, loss = 0.22422919\n",
      "Iteration 267, loss = 0.22361821\n",
      "Iteration 268, loss = 0.22315183\n",
      "Iteration 269, loss = 0.22249612\n",
      "Iteration 270, loss = 0.22203278\n",
      "Iteration 271, loss = 0.22144425\n",
      "Iteration 272, loss = 0.22095783\n",
      "Iteration 273, loss = 0.22042056\n",
      "Iteration 274, loss = 0.21985320\n",
      "Iteration 275, loss = 0.21941662\n",
      "Iteration 276, loss = 0.21888025\n",
      "Iteration 277, loss = 0.21840756\n",
      "Iteration 278, loss = 0.21781169\n",
      "Iteration 279, loss = 0.21731284\n",
      "Iteration 280, loss = 0.21678988\n",
      "Iteration 281, loss = 0.21632406\n",
      "Iteration 282, loss = 0.21576703\n",
      "Iteration 283, loss = 0.21532653\n",
      "Iteration 284, loss = 0.21475927\n",
      "Iteration 285, loss = 0.21431990\n",
      "Iteration 286, loss = 0.21379597\n",
      "Iteration 287, loss = 0.21329747\n",
      "Iteration 288, loss = 0.21280547\n",
      "Iteration 289, loss = 0.21232848\n",
      "Iteration 290, loss = 0.21185813\n",
      "Iteration 291, loss = 0.21129245\n",
      "Iteration 292, loss = 0.21088415\n",
      "Iteration 293, loss = 0.21042937\n",
      "Iteration 294, loss = 0.20988951\n",
      "Iteration 295, loss = 0.20946492\n",
      "Iteration 296, loss = 0.20902786\n",
      "Iteration 297, loss = 0.20851935\n",
      "Iteration 298, loss = 0.20800161\n",
      "Iteration 299, loss = 0.20756074\n",
      "Iteration 300, loss = 0.20704131\n",
      "Iteration 301, loss = 0.20660872\n",
      "Iteration 302, loss = 0.20614241\n",
      "Iteration 303, loss = 0.20571052\n",
      "Iteration 304, loss = 0.20526414\n",
      "Iteration 305, loss = 0.20480375\n",
      "Iteration 306, loss = 0.20435712\n",
      "Iteration 307, loss = 0.20393085\n",
      "Iteration 308, loss = 0.20343335\n",
      "Iteration 309, loss = 0.20298950\n",
      "Iteration 310, loss = 0.20258710\n",
      "Iteration 311, loss = 0.20212209\n",
      "Iteration 312, loss = 0.20173781\n",
      "Iteration 313, loss = 0.20130726\n",
      "Iteration 314, loss = 0.20086012\n",
      "Iteration 315, loss = 0.20047758\n",
      "Iteration 316, loss = 0.20008396\n",
      "Iteration 317, loss = 0.19963582\n",
      "Iteration 318, loss = 0.19913018\n",
      "Iteration 319, loss = 0.19873501\n",
      "Iteration 320, loss = 0.19830750\n",
      "Iteration 321, loss = 0.19789338\n",
      "Iteration 322, loss = 0.19751747\n",
      "Iteration 323, loss = 0.19708615\n",
      "Iteration 324, loss = 0.19665571\n",
      "Iteration 325, loss = 0.19628658\n",
      "Iteration 326, loss = 0.19585815\n",
      "Iteration 327, loss = 0.19550631\n",
      "Iteration 328, loss = 0.19506295\n",
      "Iteration 329, loss = 0.19475687\n",
      "Iteration 330, loss = 0.19431619\n",
      "Iteration 331, loss = 0.19391536\n",
      "Iteration 332, loss = 0.19347793\n",
      "Iteration 333, loss = 0.19315229\n",
      "Iteration 334, loss = 0.19269341\n",
      "Iteration 335, loss = 0.19242925\n",
      "Iteration 336, loss = 0.19201804\n",
      "Iteration 337, loss = 0.19164852\n",
      "Iteration 338, loss = 0.19125227\n",
      "Iteration 339, loss = 0.19095825\n",
      "Iteration 340, loss = 0.19047884\n",
      "Iteration 341, loss = 0.19017629\n",
      "Iteration 342, loss = 0.18981108\n",
      "Iteration 343, loss = 0.18944326\n",
      "Iteration 344, loss = 0.18898744\n",
      "Iteration 345, loss = 0.18874145\n",
      "Iteration 346, loss = 0.18838836\n",
      "Iteration 347, loss = 0.18801830\n",
      "Iteration 348, loss = 0.18753749\n",
      "Iteration 349, loss = 0.18729181\n",
      "Iteration 350, loss = 0.18698950\n",
      "Iteration 351, loss = 0.18663479\n",
      "Iteration 352, loss = 0.18621270\n",
      "Iteration 353, loss = 0.18578538\n",
      "Iteration 354, loss = 0.18556343\n",
      "Iteration 355, loss = 0.18529295\n",
      "Iteration 356, loss = 0.18489918\n",
      "Iteration 357, loss = 0.18443110\n",
      "Iteration 358, loss = 0.18413095\n",
      "Iteration 359, loss = 0.18386715\n",
      "Iteration 360, loss = 0.18354556\n",
      "Iteration 361, loss = 0.18318440\n",
      "Iteration 362, loss = 0.18275519\n",
      "Iteration 363, loss = 0.18237190\n",
      "Iteration 364, loss = 0.18216784\n",
      "Iteration 365, loss = 0.18189757\n",
      "Iteration 366, loss = 0.18148136\n",
      "Iteration 367, loss = 0.18100461\n",
      "Iteration 368, loss = 0.18075581\n",
      "Iteration 369, loss = 0.18055929\n",
      "Iteration 370, loss = 0.18019775\n",
      "Iteration 371, loss = 0.17982249\n",
      "Iteration 372, loss = 0.17936421\n",
      "Iteration 373, loss = 0.17887804\n",
      "Iteration 374, loss = 0.17862517\n",
      "Iteration 375, loss = 0.17843540\n",
      "Iteration 376, loss = 0.17793605\n",
      "Iteration 377, loss = 0.17741067\n",
      "Iteration 378, loss = 0.17704248\n",
      "Iteration 379, loss = 0.17685422\n",
      "Iteration 380, loss = 0.17659805\n",
      "Iteration 381, loss = 0.17620594\n",
      "Iteration 382, loss = 0.17573591\n",
      "Iteration 383, loss = 0.17527606\n",
      "Iteration 384, loss = 0.17517391\n",
      "Iteration 385, loss = 0.17485572\n",
      "Iteration 386, loss = 0.17463782\n",
      "Iteration 387, loss = 0.17418125\n",
      "Iteration 388, loss = 0.17380924\n",
      "Iteration 389, loss = 0.17337054\n",
      "Iteration 390, loss = 0.17321824\n",
      "Iteration 391, loss = 0.17286651\n",
      "Iteration 392, loss = 0.17252751\n",
      "Iteration 393, loss = 0.17216757\n",
      "Iteration 394, loss = 0.17195767\n",
      "Iteration 395, loss = 0.17163878\n",
      "Iteration 396, loss = 0.17129493\n",
      "Iteration 397, loss = 0.17098998\n",
      "Iteration 398, loss = 0.17073361\n",
      "Iteration 399, loss = 0.17043130\n",
      "Iteration 400, loss = 0.17008667\n",
      "Iteration 401, loss = 0.16973343\n",
      "Iteration 402, loss = 0.16949500\n",
      "Iteration 403, loss = 0.16920844\n",
      "Iteration 404, loss = 0.16898042\n",
      "Iteration 405, loss = 0.16862155\n",
      "Iteration 406, loss = 0.16838155\n",
      "Iteration 407, loss = 0.16804876\n",
      "Iteration 408, loss = 0.16777566\n",
      "Iteration 409, loss = 0.16743939\n",
      "Iteration 410, loss = 0.16720655\n",
      "Iteration 411, loss = 0.16694653\n",
      "Iteration 412, loss = 0.16661108\n",
      "Iteration 413, loss = 0.16640956\n",
      "Iteration 414, loss = 0.16614806\n",
      "Iteration 415, loss = 0.16578960\n",
      "Iteration 416, loss = 0.16564695\n",
      "Iteration 417, loss = 0.16544599\n",
      "Iteration 418, loss = 0.16517118\n",
      "Iteration 419, loss = 0.16483925\n",
      "Iteration 420, loss = 0.16451022\n",
      "Iteration 421, loss = 0.16428900\n",
      "Iteration 422, loss = 0.16405611\n",
      "Iteration 423, loss = 0.16372970\n",
      "Iteration 424, loss = 0.16345832\n",
      "Iteration 425, loss = 0.16322024\n",
      "Iteration 426, loss = 0.16300070\n",
      "Iteration 427, loss = 0.16272105\n",
      "Iteration 428, loss = 0.16240834\n",
      "Iteration 429, loss = 0.16220725\n",
      "Iteration 430, loss = 0.16193952\n",
      "Iteration 431, loss = 0.16169062\n",
      "Iteration 432, loss = 0.16146562\n",
      "Iteration 433, loss = 0.16118126\n",
      "Iteration 434, loss = 0.16092873\n",
      "Iteration 435, loss = 0.16074655\n",
      "Iteration 436, loss = 0.16046399\n",
      "Iteration 437, loss = 0.16021778\n",
      "Iteration 438, loss = 0.16002693\n",
      "Iteration 439, loss = 0.15980431\n",
      "Iteration 440, loss = 0.15949560\n",
      "Iteration 441, loss = 0.15922404\n",
      "Iteration 442, loss = 0.15905308\n",
      "Iteration 443, loss = 0.15885318\n",
      "Iteration 444, loss = 0.15853057\n",
      "Iteration 445, loss = 0.15835002\n",
      "Iteration 446, loss = 0.15816555\n",
      "Iteration 447, loss = 0.15795466\n",
      "Iteration 448, loss = 0.15772906\n",
      "Iteration 449, loss = 0.15746784\n",
      "Iteration 450, loss = 0.15718992\n",
      "Iteration 451, loss = 0.15697733\n",
      "Iteration 452, loss = 0.15678109\n",
      "Iteration 453, loss = 0.15649487\n",
      "Iteration 454, loss = 0.15630659\n",
      "Iteration 455, loss = 0.15606208\n",
      "Iteration 456, loss = 0.15589123\n",
      "Iteration 457, loss = 0.15563704\n",
      "Iteration 458, loss = 0.15544610\n",
      "Iteration 459, loss = 0.15520165\n",
      "Iteration 460, loss = 0.15501864\n",
      "Iteration 461, loss = 0.15480615\n",
      "Iteration 462, loss = 0.15463870\n",
      "Iteration 463, loss = 0.15437804\n",
      "Iteration 464, loss = 0.15413475\n",
      "Iteration 465, loss = 0.15392861\n",
      "Iteration 466, loss = 0.15368514\n",
      "Iteration 467, loss = 0.15344917\n",
      "Iteration 468, loss = 0.15329280\n",
      "Iteration 469, loss = 0.15308263\n",
      "Iteration 470, loss = 0.15286316\n",
      "Iteration 471, loss = 0.15261030\n",
      "Iteration 472, loss = 0.15240550\n",
      "Iteration 473, loss = 0.15220125\n",
      "Iteration 474, loss = 0.15206655\n",
      "Iteration 475, loss = 0.15179313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 476, loss = 0.15164946\n",
      "Iteration 477, loss = 0.15146889\n",
      "Iteration 478, loss = 0.15128068\n",
      "Iteration 479, loss = 0.15107409\n",
      "Iteration 480, loss = 0.15080641\n",
      "Iteration 481, loss = 0.15057833\n",
      "Iteration 482, loss = 0.15046747\n",
      "Iteration 483, loss = 0.15034186\n",
      "Iteration 484, loss = 0.14987475\n",
      "Iteration 485, loss = 0.14969051\n",
      "Iteration 486, loss = 0.14945768\n",
      "Iteration 487, loss = 0.14921273\n",
      "Iteration 488, loss = 0.14894993\n",
      "Iteration 489, loss = 0.14875375\n",
      "Iteration 490, loss = 0.14857701\n",
      "Iteration 491, loss = 0.14835426\n",
      "Iteration 492, loss = 0.14817097\n",
      "Iteration 493, loss = 0.14781022\n",
      "Iteration 494, loss = 0.14774648\n",
      "Iteration 495, loss = 0.14752287\n",
      "Iteration 496, loss = 0.14745243\n",
      "Iteration 497, loss = 0.14721029\n",
      "Iteration 498, loss = 0.14696238\n",
      "Iteration 499, loss = 0.14684159\n",
      "Iteration 500, loss = 0.14658427\n",
      "Iteration 501, loss = 0.14629733\n",
      "Iteration 502, loss = 0.14608167\n",
      "Iteration 503, loss = 0.14605318\n",
      "Iteration 504, loss = 0.14574008\n",
      "Iteration 505, loss = 0.14559893\n",
      "Iteration 506, loss = 0.14534732\n",
      "Iteration 507, loss = 0.14521013\n",
      "Iteration 508, loss = 0.14503312\n",
      "Iteration 509, loss = 0.14493232\n",
      "Iteration 510, loss = 0.14472805\n",
      "Iteration 511, loss = 0.14453071\n",
      "Iteration 512, loss = 0.14427711\n",
      "Iteration 513, loss = 0.14411240\n",
      "Iteration 514, loss = 0.14404950\n",
      "Iteration 515, loss = 0.14381556\n",
      "Iteration 516, loss = 0.14368398\n",
      "Iteration 517, loss = 0.14345241\n",
      "Iteration 518, loss = 0.14337528\n",
      "Iteration 519, loss = 0.14319328\n",
      "Iteration 520, loss = 0.14299595\n",
      "Iteration 521, loss = 0.14279886\n",
      "Iteration 522, loss = 0.14261235\n",
      "Iteration 523, loss = 0.14243649\n",
      "Iteration 524, loss = 0.14227725\n",
      "Iteration 525, loss = 0.14215623\n",
      "Iteration 526, loss = 0.14199039\n",
      "Iteration 527, loss = 0.14179533\n",
      "Iteration 528, loss = 0.14163158\n",
      "Iteration 529, loss = 0.14149969\n",
      "Iteration 530, loss = 0.14134182\n",
      "Iteration 531, loss = 0.14115171\n",
      "Iteration 532, loss = 0.14098537\n",
      "Iteration 533, loss = 0.14083705\n",
      "Iteration 534, loss = 0.14070830\n",
      "Iteration 535, loss = 0.14054385\n",
      "Iteration 536, loss = 0.14041558\n",
      "Iteration 537, loss = 0.14022036\n",
      "Iteration 538, loss = 0.14009220\n",
      "Iteration 539, loss = 0.13994386\n",
      "Iteration 540, loss = 0.13980388\n",
      "Iteration 541, loss = 0.13965685\n",
      "Iteration 542, loss = 0.13951456\n",
      "Iteration 543, loss = 0.13934297\n",
      "Iteration 544, loss = 0.13921804\n",
      "Iteration 545, loss = 0.13907623\n",
      "Iteration 546, loss = 0.13892121\n",
      "Iteration 547, loss = 0.13880575\n",
      "Iteration 548, loss = 0.13864266\n",
      "Iteration 549, loss = 0.13851372\n",
      "Iteration 550, loss = 0.13837205\n",
      "Iteration 551, loss = 0.13823036\n",
      "Iteration 552, loss = 0.13807789\n",
      "Iteration 553, loss = 0.13796197\n",
      "Iteration 554, loss = 0.13781136\n",
      "Iteration 555, loss = 0.13768362\n",
      "Iteration 556, loss = 0.13753916\n",
      "Iteration 557, loss = 0.13747854\n",
      "Iteration 558, loss = 0.13723609\n",
      "Iteration 559, loss = 0.13715544\n",
      "Iteration 560, loss = 0.13702464\n",
      "Iteration 561, loss = 0.13687598\n",
      "Iteration 562, loss = 0.13674686\n",
      "Iteration 563, loss = 0.13663060\n",
      "Iteration 564, loss = 0.13653667\n",
      "Iteration 565, loss = 0.13634601\n",
      "Iteration 566, loss = 0.13620435\n",
      "Iteration 567, loss = 0.13611210\n",
      "Iteration 568, loss = 0.13594636\n",
      "Iteration 569, loss = 0.13581789\n",
      "Iteration 570, loss = 0.13567826\n",
      "Iteration 571, loss = 0.13559942\n",
      "Iteration 572, loss = 0.13547062\n",
      "Iteration 573, loss = 0.13534366\n",
      "Iteration 574, loss = 0.13521570\n",
      "Iteration 575, loss = 0.13512774\n",
      "Iteration 576, loss = 0.13496415\n",
      "Iteration 577, loss = 0.13484074\n",
      "Iteration 578, loss = 0.13475219\n",
      "Iteration 579, loss = 0.13468679\n",
      "Iteration 580, loss = 0.13451424\n",
      "Iteration 581, loss = 0.13439538\n",
      "Iteration 582, loss = 0.13428328\n",
      "Iteration 583, loss = 0.13420988\n",
      "Iteration 584, loss = 0.13405169\n",
      "Iteration 585, loss = 0.13392217\n",
      "Iteration 586, loss = 0.13380904\n",
      "Iteration 587, loss = 0.13369023\n",
      "Iteration 588, loss = 0.13351930\n",
      "Iteration 589, loss = 0.13350696\n",
      "Iteration 590, loss = 0.13329100\n",
      "Iteration 591, loss = 0.13325096\n",
      "Iteration 592, loss = 0.13309286\n",
      "Iteration 593, loss = 0.13297175\n",
      "Iteration 594, loss = 0.13286948\n",
      "Iteration 595, loss = 0.13275903\n",
      "Iteration 596, loss = 0.13262375\n",
      "Iteration 597, loss = 0.13249879\n",
      "Iteration 598, loss = 0.13237862\n",
      "Iteration 599, loss = 0.13223304\n",
      "Iteration 600, loss = 0.13211814\n",
      "Iteration 601, loss = 0.13209311\n",
      "Iteration 602, loss = 0.13191744\n",
      "Iteration 603, loss = 0.13179801\n",
      "Iteration 604, loss = 0.13171524\n",
      "Iteration 605, loss = 0.13163799\n",
      "Iteration 606, loss = 0.13149064\n",
      "Iteration 607, loss = 0.13140115\n",
      "Iteration 608, loss = 0.13129096\n",
      "Iteration 609, loss = 0.13117188\n",
      "Iteration 610, loss = 0.13102326\n",
      "Iteration 611, loss = 0.13089736\n",
      "Iteration 612, loss = 0.13079430\n",
      "Iteration 613, loss = 0.13068195\n",
      "Iteration 614, loss = 0.13061653\n",
      "Iteration 615, loss = 0.13048183\n",
      "Iteration 616, loss = 0.13039929\n",
      "Iteration 617, loss = 0.13029216\n",
      "Iteration 618, loss = 0.13015652\n",
      "Iteration 619, loss = 0.13008484\n",
      "Iteration 620, loss = 0.12996051\n",
      "Iteration 621, loss = 0.12986886\n",
      "Iteration 622, loss = 0.12973580\n",
      "Iteration 623, loss = 0.12963704\n",
      "Iteration 624, loss = 0.12953267\n",
      "Iteration 625, loss = 0.12945026\n",
      "Iteration 626, loss = 0.12934476\n",
      "Iteration 627, loss = 0.12922505\n",
      "Iteration 628, loss = 0.12913031\n",
      "Iteration 629, loss = 0.12902588\n",
      "Iteration 630, loss = 0.12890833\n",
      "Iteration 631, loss = 0.12888610\n",
      "Iteration 632, loss = 0.12876292\n",
      "Iteration 633, loss = 0.12863314\n",
      "Iteration 634, loss = 0.12855772\n",
      "Iteration 635, loss = 0.12844742\n",
      "Iteration 636, loss = 0.12836017\n",
      "Iteration 637, loss = 0.12825739\n",
      "Iteration 638, loss = 0.12817089\n",
      "Iteration 639, loss = 0.12806318\n",
      "Iteration 640, loss = 0.12797694\n",
      "Iteration 641, loss = 0.12787727\n",
      "Iteration 642, loss = 0.12778866\n",
      "Iteration 643, loss = 0.12767160\n",
      "Iteration 644, loss = 0.12757738\n",
      "Iteration 645, loss = 0.12755067\n",
      "Iteration 646, loss = 0.12743823\n",
      "Iteration 647, loss = 0.12730192\n",
      "Iteration 648, loss = 0.12721686\n",
      "Iteration 649, loss = 0.12719592\n",
      "Iteration 650, loss = 0.12705949\n",
      "Iteration 651, loss = 0.12693913\n",
      "Iteration 652, loss = 0.12686718\n",
      "Iteration 653, loss = 0.12679512\n",
      "Iteration 654, loss = 0.12670758\n",
      "Iteration 655, loss = 0.12660548\n",
      "Iteration 656, loss = 0.12650563\n",
      "Iteration 657, loss = 0.12638984\n",
      "Iteration 658, loss = 0.12631842\n",
      "Iteration 659, loss = 0.12624489\n",
      "Iteration 660, loss = 0.12615820\n",
      "Iteration 661, loss = 0.12605848\n",
      "Iteration 662, loss = 0.12595396\n",
      "Iteration 663, loss = 0.12589373\n",
      "Iteration 664, loss = 0.12579932\n",
      "Iteration 665, loss = 0.12567362\n",
      "Iteration 666, loss = 0.12559931\n",
      "Iteration 667, loss = 0.12552312\n",
      "Iteration 668, loss = 0.12545677\n",
      "Iteration 669, loss = 0.12537727\n",
      "Iteration 670, loss = 0.12531980\n",
      "Iteration 671, loss = 0.12522854\n",
      "Iteration 672, loss = 0.12515724\n",
      "Iteration 673, loss = 0.12505407\n",
      "Iteration 674, loss = 0.12496346\n",
      "Iteration 675, loss = 0.12486309\n",
      "Iteration 676, loss = 0.12477064\n",
      "Iteration 677, loss = 0.12470752\n",
      "Iteration 678, loss = 0.12462289\n",
      "Iteration 679, loss = 0.12455171\n",
      "Iteration 680, loss = 0.12446016\n",
      "Iteration 681, loss = 0.12436988\n",
      "Iteration 682, loss = 0.12425750\n",
      "Iteration 683, loss = 0.12416651\n",
      "Iteration 684, loss = 0.12408908\n",
      "Iteration 685, loss = 0.12397797\n",
      "Iteration 686, loss = 0.12376115\n",
      "Iteration 687, loss = 0.12346985\n",
      "Iteration 688, loss = 0.12323276\n",
      "Iteration 689, loss = 0.12322658\n",
      "Iteration 690, loss = 0.12318258\n",
      "Iteration 691, loss = 0.12311981\n",
      "Iteration 692, loss = 0.12309122\n",
      "Iteration 693, loss = 0.12296758\n",
      "Iteration 694, loss = 0.12284295\n",
      "Iteration 695, loss = 0.12275112\n",
      "Iteration 696, loss = 0.12260202\n",
      "Iteration 697, loss = 0.12250075\n",
      "Iteration 698, loss = 0.12235148\n",
      "Iteration 699, loss = 0.12232559\n",
      "Iteration 700, loss = 0.12221489\n",
      "Iteration 701, loss = 0.12208988\n",
      "Iteration 702, loss = 0.12201814\n",
      "Iteration 703, loss = 0.12192308\n",
      "Iteration 704, loss = 0.12179126\n",
      "Iteration 705, loss = 0.12174725\n",
      "Iteration 706, loss = 0.12167696\n",
      "Iteration 707, loss = 0.12152088\n",
      "Iteration 708, loss = 0.12144729\n",
      "Iteration 709, loss = 0.12140156\n",
      "Iteration 710, loss = 0.12129901\n",
      "Iteration 711, loss = 0.12121942\n",
      "Iteration 712, loss = 0.12109363\n",
      "Iteration 713, loss = 0.12095368\n",
      "Iteration 714, loss = 0.12086329\n",
      "Iteration 715, loss = 0.12081152\n",
      "Iteration 716, loss = 0.12063401\n",
      "Iteration 717, loss = 0.12059248\n",
      "Iteration 718, loss = 0.12052708\n",
      "Iteration 719, loss = 0.12043151\n",
      "Iteration 720, loss = 0.12032949\n",
      "Iteration 721, loss = 0.12018166\n",
      "Iteration 722, loss = 0.12008496\n",
      "Iteration 723, loss = 0.11997808\n",
      "Iteration 724, loss = 0.11993023\n",
      "Iteration 725, loss = 0.11984632\n",
      "Iteration 726, loss = 0.11970113\n",
      "Iteration 727, loss = 0.11962261\n",
      "Iteration 728, loss = 0.11955316\n",
      "Iteration 729, loss = 0.11944253\n",
      "Iteration 730, loss = 0.11931197\n",
      "Iteration 731, loss = 0.11923332\n",
      "Iteration 732, loss = 0.11912276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 733, loss = 0.11901473\n",
      "Iteration 734, loss = 0.11898247\n",
      "Iteration 735, loss = 0.11885631\n",
      "Iteration 736, loss = 0.11877836\n",
      "Iteration 737, loss = 0.11872570\n",
      "Iteration 738, loss = 0.11858952\n",
      "Iteration 739, loss = 0.11856420\n",
      "Iteration 740, loss = 0.11847932\n",
      "Iteration 741, loss = 0.11837724\n",
      "Iteration 742, loss = 0.11819384\n",
      "Iteration 743, loss = 0.11818709\n",
      "Iteration 744, loss = 0.11814888\n",
      "Iteration 745, loss = 0.11804949\n",
      "Iteration 746, loss = 0.11789868\n",
      "Iteration 747, loss = 0.11772076\n",
      "Iteration 748, loss = 0.11766584\n",
      "Iteration 749, loss = 0.11754544\n",
      "Iteration 750, loss = 0.11752242\n",
      "Iteration 751, loss = 0.11744978\n",
      "Iteration 752, loss = 0.11733344\n",
      "Iteration 753, loss = 0.11715445\n",
      "Iteration 754, loss = 0.11718578\n",
      "Iteration 755, loss = 0.11713024\n",
      "Iteration 756, loss = 0.11702305\n",
      "Iteration 757, loss = 0.11686767\n",
      "Iteration 758, loss = 0.11675707\n",
      "Iteration 759, loss = 0.11673511\n",
      "Iteration 760, loss = 0.11662493\n",
      "Iteration 761, loss = 0.11650457\n",
      "Iteration 762, loss = 0.11642211\n",
      "Iteration 763, loss = 0.11635986\n",
      "Iteration 764, loss = 0.11624933\n",
      "Iteration 765, loss = 0.11618465\n",
      "Iteration 766, loss = 0.11614727\n",
      "Iteration 767, loss = 0.11599748\n",
      "Iteration 768, loss = 0.11585033\n",
      "Iteration 769, loss = 0.11584295\n",
      "Iteration 770, loss = 0.11580739\n",
      "Iteration 771, loss = 0.11567214\n",
      "Iteration 772, loss = 0.11553335\n",
      "Iteration 773, loss = 0.11548526\n",
      "Iteration 774, loss = 0.11536353\n",
      "Iteration 775, loss = 0.11520807\n",
      "Iteration 776, loss = 0.11519707\n",
      "Iteration 777, loss = 0.11517273\n",
      "Iteration 778, loss = 0.11494642\n",
      "Iteration 779, loss = 0.11491800\n",
      "Iteration 780, loss = 0.11489770\n",
      "Iteration 781, loss = 0.11483297\n",
      "Iteration 782, loss = 0.11466653\n",
      "Iteration 783, loss = 0.11458737\n",
      "Iteration 784, loss = 0.11446837\n",
      "Iteration 785, loss = 0.11446884\n",
      "Iteration 786, loss = 0.11434244\n",
      "Iteration 787, loss = 0.11428254\n",
      "Iteration 788, loss = 0.11418693\n",
      "Iteration 789, loss = 0.11407841\n",
      "Iteration 790, loss = 0.11403154\n",
      "Iteration 791, loss = 0.11394899\n",
      "Iteration 792, loss = 0.11385446\n",
      "Iteration 793, loss = 0.11375016\n",
      "Iteration 794, loss = 0.11370457\n",
      "Iteration 795, loss = 0.11358063\n",
      "Iteration 796, loss = 0.11353479\n",
      "Iteration 797, loss = 0.11345905\n",
      "Iteration 798, loss = 0.11334180\n",
      "Iteration 799, loss = 0.11321279\n",
      "Iteration 800, loss = 0.11312961\n",
      "Iteration 801, loss = 0.11310019\n",
      "Iteration 802, loss = 0.11305044\n",
      "Iteration 803, loss = 0.11293374\n",
      "Iteration 804, loss = 0.11288794\n",
      "Iteration 805, loss = 0.11279672\n",
      "Iteration 806, loss = 0.11263910\n",
      "Iteration 807, loss = 0.11262653\n",
      "Iteration 808, loss = 0.11246152\n",
      "Iteration 809, loss = 0.11248621\n",
      "Iteration 810, loss = 0.11233775\n",
      "Iteration 811, loss = 0.11233866\n",
      "Iteration 812, loss = 0.11214903\n",
      "Iteration 813, loss = 0.11219764\n",
      "Iteration 814, loss = 0.11211943\n",
      "Iteration 815, loss = 0.11203273\n",
      "Iteration 816, loss = 0.11189837\n",
      "Iteration 817, loss = 0.11191106\n",
      "Iteration 818, loss = 0.11184804\n",
      "Iteration 819, loss = 0.11168508\n",
      "Iteration 820, loss = 0.11166774\n",
      "Iteration 821, loss = 0.11160812\n",
      "Iteration 822, loss = 0.11154115\n",
      "Iteration 823, loss = 0.11140143\n",
      "Iteration 824, loss = 0.11126801\n",
      "Iteration 825, loss = 0.11120685\n",
      "Iteration 826, loss = 0.11110760\n",
      "Iteration 827, loss = 0.11102857\n",
      "Iteration 828, loss = 0.11097856\n",
      "Iteration 829, loss = 0.11096304\n",
      "Iteration 830, loss = 0.11078818\n",
      "Iteration 831, loss = 0.11080223\n",
      "Iteration 832, loss = 0.11068683\n",
      "Iteration 833, loss = 0.11073132\n",
      "Iteration 834, loss = 0.11064236\n",
      "Iteration 835, loss = 0.11051527\n",
      "Iteration 836, loss = 0.11038160\n",
      "Iteration 837, loss = 0.11053116\n",
      "Iteration 838, loss = 0.11046412\n",
      "Iteration 839, loss = 0.11024105\n",
      "Iteration 840, loss = 0.11023105\n",
      "Iteration 841, loss = 0.11019739\n",
      "Iteration 842, loss = 0.11017671\n",
      "Iteration 843, loss = 0.10997735\n",
      "Iteration 844, loss = 0.10985593\n",
      "Iteration 845, loss = 0.10986477\n",
      "Iteration 846, loss = 0.10985216\n",
      "Iteration 847, loss = 0.10965972\n",
      "Iteration 848, loss = 0.10957652\n",
      "Iteration 849, loss = 0.10958404\n",
      "Iteration 850, loss = 0.10952843\n",
      "Iteration 851, loss = 0.10940841\n",
      "Iteration 852, loss = 0.10923116\n",
      "Iteration 853, loss = 0.10938066\n",
      "Iteration 854, loss = 0.10933552\n",
      "Iteration 855, loss = 0.10917231\n",
      "Iteration 856, loss = 0.10901646\n",
      "Iteration 857, loss = 0.10907534\n",
      "Iteration 858, loss = 0.10893936\n",
      "Iteration 859, loss = 0.10883297\n",
      "Iteration 860, loss = 0.10874496\n",
      "Iteration 861, loss = 0.10866679\n",
      "Iteration 862, loss = 0.10861021\n",
      "Iteration 863, loss = 0.10857752\n",
      "Iteration 864, loss = 0.10847167\n",
      "Iteration 865, loss = 0.10841410\n",
      "Iteration 866, loss = 0.10836826\n",
      "Iteration 867, loss = 0.10826264\n",
      "Iteration 868, loss = 0.10823124\n",
      "Iteration 869, loss = 0.10818385\n",
      "Iteration 870, loss = 0.10807635\n",
      "Iteration 871, loss = 0.10801207\n",
      "Iteration 872, loss = 0.10791342\n",
      "Iteration 873, loss = 0.10790171\n",
      "Iteration 874, loss = 0.10782756\n",
      "Iteration 875, loss = 0.10771817\n",
      "Iteration 876, loss = 0.10772856\n",
      "Iteration 877, loss = 0.10763885\n",
      "Iteration 878, loss = 0.10757323\n",
      "Iteration 879, loss = 0.10754462\n",
      "Iteration 880, loss = 0.10745236\n",
      "Iteration 881, loss = 0.10735632\n",
      "Iteration 882, loss = 0.10731222\n",
      "Iteration 883, loss = 0.10724952\n",
      "Iteration 884, loss = 0.10718988\n",
      "Iteration 885, loss = 0.10713791\n",
      "Iteration 886, loss = 0.10705661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.950000\n",
      "Training set loss: 0.107057\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# different learning rate schedules and momentum parameters\n",
    "params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0.9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0.9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0.9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0.9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'adam', 'learning_rate_init': 0.01}]\n",
    "\n",
    "labels = [\"constant learning-rate\", \"constant with momentum\",\n",
    "          \"constant with Nesterov's momentum\",\n",
    "          \"inv-scaling learning-rate\", \"inv-scaling with momentum\",\n",
    "          \"inv-scaling with Nesterov's momentum\", \"adam\"]\n",
    "\n",
    "plot_args = [{'c': 'red', 'linestyle': '-'},\n",
    "             {'c': 'green', 'linestyle': '-'},\n",
    "             {'c': 'blue', 'linestyle': '-'},\n",
    "             {'c': 'red', 'linestyle': '--'},\n",
    "             {'c': 'green', 'linestyle': '--'},\n",
    "             {'c': 'blue', 'linestyle': '--'},\n",
    "             {'c': 'black', 'linestyle': '-'}]\n",
    "\n",
    "\n",
    "def plot_on_dataset(X, y, ax, name):\n",
    "    # for each dataset, plot learning for each learning strategy\n",
    "    print(\"\\nlearning on dataset %s\" % name)\n",
    "    ax.set_title(name)\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    mlps = []\n",
    "    if name == \"digits\":\n",
    "        # digits is larger but converges fairly quickly\n",
    "        max_iter = 201068\n",
    "    else:\n",
    "        max_iter = 196810\n",
    "\n",
    "    for label, param in zip(labels, params):\n",
    "        print(\"training: %s\" % label)\n",
    "        mlp = MLPClassifier(verbose=True, random_state=0,\n",
    "                            max_iter=max_iter, **param)\n",
    "        mlp.fit(X, y)\n",
    "        mlps.append(mlp)\n",
    "        print(\"Training set score: %f\" % mlp.score(X, y))\n",
    "        print(\"Training set loss: %f\" % mlp.loss_)\n",
    "    for mlp, label, args in zip(mlps, labels, plot_args):\n",
    "            ax.plot(mlp.loss_curve_, label=label, **args)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "# load / generate some toy datasets\n",
    "digits = datasets.load_digits()\n",
    "data_sets = [(rose_data, rose_target),\n",
    "             (digits.data, digits.target),\n",
    "             datasets.make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "             datasets.make_moons(noise=0.3, random_state=0)]\n",
    "\n",
    "for ax, data, name in zip(axes.ravel(), data_sets, ['rose', 'digits',\n",
    "                                                    'circles', 'moons']):\n",
    "    plot_on_dataset(*data, ax=ax, name=name)\n",
    "\n",
    "fig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AndreVermeulen\\Documents\\My Book\\apress\\Industrialized Machine Learning\\book\\IML\\Results\\Chapter 04\n"
     ]
    }
   ],
   "source": [
    "imagepath = os.path.join(*[os.path.dirname(os.path.dirname(os.getcwd())),'Results','Chapter 04'])\n",
    "print(imagepath)\n",
    "if not os.path.exists(imagepath):\n",
    "    os.makedirs(imagepath)\n",
    "graphName = 'Chapter-004-Example-026-01-01.jpg'\n",
    "imagename = os.path.join(*[os.path.dirname(os.path.dirname(os.getcwd())),'Results','Chapter 04',graphName])\n",
    "fig.savefig(imagename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! 2019-04-08 20:08:39.930779\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print('Done!',str(now))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
